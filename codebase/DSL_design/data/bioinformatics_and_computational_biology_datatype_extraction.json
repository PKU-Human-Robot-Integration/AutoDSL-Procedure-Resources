{"original_protocol": ["Step-by-step method details\nStep-by-step method details\nMaking germline revertant antibody expression constructs\nTiming: 3\u00a0weeks\nTo evaluate the importance of each amino acid mutation or indel in the target bnAb, we revert each of the changed residues in the heavy chain or light chain of the target bnAb back to its corresponding germline encoded sidechain one residue at a time.\nUse the coding sequence of the bnAb heavy or light chain as bait to perform IgBlast against human database.\nHeavy chain IgBlast generates an alignment of the bnAb heavy chain with its inferred germline VH (VH1-2) gene\nLight chain IgBlast generates an alignment of the bnAb light chain with its inferred germline VK (VK3-20) gene\nBased on the search results, design a point mutagenesis mutant for each of the SHM-induced residue change or indel to revert the change back to corresponding germline residue. One revertant corresponds to one amino acid mutation in the heavy or light chain. An example of the germline revertants of a VRC01-class bnAb 2411a (Chen et\u00a0al., 2021[href=https://www.wicell.org#bib3]) is shown in Figure\u00a02[href=https://www.wicell.org#fig2].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1448-Fig2.jpg\nFigure\u00a02. Amino acid sequence alignment of the HCs and LCs of a parental VRC01-class bnAb 2411a and its germline revertants (Gr1-Gr31) against the germline VH1-2\u221702 and VK3-20\u221701 respectively\n\u201c-\u201d indicates the same residue as the germline; mutated V-region residues in the bnAb and the revertants are shown as black single letter codes, whereas the germline reverted residues are marked in blue or red color. Mutation of the red-labeled residues are key mutations (Chen et\u00a0al., 2021[href=https://www.wicell.org#bib3]).Order the designed germline revertants from GenScript with their point mutagenesis service, in which they will need the original heavy and light chain expression vectors of the target bnAb as templates to make a series of mutated heavy and light chain constructs, each containing only one germline reverted residue.\nThe expression plasmids for the revertants are usually ready in two-three weeks.\nNote: The revertant constructs can also be generated using a QuickChange Lightning Site-Directed Mutagenesis Kit (Agilent Technologies) or Q5 Site-Directed mutagenesis kit with NEBaseChanger web tool (New England Biolabs). While waiting for the germline revertant heavy and light chains to be synthesized, you could perform the single B cell sorting and RT-PCR of VRC01-class IgG sequences from splenocytes of mice sacrificed at different time points of the immunization course, as described in steps 31\u201332.\nExpression and purification of germline revertant mutants of the target bnAb\nTiming: 8\u00a0days\nThis part describes how to produce and purify the germline revertant mutants of the target bnAb. We need to make germline revertant antibodies each containing only one germline-reverted residue, so we need to pair each mutant heavy chain with the parental bnAb light chain and pair each mutant light chain with the parental bnAb heavy chain for antibody production.\nTransient transfection of Expi293 cells to produce germline revertants (7\u00a0days)\nGrow 2\u00a0L Expi293F\u2122 cells in Expi293\u2122 Expression Medium to 3\u20135\u00a0\u00d7 106/mL in a cell incubator at 37\u00b0C, 8% CO2, 60% humidity with 125\u00a0rpm shaking. These cells should be sufficient to prepare 24\u201340\u00a0\u00d7 100\u00a0mL-transfections to express 24\u201340 mutant antibodies.\nOn the day of transfection, transfect 100\u00a0mL Expi293 (2.5\u00a0\u00d7 106/mL) cells for expression of each germline revertant antibody. All procedures at this step are operated in a biosafety hood.Dilute the 2\u00a0L Expi293F\u2122 cells to 2.5\u00a0\u00d7 106/mL with fresh warm Expi293\u2122 Expression Medium and split into 100mL-aliquots in 250mL-baffled-flasks.\nWarm 1\u00d7 Opti-MEM medium in 37\u00b0C water bath.\nFor each transfection, mix 50\u00a0\u03bcg heavy chain plasmid and 50\u00a0\u03bcg light chain plasmid in 5\u00a0mL 1\u00d7 Opti-MEM medium, and filter the mix through a MilliporeSigma\u2122 Steriflip\u2122 Sterile Disposable Vacuum Filter Unit (0.22\u00a0\u03bcm PES filter on a 50\u00a0mL conical tube).\nFor each transfection, add 0.27\u00a0mL ExpiFectamine\u2122 293 Reagent to 5\u00a0mL 1\u00d7 Opti-MEM medium, mix gently and incubate the mixture at room temperature for 5\u00a0min.\nAdd the ExpiFectamine\u2122 mix from iv to the plasmid mix from iii in the 50\u00a0mL conical tube, mix gently by swirling and incubate at room temperature for 20\u00a0min.\nPour the transfection mix (\u223c10\u00a0mL) to one 250\u00a0mL-flask containing 100\u00a0mL 2.5\u00a0\u00d7 106/mL Expi293F\u2122 cells.\nIncubate the transfected cells in a cell culture incubator at 37\u00b0C, 8% CO2, 60% humidity with 125\u00a0rpm shaking.\nOn day 2, add 0.5\u00a0mL ExpiFectamine\u2122 293 Transfection Enhancer 1 and 5\u00a0mL ExpiFectamine\u2122 293 Transfection Enhancer 2 to each flask of cells, and continue incubation under the same condition for 5\u20136 more days.\nNote: To improve protein yield, the cells can also be incubated at 32\u00b0C during day 3\u20137 with the other conditions unchanged. It is better to produce the parental bnAb as a reference control together with the germline revertant antibodies.\nPurification of the parental and germline revertant antibodies (1\u00a0day).\nOn day 7 post transfection, collect the \u223c115\u00a0mL of cell culture from each flask and centrifuge the cells down in one-time use centrifugal flasks at 2000\u00a0\u00d7 g for 15\u00a0min.Sterile filter each supernatant with a 0.2\u00a0\u03bcm Nalgene Rapid-Flow 250\u00a0mL-filter unit and check the pH of the supernatant to make sure that it is between 7.0 and 8.0, for optimal rProtein A binding of mouse IgG2a at next step.\nApply the filtered supernatant onto 0.8\u00a0mL rProtein A Sepharose FF (GE health) pre-equilibrated in PBS in a disposable 10\u00a0mL poly-prep column with a 250\u00a0mL funnel attached on top.\nLet the medium run through the column by gravity, remove the funnel and wash column 2\u00d7, each with 10\u00a0mL PBS (pH7.4).\nElute antibody with 5\u00a0mL GE IgG elution buffer and collect the eluate with constant shaking in a 15\u00a0mL conical tube containing 0.5\u00a0mL 1\u00a0M Tris, pH8.\nConcentrate the eluted antibodies to above 1\u00a0mg/mL, as needed, in Amicon Ultra-4 centrifugal filter units.\nDialyze the antibodies in Slide-A-Lyzer G2 cassettes (3\u00a0mL size, 20 kD molecular weight cutoff) against 4\u00a0L PBS three times, with 1.5\u00a0h stirring at room temperature or 4\u00b0C overnight each time.\nCritical: Some antibodies tend to precipitate during pH neutralization. The drop-by-drop elution into 1M Tris, pH8 with constant shaking can effectively reduce or eliminate precipitation. Constant shaking during elution is critical for quick and sufficient pH neutralization and prevention of antibody precipitation.\nBinding assay of germline revertants to identify key mutations\nTiming: 2\u00a0days\nELISA of the parental bnAb and its germline revertant mutants against a panel of antigens to assess the impact of each SHM-resulted residue change of the bnAb on its binding activity.\nChoose a panel of HIV-1 envelope protein antigens as the ELISA binding substrates for the tested bnAb and its germline revertants.Note: The panel should include envelopes from different strains of HIV-1 viruses to examine the binding breadth of the tested antibodies, and some CD4bs-disrupting mutants of the envelope as negative control to check the CD4bs-specificity of the antibodies.\nCoat 96-well costar half-area microplates with the panel of purified antigens:\nPrepare sufficient antigen coating solution in PBS at 2\u00a0\u03bcg/mL for each antigen: total volume\u00a0= number of antibodies to test \u00d7 3 (3 antibody concentrations) \u00d7 50\u00a0\u03bcL \u00d7 1.05 (5% extra)\nFor each antigen, coat a 96-well costar half-area plate with the 2\u00a0\u03bcg/mL antigen coating solution at 50\u00a0\u03bcL per well. Each coated plate is sufficient to test 32 antibodies (target bnAb and its germline revertants) at three different concentrations.\nPut all coated plates in stacks and cover the top plates with either a plate cover or plate sealing film and incubate the plates at 4\u00b0C overnight.\nOn the 2nd day, remove the coating solutions in all plates and wash them 1\u00d7 with 200\u00a0\u03bcL PBS/T (PBS with 0.05% Tween) per well in a BioTek plate washer.\nBlock all plates with 1:10 diluted (with PBS) Blocking/Diluent Solution (Immune Technology) at 50\u00a0\u03bcL per well and incubate them at room temperature for 1 h.\nDuring the blocking period, prepare solutions of the parental or germline revertant antibodies each at three different concentrations (10, 1 and 0.1\u00a0\u03bcg/mL) in PBS/T containing 2% ImmuneTech Blocking Reagent.\nDistribute 2\u00a0mL of the antibody solutions from step 11 into each well of a 96-well deep-well plate according to Figure\u00a03[href=https://www.wicell.org#fig3]A.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1448-Fig3.jpg\nFigure\u00a03. Identify key mutations by ELISA(A) ELISA plate/well setting for 1st antibody binding step. The graph shows the plate/well setting for testing the parent\u00a0antibody and up to 31 germline revertant (Gr) antibodies in a 96-well ELISA plate coated with one antigen. Each antibody is tested in three different concentrations, 10, 1, and 0.1\u00a0\u03bcg/mL, to avoid overdosing or underdosing and to best detect the binding difference between a revertant and the parent antibody.\n(B) Example of a heat map showing the binding (OD450) of a VRC01-class bnAb (Parental) and its 31 germline revertants (Gr) to a panel of 23 HIV envelope-based antigens at three different concentrations: 10, 1 and 0.1\u00a0\u03bcg/mL. \u2217 marks the optimal concentration for detecting changes between Gr and Parent.\nWash blocked ELISA plates 5\u00d7 with PBS/T (200\u00a0\u03bcL/well per wash) in a BioTek plate washer.\nTransfer 50\u00a0\u03bcL/well from the deep-well plate to each well of the antigen-coated ELISA plate.\nIncubate the plates at room temperature for 1 h.\nRepeat step 13.\nAdd 50\u00a0\u03bcL/well SureBlue\u2122 TMB Microwell Peroxidase Substrate (KPL) to all plates with a 96-well liquidator in order, wait for 10\u00a0min for color development at room temperature.\nAdd 50\u00a0\u03bcL/well 1\u00a0N H2SO4 to all plates in the same order, to quench the color development.\nRead all plates for OD450 using a microplate reader.\nData analysis:\nVisualization of ELISA data in a heat map like Figure\u00a03[href=https://www.wicell.org#fig3]B.\nFor each antigen, determine the antibody concentration at which the OD450 display the best dynamic range among the tested antibodies. For example, data obtained at 0.1\u00a0\u03bcg/mL for UG037.8 and 10\u00a0\u03bcg/mL for ZM53 core should be chosen for further analysis of binding titer changes of germline revertant antibodies.For each germline revertant, determine the OD450 ratio of the parental antibody to the germline revertant at the best antibody concentration for each antigen (as determine in 20b), and take the average of all the ratios for every antigen as the binding fold change index, Fb, for that germline revertant antibody.\n  F b  =  1 n  \u2217  \u2211  i = 1  n     O D  450 i P    O D  450 i  G r      ,  n  =  the  number  of  antigens \nWe artificially define any fold change larger than 1.5 as significant, demonstrating that the SHM in the corresponding germline revertant has a significant impact on the binding of the target bnAb to its antigens.\nIdentify all key mutations that cause >= 1.5-fold change in binding index.\nTZM-bl neutralization assay of germline revertants to identify key mutations\nTiming: 1\u00a0week\nVirus panel selection: based on the large-panel neutralization data of the target bnAb, select a panel of 10 bnAb-sensitive viruses from different clades with at least moderate neutralization titers by the bnAb. An exemplar panel for VRC01-class bnAb 2411a is shown in Figure\u00a04[href=https://www.wicell.org#fig4]A. The criteria for the virus panel selection include: 1) select from the bnAb -sensitive viruses, since a non-sensitive virus is not helpful for the antibody loss-of-function analysis; 2) select from different viral clades and be as representative as possible to the full panel; 3) avoid virus strains already sensitive to unmutated bnAb precursors; 4) choose viruses with moderate-high neutralizing titers to the bnAb: viruses with too high titers are often also sensitive to the germline precursor, whereas those with too low titers do not have the range to show significant titer change in response to SHM revertant.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1448-Fig4.jpg\nFigure\u00a04. Identify key mutations by neutralization assays(A) Exemplar neutralization results showing IC50s of a parental VRC01-class bnAb (2411a) and its germline revertants against a panel of 10 sensitive viruses from different clades. The color-code for IC50 goes from strong neutralization (red) to weak (green). No neutralization observed at greater than 50\u00a0\u03bcg/mL is shown in white.\n(B) Exemplar neutralization curves of a parental bnAb 2411a and its germline revertants of non-key mutations (gray curves) vs. key mutations (red curves) against virus strain RHPA.7.\nPseudovirus preparation: Pseudoviruses for use in TZM-bl neutralization assays were produced in 293T cells by cotransfection of a pSG3\u0394Env backbone plasmid and a full HIV-1 Env gp160-encoding plasmid (Li et al., 2005[href=https://www.wicell.org#bib13]).\n2\u00a0\u00d7 106 cells in 20\u00a0mL cDMEM (see materials and equipment[href=https://www.wicell.org#materials-and-equipment]) were seeded in T75 flasks the day prior to cotransfection.\nFor transfection, 40\u00a0\u03bcL of FuGene 6 reagent (Promega) was diluted into 800\u00a0\u03bcL of room-temperature Opti-MEM I reduced serum medium (Thermo Fisher), followed by addition of 10\u00a0\u03bcg of pSG3\u0394Env backbone plasmid.\n3.3\u00a0\u03bcg of HIV Env plasmid was then added to the mixture, mixed, and incubated for 30\u00a0min at room temperature.\nTransfection mixture was then added to media of previously seeded 293T cells in the T75 flask and then distributed evenly on cells.\nThe following day, media was replaced with 20\u00a0mL fresh cDMEM.\nVirus was harvested the following day by filtering cell supernatants with 0.45\u00a0\u03bcm Steriflip units (EMD Millipore) and aliquoted.\nTo measure neutralization of purified antibodies,\n10\u00a0\u03bcL of five-fold serially diluted mAbs in cDMEM was incubated with 40ul of diluted HIV-1 Env-pseudotyped virus and incubated for 30\u00a0min at 37oC in a 96-well CulturPlate (Perkin Elmer).\n20\u00a0\u03bcL of TZM-bl cells (10,000 cells/well) with or without 70\u00a0\u03bcg/mL DEAE-Dextran was then added and incubated overnight at 37oC.Each experiment plate also had a column of cells only (no Ab or virus) and a column of virus only (no Ab) as controls for background TZM-bl luciferase activity and maximal viral entry, respectively.\nCritical: Serial dilutions were performed with a change of tips at each dilution step to prevent carryover.\nThe following day, all wells received 100\u00a0\u03bcL of fresh cDMEM and were incubated overnight at 37oC.\nThe following day, 50\u00a0\u03bcL of Steadylite Plus Reporter Gene Assay System (PerkinElmer) was added to all wells, and plates were shaken at 600\u00a0rpm for 15\u00a0min.\nLuminometry was then performed on a SpectraMax L (Molecular Devices) luminometer. Percent neutralization is determined by calculating the difference in average Relative Light Unit (RLU) between virus only wells (cells\u00a0+ virus column) and test wells (cells\u00a0+ plasma/Ab sample\u00a0+ virus), dividing this result by the average RLU of virus only wells (cell\u00a0+ virus column) and multiplying by 100. Background is subtracted from all test wells using the average RLU from the uninfected control wells (cells only column) before calculating the percent neutralization.\nNeutralizing antibody titers are expressed as the antibody concentration required to achieve 50% or 80% neutralization (IC50 or IC80) and calculated using a dose-response curve fit with a 5-parameter nonlinear function.\nFor each germline revertant, determine the IC50 ratio of the germline revertant to the parental antibody against each virus, and take the average of all the ratios for every tested virus as the neutralization fold change index, Fn, for that germline revertant antibody.\n  F n  =  1 m  \u2217  \u2211  j = 1  m     I C  50 j  G r     I C  50 j P     ,  m  =  the  number  of  virusesIdentify all key mutations that cause >= 2-fold change in Fn, and especially focus on those that also cause >=1.5-fold change in binding (Fb).\nOptional: Structural analysis of identified key mutations.\nIf the structure of the parental bnAb in complex with its antigen is available or can be modeled based on known structures of similar antibodies, one can examine structurally why the identified key mutations contribute to improved antibody binding and neutralization activity, such as creating new antigen contact sites or reducing potential clash with antigens.\nLongitudinal analysis of antibody sequences to determine immunogens that elicit the key mutations\nTiming: 3\u00a0weeks\nThis part describes steps for obtaining VRC01-class antibody heavy and light chain sequences from different stages of sequential immunization and how to perform the longitudinal analysis of the SHM and key mutations in these antibodies.\nSingle B cell sorting with VRC01-class specific probes\nThaw frozen splenocytes (5\u201310 million cells) in 2-mL cryovials collected from different immunization stages in 37\u00b0C water bath.\nSpin down the cells at 500\u00a0\u00d7 g for 3\u00a0min and remove the supernatants.\nWash the cells with 1\u00a0mL PBS by vertexing to resuspend the cells and re-spinning them down at 500\u00a0\u00d7 g for 3\u00a0min.\nRemove the PBS and resuspend the cells in 100\u00a0\u03bcl of 1:1,000 diluted (in PBS) ViViD dye (40-test vial) that was reconstituted in 50\u00a0\u03bcL DMSO. Incubate in dark for 15\u00a0min.\nIn the meantime, prepare cell staining mix based on Table 1[href=https://www.wicell.org#tbl1], and store on ice in dark.\nWash off extra ViViD by adding 1\u00a0mL cold PBS and spinning the cells down at 500\u00a0g for 3\u00a0min.Resuspend cells in 100\u2013200\u00a0\u03bcL of staining mix based on the number of cells (107 cells per 100\u00a0\u03bcL staining mix), incubate on ice in darkness for 30\u201340\u00a0min.\nDuring the cell staining period, prepare compensation beads for each color used in the staining panel with an unstained bead control, following the recipe of 250\u00a0\u03bcL PBS\u00a0+ 1 drop of BD anti-mouse IgK CompBeads\u00a0+ specified amount of anti-mouse mAb as described in the last column of Table 1[href=https://www.wicell.org#tbl1]. Use 50\u00a0\u03bcL ViViD-conjugated Amide beads (BD Pharmingen) diluted in 250\u00a0\u03bcL PBS as compensation control for V450 channel.\nWash the stained cells twice with 1\u00a0mL ice-cold PBS as f.\nResuspend the cells in 0.5\u20130.7\u00a0mL of ice-cold PBS/1% BSA and filter through a 40\u00a0\u03bcm cell strainer.\nSet up the compensation in a BD FACSAria Fusion cell sorter with the prepared CompBeads, and sort ViViD-/CD3-CD4-CD8-F4/80-/B220+/IgD-IgM-/IgG+/eOD-GT6+/eOD-GT6 KO- B cells (Figure\u00a05[href=https://www.wicell.org#fig5]) into 96 well plates containing 10\u00a0\u03bcL of freshly made reverse transcription (RT) /Lysis Buffer per well (see materials and equipment[href=https://www.wicell.org#materials-and-equipment]).Note: Sorting strategy rationale is as follows: ViViD- for live cells, CD3- CD4- and CD8- to gate out T\u00a0cells, F4/80- to gate out macrophages, B220+ for B cells, IgD- IgM- and IgG+ for IgG+ memory B cells, eOD-GT6+ and its CD4bs-disrupting mutant eOD-GT6-KO- for eOD-GT6-binding CD4bs-specific B cells (see Figure\u00a05[href=https://www.wicell.org#fig5]). We chose eOD-GT6 over eOD-GT8 because the former is more similar to gp120, has less engineered mutations and lower affinity to the unmutated VRC01-class precursors in the VH1-2/LC mice (Chen et\u00a0al., 2021[href=https://www.wicell.org#bib3]; Jardine et\u00a0al., 2013[href=https://www.wicell.org#bib7], 2015[href=https://www.wicell.org#bib8]), and can thus better select for and enrich VRC01-class B cell receptors with vaccine-elicited SHMs. However, do not use the additional BG505.SOSIP probe (Figure\u00a05[href=https://www.wicell.org#fig5], step 8) for acquiring antibody sequences used for longitudinal mutation analysis, because the added selection by the BG505 probe artificially increases the frequency of more affinity matured antibodies among all acquired antibodies and miss out those with lower SHMs, which interferes with the evaluation of immunization impact on antibody SHM and key mutations.\nSeal the 96 well plate with a sealing film, vortex and centrifuge the plate to make sure the sorted cells are in the lysis buffer.\nFreeze the plate on dry ice or in a \u221280\u00b0C freezer and store it frozen at \u221280\u00b0C until next step.\nSingle B cell RT-PCR\nReverse Transcription (RT): For each 96 well plate, makes a master mix of RT/Superscript mix (see materials and equipment[href=https://www.wicell.org#materials-and-equipment]) and add 2.5\u00a0\u03bcL to each well with a multichannel pipet; perform RT reaction as described in Table 2[href=https://www.wicell.org#tbl2].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1448-Fig5.jpg\nFigure\u00a05. B cell sort gating steps for isolating CD4bs-specific B cells and VRC01-class antibodiesSorting steps 1\u20137 are used to sort for eODGT6-reactive CD4bs-specific B cells which most likely encode VRC01-class antibodies in the VH1-2/LC mice. These antibodies should cover a wide range of SHM levels. Step 8 is added to select for B cells expressing more affinity matured and more cross-reactive VRC01-class antibodies, for example the target bnAb, that can bind the glycan276-containing BG505.SOSIP trimer.\ntable:files/protocols_protocol_1448_6.csv\n\u2217Only use this probe when sorting for bnAb.\ntable:files/protocols_protocol_1448_7.csv\n1st PCR: Set up the 1st PCR reactions based on recipe in materials and equipment[href=https://www.wicell.org#materials-and-equipment] (make 100\u00d7 master mix for each 96 well PCR plate) and run PCR under the conditions as described in Table 3[href=https://www.wicell.org#tbl3]. Choose corresponding 1st PCR 5\u2019 (forward) and 3\u2019 (reverse) primers to amplify the heavy and light chains of VRC01-class BCRs (see key resources table[href=https://www.wicell.org#key-resources-table]).\ntable:files/protocols_protocol_1448_8.csv\n2nd PCR: Set up the 2nd PCR reactions (see materials and equipment[href=https://www.wicell.org#materials-and-equipment], make 100\u00d7 master mix for each 96 well PCR plate) and run PCR under the conditions as described in Table 3[href=https://www.wicell.org#tbl3]. Choose corresponding 2nd PCR 5\u2019 (forward) and 3\u2019 (reverse) primers to amplify the heavy and light chains of VRC01-class BCRs (see key resources table[href=https://www.wicell.org#key-resources-table]).\nGel electrophoresis: load 5\u00a0\u03bcl of the 2nd PCR products to 4\u00d7 (24+1)-well premade 2% agarose gel with EtBr, and run in 1\u00d7 TAE buffer at 150V for 20\u00a0min. Take gel images on UV transilluminator in a Bio-Rad ChemiDoc imaging system.\nCritical: Ethidium Bromide (EtBr) is a carcinogen. Wear gloves when handling EtBr-Agarose gels.\nIdentify wells with positive PCR band of immunoglobulin heavy or light chains and send the remaining 2nd PCR products in those wells for Sanger sequencing (by Genewiz) with the corresponding 2nd PCR 3\u2032primers.\nOptional: Can also obtain VRC01-class antibody sequences by H/L-paired deep sequencing or 10\u00d7 genomics.\nSequence analysesSubmit raw sequencing results to IgBlast and display mismatches with the germline (VH1-2 or VK3-20) sequences and check the sequencing chromatograms to correct any misread of nucleotides (including indels) and ensure that the mismatches from the germline sequences are not from sequencing errors.\nSubmit all corrected sequences in FASTA format to IMGT/V-QUEST (<= 50 sequences) or IMGT/HighV-QUEST (> 50 sequences) to define V-genes, D- genes, J-genes and CDRs of the queries; IMGT will also export the translated VH and VK sequences of the queries and align all sequences derived from the same V-gene.\nAlign all heavy and light chains against their respective germline V-genes, VH1-2 or VK3-20, only displaying mismatched residues while showing all identical residues as a dot (.) or hyphen (-) in alignment. This can be done in public sequence alignment servers such as MULTALIN. In fact, the IMGT/V-QUEST protein sequence alignment is displayed this way and can be copied and pasted into cells [A3:A102] of a Microsoft Excel file template that we created specifically for analyzing VRC01-class Ab sequences and key mutations (Table S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1448-Mmc1.extension]). This file contains two sheets, each for analyzing VH1-2 HCs and VK3-20 LCs, respectively. It will output trimmed sequence alignment for making logo graphs and calculate frequencies of VRC01-class key mutations.\nThe aligned antibody sequences in Cells [A3:A102] of our Excel template file are automatically trimmed to remove the space and extra dot (.) in the IMGT/V-QUEST alignment and to keep the same length as the corresponding germline V-gene. The trimmed sequences are outputted in corresponding rows in column D. This trimming step is necessary for the following key mutation frequency analysis and logograph display of mutations induced by immunizations.The frequency of each VRC01-class key mutation, as defined in our previous study (Chen et\u00a0al., 2021[href=https://www.wicell.org#bib3]), among all antibody sequences obtained from a certain immunization stage will be automatically calculated and output in cells [E103:M103] of \u201csheet1_VH1-2 HCs\u201d and cells [E103:K103] of \u201csheet2_VK3-20 LCs\u201d (Table S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1448-Mmc1.extension]). The total number of key mutations in each antibody heavy and light chains is also calculated and displayed in sheet1 [ N3:N102] and sheet2 [ L3:L102] VK3-20, respectively. The average number of key mutations in a HC or LC of the analyzed sequences is shown in Sheet1 [N103] or Sheet2 [L103], respectively.\nPerform the above key mutation frequency analysis for every immunization stage and make a table to show how key mutation frequency changes with different immunization stages and last injected immunogens (Figure\u00a06[href=https://www.wicell.org#fig6]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1448-Fig6.jpg\nFigure\u00a06. Exemplar key mutation frequencies and VH1-2 mutation logo graphs of VRC01-class Abs at different immunization stages in response to different immunogens\n(A) Exemplar key mutation analysis results at different immunization stages. For the Key SHMs \"H-\" denotes heavy chain and \"K-\" denotes light chain mutations.\n(B) Exemplar VH1-2 mutation logo graphs. Germline VH1-2\u221702 sequence is shown at the bottom and the key mutation residues identified in Chen et\u00a0al. (2021)[href=https://www.wicell.org#bib3] are highlighted in red. Immunization stage (Post X), number of antibody sequences (n) and last injected immunogen are listed above each logo graph.\nMake logo graphs showing all the amino acid mutations (vs. the germline V-gene) and their frequencies in all the antibody sequences from a certain immunization stage.\nGenerate FASTA format of the aligned and trimmed antibody sequences:\nCopy the sequence numbers and the corresponding aligned and trimmed antibody sequences in cells [C3:D100] of the Excel template (Table S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1448-Mmc1.extension]) and paste them into a TextEdit file in Mac or a Word file in Windows.Use letter \u201cO\u201d to replace any deleted residues (shown as \u201c.\u201d) compared to the germline V-gene in the aligned sequences in Table S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1448-Mmc1.extension]the TextEdit or Word file with the \u201cReplace\u201d function. In the logo graph, \u201cO\u201d would represent a residue deletion.\nIn the Mac TextEdit file, select and copy all content to the designated sequence input field in WebLogo server, the sequences will be in FASTA format.\nIn Windows, save the Word file as a plain text file, and in the popup window, check the box under \u201coptions\u201d for \u201cinsert line breaks\u201d, and click OK. Reopen the saved plain text file with NotePad, select and copy all content to WebLogo server, and the pasted sequences will also be in FASTA format.\nUse the default setting, including checked \u201cSmall Sample Correction\u201d, to submit the pasted sequences.\nA mutation logo graph will be displayed for the pasted sequences and can be copied.\nMake a logo graph for VRC01-class Ab sequences obtained from every immunization stage, including na\u00efve or preimmune stage. Align the graphs together as in Figure\u00a06[href=https://www.wicell.org#fig6]B, and it will be visually obvious at which stage or following injection of which immunogen each amino acid mutation, including the key mutations, initially appear or significantly rise in frequency.\nCritical: Only when aligned sequences all have the same number of characters and are in FASTA format, can the WebLogo output the correct logograph. To display only mutated residues, it is important to exclude the germline V-gene sequence in the alignment and display all germline residues in the aligned antibody sequences as a (.) or (-).", "Step-by-step method details\nStep-by-step method details\nDownload codes\nTiming: 10\u00a0min\nThe MATLAB codes for simulating gene expression noise and the R codes for analyzing flow cytometry data are available at https://github.com/XWangLabTHU/microRNA_noise[href=https://github.com/XWangLabTHU/microRNA_noise]. Please notice that all MATLAB codes should be downloaded into one same folder as the execution of solve_noise.m relies on the function steady_state.m. See troubleshooting 1[href=https://www.wicell.org#sec5.1] for details.\nSimulating gene expression noise modulated by miRNA\nTiming: 20\u00a0min\nIn this section, we will describe how to set up a mathematical model to simulate the influence of miRNA properties on gene expression noise in MATLAB, using the code we provided.\nThe parameters of the mathematical model can be modified to fit the purpose of simulation. There are four main types of parameters that should be set before simulation: the kind of model, the fixed parameters, the altered parameters, and the simulation step size and range. After, the model could be simulated and visualized.\nSet parameters for simulation.\nOpen MATLAB. Open the folder where the MATLAB codes are deposited.\nOpen the code file solve_noise.m. Please notice that in MATLAB, lines beginning with \u201c%\u201d denote comment lines in the code that will not be read as code in the software.\nSet the type of model on Line 7. There are three types of preset miRNA regulation models:\nfor competing RNAs:\n> type\u00a0= 1;\nfor repetitive targets of same miRNAs:\n> type\u00a0= 2;\nfor multiple targets of different miRNAs:\n> type\u00a0= 3;Set the fixed and altered parameters. These parameters describe the production and degradation rates of all components in the biochemical reactions, which are shown in Figure\u00a02[href=https://www.wicell.org#fig2]. All parameters involved in the model should be set before simulation in the \u201cset fixed parameters\u201d section. All parameters that should be altered for simulating different conditions should be set in the \u201cset altered parameters\u201d section. We provide samples of altered parameters for all three model types, of which parameters for type 2 and 3 are commented on by default. If users are interested in type 2 or 3, they should uncomment (remove the \"%\" from the beginning of the line) the corresponding codes of the type they are interested and comment (add a \"%\" to the beginning of the line) the unrelated codes of other types. We provided default parameter settings in the code according to previous studies that describes perfect complementary miRNA target sites (Wei et\u00a0al., 2019[href=https://www.wicell.org#bib10], 2021[href=https://www.wicell.org#bib11]; Yuan et\u00a0al., 2015[href=https://www.wicell.org#bib12]). Users can modify these parameters by comparing their interested miRNA targets and the perfect complementary miRNA targets in this study.\nThe simulation step size and range should be assigned in the \u201cset simulation parameters\u201d section. The simulation range determines the range that the gene expression is simulated in by setting the production rate of the target gene\u2019s mRNA (kT). We recommend users to set a wide range with a large step size to quickly determine the lower and upper limit of the range, and then narrow the step size to gain a refined simulation result.Run solve_noise.m to simulate the model with the set parameters by clicking Run in the Contextual Tab Editor. The results will be written into the file exp_mean.mat and exp_CV.mat. exp_mean.mat is a matrix that stored the expression level of the target gene with different kT under different parameter settings that are set in step 2d. exp_CV.mat is a three-dimensional array that stored the expression noise (CV) of the target gene with different kT under different parameter settings contributed by different reactions. The first component of the third dimension of the array represents the total noise contributed by all reactions. All the other components of the third dimension of the array represent the noise contributed by a single reaction, following the order shown in Figure\u00a02[href=https://www.wicell.org#fig2].\nOpen and run plot_noise.m to visualize the results stored in the file exp_mean.mat and exp_CV.mat.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1459-Fig2.jpg\nFigure\u00a02. The definition of parameters for simulation\nThe number represent the order of reactions for calculating their contributions in step 3. Reactions employed in each miRNA regulation model are shown. Types of all components and reactions are shown with different colors, shapes and line types.\nTransient transfection of HeLa cells\nTiming: 2\u00a0days\nHere we will introduce how to transfect Hela cells with three plasmids. The first plasmid can express two fluorescent proteins, mKate2 and EYFP, driven by a bidirectional promoter, which can be activated by binding of reverse tetracycline-controlled transactivator (rtTA) in the existence of doxycycline (Figure\u00a01[href=https://www.wicell.org#fig1]). The second plasmid can express rtTA constantly. The third plasmid does not encode any proteins and is used as a blank plasmid to increase transfection efficiency and reduce variation of plasmid transfection. miRNAs that can bind to 3\u2032UTR of EYFP are endogenous.Plasmids should be transfected into cells for further noise observation and quantification. Here we transfect HeLa cells via Lipofectamine LTX with PLUS Reagent according to the manufacturer\u2019s manual protocol[href=https://assets.thermofisher.com/TFS-Assets/LSG/manuals/LipofectamineLTX_PLUS_Reag_protocol.pdf] with the cell density, amount of DNA and the reagents specified specifically for Hela cells transfected in 12-well plates compared to the manual (see next steps for details).\nCulture the cell lines into 12-well plates that you are interested in on the day before transfection with a proper density. The density of HeLa cells should be around 1.6\u00a0\u00d7 105 cells/mL. The culture medium of HeLa cells is Dulbecco's Modified Eagle Medium (DMEM) with 10% Fetal Bovine Serum (FBS), 1% MEM Non-Essential Amino Acids Solution (NEAA) and 1% Penicillin-Streptomycin. The total volume of the medium per well is 1\u00a0mL.\nAbout 24\u00a0h later, make sure that the density of cells is around 70%.\nFor each well, Dilute 2\u00a0\u03bcL Lipofectamine\u00ae LTX Reagent in 50\u00a0\u03bcL serum-free DMEM.\nFor each well, add 40\u00a0ng of the plasmids carrying the dual-fluorescent reporter, 40\u00a0ng of the plasmids carrying the rtTA gene and 420\u00a0ng of blank plasmids with no protein-coding sequences (for example, pDT7004 from (Li et\u00a0al., 2015[href=https://www.wicell.org#bib4])) into 50\u00a0\u03bcL serum-free DMEM, and then add 1\u00a0\u03bcL PLUSTM Reagent.\nMixed the reagents prepared in step 7 and 8.\nIncubate for 5\u00a0min at 20\u00b0C\u201325\u00b0C.\nChanged the medium of cells in the 12-well plates with 900\u00a0\u03bcL media, and then add 100\u00a0\u03bcL mixture prepared in step 9 to cells.\nAdd doxycycline to the medium with a final concentration of 1\u00a0\u03bcg/mL after adding the mixed reagents immediately.\nChange the medium one day after transfection. Do not forget to add doxycycline into the medium again with the same concentration described in step 12.Note: Make sure that the order of adding the reagents is strictly the same as the manufacturer\u2019s manual, which means that do not add the PLUSTM Reagent first to the medium and then add plasmids sequentially, because adding the PLUSTM Reagent first will decrease the co-transfection efficiency of the plasmids.\nNote: It is important to add the blank plasmid with no protein-coding sequence. This procedure could supplement the total amount of the plasmid to enhance the transfection efficiency and reduce the variation of transfection. The amount of the plasmids could be adjusted to ensure a high transfection efficiency. For example, when the transfection efficiency is low, try to improve the relatively amount of the plasmids that express the reporter and rtTA in the total 500\u00a0ng plasmids (e.g., 80\u00a0ng of the plasmid that express the reporter\u00a0+ 80\u00a0ng of the plasmid that express rtTA\u00a0+ 340\u00a0ng of the blank plasmid per well).\nFlow cytometry\nTiming: 4 h\nThe fluorescent intensity of cells can be observed by flow cytometry two days after transfection.\nDigest cells from the 12-well plates using trypsin-EDTA (200\u00a0\u03bcL for each well, 3\u00a0min), and then stop the reaction using 300\u00a0\u03bcL culture medium described in step 5.\nCentrifuge cells at 300\u00a0g for 5\u00a0min, wash cells using 500\u00a0\u03bcL phosphate buffered saline (PBS).\nRepeat step 15.\nResuspend cells with PBS and transfer the cells into separate FACS tubes.\nDetect fluorescent intensities of cells using flow cytometry machines and record no fewer than 105 singlets.\nExport all records with the fcs format.Note: The type of flow cytometer is flexible and the settings may need to be altered according to the equipment. We do not recommend using the high-throughput screening (HTS) way of flow cytometry to perform these procedures. When using HTS, the cells can hardly be mixed well and may cause poor data quality or damage the flow cytometry machine.\nNote: Make sure cells have been washed twice before running in the flow cytometry machine. If the medium is not washed thoroughly, cells may clump together such that the machine may be blocked by the clumped cells and bubbles can be generated, making the measurement of fluorescent intensity unreliable.\nNote: Vortex cells to avoid clumped cells right before running samples in the flow cytometry machine.\nNote: If the recording speed showing in the flow cytometry machine is too high (> 3,000 total events per second), dilute cells properly to make sure around 2,000 total events are recorded per second at the high-speed mode of the machine. Otherwise, too many cells might block the machine and generate bubbles in the machine.\nData analysis of flow cytometry\nTiming: 1 h\nGate living cells according to the forward scatter (FSC) and side scatter (SSC) value from the flow cytometry data via data analysis software. Save the fluorescent intensity values of these cells in a csv file. For instance, when analyzing the data by Floreada.io, press File/Open File(s) to import fcs files, gate living cells with FSC-A/SSC-A, FSC-H/FSC-W, SSC-H/SSC-W sequentially, and then store the data by pressing File/Save CSV Event File. Arrange all csv files into a folder named data in the same folder with analysis.r.Run analysis.r to analyze all files in the folder data. For instance, when using Terminal, first direct the working directory to the working folder where the file analysis.r and the folder data are, and then execute\n> Rscript analysis.r\nWhen using RStudio, set the working folder as the working path and then execute analysis.r. The results will be stored in a folder named result.", "This protocol will introduce the basic navigational techniques needed to browse the Reactome Web site.\nNecessary Resources\nHardware\nComputer capable of supporting a Web browser and an Internet connection\nSoftware\nAny modern Web browser such as Firefox, Safari, and Chrome will work to display Reactome Web pages\n1. Point the browser to the Reactome home page at https://reactome.org[href=https://reactome.org].\nThe home page (Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0001]) has several elements.\nAt the top left of the home page is the Reactome logo. Clicking on this from any page on the Web site will return the user to the home page.The navigation bar, at the very top of the page, provides access to the top-level sections, tools, and resources of the Reactome site. \u201cAbout\u201d is a description of the project as a whole, including Reactome team and Scientific Advisory Board members, details of our open source licenses, the upcoming editorial calendar and current statistics; \u201cContent\u201d provides links to resources within the database, including the table of contents, database object identifiers, a detailed description of the Reactome data schema as well as information on specific features such as the Reactome Research Spotlight, the COVID-19 Disease project and the ORCID integration project; \u201cDocs\u201d provides access to user guides and information about the Reactome data model, icon library and computationally inferred events (described more fully in step 3, below), as well as instructions on how to link to or cite Reactome material; \u201cTools\u201d provides links to key Reactome functions, including the pathway browser, tools for analyzing gene lists and gene expression data, for species comparisons, tissue distribution and for disease overlay. This tab also has links to the Reactome analysis and content services and Reactome FIViz, the Reactome functional interaction network app (Wu et\u00a0al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0023]). \u201cCommunity\u201d has information on outreach and events, Reactome publications, partnerships and collaborations as well as access to training guides and tutorials; \u201cDownload\u201d provides access to the whole database as a single bulk or individual data set download, pathway downloads in a variety of formats including BioPAX, SMBL, and PDF, as well as physical entity and event mapping files.Below the header is a simple search bar that permits flexible keyword, accession number, and database identifier queries on the Reactome database. Below the search bar are four large buttons linking to key features of the Reactome Web site: \u201cPathway Browser\u201d, which takes the user into the curated pathway hierarchy of human biological pathways (described in step 2 below), \u201cAnalysis Tools\u201d, which opens the analysis window allowing users to analyze gene lists and expression patterns and to conduct species comparisons and examine tissue distribution, \u201cReactomeFIViz\u201d, which takes users to the documentation page for the Reactome functional interaction network app (Wu et\u00a0al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0023]), and \u201cDocumentation\u201d, which links to useful information about the Web site for users and developers.\nBelow these buttons is a section of the home page (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0002]) that contains the \u201cReactome Research Spotlight,\u201d (a feature that highlights recent publications that have incorporated Reactome data or tools into their research), as well as news items, the Twitter feed, curation statistics from the most recent release, and information about the project.\nScrolling farther down on the home page (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0003]) reveals a \u201chelp\u201d panel, with buttons linking to guides for users and developers, a button linking to information on citing Reactome and a \u201cContact Us\u201d button, for users requiring help. Below the help panel is a panel for API and data access, including the Content and Analysis Services, the icon library and the graph database.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/b10b4554-b8c3-42ae-8d19-67b7f47d6593/cpz1722-fig-0001-m.jpg</p>\nFigure 1\nThe Reactome home page (https://reactome.org[href=https://reactome.org]) features a header panel with drop-down menus to access Web content, a search bar and four large buttons linking to key features of the Web site: Pathway Browser, Analysis Tools, the Reactome FIViz app, and documentation.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a6952302-e61d-449c-881f-652c09567351/cpz1722-fig-0002-m.jpg</p>\nFigure 2The Reactome home page also contains announcements (news, Twitter, research Spotlight, project information), and statistics from the most recent release.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4ab82c43-9021-4039-8893-b1f6f29af6f9/cpz1722-fig-0003-m.jpg</p>\nFigure 3\nLower down on the home page is a help panel with buttons linking to guides for users and developers, and buttons for API and data access.\n2. To begin exploring the curated Reactome pathways, click on the \u201cPathway Browser\u201d button on the home page. This will load the page shown in Figure 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0004].\nThe Reactome Pathway Browser consists of four key elements:\n1.The header bar, at the top of the page. This has the Reactome logo, which returns users to the home page when clicked. Next to this is a species selector, with a drop-down list of species. Selecting an organism from the species selector will refresh the pathway browser with the inferred pathway diagram from the selected model organism if it is conserved. Reactome data is human-centric. Data for other species is inferred from human pathways and pathway steps may be missing for other organisms if they are not identified by the inference process. The \u201cAnalysis\u201d button provides access to the interactive tools associated with the pathway diagrams, described below in Basic Protocols \nBasic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0006], \nBasic Protocol 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0007], \nBasic Protocol 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0008], and \nBasic Protocol 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0009]. Clicking the \u201cTour\u201d button in the header opens a brief video tutorial on the key Reactome website functions, while selecting one of the layout buttons in the top right of the header bar allows users to personalize the Web site panels that are displayed to optimize viewing.2.The pathway hierarchy panel, occupying the vertical rectangle on the far left of the screen, provides a scrolling display of all Reactome pathways in a hierarchy. The plus (\u201c+\u201d) symbol indicates that there are subheadings underneath the pathway headings. Clicking on a plus (\u201c+\u201d) symbol will expand the topic to show its subsections. The subpathways and reactions within each pathway can be hidden by clicking on the minus (\u201c\u2013\u201d) symbol to the left of the pathway name. Next to the plus/minus signs is a small pathway icon in blue or black, indicating the presence or absence of an \u201cEnhanced High Level Diagram\u201d (EHLD, see below) associated with that pathway. A red \u201cN\u201d or \u201cU\u201d next to a pathway name indicates that the pathway is new or has been updated since the last release, respectively. A red cross next to a pathway name indicates that the pathway contains disease annotations.3.The visualization panel, to the right of the hierarchy panel, displays an interactive pathway diagram that can be panned and zoomed in Google Map style. The visualization panel is synced with the pathway hierarchy on the left, such that selecting pathways or subpathways in the hierarchy will change what is displayed in the visualization panel. There are three primary views that can be displayed in this panel. The first view, \u201cPathway Overview\u201d, displays the entire pathway hierarchy as interconnected nodes, with nodes representing pathways and edges representing relationships. If a user selects a pathway in the hierarchy or in the graphical display, the corresponding node is outlined in orange. The second view, \u201cEnhanced High Level Diagram (EHLD)\u201d, where available, displays a textbook style interactive illustration of a user-selected pathway (Sidiropoulos et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0022]). The third view, \u201cEntity Level View\u201d (ELV), displays the reaction-level molecular details of the user-selected pathway. The ELV pathway diagrams apply the conventions of the Systems Biology Graphical Notation (SBGN) format (Le Nov\u00e8re et\u00a0al., 2009[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0015]) to distinguish the molecules and reactions by shape and cellular location, providing a dynamic framework for pathway visualization and data analysis. Users can toggle between the Pathway Overview and the ELV views by clicking the second of three blue icons beside the search bar at the top left of the visualization panel. EHLDs, where available, appear as a thumbnail in the bottom left of the visualization panel, and can be accessed by clicking the pathway name in the pathway hierarchy at the left of the visualization panel. An alternate view of the entire pathway hierarchy can be accessed by clicking the third blue icon to the right of the search bar from the Pathway Overview view.This opens a separate window and displays the Reactome pathways as a Voronoi diagram, with sizes of pathway clusters proportional to the number of events the pathway contains (Jassal et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0013]). To move from the Voronoi diagram back to the Pathway Browser, click and hold on a pathway name within the Voronoi image.At the top left of the visualization panel is a search bar, featuring results grouping and filtering, hit highlighting and text auto-completion (Fabregat et\u00a0al., 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0007]). The visualization panel also contains an icon (top right, first blue icon) that allows export of the pathway visualization in several different formats, a \u201ccompass\u201d icon that provides a pathway overview key (top right, second blue icon) and an expandable panel on the right side which provides information and allows users to customize color preferences.\nAt the bottom right of the visualization panel are navigation arrows and zoom features. Users can also zoom using the mouse wheel and can click and drag the diagram. The thumbnail image, in the lower left corner of the visualization panel, can be used to navigate quickly to a region of interest in the pathway diagram.4.The details panel is located below the visualization panel, and its contents change in sync with user selections in the visualization panel or the pathway hierarchy. The details panel has 6 tabs, each of which contains a general description of what will be displayed in that panel once an event or entity is selected. The \u201cDescription\u201d tab displays molecular details related to the selected event or entity, including inputs and outputs of reactions, catalysts, regulators, preceding, and following events, linkouts to other databases with entity information, etc. This tab also displays event summations, literature references, and editorial information. The \u201cMolecules\u201d tab shows downloadable details of the molecules (proteins, small molecules/chemical compounds, nucleic acid sequences) involved in the selected event. The \u201cStructures\u201d tab shows reaction details from Rhea (Bansal et\u00a0al., 2022[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0002]) or structural information from ChEBI (Hastings et\u00a0al., 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0011]) or PDBe (Armstrong et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0001]), as appropriate. The \u201cExpression\u201d tab displays gene expression information from Gene Expression Atlas for genes corresponding to the selected items. The \u201cAnalysis\u201d tab displays the pathway-specific results generated by the Reactome analysis tools, and finally, the \u201cDownload\u201d tab allows users to download the selected pathway in several different formats.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/cc509310-f827-47bf-9660-9f081f95c915/cpz1722-fig-0004-m.jpg</p>\nFigure 4\nThe Pathway Browser, including the event hierarchy (left panel), the details panel (bottom) and the visualization panel displaying the \u201cPathway Overview\u201d view.\n3. This protocol will illustrate features of the Reactome pathway browser by exploring the events contained within the \u201cDNA Repair\u201d pathway. To begin, click on the \u201cDNA Repair\u201d pathway title in the pathway hierarchy at right.In response to this selection, the visualization panel zooms in on the DNA Repair node in the Pathway Overview (see Fig. 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0005]) and brings up the pathway level information in the \u201cDescription\u201d tab of the details panel, including pathway level summation, editorial attributes (authors and reviewers), literature references, GO Biological Process where appropriate, and cellular compartment. Each of these attributes is expandable: clicking on the plus (\u201c+\u201d) symbol on the right reveals further details, including linkouts to PubMed, ORCID, GO, or other cross-referenced resources. The Description tab of the details panel also contains a stable identifier for the event displayed, including the three letter species code (HSA for Homo sapiens is the default unless species is changed). Scrolling down in the details panel to the section labeled \u201cView computationally predicted event in\u201d reveals a species selector bar. Reactome's manual annotations are human-focused but are computationally extended to other species based on protein conservation as described under \u201cComputationally inferred events\u201d in the \u201cDocs\u201d section of the Web site. Selecting a different species from the species selector bar will update the events and information displayed in the visualization and details panel for the selected species. Not that the species may also be changed using the drop-down menu to the right of the Reactome logo in the header of the pathway browser.Other tabs in the details panel are also updated in response to the selection of \u201cDNA Repair\u201d pathway in the hierarchy (note that the \u201cStructures\u201d tab is not populated with data when a pathway-level event is selected in the hierarchy, and the \u201cAnalysis\u201d tab is only populated once an analysis has been performed). The \u201cMolecules\u201d tab is updated to provide information on all the chemical compounds (55 in DNA Repair pathway), proteins (315) drugs (28), sequences (0) or other entities (0) contained in the pathway, and the total number of molecules is displayed as in a red bubble at the top of the \u201cMolecules\u201d header (398). Inside the \u201cMolecules\u201d panel, expandable sections provided detailed information on each of the entities contained in the event linked out to the appropriate reference database. This information is downloadable in full or in part from within the \u201cMolecules\u201d tab. The \u201cExpression\u201d tab displays expression data from Gene Expression Atlas for each of the genes contained within the pathway, and the \u201cDownload\u201d tab has pathway reports and diagrams for the selected pathway available for download in a variety of formats.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/c5049da0-2263-43cd-a3ba-f45fc69077a5/cpz1722-fig-0005-m.jpg</p>\nFigure 5\nHigh-level view of the DNA Repair pathway in the Pathway Browser, showing pathway-level summation in the details panel, a zoomed-in view of the Pathway Overview and a thumbnail of the textbook-style illustration in the visualization panel.\n4. Double click on the \u201cDNA Repair\u201d pathway title in the hierarchy, or double click on the corresponding node in the visualization panel.Double clicking on the DNA Repair pathway title in the hierarchy opens the interactive EHLD for this pathway (users know an EHLD is available because the pathway icon between the plus sign and the pathway name in the hierarchy is blue rather than black; pathways with black icons do not have EHLDs but may have static, non-interactive illustrations).\n5. Click on the plus (\u201c+\u201d) symbol beside \u201cDNA Repair\u201d in the hierarchy to reveal the subpathways.\nUsers can navigate to any of the 7 subpathways of DNA Repair by clicking on the plus (\u201c+\u201d) symbol beside DNA Repair in the hierarchy or by clicking on the subpathway name in the EHLD. Clicking on a subpathway name either in the hierarchy or from the EHLD will either open another EHLD, if the selected subpathway itself contains multiple subpathways with ELV diagrams (as is the case for the Base Excision Repair subpathway) or will open an ELV pathway as is the case for the other 6 subpathways of DNA Repair.\n6. Click on the \u201cNucleotide Excision Repair\u201d pathway either in the hierarchy or from within the \u201cDNA Repair\u201d EHLD.\nAn ELV diagram containing reactions curated at the level of molecular participants appears in the visualization panel, and the details panel updates to reflect information appropriate for this pathway. Note that the total molecules displayed in the \u201cMolecules\u201d tab for this pathway is 119 (9 \u201cChemical Compounds\u201d, 110 \u201cProteins\u201d), fewer than the parent DNA Repair pathway, as expected.ELV pathways open at a fully zoomed out level. Diagram zoom level is controlled either by a mouse or with the zoom icons in the bottom right of the visualization panel. Diagrams can be easily recentered to the fully zoomed out view by clicking on the icon to the immediate right of the search bar at the top of the visualization panel.\nDepending on the diagram size, commonly occurring reaction participants such as H2O or ATP may not be displayed in the diagram at the fully zoomed out level. As the user zooms in, these entities are dynamically added to the pathway diagram.\nNucleotide Excision Repair has two subpathways, both laid out in the same ELV: \u201cGlobal Genomic NER (GG-NER)\u201d and \u201cTranscription-coupled NER (TC-NER)\u201d. These subpathway names are displayed in the zoomed-out view of the ELV and are contained by colored subpathway boundary boxes. Clicking on either of the NER subpathway names in the hierarchy highlights the events encompassed by that subpathway in the visualization panel in blue, while hovering over a pathway name highlights the corresponding events in yellow.\nThe pathway diagram uses the conventions of the Systems Biology Graphical Notation (SBGN) format to distinguish the molecules and reactions by shape and cellular location, to provide a dynamic framework for pathway visualization and data analysis. A key to the shapes used in the ELV diagrams is available by clicking on the compass arrow at the top right of the visualization panel; doing so reveals the key shown in Figure 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0006].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/91a90bf6-5fa9-4447-9e09-d342dcd0dc94/cpz1722-fig-0006-m.jpg</p>\nFigure 6\nA key to the SGBN-based icons used in the molecular level events in Reactome.7. Continue to drill down into the hierarchy to reach reaction level events as follows: Click on the pathway title \u201cGlobal Genomic Nucleotide Excision Repair\u201d. Note that the details panel shows that this pathway contains 92 of the 119 molecules present in the NER pathway (8/9 \u201cChemical Compounds\u201d and 84/110 \u201cProteins\u201d). Expand this subpathway in the hierarchy by clicking on the adjacent plus sign to reveal the four subpathways (\u201cDNA Damage Recognition in GG-NER\u201d, \u201cFormation of Incision Complex\u2026\u201d, etc.). Continue to expand the hierarchy by clicking the \u201cDNA Damage Recognition in GG-NER\u201d pathway in the hierarchy to reveal the five molecular level events contained within, noticing that at each subsequent pathway level the fraction of molecules represented is adjusted relative to the parent NER pathway.\n8. Click on \u201cXPC binds RAD23 and CETN2\u201d, the first reaction in the \u201cDNA Damage Recognition in GG-NER\u201d pathway, as shown in Figure 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0007].\nClicking a reaction in the pathway hierarchy will cause the reaction name and the name of the subpathway(s) and parent pathways to be highlighted, as seen in Figure 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0007], above. The visualization panel will recenter on the selected reaction and the reaction will be highlighted in blue. Furthermore, the description tab of the \u201cDetails\u201d panel will update to show particulars of the selected reaction, which will include some or all of the following, where appropriate: inputs, outputs, catalysts and positive and negative regulators, preceding event, and \u201cinferred from\u201d reaction.In the case of the \u201cXPC binds RAD23 and CETN2\u201d reaction, there are three inputs, one output, no catalyst or regulators, no preceding event, and no \u201cinferred from\u201d reaction. The inputs to the reaction are the proteins XPC and CETN2 and a set of RAD23 proteins. Reactome sets are groups of molecules that are known or predicted to function in the same way in a given reaction and may either be \u201cmember\u201d sets, where each participant is known to have the indicated role, or \u201ccandidate\u201d sets, where some participants are verified members, and others are candidates based on sequence or structural similarity. Here the RAD23 set is a \u201ccandidate\u201d set, with RAD23B a verified member and RAD23A a candidate. The output of this reaction is the complex formed by the binding of the three inputs (XPC, the RAD23 set, and CETN2).\nClicking on the \u201cXPC binds RAD23 and CETN2\u201d reaction simultaneously updates information in the other details panel tabs:\n-The red bubble on the \u201cMolecules\u201d header now indicates that the reaction contains 4/119 molecules from the Nucleotide Excision Repair pathway, all of them proteins, and the \u201cMolecules\u201d panel is updated with information on these four proteins.\n-The \u201cStructures\u201d header has been decorated with a red bubble indicating the fraction of the molecules in the reaction for which structural information is available (here, 4/4), and the panel provides linkouts to those structures in Protein Data Bank. The expression for these four proteins is displayed in the \u201cExpression\u201d tab.\n-The \u201cAnalysis\u201d tab continues to be blank unless an analysis has been performed.\n-The \u201cDownload\u201d tab: note that although a reaction is selected in hierarchy, the download available from this tab remains the immediate parent ELV pathway (here the \u201cNucleotide Excision Repair\u201d subpathway of the top-level \u201cDNA Repair\u201d).In the context of the \u201cDNA Damage and Recognition in GG-NER\u201d pathway, the reaction \u201cXPC binds RAD23 and CETN2\u201d is the first in a series of connected reactions, in which the output of one reaction is the input to the next reaction. Because there are no annotated reactions that occur prior to the binding of RAD23 and CETN2 to XPC, there is no indication of a \u201cpreceding event\u201d in the \u201cDescription\u201d tab of the details panel for this reaction. If the following reaction in the hierarchy (\u201cXPC:RAD23:CETN2 and UV-DDB bind distorted dsDNA site\u201d) is selected, the \u201cDescription\u201d tab of the details panel is updated with reaction-specific information. In particular, a new field describing the positive regulator (INO80 complex) is added, as is the new field \u201cPreceding Event\u201d, which lists the \u201cXPC binds RAD23 and CETN2[Homo sapiens]\u201d reaction described above. Clicking on the plus (\u2018+\u2019) symbol to the right of this \u201cpreceding event\u201d reaction name expands this field to reveal the summation and literature references associated with the reaction. This allows users to put the current reaction into a more complete biological context.\nThe relationship between the \u201clevels\u201d of the pathway hierarchy on the one hand and the \u201cPreceding event(s)\u201d links, on the other hand, may not be immediately clear. The nested levels of the pathway hierarchy reflect levels of abstraction in the conceptual organization of pathways. As one moves deeper into the hierarchy, the contents of the pathway diagram become more and more specific and move closer to the biochemical reaction level. The \u201cPreceding event(s)\u201d link only appears at the reaction level.Reactions in Reactome are human-centric. Wherever possible, the molecular events that are depicted in Reactome are supported by direct experiments that make use of human cells, tissues, protein, or other entities. This evidence is cited in the literature references associated with the reaction. Often, however, knowledge of human biology is derived indirectly from work using model organisms. If the direct experimental evidence supporting a given reaction is model organism-based, an \u201cinferring\u201d reaction is created using the molecules from the relevant species, and a corresponding human reaction is inferred from it. Human reactions that are inferred in this way are indicated with a double arrow adjacent to the reaction title in the hierarchy, and by the presence of an \u201cinferred from another species\u201d field in the \u201cDescription\u201d tab of the details panel. Expanding the field by clicking on the plus (\u2018+\u2019) symbol at the right reveals the summation and references for the experiment(s) in the other species. \u201cInferred from\u201d reactions may also be marked as chimeric if the experiment being cited as evidence contains molecules (proteins, nucleic acids, etc.) from multiple species.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/46090d3a-63dc-496a-b7e9-04e9cbe9ef6e/cpz1722-fig-0007-m.jpg</p>\nFigure 7\nSelection of a particular reaction (\u201cXPC binds RAD23 and CETN2\u201d in the \u201cDNA damage and recognition in the GG-NER\u201d pathway) updates the Pathway Browser appropriately.\n9. There are no inferred reactions within the \u201cNucleotide Excision Repair\u201d subpathway. To see an example in the \u201cBase Excision Repair\u201d pathway, expand the \u201cBase Excision Repair\u201d hierarchy to reveal the subpathways \u201cBase-Excision Repair, AP Site Formation\u201d, and continue to unfurl its child pathway \u201cDepurination\u201d, and its child pathway \u201cRecognition and association of DNA glycosylase with site containing an affected purine\u201d.This pathway contains 10 reactions, the ninth of which is \u201cNEIL3 recognizes and binds to spiroiminodihydantoin in telomeric DNA\u201d, with the inferred double arrow indicator beside its event name in the hierarchy. The \u201cDescription\u201d tab of the details panel contains an \u201cInferred from another species\u201d field, listing the corresponding reaction from Mus musculus, along with its summation and literature references, as shown in Figure 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0008].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/bfae08b4-2492-4015-a48e-9774b92222bc/cpz1722-fig-0008-m.jpg</p>\nFigure 8\nThe reaction \u201cNEIL3 recognizes and binds to spiroiminodihydatoin in telomeric DNA\u201d is inferred from a corresponding reaction in Mus musculus, as evidenced by the double arrowhead beside the event name in the hierarchy.\n10. In addition to the reaction level information described above (summations, literature references, editorial attributes, etc.), Reactome also provides detailed information about each of the entities involved in a reaction. To explore this, return to the first reaction of the \u201cDNA Damage Recognition in GG-NER pathway\u201d, \u201cXPC binds RAD23 and CETN2\u201d. Clicking on any of the inputs or outputs of the reaction (or regulators and catalysts where applicable) updates the \u201cDescription\u201d tab of the details panel with information and linkouts for the corresponding molecule. In the \u201cXPC binds RAD23 and CETN2\u201d reaction, click on the \u201cXPC\u201d entity in the pathway diagram.\nThis will update the Details panel to display information about the protein, including synonyms, cellular compartment (linked out to the GO ontology), external identifiers to resources such as UniProtKB, Ensembl, GeneCards, HMDB Protein, HPA, OpenTargets, Orphanet, PDB, PRO, Pharos, and RefSeq and a selector bar to view the entity in other species. Small molecule inputs or outputs (none in this reaction) are similarly linked to appropriate reference databases and provided with synonyms and cellular compartments in the \u201cDescription\u201d tab of the details panel. Small molecules are given species-agnostic identifiers beginning with R-ALL.Note that in the \u201cXPC binds RAD23 and CETN2\u201d reaction, the XPC and CETN2 icons in the visualization panel are decorated with red circles on the upper right corner. This mark indicates the number of interacting proteins in the IntAct database for that entity. Clicking on the red icon displays the interacting proteins in a halo around the pathway entity, as shown for XPC in Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0009]; clicking on an interactor takes the user to the UniProtKB record for that protein. In cases where there are too many interactors to display in the context of the pathway diagram, only the top 18 interactors are shown. Users can customize the confidence level for these interactors with the sliding scale at the bottom of the visualization panel; this toolbar also allows users to download all pathway interactors as a CSV file.\nAlthough the interacting protein overlay by default is set to IntAct, users can change the referring database by opening the panel on the right side of the visualization panel as shown in Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0009] and selecting one of the other highlighted resources from the middle \u201cInteractors\u201d tab. These interactors are loaded on an on-demand basis through PSICQUIC. Note that at this time, interactors can only be displayed for individual pathway protein entities and not for complexes or molecule sets.\nAlso note that zooming in on XPC or other proteins within the visualization panel reveals within the pathway icon the UniProtKB identifier and, if available, an associated structure from PDB.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/7278d7fa-59b1-475b-a1c8-51406e81df30/cpz1722-fig-0009-m.jpg</p>\nFigure 9\nProteins that interact with XPC as described in the IntAct database are displayed as a halo around XPC in the visualization panel.11. Reactome provides information about the subunits of a complex, as well as the larger ensembles of proteins that a complex participates in. In this example, from the \u201cXPC binds RAD23 and CETN2\u201d page, click on the \u201cXPC:RAD23:CETN2\u201d entity in the pathway diagram. This will update the Details panel to display information about this complex.\nThe \u201cDescription\u201d tab of the details panel will now include a new field \u201cComponents\u201d, in which each of the constituents of the complex are listed, each with an expandable window that links to further information about that entity (synonyms, compartment, reference entities, external identifiers, etc. as outlined above for XPC; in addition, clicking on the icon to the left of the component name\u2014here, the green protein circle for XPC also reveals a window with entity-specific information).\nIn addition, the \u201cDescription\u201d tab of the details panel now includes a \u201cProduced by\u201d and a \u201cConsumed by\u201d field, listing events across Reactome as a whole in which the complex is either an output or an input, respectively. These fields are expandable [plus (\u2018+\u2019) symbol at right] to reveal summation and literature references for the reaction-like-event in question. Clicking on the reaction icon to the left of the \u201cProduced by\u201d or \u201cConsumed by\u201d reaction titles will highlight the reaction node and recenter the visualization panel on the corresponding reaction. Note that this may move the user to a different pathway diagram.\nMore detailed information is also provided about the components of sets; to see this click on the RAD23 set that is an input to the \u201cXPC binds RAD23 and CETN2\u201d reaction. Note, however, that sets are not associated with \u201cProduced by\u201d or \u201cConsumed by\u201d fields as complexes are.12. To explore the information Reactome provides about catalysts, click on the third reaction in the \u201cDNA Damage Recognition in GG-NER\u201d pathway, \u201cUV-DDB ubiquitinates XPC\u201d. Catalysts are shown regulating the reaction node by virtue of an edge ending in a circle (see Fig. 10[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0010]). Catalysts may either be independent of other reaction participants, or, as in this case, may be one of the reaction inputs. Reflecting this dual role, the \u201cXPC:RAD23:CETN2:Distorted ds DNA:UV-DDB\u201d complex has both a reaction edge and a catalyst edge associated with it.\nIn addition to the fields described above for reactions without catalysts, the \u201cDescription\u201d tab of the details panel for an enzyme-catalyzed reaction also contains the following information about the catalyst (Fig. 10[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0010]):\n         \n-Physical Entity: whichever molecule in the pathway diagram is associated with the catalyst activity. This may be a single protein, a set of proteins, or a complex (here the complex \u201cXPC:RAD23:CETN2:Distorted ds DNA:UV-DDB\u201d).\n-Active Unit: in cases such as this one where a complex is the catalyst, the specific component that contributes the catalytic activity is identified. Here, the active unit is the UV-DDB subcomplex consisting of DDB1 and 2, RBX1 and CUL4.\n-Molecular Function: the most appropriate term is taken from (and linked out to) the GO. The catalyst name is a concatenation of the GO Molecular Function and the name of the Physical entity.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a66f3d7a-49b8-42d8-8167-da3da02302a6/cpz1722-fig-0010-m.jpg</p>\nFigure 10\nThe details of a Reactome catalyst are shown in the context of the reaction \u201cUV-DDB ubiquitinates XPC\u201d.13. Reactome provides inter-pathway connections for physical entities contained within a given pathway diagram. Hovering over any entity in the visualization panel reveals an arrowhead at the right side of the entity icon. Clicking on this arrowhead reveals an interactive information panel (Contextual Information Panel, CIP) with three tabs (see Fig. 11[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0011]): \u201cMolecules\u201d, \u201cPathways\u201d, and \u201cInteractors\u201d. Similar to the descriptions above, the \u201cMolecules\u201d tab provides the components of the selected entity, and the \u201cInteractors\u201d tab provides a table listing the interacting proteins along with scores and evidence (note that display name of components or interactors can be toggled between common name and reference identifier by clicking on the small \u201cid\u201d button at the top right of the interactive panel; clicking on the pin icon locks the interactive panel to the pathway diagram; to close the panel, click on the \u201cx\u201d icon).\nTo explore the \u201cPathways\u201d tab, click on the arrowhead revealed by hovering above the XPC protein input in the first reaction of the \u201cDNA Damage Recognition in GG-NER\u201d subpathway, \u201cXPC binds RAD23 and CETN2\u201d.\nThe \u201cPathways\u201d tab lists other Reactome pathways in which the selected entity takes part; here the only other pathway in which XPC has an annotated function in Reactome is the \u201cSUMOylation of DNA damage response and repair proteins\u201d pathway. Clicking on the pathway name within the interactive panel moves the user to the new pathway in which that entity participates.\nThis connection between pathways mediated by shared participants highlights potentially unexpected linkages between disparate areas of biology and illustrates the power of Reactome to bridge the domains.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/8060dda3-874d-4983-b657-3c4b03b9430e/cpz1722-fig-0011-m.jpg</p>\nFigure 11\nThe Contextual Information Panel (CIP) displays information about a given pathway entity, including constituent molecules, other Reactome pathways where that entity occurs, and interactors.In addition to normal human biology, Reactome annotates abnormal or pathological events arising from genetic mutation or interaction with an infectious agent in a separate top-level pathway called \u201cDisease\u201d. Reactome disease pathways are designated with a red \u201c+\u201d symbol to the left of the pathway name and include cancer, metabolic, immune, and infectious diseases, among others. Where possible, Reactome disease pathways also include the interaction of relevant therapeutic drugs.\nConsistent with Reactome's pathway-centric view, disease events (with the exception of infectious processes) are annotated as changes to normal molecular level reactions and are displayed in the context of the relevant non-disease pathway background. As a result, there is no single diagram representing a given disease (e.g., bladder cancer or diabetes) but rather individual events that are perturbed in the course of that disease are labeled with the appropriate disease tag and displayed as overlays to normal pathway events. Events with the same disease tag may, therefore, be distributed across multiple normal pathways and pathway diagrams. Infectious diseases represent novel events that do not have a corresponding normal state and have their own pathway diagrams.\nReactome's disease and drug annotations will be explored using the disease pathways \u201cSignaling by ERBB2 in Cancer\u201d and \u201cSARS-CoV-2 Infection\u201d. This module will highlight where and how the disease pathway annotations diverge from those of normal pathways; many of the key annotation features, however, are functionally equivalent and these will not be detailed here.\nNecessary Resources\nHardware\nComputer capable of supporting a Web browser and an Internet connection\nSoftware\nAny modern Web browser such as Firefox, Safari, and Chrome will work to display Reactome Web pages\n1. To begin exploring Reactome's disease and drug annotations, point the browser to the Reactome home page at https://reactome.org[href=https://reactome.org].2. Click on the \u201cPathway Browser\u201d button on the home page and unfurl the events under the \u201cDisease\u201d top level pathway in the hierarchy by clicking on the \u201c+\u201d symbol to the left of the pathway name.\nNotice that subpathways are grouped by general biological processes (\u201cDiseases of signal transduction by growth factor receptors and second messengers\u201d, \u201cDiseases of metabolism\u201d, \u201cInfectious diseases\u201d, etc.) rather than by specific diseases (diabetes, bladder cancer, etc.). These groupings generally mirror the pathway groupings of the normal event hierarchy (See Fig. 12[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0012]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d9f43c8c-3fa9-42a8-8271-346271044ea2/cpz1722-fig-0012-m.jpg</p>\nFigure 12\nThe textbook-style illustration and the pathway summation for the top-level Disease pathway.\n3. Continue to unfurl the disease hierarchy, first expanding the \u201cDiseases of signal transduction by growth factor receptors and second messengers\u201d pathway, and then the child of that subpathway \u201cSignaling by ERBB2 in Cancer\u201d. This is an EHLD-level pathway with 6 subpathways.Notice that in the \u201cDescription\u201d tab of the details panel for this pathway, there is a new field \u201cDisease\u201d, with \u201ccancer\u201d as the entry. Expanding this field with the \u201c+\u201d at the right reveals a disease definition and synonyms along with a hyperlinked identifier for the disease that refers back to the corresponding page in the Human Disease Ontology at the OLS (Schriml et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0021]). DOIDs are applied to all disease entities and events. In the case of disease entities, such as proteins that arise as the result of genetic mutation, the entity may be labeled with as many separate DOIDs as there is evidence to support; in the case of sets of disease entities AND in the case of disease events, only DOIDs that are applicable to ALL the contained entities/events are applied. For this reason, pathway-level DOIDs are often quite general (\u201ccancer\u201d in this case). Drilling down into the hierarchy usually corresponds to more and more specific disease terms.\n4. Click on the first subpathway \u201cConstitutive Signaling by Overexpressed ERBB2\u201d. This opens an ELV level pathway with molecular level reactions laid out.Relative to a wild-type pathway, ELV-level disease pathways have two new fields in the \u201cDescription\u201d tab of the details panel, \u201cDisease\u201d as described above and \u201cNormal pathway\u201d. The \u201cNormal Pathway\u201d field lists the non-disease pathway from the normal hierarchy upon which the disease events are overlaid - in this case \u201cSignaling by ERBB2\u201d. Expanding the panel reveals the summation for the wild-type pathway, while clicking on the pathway icon to the left of the normal pathway name in the details panel moves the user to the pathway diagram for that normal pathway. To return to the disease ELV from the normal pathway, click on the browser's back button. Notice that the events of the normal pathway, while visible in the disease ELV, are not clickable or interactive. To demonstrate this, hover over or try to click any of the greyed-out reaction lines or entities from the normal pathway.\nDisease entities and disease reaction lines are highlighted in the disease ELV in red, while drugs and drug-containing entities are colored purple and have a small Rx in the bottom right of the icon. All disease and drug entities and events displayed in the ELV are fully interactive as expected.\n5. Click on the first reaction of the \u201cConstitutive Signaling by Overexpressed ERBB2\u201d pathway, \u201cERBB2 homodimerization\u201d.\nThis reaction has as input a complex of ERBB2 with the protein ERBIN and the chaperone proteins HSP90 and CDC37; during the reaction, 2 such complexes (note the stoichiometry displayed on the reaction line) associate, the chaperone proteins and ERBIN are released and an ERBB2 homodimer is formed (see Fig. 13[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0013]).Notice that the input complex (\u201cERBB2:ERBIN\u201dHSP90:CDC37\u2033) and the outputs ERBIN, HSP90, and CDC37 are not colored red. These are all genetically normal proteins or protein complexes, are not labeled with a disease tag and consequently are not colored red. In contrast, the output (ERBB2 homodimer) is colored red, and is associated with a disease tag (although this annotation is not currently displayed in the \u201cDescription\u201d section of the details panel when the ERBB2 homodimer is selected in the pathway diagram). In this example, although all the participating input entities are genetically normal, the process is not: disease-associated amplification of the ERBB2 gene leads to protein overexpression. This allows the receptor tyrosine kinase to homodimerize in the absence of ligand, something that does not occur under normal conditions (the corresponding complex under normal conditions would be a heterodimer consisting of a monomer of ERBB2 and a ligand-activated monomeric member of the ERBB family (EGFR, ERBB3, or ERBB4)). For this reason, the ERBB2 homodimer is a disease entity, is marked with a disease tag and is colored red.\nThis reaction represents a gain-of-function or novel reaction, in that the protein is carrying out a new role that is not seen in the normal pathway. Infectious disease events are another example of gain-of-function events, as by definition the presence of an infectious agent is a new, disease-associated attribute.Although gain-of-function disease reactions are displayed in the context of the normal, grayed-out pathway (and may make use of other components of the normal pathway, as in this case), the actual reaction is generally laid out separately in its own space in the pathway diagram. This is in contrast to loss-of-function disease reactions, described below in point 9, which are overlaid directly on top of the corresponding normal reactions. Exceptions to this \u201cseparate space for gain-of-function reactions\u201d rule are explained below in point 8.\nDisease reactions display a new field in the \u201cDescription\u201d tab of the details panel relative to normal events: \u201cFunctional Status\u201d. This expandable field identifies the disease physical entity, as well as the underlying genetic structural variation and the functional outcome of that variation, with terms pulled from Sequence types and features ontology (SO; Eilbeck et\u00a0al., 2005[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0006]). This is shown in Figure 14[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0014].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f6ec7224-cff0-4da3-9427-50b7584bb22b/cpz1722-fig-0013-m.jpg</p>\nFigure 13\nA gain-of-function reaction in the \u201cConstitutive Signaling by Overexpressed ERBB2\u201d pathway.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/3467689f-02cc-4629-96fc-2c9ec6d97189/cpz1722-fig-0014-m.jpg</p>\nFigure 14\nThe details panel identifies the \u201cFunctional Status\u201d of the disease entity (here, p-ERBB2 homodimer), describing the underlying genetic changes that result in abnormal molecular behavior and disease outcomes.\n6. Click on the next reaction in the \u201cConstitutive Signaling by Overexpressed ERBB2\u201d, \u201cTrans-autophosphorylation of ERBB2 homodimer\u201d.This reaction shows the post-translational modification of the receptor through autophosphorylation, and is highlighted here not to illustrate a disease-specific reaction element, but rather as a demonstration of the ability of Reactome to capture detailed post-translational modifications. Scroll down in the \u201cDescription\u201d tab of the details panel for this reaction and unfurl the field for the output, \u201cp-ERBB2 homodimer\u201d. Notice that this complex consists of two copies of the phosphorylated ERBB2 monomer. Phosphorylations are listed under the \u201cPost-translational modification\u201d field, which includes the coordinate(s) of amino acid residues from the reference sequence that are modified and details of the specific modifications with terms taken from PSI MOD (see Fig. 15[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0015]). Notice also that this reaction has two negative regulators, the complexes \u201cERBB2:TKIs:ERBIN:HSP90:CDC37\u201d and \u201cERBB2:trastuzumab:ERBIN:HSP90:CDC37\u201d. These therapeutic-containing complexes are appropriately shaded in purple to reflect the inclusion of drug(s) and will be described in more detail in step 7 of this protocol.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d11d7874-bbbd-4c3a-a255-255a766bfe6d/cpz1722-fig-0015-m.jpg</p>\nFigure 15\nThe details panel captures precise information about post-translational modifications to pathway entities.\n7. In the \u201cConstitutive Signaling by Overexpressed ERBB2\u201d, click on the reaction \u201cERBB2 binds trastuzumab\u201d. Scroll down in the \u201cDescription\u201d tab of the details panel and expand the field for the \u201ctrastuzumab\u201d reaction input.\nReactome annotates therapeutics in three classes - protein, small molecule, and RNA drugs. Drugs are cross-referenced to Guide to Pharmacology, ChEBI and/or PubChem where possible and applicable, and this information is displayed in the \u201cDescription\u201d tab of the Details panel as shown in Figure 16[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0016]. Similar information can also be viewed by clicking on the red circle icon beside the trastuzumab name.The effect of the drug on the disease pathway is annotated where possible. In this pathway, the complex formed by the binding of the monoclonal antibody Trastuzumab to the ERBB2-chaperone complex inhibits the ability of the ERBB2 homodimer to autophosphorylate, thus preventing signaling downstream of the abnormally activated receptor. In the case of the \u201cERBB2 binds TKIs\u201d reaction, Reactome shows the binding of a set of small molecule tyrosine kinase inhibitors (TKIs) to the ERBB2:chaperone complex; this binding inhibits the tyrosine kinase activity of the receptor, similarly preventing autophosphorylation and downstream signaling.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e6435678-e45e-4b35-99cd-347524881719/cpz1722-fig-0016-m.jpg</p>\nFigure 16\nReactome captures the effect of therapeutics on pathway events where possible and links the therapeutics to appropriate external resources in the details panel.\n8. Reactome captures detailed molecular information about individual proteins that are implicated in abnormal biochemical reactions in disease. To explore this, unfurl the second pathway of \u201cSignaling by ERBB2 in Cancer\u201d, \u201cSignaling by ERBB2 KD Mutants\u201d, and click on the first reaction \u201cERBB2 KD mutants heterodimerize\u201d.\nThis pathway describes ERBB2 proteins with mutations in the kinase domain (KD) that increase the catalytic activity of the enzyme, resulting in elevated autophosphorylation and downstream signaling. In this case, the mutant proteins are performing the same biochemical role as their normal counterparts but at an elevated rate or efficiency. In cases like this, the disease reactions are overlaid directly on top of the corresponding normal ones in the diagram, unlike in the case of novel functions described in step 5, above, for overexpressed ERBB2.To see the details of the ERBB2 mutants annotated in this reaction, click on the complex \u201cERBB2 KD mutants:ERBIN:HSP90:CDC37\u201d in the pathway diagram. In the \u201cDescription\u201d tab of the details panel, select the set \u201cERBB2 KD mutants\u201d to reveal the list of member and candidate proteins (Fig. 17A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0017]). Expand the field for the first member, \u201cERBB2 L775P\u201d with the \u2018+\u2019 on the right. This reveals detailed information about the protein including the genetic alteration (here L-leucine 755 replaced with L-proline) (See Fig. 17B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0017]). Clicking on the green circle beside the \u201cERBB2 L755P\u201d name (indicated with the red square in Fig. 17B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0017]) reveals further information including linkouts to COSMIC (or OMIM, or ClinGen, where appropriate), as well as the literature reference specific for that mutant (scroll down to bottom of the panel to see this) (Fig. 17C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0017]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/c37f0b4f-44ac-4a04-8535-26f891a6e529/cpz1722-fig-0017-m.jpg</p>\nFigure 17\n(A) A list of the members and candidates of the \u201cERBB2 KD mutants\u201d set is revealed by clicking on the \u201c+\u201d symbol on the right side of the details panel after the disease complex is selected in the pathway diagram. (B) Detailed molecular information about the genetic changes that give rise to mutant ERBB2 L755P is displayed by clicking on the \u201c+\u201d symbol to the right of the variant name in the details panel Linkouts to appropriate databases and ontologies are provided. (C) Further cross-references are available by clicking on the green circle to the left of the disease entity name.9. Reactome also annotates loss-of-function events, where a protein has lost all or most of its normal functional activity. To explore this, unfurl the third subpathway of \u201cSignaling by ERBB2 in Cancer\u201d, \u201cDrug Resistance in ERBB2 KD mutants\u201d. This reveals a further 8 subpathways, each describing the resistance of sets of ERBB2 KD mutants to 8 different drugs. Select the first pathway, \u201cResistance of ERBB2 mutants to trastuzumab\u201d and unfurl that pathway to reveal the single \u201cfailed reaction\u201d, \u201cResistant ERBB2 KD mutants do not bind trastuzumab\u201d (Fig. 18[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0018]).\nIn a loss-of-function event, the relevant disease entity (here the complex \u201cERBB2 KD mutants (trastuzumab resistant):ERBIN:HSP90:CDC37\u201d) is outlined with a dashed red line, and products that are no longer made are greyed out with a superimposed red \u201cX\u201d. These loss-of-function events represent \u201cstop points\u201d in the pathway and are overlaid directly on top of the corresponding normal event in the pathway diagram.\nThe \u201cDescription\u201d tab of the details panel provides \u201cFunctional Status\u201d information for the disease entity and unfurling the complex to reveal the member and candidate proteins will provide access to detailed mutational and linkout information as described above for gain-of-function mutants.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a1672452-4e7d-42df-b21c-a179344201ee/cpz1722-fig-0018-m.jpg</p>\nFigure 18\nLoss-of-function reaction \u201cResistant ERBB2 KD mutants do no bind trastuzumab\u201d is shown with the loss-of-function entity bordered by a broken red line and the output of the failed reaction crossed out.\n10. Infectious processes are, by definition, novel events that do not occur in the absence of the initiating pathogen. As such, these are represented in their own diagrams with no corresponding normal pathway. In addition to annotating the infection process itself, Reactome also shows how these infectious agents modulate normal biological processes.To explore this, unfurl the \u201cInfectious disease\u201d child of \u201cDiseases\u201d, then continue to unfurl \u201cSARS-CoV Infections\u201d, \u201cSARS-CoV-2 Infection\u201d, and \u201cSARS-CoV-2-host interactions\u201d. This reveals an ELV pathway diagram displaying five subpathways. Expand the subpathway \u201cSARS-CoV-2 activates/modulates innate and adaptive immune responses\u201d and click on the reaction \u201cSARS-CoV-2 8 binds class I MHC\u201d, shown in Figure 19A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0019].\nThis reaction shows the first of two steps in the formation of a host-CoV-2 complex that has been demonstrated to play a role in immune evasion. In the next step, the output of this reaction is bound by the Beclin-1 complex and the final complex, \u201c8:class I MHC:BECN1 complex\u201d, is shown negatively regulating the normal human reaction \u201cCapturing cargo and formation of prebudding complex\u201d. This cargo capture reaction is part of the normal \u201cClass I MHC mediated antigen processing and presentation\u201d pathway, as indicated by the flow arrow from the normal reaction to the green-bordered interactive pathway icon seen in Figure 19A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0019]. Users can navigate between the normal and disease diagrams by clicking on the encapsulated pathway icon to open the corresponding pathway diagram. In the normal human pathway diagram, a reciprocal view is shown (Fig. 19B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0019]): here, the output of the two-step human-CoV-2 binding reactions is shown negatively regulating the normal reaction in the context of its normal biological pathway and the chimeric complex is associated with the pathway icon for \u201cSARS-CoV-2-host interactions\u201d. Note that pathway connections of this kind are not limited to normal:disease diagrams, but are used throughout Reactome to indicate bridges between any biological processes that are depicted in different pathway diagrams.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/71675f76-4090-404d-a9fb-57f29f3891d6/cpz1722-fig-0019-m.jpg</p>\nFigure 19(A) Infectious diseases and processes are novel events with no normal counterparts. They are laid out in their own diagrams and are shown interacting with and modulating the function of normal entities and events, as shown here for the SARS-CoV-2 subpathway \u201cSARS-CoV-2 host interactions\u201d. (B) Encapsulated pathway icons like \u201cSARS-CoV-2 host interactions\u201d shown here provide connections between pathways that share entities or events.\n11. Reactome supplements its manual disease annotation with an overlay feature that makes use of data from DisGeNet, a public database of associations between human genes or variants and disease (Pi\u00f1ero et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0018]). This overlay is similar to the protein interactor overlay from IntAct described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0001], Step 10 and shown in Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0009].\nTo explore this feature within the \u201cSARS-CoV-2 host interactions\u201d pathway diagram, open the interactive panel on the right side of the visualization panel, select the middle tab with the data overlay options, and select \u201cDisGeNet\u201d from the available data sources.\nProtein icons in the pathway are now decorated in the upper right with a red circle indicating the number of diseases from DisGeNet that are associated with that protein.\n12. Navigate to the reaction \u201cSARS-CoV-2 M protein bind MAP1LC3B\u201d under the \u201cSARS-CoV-2 modulates autophagy\u201d subpathway and click on the red circle on the human protein input MAP1LC3B to reveal the six associated diseases, as shown in Figure 20[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0020].\nNote that the confidence levels may need to be adjusted on the slider bar to reveal all the associated diseases. Clicking on any of the disease icons takes the user to the corresponding record in DisGeNet.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/08c296b9-d2d5-4d3e-af35-538b9181f563/cpz1722-fig-0020-m.jpg</p>\nFigure 20\nPathway diagrams can be overlaid with disease associations taken from DisGeNet, as shown here for the protein MAP1LC3B.This protocol will describe how to identify pathways and reactions that involve a gene or protein of interest. For the purposes of illustration, the cyclin-dependent kinase 7 gene will be used, which has the following identifiers:\n         \ntable:\n\ufeff0,1\nProtein product:,Common name: Cdk7\n,UniProtKB (SwissProt): P50613 (CDK7_HUMAN)\nGene:,HGNC: 1778 Entrez Gene: 1022\n,GenBank: NM_001799\n,Ensembl: ENSG00000134058.\nSee Alternate Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0004] to search by a database accession number rather than by a common name.\nNecessary Resources\nHardware\nComputer capable of supporting a Web browser and an Internet connection\nSoftware\nAny modern Web browser such as Firefox, Safari, and Chrome will work to display Reactome Web pages\n1. Point the browser to the Reactome home page at https://reactome.org[href=https://reactome.org].\n2. On the home page, in the search bar near the top of the page, click the text box, type CDK7, then press the \u201cGo!\u201d button. After a few seconds, you will be presented with a results page similar to Figure 21[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0021].\nSearch results are organized based upon record type, i.e., Protein, Reaction, Pathway; users can filter search results according to the criteria listed at the left of the panel (species, types, compartment, reaction types) and also elect to display search results grouped by type (default) or not. The CDK7 search reveals two \u201cProtein\u201d, ten \u201cReaction\u201d, and three \u201cPathway\u201d results. Note that when using a text string, the search results find matches for that text in any Reactome data model field. For instance, the second \u201cProtein\u201d hit is MNAT1, which has \u201cCDK7\u201d as part of one of its identified synonyms; similarly, identified reactions or pathways may have the text string \u201cCDK7\u201d in the summation.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/9495ee8d-db00-4e2e-ac6b-218e814b621a/cpz1722-fig-0021-m.jpg</p>\nFigure 21\nThe results page from a simple CDK7 search on the Reactome home page are shown here.3. Click on the protein \u201cCDK7\u201d hit from the search results to reveal the page shown in Figure 22[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0022].\nThis page describes everything that Reactome knows about this protein. In addition to extensive cross-referencing to external resources, the page identifies all the \u201cLocations in the Pathway Browser\u201d where CDK7 is found in Reactome. Expanding each of the top-level pathway hits will pinpoint exactly where CDK7 is found in each and clicking on any of the hits will move the user to the relevant pathway diagram. The CDK7 search page also details all the ways that the protein participates in events in Reactome, identifying its role \u201cas an input\u201d or \u201cas a component of\u201d, in this case.\nClicking on a reaction or pathway hit from the initial CDK7 search reveals a similar summary page for that event, including an interactive reaction or pathway diagram, literature references and editorial attributes in addition to the features shown for an entity described above.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/9669a7cb-5d3b-4f3b-94b1-299383ec9097/cpz1722-fig-0022-m.jpg</p>\nFigure 22\nThe CDK7 reference entity page.Instead of searching for a gene or protein using its common name, as described in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0003], one may wish to use the accession number by which it is known in UniProtKB (SwissProt), GenBank, Ensembl, Entrez, or HGNC. The steps for doing so, using a UniProtKB (SwissProt) accession number, are presented here. The same procedure works for GenBank, Ensembl, Entrez or HGNC identifiers. Note that searching with an identifier rather than a gene name provides more targeted information about the protein of interest but does not identify locations in Reactome where the protein is mentioned in summations or synonyms, as described above in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0003].\nNecessary Resources\nHardware\nComputer capable of supporting a Web browser and an Internet connection\nSoftware\nAny modern Web browser such as Firefox, Safari, and Chrome will work to display Reactome Web pages\n1. Point the browser to the Reactome home page at https://reactome.org[href=https://reactome.org].\n2. On the home page, in the search bar near the top of the page, click the text box, type P50613, then press the \u201cGo!\u201d button.\nThis brings up the search results page listing the CDK7 protein as the single hit.\n3. Clicking on the CDK7 search result loads the reference entity page as shown in Figure 21[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0021].\nFrom here, it is possible to navigate to the pathways and reactions in which Cdk7 takes part, and to view the complexes that contain Cdk7.The simple searches shown in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0003] and Alternate Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0004] will suffice for many situations. However, the default search casts a very wide net and may return more hits than one wants. If this is the case, one may wish to use the Advanced Search, which gives much finer control over the search.\nNecessary Resources\nHardware\nComputer capable of supporting a Web browser and an Internet connection\nSoftware\nAny modern Web browser such as Firefox, Safari, and Chrome will work to display Reactome Web pages\n1. Point the browser to the Reactome home page at https://reactome.org[href=https://reactome.org].\n2. On the home page under the \u201cTools\u201d in the Navigation bar, select \u201cAdvanced Data Search.\u201d\nThe advanced search method permits Boolean-based queries of the Reactome data set. Combining desired search terms with the appropriate Boolean operators \u201cAND\u201d, \u201cOR\u201d, and \u201cNOT\u201d in combination with quotation marks (for exact term searches), brackets (for grouping terms), the single and multiple wildcard operators \u201c?\u201d and \u201c*\u201d, respectively, and \u201c+\u201d (for \u201cmust contain\u201d) or \u201c\u2212\u201d (for \u201cmust not contain\u201d) allows users to create precise searches.\nA sample search is preloaded into the search field, querying for Reactome data that contains either the terms \u201craf\u201d and \u201cmap\u201d together, or the term \u201capoptosis\u201d, or records whose name contains the exact phrase \u201cPTEN S170N\u201d or whose standard IDs are \u201cREACT_1258.1\u201d or \u201cR-HSA-198344,1\u201d. This search combination returns 1166 hits when the \u201cSEARCH!\u201d button at the bottom of the page is clicked. Users can also restrict the Boolean search by selecting various criteria from the Filtering Parameters below the search box.The Pathway Analysis tool allows one to analyze lists of genes, proteins or small molecules by providing services for ID mapping and pathway assignment and overrepresentation analysis. It is a powerful exploratory tool that is linked to the Reactome Pathway Browser. To illustrate how it works, this protocol will describe the analysis of a list of UniProtKB identifiers to identify enriched Reactome pathways.\nNecessary Resources\nHardware\nComputer capable of supporting a Web browser and an Internet connection\nSoftware\nAny modern Web browser such as Firefox, Safari, and Chrome will work to display Reactome Web pages\n1. Point the browser to the Reactome home page at https://reactome.org[href=https://reactome.org].\n2. Click on the \u201cAnalysis Tools\u201d button on the home page; alternately, select \u201cAnalyse gene list\u201d from under the \u201cTools\u201d dropdown menu in the home page header.\nThis will open a submission form where users can upload sample data sets or user supplied data into any of the Reactome analysis tools listed at the left. \u201cAnalyse gene list\u201d is selected by default; other tool options (\u201cAnalyse gene expression\u201d, \u201cSpecies Comparison\u201d, and \u201cTissue Distribution\u201d) are described in later protocols. Below the tool buttons on the left of the analysis window is a \u201cclick to learn more about our analysis tools\u201d option. This takes users to a detailed description of the tools, including some \u201cgetting started\u201d tutorials.\n3. Select the UniProtKB accession list sample data from the panel at the right of the analysis window and click the \u201cContinue\u201d button.The \u201cAnalyse Gene list\u201d tool maps a list of identifiers (generally, UniProtKB IDs for proteins, ChEBI IDs for small molecules, and either HGNC gene symbols or Ensembl IDs for DNA/RNA molecules, although other IDs are also supported) to the Reactome pathways that contain them, and overrepresentation and pathway topology analyses are performed. Overrepresentation analysis indicates the probability that the data set contains more of the participants of a given pathway than would be expected by chance, while the topology analysis highlights any reaction whose participants include at least one match to a molecule from the submitted data set.\n4. This moves the analysis panel to the \u201cOptions\u201d step, where \u201cproject to human\u201d is checked by default. In this mode, any non-human identifiers are converted by the analysis service to their human equivalents. The second option \u201cinclude interactors\u201d is by default left unchecked; if this option is selected, the analysis will include protein-protein interactions from the IntAct database for all proteins in all pathways, increasing the potential coverage with the query.\nLeaving these options set at their default values, click \u201cAnalyse!\u201d to reveal the overrepresentation analysis overlaid on the Pathway Overview diagram, as shown in Figure 23[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0023].\nNote that the color scheme for the analysis may be customized by clicking on the artist's palette on the pop-out Settings panel at the right. The overview may also be viewed in the space-filling Voronoi diagram by clicking on the right-most icon at the top left of the visualization panel.A pathway is considered \u201coverrepresented\u201d or \u201cenriched\u201d if the submitted data set has more participants from that pathway than would be expected by chance. The overrepresentation analysis calculates a probability score for each pathway, corrected for false discovery rate by the Benjamini-Hochberg method (Benjamini & Hochberg, 1995[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0003]), and colors the pathways according to the scale shown on the right in the visualization panel. Analysis results are shown in the \u201cAnalysis\u201d tab in the Details panel. All Reactome pathways are shown, in groups of 20, ranked by the p-value obtained from the overrepresentation analysis. Visualization may also be set to represent pathway coverage in the query set, by adjusting the toggle in the bottom of the visualization panel.\nThe details panel lists the results by pathway for: \u201cEntities found\u201d (number of submitted entities found: clicking on the number in this column will open a new window displaying the identifiers found in that pathway, including mapping to isoform-specific versions where applicable); \u201cEntities total\u201d (total number of pathway entities); \u201cEntities Ratio\u201d (representing the fraction of entities in the pathway relative to Reactome as a whole); \u201cEntities p-value\u201d (probability score as described above); \u201cEntities FDR\u201d (p-value adjusted for multiple comparisons based on the Benjamini-Hochberg procedure); \u201cReactions found\u201d (the number of reactions in the pathway containing at least one entity from the data set); \u201cReactions total\u201d (total number of reactions in the pathway); \u201cReactions ratio\u201d (fraction of pathway reactions relative to Reactome as a whole); and Species name.In the pathway hierarchy panel, pathway names are labeled with the number of molecules from the data set found in that pathway as a fraction of the pathway total, and FDR values are added to the right side of the pathway names. Results from the Topology analysis are also overlaid onto the hierarchy - any reaction in the hierarchy that contains as a participant at least one identifier from the data set is boxed in orange.\nIdentifiers from the query set that were not found in a Reactome pathway are listed under the \u201cNot found\u201d tab to the left of the ranked pathways list. This list, as well as the analysis results can be downloaded by selecting the files of choice from the \u201cDownloads\u201d tab to the left of the ranked pathway list. Analysis results are temporarily stored on the Reactome server. The storage period depends on usage of the service but is at least 7 days. Stored results are available via the token assigned to the results file when it is created and displayed in the URL for the results report. The token can be shared and allows later access through the API.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/9846571b-ddfa-4705-8503-f5fecfb88cc9/cpz1722-fig-0023-m.jpg</p>\nFigure 23\nThe Pathway Overview showing results of an overrepresentation analysis using the \u201cAnalyse Gene list\u201d tool with a list of UniProtKB identifiers.\n5. Results can be filtered to allow users to customize results based on resource (in cases where the data set contains IDs from multiple resources\u2014in this case, this filter is not relevant because all the IDs in the submitted data set are from UniProtKB). Results can also be filtered on the basis of pathway size, species, p-value, and to include or exclude disease pathways.Make use of this feature to hide disease pathways from the results, as follows: click on the funnel displayed at the top right of the ranked pathways list in the \u201cAnalysis\u201d tab of the Details panel, unchecked the default option \u201cInclude disease pathway in the results\u201d, and click apply. The filter can be removed again by clicking on the \u201cx\u201d at the bottom right of the ranked pathway list in the details panel.\n6. The analysis view provides an overview of all the Reactome pathways at once. To see the details of a specific pathway, double click on the node representing the pathway in the ranked list or in the hierarchy. To see this, click on the top pathway, \u201cSignaling by Receptor Tyrosine Kinases\u201d.\nThis opens the EHLD for the pathway as shown in Figure 24[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0024]. The yellow band in the subpathway name boxes indicates the proportion of the pathway that is represented in the query data set, while the gray bar above the label indicates the number of pathway entities that are represented in the submitted data, the total number of entities in the pathway and the FDR corrected probability score.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f99bae8a-dc39-4ed3-ae04-12b6c6e6e97b/cpz1722-fig-0024-m.jpg</p>\nFigure 24\nResults of an overrepresentation analysis overlaid on the interactive textbook-style illustration for the \u201cSignaling by Receptor Tyrosine Kinases\u201d pathway.\n7. Click on the subpathway \u201cSignaling by EGFR\u201d to reveal the reaction-level diagram.In this view, entities that are part of the query data set, such as the protein EGFR, are re-colored. Similarly, complexes, sets, and subpathway icons are colored to represent the fraction of their components that are represented in the submitted data set. If interactors were included in the analysis, or if the interactor icon on the top right of an entity in the pathway diagram is selected, interactors are also colored to reflect their status in the submitted data, as shown in Figure 25[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0025] for the EGF interactors MGST1 and EGFR.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/511d98ed-20be-43df-9842-89ae0bece498/cpz1722-fig-0025-m.jpg</p>\nFigure 25\nOverrepresentation analysis displayed at the entity level for the reactions in the \u201cSignaling by EGFR\u201d pathway. Interactors for EGF are displayed.There are two ways to analyze gene expression data in Reactome. The first method, described in this section, uses an appropriately formatted data set uploaded into the \u2018Analyze Gene List\u2019 tool described in Protocol 3 above for Overrepresentation analysis.\nThe second way to analyze gene expression data in Reactome makes use of the Reactome Gene Set Analysis (Reactome GSA) tool, accessed through the \u201cAnalyze Gene Expression\u201d button after \u201cAnalysis\u201d is selected from the home page. Reactome GSA performs quantitative pathway analyses, increasing the statistical power of the differential gene expression analysis. This tool is out of scope for this tutorial, but is described in detail in the corresponding publication (Griss et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-bib-0010]) and on the Reactome Web site under \u201cDocs\u201d, \u201cUser guide\u201d, \u201cAnalysis Tools\u201d, \u201cAnalysis Gene Expression\u201d).\nNecessary Resources\nHardware\nComputer capable of supporting a Web browser and an Internet connection\nSoftware\nAny modern Web browser such as Firefox, Safari, and Chrome will work to display Reactome Web pages\n1. Point the browser to the Reactome home page at https://reactome.org[href=https://reactome.org].\n2. Click on the \u201cAnalysis Tools\u201d button on the home page; alternately, select \u201cAnalyse gene list\u201d from under the \u201cTools\u201d dropdown menu in the home page header to open the submission form as described in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0003].\n3. Ensure the \u201cAnalyse gene list\u201d tool is selected (this is the default tool) and click on the sample data set \u201cMicroarray data\u201d from the panel at the right.Expression data sets are distinguished from overrepresentation data sets by virtue of having multiple columns. The first column must contain the identifiers for the protein, small molecule or other entity, as described in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0003] for Overrepresentation analysis, and the first entry in column 1 must start with the \u2018#\u2019 symbol. The remaining columns of the data set must contain only numeric values, with no alphabetical characters. These columns can represent any data that can be inputted as numerical entries: time-course microarray expression, as in this data set, but also fold change, abundance, or statistical value. Examples include quantitative proteomics, GWAS scores, numbers of somatic or germline mutations (as in the Cancer Gene Census sample data set) or tissue-specific expression (as in the final sample data set from HPA), among others. Headers for the numerical data columns are supported but not required. For user uploaded data sets, the data should be formatted as a tab-delimited file, where the first column contains the identifiers and subsequent columns contain the numerical values.\n4. Click \u201cContinue\u201d after selecting the Microarray data set, and click \u201cAnalyse!\u201d from the Options panel, keeping the default settings of \u201cProject to human\u201d checked and \u201cInclude Interactors\u201d unchecked.\nThis reveals the results of the analysis, beginning as an overlay on the Pathway Overview display. As above, the user can customize the color selection with the artist palette on the pop-out Selection panel at the right, and view the results as a Voronoi diagram.\nPathways are re-colored according to the numeric values submitted in the test data. The results of the analysis are very similar to those described above for Overexpression analysis, with the following additions:- The details panel has additional columns to the right of those described for Overrepresentation, above. These columns contain the submitted expression values or other numerical data.\n- The \u201cOverrepresentation/pathway coverage\u201d toggle in the bottom of the visualization panel has been replaced by a control panel allowing the user to step forward or backward through the columns of data; alternately, the play button may be selected and the series will be shown automatically. The color overlay on the Pathway Overview is adjusted accordingly.\n5. Select the top hit from the details panel \u201cFormation of the HIV-1 Early Elongation Complex\u201d to open the reaction-level diagram.\nEntities are colored according to their expression (or other numeric) values from the submitted data, and numeric data can be displayed for any affected entity by clicking on the small arrowhead at the right side of the entity icon in the pathway diagram. This opens the contextual information panel (CIP), with molecules, pathways, and interactors tabs; the numeric analysis data is presented in the molecules tab, as shown for a complex in Figure 26[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0026]. CIPs can be pinned to the diagram by clicking on the pin icon at the top, and multiple CIPs can be opened and pinned at the same time.As for \u201cOverrepresentation\u201d analysis results, above, complexes and sets are overlaid with a bar of color that represents the fraction of their components that have expression data in the query set. At a coarse-grained focus, this appears as a single block of color representing the average expression of all the components. Zooming in on a complex or set reveals individual bars for each component with expression data, as shown in Figure 26[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0026]. Changes to expression across the time course can be visualized using the forward/backward arrows or the play button at the bottom of the visualization panel; within each complex or set, the bar representing a given component remains in the same position relative to the others. When an entity is selected in the diagram, its icon is outlined in blue as usual (as for the \u201cRNA Pol II (hypophosphorylated complex bound to DSIF protein\u201d complex in the middle right of Fig. 26[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0026]), and the expression levels of its components are indicated with arrowheads on the right side of the color scale bar at the right of the visualization panel. If an entity in the diagram is hovered over, its icon is shaded in yellow [as for the \u201cRNA Pol II (hypophosphorylated):capped pre-mRNA complex\u201d at the upper middle in Fig. 26[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0026]] and the expression values of its components are indicated with arrowheads on the left side of the color scale bar.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/26dc5529-0669-4819-98c0-3b562ede5709/cpz1722-fig-0026-m.jpg</p>\nFigure 26\nResults from a gene expression analysis using the \u201cAnalyse Gene list\u201d tool are overlaid on entities in a reaction from the \u201cHIV Infection\u201d pathway.The comparative analysis of pathways and biological processes offers important information on their evolution and supports metabolic engineering and the study of human disease. Reactome uses manually curated human pathways to electronically infer equivalent events in 15 other species. The Species Comparison tool allows users to compare the predicted model organism pathways with human ones to find pathways conserved (or not) between both species.\nNecessary Resources\nHardware\nComputer capable of supporting a Web browser and an Internet connection\nSoftware\nAny modern Web browser such as Firefox, Safari, and Chrome will work to display Reactome Web pages\n1. Point the browser to the Reactome home page at https://reactome.org[href=https://reactome.org].\n2. Click on the \u201cAnalysis Tools\u201d button on the home page and click on the third tool \u201cSpecies Comparison\u201d to launch the data selection page for the Species Comparison Analysis. Alternatively, select \u201cSpecies Comparison\u201d from under the \u201cTools\u201d dropdown menu in the home page header and click on the \u201cSpecies Comparison\u201d button to the left of the analysis window (note that the \u201cAnalyse gene list\u201d tool is selected by default).\n3. On the \u201cSpecies Comparison\u201d page is a selection tool that reveals a drop-down list of species. Select species \u201cMus musculus\u201d from the drop-down menu and click the \u201cGo!\u201d button to reveal the Reactome-wide pathway conservation data.The \u201cSpecies Comparison\u201d results are overlaid on the Pathway Overview with pathways colored by the p-value for entities conserved (as determined by the protocol for computationally inferred events, described here), or by % coverage, depending on which metric is chosen from the dropdown at the bottom of the visualization panel. The color of entities for which no inference was made is left unchanged. Unlike the other tools, the \u201cSpecies Comparison\u201d analysis infers only on the basis of protein entries with UniProtKB IDs. Small molecules, DNA and other entities that do not have a UniProtKB ID are not considered and are not colored in the analysis.\nThe details panel lists the results of the analysis by pathway for: \u201cEntities found\u201d (the number of mouse proteins inferred for that pathway - clicking on the number in this column opens a mapping file that identifies the mouse proteins in that pathway by UniProtKB ID and provides the corresponding human UniProtKB); \u201cEntities total\u201d(the number of human proteins in that pathway); \u201cEntities ratio\u201d (the ratio of proteins from the selected species (here, mouse) in that pathway as a proportion of the total mouse proteins inferred across Reactome as whole); \u201cEntities p-value\u201d (the probability score for the pathway); \u201cEntities FDR\u201d (p-value adjusted for multiple comparisons based on the Benjamini-Hochberg procedure);\u201cReactions found\u201d(the number of reactions in that pathway containing at least one inferred protein); \u201cReactions total\u201d (the total number of human reactions in that pathway); \u201cReactions ratio\u201d (the number of mouse reactions from that pathway as a fraction of the inferred mouse reactions across Reactome as a whole); and \u201cSpecies name\u201d.4. Click on \u201cCircadian Clock\u201d in the pathway hierarchy to open the reaction-level diagram. Entities are colored according to their conservation in mouse as described above. Clicking on the small arrowhead at the right of the icon for a set or complex reveals the Contextual Information Panel (CIP), which provides inference detail on each of the components of the entity, as shown in Figure 27[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0027].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/52a4d853-8a8f-497d-a25c-9ce2968368da/cpz1722-fig-0027-m.jpg</p>\nFigure 27\nResults from the Species Comparison tool, showing conservation of entities and events from the \u201cCircadian Clock\u201d pathway in mouse.Currently, reactions in Reactome represent events that occur within a generic human cell. To facilitate analysis of tissue specific expression, protein expression data has been imported from Human Protein Atlas for overlay on Reactome data. The HPA data reflects the expression of the protein-coding genes in 44 different human tissues and can be visualized through the \u201cTissue Distribution\u201d analysis tools.\nNecessary Resources\nHardware\nComputer capable of supporting a Web browser and an Internet connection\nSoftware\nAny modern Web browser such as Firefox, Safari, and Chrome will work to display Reactome Web pages\n1. Point the browser to the Reactome home page at https://reactome.org[href=https://reactome.org].\n2. Click on the \u201cAnalysis Tools\u201d button on the home page and click on the fourth tool \u201cTissue Distribution\u201d to launch the data selection page for the analysis. Alternatively, select \u201cTissue Distribution\u201d from under the \u201cTools\u201d dropdown menu in the home page header and click on the \u201cTissue Distribution\u201d button to the left of the analysis window (note that the \u201cAnalyse gene list\u201d tool is selected by default).\nThis will reveal the window shown in Figure 28[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0028]. There is currently one experimental data set available from the dropdown list HPA (E-PROT-3). Below the dropdown is an interactive table listing the 44 tissue and cell types from HPA in the left panel. Users can select any of the available tissues and cells by clicking on them in the left panel and then clicking the \u201cadd\u201d button from the middle panel. This will duplicate the tissue or cell name in the panel on the right side of the window. Users can also choose to \u201cAdd all\u201d, \u201cRemove\u201d, or \u201cRemove all\u201d to tailor the list to their satisfaction. Once the appropriate tissues are selected, the analysis is initiated by pressing the \u201cGo!\u201d button.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/cb0047d4-5027-4e53-bd51-3494f14c26c5/cpz1722-fig-0028-m.jpg</p>\nFigure 28View of the data selection panel in the Tissue Distribution tool.\n3. For this protocol, select all the tissues using the \u201cAdd all\u201d button, and then click \u201cGo!\u201d.\nThis will generate the analysis results page as shown in Figure 29[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0029], beginning at the Pathway Overview. Pathways are colored according to numeric values submitted in the query data set, as indicated on the scale bar at the right of the visualization panel. The standard details panel is supplemented with additional columns to the right listing the selected tissues and the average value for the submitted identifiers in that pathway for that tissue. Users can view the results for each tissue or cell type by clicking on the forward and backward arrows on the Experiment Browser toolbar at the bottom of the visualization panel.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/749afbdb-b01c-4ee5-a152-c38ea87fd07e/cpz1722-fig-0029-m.jpg</p>\nFigure 29\nPathway Overview display of the Tissue Distribution analysis results.\n4. Click on \u201cmRNA Splicing - Major Pathway\u201d to open the reaction-level diagram.\nObjects in the Pathway Diagram are re-colored according to the numeric values from the data set, shown on the scale at the right side of the visualization panel. Entities that are not represented in the input data are not re-colored. Clicking the forward and backward arrows or the play button in the Experiment Browser toolbar allows users to step through the results for each of the selected tissues and cells.\nAs for the \u201cGene Expression\u201d analysis described in Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-prot-0006], complexes and sets are overlaid with colored bars corresponding to the proportion of their components represented in the data set, as seen in Figure 30[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.722#cpz1722-fig-0030]. At a coarse-grained zoom, the color represents the average of all the components of that set or complex; zooming in reveals individual bars for each constituent represented in the data set.Details of the components of a complex or set can be visualized by clicking on the small arrowhead at the right of the entity icon to open the Contextual Information Panel (CIP). Multiple CIPs can be opened and pinned so they remain visible when other entities are selected.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/fb31a143-23f2-4070-a85b-e455135cc5ca/cpz1722-fig-0030-m.jpg</p>\nFigure 30\nResults of a Tissue Distribution analysis displayed at the entity and reaction level on the pathway \u201cProcessing of Capped Intron-containing Pre-mRNA\u201d.", "Step-by-step method details\nStep-by-step method details\nSpinal cord dissection\nTiming: 2\u20133\u00a0h for a litter of embryos\nDuring these steps, spinal cords are microdissected from mouse embryos collected from pregnant female mice. Specific care is taken to remove the meninges from spinal tissues. A tail snip from each embryo is also taken for genotyping.\nEuthanize the pregnant dam on embryonic day (E) 12 following your institutionally approved method.\nRemove embryos.\nSpray the mouse abdomen with 70% ethanol until hair is matted down.\nUsing scissors, make an incision along the abdominal midline, and remove the uterine horns.\nTransfer uterine horns to a 10\u00a0cm dish and rinse twice with ice cold PBS. Then transfer to a new 10\u00a0cm dish with PBS on ice.\nUsing dissection scissors and forceps, remove the E12 embryos from the uterus and transfer them into a new 10 cm dish with PBS on ice.\nNote: PBS may become cloudy with blood or other tissue debris. Prepare three 10\u00a0cm dishes with PBS per mouse. Transfer the uterine horns to a new dish as needed to clearly visualize and cleanly dissect the embryos.\nSelect the embryos desired for spinal dissection (15\u00a0min).\nVisualize the embryos under a fluorescent stereoscope and identify embryos that have fluorescent signals (Figures 1[href=https://www.wicell.org#fig1]B\u20131D).\nSave one Hb9::gfp negative embryo as a negative control sample to set the FACS gating.\nNote: Embryos may need to be taken off ice for brief periods of time (several minutes) for this step. Try to limit the time embryos are at room temperature.\nOptional: GFP goggles may also be used to identify embryos carrying the Hb9::gfp reporter.\nTransfer Hb9::gfp embryos to individually numbered wells of a 12 well plate with 2\u00a0mL PBS on ice.\nClean forceps thoroughly with 70% ethanol.Remove a piece of the tail from each embryo with forceps and transfer to a pre-labeled PCR strip tube for genotyping.\nCritical: Clean the forceps by spraying them with 70% ethanol once and wiping down. This should be repeated for each embryo to avoid cross-contaminating samples for genomic DNA extraction.\nNote: You may also submit tail samples for genotyping by RT-PCR based methods (e.g. as provided by Transnetyx, Inc.) as a more accurate post hoc verification of genotypes.\nPerform rapid genotyping (2 h).\nExtract genomic DNA with QuickExtract according to the manufacturer\u2019s protocol.\nAdd 100 uL of QuickExtract buffer to each PCR tube with tail snips.\nHeat the samples on a thermocycler to 65C for 8\u00a0min.\nRemove from the thermocycler and vortex for 30 s.\nHeat the samples on a thermocycler to 98C for 2\u00a0min.\nRemove the samples from the thermocycler and vortex for 30 s.\nNote: Only a very small amount of starting material is needed for genotyping, such as the last 0.5\u00a0mm of the tail of an E12 embryo. Using excess tissue (such as 4\u20135\u00a0mm of tail length) may interfere with genomic DNA extraction or subsequent PCR.\nNote: DNA can also be isolated with commercially available DNA extraction kits. Many commonly used kits will produce DNA with high purity, though these kits typically take more time to complete.\nPerform genotyping PCR for miR-218-1 and miR-218-2 alleles\nIf there is any large cellular debris remaining in the genomic DNA, spin the samples down and use the supernatant\nFollow the manufacturer\u2019s protocol for 2\u00d7 GoTaq Master Mix using the following primers:\ntable:files/protocols_protocol_1389_1.csv\nStart the thermocycler using the following protocol\ntable:files/protocols_protocol_1389_2.csv\nRun the PCR product on an agarose gel to resolve WT and KO bandsNote: The expected frequency of embryos carrying the desired alleles may be 1/8 or lower. Thus, having genotyping results prior to performing FACS will reduce the time required on the FACS machine and the time that samples spend on ice prior to processing. It is helpful to have another lab member set up the genotyping PCR while the dissection and dissociation are ongoing. Performing genotyping prior to loading cells onto the 10\u00d7 controller is important to reduce the cost of preparing samples of the incorrect genotype.\nPrepare the embryo for removal of spinal tissue (1\u20133\u00a0min per embryo).\nRemove the head and discard by cutting at the indicated position (Figures 2[href=https://www.wicell.org#fig2]A and 2B).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1389-Fig2.jpg\nFigure\u00a02. Decapitation and evisceration of thoraco-abdominal contents\n(A) Brightfield image of a lateral view of an E12 embryo. The dotted line designates the optimal location for decapitation to separate the meninges in later steps.\n(B) Embryo after decapitation.\n(C) The embryo was pinned to a Sylgard dish ventral side up using dissection pins securely placed in the proximal area of the limbs.\n(D) Same embryo after evisceration of the thoracic and abdominal contents. Scale bar 1\u00a0mm.\nPin embryo ventral side up onto a Sylgard dish filled with ice cold PBS (Figure\u00a02[href=https://www.wicell.org#fig2]C).\nEviscerate the contents of the thoracic and abdominal cavities using forceps to first remove the overlying ectoderm, then pinching and pulling organs until they are released from their attachment points (Figure\u00a02[href=https://www.wicell.org#fig2]D).\nNote: These steps are difficult to perform at 4\u00b0C since they require working under a stereomicroscope which often does not accommodate ice buckets. Dissection dishes can be pre-chilled on ice and filled with ice-cold PBS to reduce the temperature of the tissues during dissection.\nRemove the spinal cord (2\u20135\u00a0min per embryo).Remove dissection pins and place the eviscerated embryo dorsal side up.\nSecurely pin each limb to the dish (Figure\u00a03[href=https://www.wicell.org#fig3]A).\nUsing a microdissection knife, cut along the dorsal midline along the full length of the embryo to open the dorsal neural tube (Figure\u00a03[href=https://www.wicell.org#fig3]B).\nCut along the left and right sides of the spinal cord along the full length of the embryo to free the spinal cord from the rest of the embryo (Figures 3[href=https://www.wicell.org#fig3]C and 3D). Note that the left and the right hemi-cords are still connected ventrally.\nRemove the meninges.\nNote: Removal of the meninges is technically challenging and may require some practice. The meninges will adhere more tightly to the spinal cord as embryos sit out over time, making meninges removal very challenging. Therefore, it is ideal to be time efficient with dissections.\nPlace the spinal cord on its side and identify the meninges on the ventral surface (Figure\u00a04[href=https://www.wicell.org#fig4]A).\nUsing forceps, gently loosen the meninges from the rostral region.\nUsing one set of forceps, grip the spinal cord gently but securely, so as not to crush the tissue.\nWith another set of forceps, secure the meninges tightly (Figure\u00a04[href=https://www.wicell.org#fig4]B).\nQuickly and smoothly detach the meninges by pulling in the caudal direction (Figure\u00a04[href=https://www.wicell.org#fig4]C). Problem 1[href=https://www.wicell.org#sec5.1].\nDiscard the meninges and any non-spinal tissues.\nCritical: Inspect the spinal cord to ensure all the spinal cord regions are intact by placing pins in the ends of the spinal cord. Loss of particular rostro-caudal spinal regions or regions of the spinal cord will affect the capture of the same motor neuron populations across replicates (Figures 4[href=https://www.wicell.org#fig4]D\u20134G).\nTransfer the spinal cord to a labeled Eppendorf tube with 500 uL PBS and keep on ice.\nRepeat steps 5 and 6 for the remainder of the embryos.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1389-Fig3.jpgFigure\u00a03. Removal of spinal cord from the embryo\n(A) The embryo was reoriented dorsal side up and again securely attached to the Sylgard dish with dissection pins.\n(B) The dorsal axis of the spinal cord is cut vertically using a dissection knife.\n(C and D) After cutting both sides of the spinal cord with microdissection scissors, the spinal cord (D) is released leaving the rest of the embryo attached to the Sylgard (C). Scale bar 1mm.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1389-Fig4.jpg\nFigure\u00a04. Removal of the meninges from the spinal cord\n(A) The rostral meninges (arrow) is loosely attached to the spinal cord.\n(B) One forceps was used to gently secure the rostral end of the spinal cord and the other forceps was used to firmly grasp the meninges on the ventral side.\n(C) With the spinal cord secured, the meninges was pulled down in a firm and steady motion until completely removed.\n(D) Brightfield image of the isolated spinal cord that is secured to the Sylgard dish with insect pins.\n(E) GFP expression from the Hb9::gfp allele enables visualization of cervical (1), thoracic (2) and lumbar (3) regions of the spinal cord.\n(F and G) Brightfield (F) and GFP (G) views of the lumbar spinal cord at higher magnification enable visualization of the lateral (LMC) and medial motor columns (MMC). Scale bar (A\u2013E) 1\u00a0mm; (F-G) 0.5\u00a0mm.\nSpinal tissue dissociation and FACS purification\nTiming: 2\u20133\u00a0h (1\u00a0h for dissociation and 1\u20132\u00a0h for FACS purification of all samples)\nThe goal of these steps is to dissociate the spinal tissue into single cells for FACS purification of GFP+ motor neurons.\nPrepare Papain dissociation reagents according to the manufacturer\u2019s protocol.\nAspirate PBS from spinal cord samples.Add 1mL reconstituted papain solution (20 units per mL) to each spinal sample, place at 37\u00b0C, and set timer for 30\u00a0min.\nTriturate the spinal tissue with a P1000 pipette tip about 10 times. If large tissue chunks remain, triturate with a P200 or incubate at 37\u00b0C for another 10\u00a0min and triturate again until no large pieces remain.\nCentrifugate dissociated cells at 200 rcf for 5\u00a0min at room temperature (20\u00b0C\u201322\u00b0C) in a spinning bucket centrifuge and discard the supernatant.\nAdd 1\u00a0mL reconstituted and diluted albumin-ovomucoid inhibitor solution (10\u00a0mg per mL) with DNase (10 units DNase) per the manufacturer\u2019s protocol.\nCentrifugate cells at 200 rcf for 5\u00a0min at room temperature in a spinning bucket centrifuge and discard the supernatant.\nResuspend cell pellet in sorting buffer (1:1 Neurobasal:DMEM/F12 without phenol red with 3% Horse Serum and 10 units per mL DNase solution).\nCritical: Sorting buffer must contain DNase or cells may clump, causing yield to significantly decrease.\nNote: The addition of 3\u00a0\u03bcM DAPI to the sorting buffer can aid in the exclusion of dead or dying cells by FACS gating. Ensure the cells have been incubating in DAPI for at least 15\u00a0min. DAPI can be left in the sorting buffer during FACS.\nPass cells through a 40 \u03bcm cell strainer.\nSort dissociated cells from each embryo separately on a FACS machine.\nCritical: Setting the gating on FACS machine and maintaining the same gates for all samples is important to reduce variability in motor neuron capture. Motor neuron subpopulations express different levels of endogenous Hb9 protein and, resultingly, of the Hb9::gfp reporter. Therefore, changing the gating settings may enrich or deplete some motor neuron pools.Collect GFP+ motor neurons into 1\u00a0mL chilled sorting buffer (1:1 Neurobasal:DMEM/F12 without phenol red with 3% Horse Serum and 10 units per mL DNase solution) in an Eppendorf tube for single cell RNA sequencing preparation.\nNote: We typically capture 5,000\u201310,000 motor neurons per E12 spinal cord. The number of motor neurons captured is affected by the age of the embryo, precision of the spinal dissection, and completeness of the papain dissociation.\n10\u00d7 genomics cell preparation\nTiming: 2\u20133\u00a0days\nQuantify motor neuron cells per volume.\nCalculate the volume of media required to obtain the desired concentration of cells.\nSpin collected cells at 200 rcf for 5\u00a0min at 4\u00b0C.\nGently remove media to desired volume with 10 uL excess for hemocytometer cell counting.\nMix 5 uL of the cell suspension with 5 uL trypan blue in a separate tube, load onto a hemocytometer, and count cells.\nRepeat once for confirmation of cell count. Problem 2[href=https://www.wicell.org#sec5.3]:\nLoad the resuspended cells into the 10\u00d7 Chromium Controller following the manufacturer\u2019s instructions.\nPrepare sequencing libraries following the manufacturer\u2019s instructions (10\u00d7 Genomics). In Amin et\u00a0al. (2021)[href=https://www.wicell.org#bib5], 14 cycles were used for cDNA amplification, and 10 cycles were used for library amplification using V2 reagents.\nPause point: Libraries can be stored at \u221220C prior to sequencing.\nSequence the resulting sequencing library following the manufacturer\u2019s instructions. In Amin et\u00a0al. (2021)[href=https://www.wicell.org#bib5], the libraries were sequenced with paired end reads, with a Read 1 of 26 basepairs and a Read 2 of 98 basepairs, on an Illumina HiSeq 4000 at the UCSD IGM Genomics Center.Note: When performing differential expression with single cell RNA sequencing data, it is important to have biological replicates to limit the detection of spurious associations (Squair et\u00a0al., 2021[href=https://www.wicell.org#bib12]). We found a WT versus DKO (n\u00a0= 2,2) comparison with single cell RNA sequencing resulted in pseudobulk differential expression results that detected many of the same differentially expressed target genes as our WT versus DKO (n\u00a0= 3,3) bulk RNA sequencing results (Amin et\u00a0al., 2021[href=https://www.wicell.org#bib5]). The ability to accurately identify differentially expressed genes will be significantly higher when having even more replicates but will substantially increase the cost.\nPreparing single cell RNA sequencing data for downstream analysis\nTiming: 1\u00a0week\nThe goal is to filter out low quality cells and prepare the data for downstream analysis.\nDemultiplex raw sequencing data. In Amin et\u00a0al. (2021)[href=https://www.wicell.org#bib5], bcl2fastq v2.18.0.12 was used.\nAlign and quantify reads from the samples separately for gene expression using Cellranger version 3 (10\u00d7 Genomics) using the mm10 mouse transcriptome annotation.\nCombine samples into a single expression matrix using Cellranger\u2019s \u2018aggr\u2019 program by equalizing aligned read counts. Use the estimated cell count from Cellranger for downstream analysis.\nImport the resulting UMI matrix and sample identities into R.\nFilter out low quality cells and doublets.\nWe generated a histogram of the number of detected features per-cell. We then selected lower and upper bounds that retained the dominant population of cells in order to eliminate low quality cells and doublets. Problem 3[href=https://www.wicell.org#sec5.5].\nWe also generated a histogram of the percentage of mitochondrial gene expression per cell and selected an upper limit of 8%.\nThese two filters removed 796 cells, leaving 8,247 cells for downstream analysis.Note: Lower sequencing depths are often sufficient for identifying cellular populations using single cell RNA sequencing, however, higher sequence depths are likely needed for effective differential gene expression testing. We sequenced a total of 1,128,753,214 reads across four samples. After filtering steps, we detected 4762 (median) genes per cell, 21,268 (median) unique molecular identifiers (UMIs) per cell, and 19,410 total genes detected (minimum of three cells).\nPerform dimensionality reduction.\nIdentify highly variable genes (HVG) using a method previously described (Brennecke et\u00a0al., 2013[href=https://www.wicell.org#bib7]). We used FDR\u00a0= 0.1.\nNote: If using software such as Seurat, p values may not be provided. You can test a range of HVGs in these steps to see if and how it changes the interpretation of your analyses. Adjusting the HVG cutoff can help reveal detail in the data that you would otherwise miss if all genes were included, or if you use all genes over certain expression levels (as is often done with bulk RNA-seq analyses).\nPerform normalization by dividing each cell\u2019s UMI counts by its respective total UMI count across all genes and scaling by 10,000, resulting in an expression value comparable to TPM in bulk RNA sequencing.\nPerform PCA on normalized and log2+1 scaled UMI using the identified HVGs.\nIdentify significant components by comparing the eigenvalues to the 95th percentile of the appropriate Wishart distribution.\nNote: We used the Wishart method but it\u2019s advisable to observe a scree plot of the eigenvalues and find the \u201celbow\u201d in the curve as a second opinion. Sometimes the Wishart method will over-estimate the number of significant components.\nGenerate UMAP. We used \u2018n_neighbors=20\u2019.Note: Changing the number of neighbors used in the UMAP algorithm will affect the outcome. Lower values will result in smaller and more numerous groupings of cells (better for fine detail analysis such as finding subtypes of a single cell type) while higher values will result in larger and fewer groups of cells (better for large scale differences such as different cell types). These setting alterations will need to be adjusted to address the specific biological question being asked and the inherent diversity of the cell population that was sequenced.\nPerform dimensionality reduction separately for all cells and the interneuron-excluded subset (if contaminating interneurons are captured).\nNote: Hb9::gfp may have background levels of expression in some spinal interneuron populations and may also be expressed in Hb9 spinal interneurons. Thus, you may capture some interneuron populations despite optimizing FACS gating settings. The transcriptomic identity of interneurons is very distinct from motor neurons and can be clearly distinguished by UMAP clusters or expression of neurotransmitters and transcription factor marker genes.\nPseudo-bulk gene expression comparison\nTiming: 1\u00a0day\nThe goal of these steps is to compare and perform differential gene expression of a subset of neurons isolated from mice of different genotypes (e.g., differential expression of medial motor column neurons from control and miR-218 DKO backgrounds).\nWe summed the expression of individual cells within an identified cluster (e.g., medial motor column or phrenic/hypaxial motor neurons) on the gene level into pseudo-bulk expression tables. Each gene\u2019s un-normalized UMI counts were summed into bins based on sample identity and cluster assignment (e.g., medial motor column or phrenic/hypaxial motor neurons) resulting in a pseudo-bulk count matrix (4 samples per cluster assignment) Problem 4[href=https://www.wicell.org#sec5.7].\nThe pseudo-bulk expression data were treated like bulk RNA-seq for expression normalization and differential expression testing performed using DESeq.Perform Sylamer (Bartonicek and Enright, 2010[href=https://www.wicell.org#bib6]; van Dongen et\u00a0al., 2008[href=https://www.wicell.org#bib14])\nUse DESeq normalized gene expression for each population by genotype as data input.\nLimit analysis to the top 10,000 most highly expressed genes. Problem 5[href=https://www.wicell.org#sec5.9]:\nNote: If the quality and depth of sequencing reads is low, the number of genes included in Sylamer analysis might need to be reduced so that only the genes with the most accurately determined gene expression are included. We used this cutoff (i.e. top 10,000 most highly expressed genes) for both bulk RNA sequencing analysis and pseudo-bulk RNA sequencing analysis from single cell data for consistency.\nRank the genes from highest enriched to lowest enriched in a given genotype (i.e., most highly enriched in WT medial motor column compared with DKO medial motor column neurons).\nObtain a FASTA file of 3\u2032UTR sequences. This can be obtained from UCSC Table Browser for the GENCODE_VM18 database with masked repeats with \u201cN\u201d.\nNote: 3\u2032UTR sequences often contain low complexity repeated nucleotides that can reduce the quality of detection of enriched motifs. Masking repeats is an effective way to reduce this source of spurious correlations.\nFor genes with multiple isoforms, the longest UTR sequence among annotated transcripts per gene was retained.\nNote: For a given mRNA, there can be many annotated isoforms that vary with their 3\u2032UTR sequence length. In any cell, multiple RNA isoforms may be present at varying abundance for a single gene. Despite the complexity and diversity of RNA isoforms, we have found it effective to use the longest annotated UTR sequence for Sylamer analysis. It is also possible to generate isoform abundances (either de novo or annotated) from RNA sequencing data using available software, though we have not found this necessary to obtain robust data from Sylamer.FASTA sequence files and ranked gene lists were used with Sylamer software (Enright Lab) with the following settings: 8mer search (Markov correction: 5), 7mer search (Markov correction: 4), and 6mer search (Markov correction: 4) including known miRNA seed sequences.", "Both the Matrix and the Search pages provide an intuitive method to perform metadata-based searches for ENCODE datasets in the form of facet filters, which are categorized lists of commonly used experimental metadata.\nNecessary Resources\nHardware\nComputer with internet access\nSoftware\nUp-to-date web browser (Chrome, Microsoft Edge, Firefox, Safari)\nUse the matrix to navigate to a search page\n1. Navigate to the ENCODE portal home page at https://www.encodeproject.org[href=https://www.encodeproject.org]. A clickable widget labeled \u201cHelp\u201d automatically loads in the lower right corner of the browser window, which contains links to interactive tutorials, frequently asked questions, and other documentation on topics not addressed in this article. Users are encouraged to explore the widget for additional help with the portal.\n2. In the toolbar along the top of the page, click \u201cData.\u201d This opens a drop-down menu with multiple options (Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0001]).\nThe menu options in the toolbar provide access to key resources on the portal, including the Experiment matrix page and a link back to the home page. For example, the Data drop-down menu also has links to Search and Summary pages, which represent the same data as the Experiment matrix page but in different layouts. The Encyclopedia menu contains information about and links to integrative-level annotations generated using ENCODE data. The Materials & Methods menu links to information about experimental components used, data-processing methods, and portal-organization methodology. The Help menu contains links to information about ENCODE, portal usage, past ENCODE workshops, and contact information. A cart menu will also appear here if there are experiments added to the cart, which is discussed in Support Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0003].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d791bbf7-c167-49b1-9def-66805e5a2d85/cpbi89-fig-0001-m.jpg</p>\nFigure 1\nThe ENCODE home page. This image shows the Data drop-down menu in the toolbar opened. The first item in the menu is a link to the Experiment Matrix page.3. In the drop-down menu, click \u201cExperiment Matrix\u201d to navigate to the Experiment matrix page (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0002]).\n         \nThe Experiment matrix page lists biosample types, which refer to the biological material used such as a cell culture or tissue sample along the y axis, and various assays along the x axis, with each cell indicating the number of experiments of that combination of assay and biosample type.\nOnly a subsection of the matrix is visible upon loading the page. To view more, click the arrows along the left side of the biosample category headers to expand the categories and reveal an extended list of available biosample types. With the mouse cursor hovering on top of the matrix, scroll horizontally to view more available assays.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/5b0a9607-d06d-459b-b8a1-53b718a68216/cpbi89-fig-0002-m.jpg</p>\nFigure 2\nThe Experiment Matrix page displays available ENCODE data in a matrix with biosample types and assays as the axes. Each cell of the matrix is clickable and leads to a list of experiments matching the given combination of biosample type and assay.\n4. Click on the cell for transcription factor ChIP-seq (abbreviated as TF ChIP-seq) experiments on K562. As of September 2019, the ENCODE portal had 530 experiments in this group. This link leads to a search page with a list of the 530 experiments, shown in Figure 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0003].\nBy default, only experiments with the status \u201creleased\u201d are included in a search. Explanations of the meaning of different experimental statuses are available in Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-tbl-0001].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/314bd88e-5026-42bd-baaa-9f97b8583b19/cpbi89-fig-0003-m.jpg</p>\nFigure 3The Experiment search page displays ENCODE data as a list of search results. Each experiment is shown with a brief summary of the biological material and assay name, and a link to its individual experiment summary page with more metadata details (see Fig. 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0005]). On the left is the facet sidebar, which can be used to modify and refine the search results. The \u201cAdd all items to cart\u201d button is a Cart function, explained further in Support Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0004].\nTable 1.\n                Dataset Statusesa[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-tbl1-note-0001_27]\ntable:\n\ufeffStatus,Meaning\nReleased,\"Publicly available datasets are marked with the \u201creleased\u201d status. Datasets become publicly available after automatic and manual review to make sure they meet the standards and do not have data or metadata issues and inconsistencies. This status is selected by default when visiting all search views, including the Matrix page (refer to Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001], step 2).\"\nRevoked,\"An error was found with the experiment after it became publicly available, so the status was changed to \u201crevoked\u201d to indicate that caution should be exercised before using the data. Some examples of errors are:  1.The data was not compliant with the ENCODE quality requirements 2.Issue discovered with experimental elements (antibody, biosample, etc.)\"\nArchived,\"The dataset was superseded by another dataset that has higher quality, was collected and/or processed with newer technology, etc. The ENCODE DCC encourages use of the superseding experiment instead of the archived one.\"\na Additional information is available at https://www.encodeproject.org/help/getting-started/status-terms/[href=https://www.encodeproject.org/help/getting-started/status-terms/].\nFilter search results using facets\n5. The sidebar on the left side of the search page is populated with facets, which allow users to filter search results by different properties. Locate the \u201cAssay title\u201d facet and observe that the facet term \u201cTF ChIP-seq\u201d is highlighted in blue, indicating that the search results have been filtered for experiments with an assay title of TF ChIP-seq.Clicking a cell in the \u201cTF ChIP-seq\u201d column on the Experiment matrix page (see step 4) automatically selects the \u201cTF ChIP-seq\u201d facet term.\nIn general, selecting a facet term applies that term as a filter, and automatically updates the displayed search results.\n6. Scroll further down on the page and locate the \u201cBiosample term name\u201d facet. Observe that \u201cK562\u201d is already selected, indicating that the search results have been filtered for experiments with a biosample term name of K562.\n         \nClicking a cell in the \u201cK562\u201d row on the Experiment matrix page (see step 4) automatically selects the \u201cK562\u201d facet term.\nSelections can be made in more than one facet at a time. When such a selection is made, the combined filters possess an AND relationship. For example, the selection of TF ChIP-seq in the \u201cAssay title\u201d facet and K562 in the \u201cBiosample term name\u201d facet returns only experiments that are TF ChIP-seq assays AND use K562 cells as the biosample.\n7. In the type-ahead search box below the \u201cBiosample term name\u201d label, type DND-41. The list of terms below the search box will be dynamically filtered.\nBecause there are many biosample types to choose from, a type-ahead search is available for this facet to help speed up the search process. This also applies to other facets with many terms, such as \u201cTarget of assay.\u201d\n8. Click \u201cDND-41\u201d in the \u201cBiosample term name\u201d facet.\n         \nMore than one facet term can be selected in a single facet simultaneously. Multiple selections in one facet represent an OR relationship between the selected facet terms. In this example, the selection of \u201cK562\u201d and \u201cDND-41\u201d terms means the returned experiments may be on K562 or DND-41 cells.\nThe URL for the current search is https://www.encodeproject.org/search/?type=Experiment&status=released&assay_title=TF+ChIP-seq&biosample_ontology.term_name=K562&biosample_ontology.term_name=DND-41[href=https://www.encodeproject.org/search/?type=Experiment&status=released&assay_title=TF+ChIP-seq&biosample_ontology.term_name=K562&biosample_ontology.term_name=DND-41].9. Click \u201cDND-41\u201d in the facet term list a second time to remove the filter for DND-41 biosamples.\n         \nUsers can also click the \u201cDND-41\u201d link at the top of the facet after the words \u201cSelected filters\u201d to remove the filter. Both methods have the same result.\nThe URL for the current search is https://www.encodeproject.org/search/?type=Experiment&status=released&assay_title=TF+ChIP-seq&biosample_ontology.term_name=K562[href=https://www.encodeproject.org/search/?type=Experiment&status=released&assay_title=TF+ChIP-seq&biosample_ontology.term_name=K562].\n10. Locate the \u201cTarget category\u201d facet. Scroll through the list of terms and click on \u201ccohesin.\u201d\n         \nTargets have been categorized based on their Gene Ontology annotations in accordance with the methods described at https://www.encodeproject.org/target-categorization/[href=https://www.encodeproject.org/target-categorization/].\nThe URL for the current search is https://www.encodeproject.org/search/?type=Experiment&status=released&assay_title=TF+ChIP-seq&biosample_ontology.term_name=K562&target.investigated_as=cohesin[href=https://www.encodeproject.org/search/?type=Experiment&status=released&assay_title=TF+ChIP-seq&biosample_ontology.term_name=K562&target.investigated_as=cohesin].\n11. Locate the \u201cTarget of assay\u201d facet. Hover the cursor over the \u201cRAD21\u201d facet term so that a red icon appears to the right. Click on the red icon to exclude experiments targeting RAD21.\n         \nThe \u201cTarget of assay\u201d most commonly applies to assays that utilize immunoprecipitation in their protocol, such as ChIP-seq. In this context, the \u201ctarget\u201d refers to the DNA-binding molecule targeted by the antibody for precipitation. However, the target property is sometimes also used for knockdown experiments or assays on genetically modified biosamples, such as shRNA RNA-seq, in which case it refers to the gene target of the knockdown or modification.\nThe URL for the current search is https://www.encodeproject.org/search/?type=Experiment&status=released&assay_title=TF+ChIP-seq&biosample_ontology.term_name=K562&target.investigated_as=cohesin&target.label%21=RAD21[href=https://www.encodeproject.org/search/?type=Experiment&status=released&assay_title=TF+ChIP-seq&biosample_ontology.term_name=K562&target.investigated_as=cohesin&target.label%21=RAD21].\n12. Scroll to the top of the page. At the top of the facet sidebar below the words \u201cExperiment search,\u201d click \u201cClear filters\u201d to remove all selected filters.\n13. Click the back button of the browser to undo the previous action. This will return to the previous query state for TF ChIP-seq on K562 targeting cohesin-related targets except for RAD21. The facet terms which should be selected at this stage are shown in Figure 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0004]. As of September 2019, this search returned three experiments.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6d306185-2233-4e62-aa5f-3a5777f72de5/cpbi89-fig-0004-m.jpg</p>\nFigure 4A truncated view of the facets with the items that should be selected after step 13 of the Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001].\n14. Review the list of search results, which displays the summaries of the experiments satisfying the selected filters.\n         \nEach result is labeled with a short title based on the assay performed and biosample used. Because this example search has filtered for both a specific assay and a biosample type, all the results are titled with \u201cTF ChIP-seq of K562.\u201d\nBelow the short title there is a more descriptive biosample summary, followed by the target if applicable, the lab that performed the experiment, and the project the experiment belongs to, such as ENCODE or Roadmap.\nAdditional details about each experiment are located on the right side of each summary:\nObject type: In this case, all results are Experiment objects. However, there are many object types modeled in the ENCODE database, which represent different experimental components. The data model is briefly discussed in the Commentary section of this article.\nThe accession: Each experiment on the ENCODE portal is given a unique and persistent identifier known as an accession, which can be used for citing ENCODE datasets. An example of an accession is ENCSR670JDQ. Users can directly access the record page for any accessioned object by appending its accession to https://www.encodeproject.org/[href=https://www.encodeproject.org/].\nStatus of the object: Brief explanations of the different statuses are shown in Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-tbl-0001], and further information is available at https://www.encodeproject.org/help/getting-started/status-terms/[href=https://www.encodeproject.org/help/getting-started/status-terms/]. Other object types aside from Experiments also have statuses.Audit flags: The ENCODE DCC uses automated checks known as \u201caudits\u201d to flag objects in the database for potential data or metadata issues, some of which are outlined in Table 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-tbl-0002]. Once flagged, the DCC is able to work with production labs to address the issues and ensure that all data and metadata are up to quality standards. Audits are divided into different severity levels, represented as red, orange, or yellow icons, which serve as an indication to users of potential concerns to be aware of if they use the data in their own research. If flagged, the icon(s) will appear below the Status and can be clicked to show further details about the audits.\nCart button: This button allows users to add experiments to their cart. Further cart-specific information is detailed in Support Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0003].\nTable 2.\n                Audit Flag Categories\ntable:\n\ufeffCategory,Description\nRead coverage,Read depth or coverage issues for libraries. These standards were agreed upon by ENCODE production labs and are outlined in full on the data standards pages (https://www.encodeproject.org/data-standards/[href=https://www.encodeproject.org/data-standards/]).\nReplication,Issues with replicate concordance or other replicate inconsistencies\nLibrary complexity,\"Bottlenecking or library complexity issues, as outlined in the ENCODE Histone ChIP-seq (https://www.encodeproject.org/chip-seq/histone/[href=https://www.encodeproject.org/chip-seq/histone/]) and Transcription Factor ChIP-seq standards (https://www.encodeproject.org/chip-seq/transcription_factor/[href=https://www.encodeproject.org/chip-seq/transcription_factor/])\"\nEnrichment,Low SPOT scores for DNase-seq experiments as outlined in the ENCODE DNase-seq standards (https://www.encodeproject.org/data-standards/dnase-seq/[href=https://www.encodeproject.org/data-standards/dnase-seq/])\nUniform pipeline requirements,\"Various pipeline issues, such as unexpected inconsistencies in read length, insufficient read length, and unknown platforms or other missing information\"\nAntibody,Mismatches between antibody and target metadata or missing characterizations for antibodies\nMetadata,Missing required metadata\nDataset consistency,Inconsistencies between different experiments grouped together in a series\naAdditional information is available at https://www.encodeproject.org/data-standards/audits/[href=https://www.encodeproject.org/data-standards/audits/].\nVisit a single experiment page\n15. Click the short title corresponding to experiment ENCSR670JDQ to go to its experiment summary page, depicted in Figure 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0005]. This experiment is also accessible at https://www.encodeproject.org/experiments/ENCSR670JDQ/[href=https://www.encodeproject.org/experiments/ENCSR670JDQ/].This page displays a more complete picture of the metadata, which is not shown in the summaries on the search result page, including links to related experimental components, data files, and documents.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f7e6b4ae-9509-48f8-aaa0-91d787bd2fec/cpbi89-fig-0005-m.jpg</p>\nFigure 5\nAn experiment summary page. Below the page title and audits, the page is organized into distinct sections containing the following information: (A) Summary section: key info including but not limited to the assay performed, biosample used, assay target if applicable, platform, and controls. (B) Attribution section: information about the lab that performed the experiment and when the experiment was released. (C) Replicates section: table of experimental replicates with links to biosamples, antibodies, libraries, and genetic modifications when applicable. (D) Files section: information about the raw and processed data files generated from this experiment and subsequent analysis, provenance of data files as reflected in file association graph, and visualization of experiment-specific genome tracks when applicable. (E) Documents section: links to additional protocol documents describing the experimental methods.\n16. Audit flags are also displayed on the experiment summary pages. Click the audit button, as labeled in Figure 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0005], to display a list of audits. For this experiment, the button appears as an unlabeled yellow icon with a number corresponding to the severity and number of the audit flags. The plus symbol to the left of each audit expands the audit further to display the audit flag details.\n17. Scroll down the page to the Summary and Attribution sections.\nThese sections contain general information about the experiment, such as the biosample, assay, and target, a link to a control experiment, and information about the lab that performed the experiment.\n18. Scroll down the page to the Replicates section.This section contains information about the replicate(s) of the experiment and links to biosamples, genetic modifications, and antibodies used when applicable.\n19. Scroll down the page to the Files section.\n         \nThe Files section is divided into three tabs: Genome browser, Association graph, and File details. By default, the Files section displays the Association graph, which shows data provenance and derivation of downstream processed files. Use the slider above the graph to zoom in or out of the graph.\nThe Files details tab is discussed in step 22, and the Genome browser tab is described in more detail in Support Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0004].\n20. Each node in the Association graph can be clicked to view more information about the node. Click on a yellow node, which represent files, to view a pop-up containing the file's unique accession as well as other metadata such as the file size, output type, and submission date.\nSome file nodes may contain smaller, green circles representing quality control metrics associated with that file. These green circles are clickable and, like the file nodes, open a pop-up with the quality metric (QC metric) values, plots, and other information if applicable. Click \u201cClose\u201d in the lower right-hand corner of the pop-ups when finished viewing the information.\n21. Click on a blue node, which represents a step in the computational analysis pipeline, to view information about the software used, the inputs and outputs, and the general purpose of the relevant pipeline analysis step.\n22. Click the \u201cFile details\u201d tab to view a list of all files linked to this experiment.The files are presented in a table containing information about the file type, reference genome assembly for mapping, and submission date, with one file per row. A small download icon next to each file accession allows users to download a single file at a time.\n23. Scroll further down to view the Documents section. Experiments may also have attached documents describing the experimental and/or computational protocols. Click the link for a particular document to download it.After identifying experiments of interest, users can download data associated with these experiments. Batch downloading provides a quick and simple way to download multiple files.\nNecessary Resources\nHardware\nComputer with internet access\nSoftware\nUp-to-date web browser\nCommand-line terminal\nCommand-line UNIX utilities, available by default in macOS and Linux operating systems. For other systems, downloads for the utilities are available at:\n               \ncurl: https://curl.haxx.se/[href=https://curl.haxx.se/]\nxargs: http://gnuwin32.sourceforge.net/packages/findutils.htm[href=http://gnuwin32.sourceforge.net/packages/findutils.htm], https://www.gnu.org/software/findutils/findutils.html[href=https://www.gnu.org/software/findutils/findutils.html]\nA text editor\nA spreadsheet application (optional)\n1. Navigate to a search result page as described in the Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001], steps 1 to 14.\n2. Click the \u201cDownload\u201d button located above the list of results (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0003]). A pop-up containing detailed instructions will appear.\n3. In the lower right corner of the pop-up, click the \u201cDownload\u201d button to download a text file named files.txt containing a list of URLs of all files of each experiment returned in the search.\n4. Open files.txt in a text editor.\n5. The first line in the file is a metadata.tsv file download link. Enter this link into a web browser to download the file. Then, open metadata.tsv in a text editor or spreadsheet program.\n         \nMetadata.tsv is a table of metadata for each file listed in the files.txt file. This includes information about the file itself and its dataset, download links, and many other properties. The provided metadata fields are subject to occasional updates. The \u201cFile download URL\u201d field contains HTTP links to the files, which are the same as those listed in the files.txt file. Another field labeled \u201cs3_uri\u201d contains Amazon Web Services (AWS) S3 Uniform Resource Identifiers (URIs) for each file, which can be used to access the files via cloud-based tools. As of September 2019, these fields are located in columns 43 and 49, respectively.Use a spreadsheet application or other tools to filter metadata.tsv by different properties for a subset of files of interest. For example, the grep UNIX command can be used to locate entries of files with the .bed.gz extension:\n               \ngrep \u201c.bed.gz\u201d metadata.tsv\nMultiple commands can be piped together to extract only the \u201cFile download URL\u201d field:\n         \ngrep \u201c.bed.gz\u201d metadata.tsv | cut -f 43\n6. The remaining lines in files.txt are download links for every file associated with the experiments returned in the initial search. There are multiple ways to download the files utilizing the provided links. One method is to use command-line utilities such as curl to download every link in the text file. An example of such a command is:\n         \nxargs -L 1 curl -O -L < files.txt\nNote that there may be many files to download. Before beginning to download, check that there is sufficient free disk space on the machine.\nThe method presented here is only one of many options. Many programming languages also have utilities or libraries that can be used to request HTTP links.\nUsers can also visit the links using a web browser to download files individually, similarly to the download of metadata.tsv in step 5.The Cart feature allows users to save a set of experiments of interest and download files from these experiments later. Carts are saved per active browser session, so closing the browser tab or refreshing the tab erases items in the cart.\nNecessary Resources\nHardware\nComputer with internet access\nSoftware\nUp-to-date web browser\n1. Navigate to a search result page as described in the Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001], steps 1 to 14.\n2. Add a single experiment from a list of results to the cart by clicking the cart icon, which appears on the right side of each experiment summary.\nThe cart icon is shaded in black when selected. The cart icon in the top toolbar displays the number of items in the cart.\n3. Click the cart icon a second time to remove the experiment from the cart. The cart icon is no longer shaded in black after being deselected.\n4. Click \u201cAdd all items to cart\u201d in the upper-right corner above the first search result to add every experiment in the list of search results to the cart (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0003]).\nAdding a large number of experiments could slow the browser down significantly. It is advised, though not mandatory, to have fewer than 500 experiments in the cart to avoid this issue.\n5. Follow step 15 of the Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001] to reach the individual experiment summary page for experiment ENCSR670JDQ. Click the cart icon located in the upper-right corner of the page to remove it from the cart. Click the cart icon a second time to add the experiment back into the cart.6. In the top toolbar, click the cart icon to open a drop-down menu. In the menu, click \u201cView cart\u201d to visit the Cart page shown in Figure 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0006] and review the items in the cart. There should be three experiments listed on the right side of the page.\nIf there are unwanted items in the cart, remove them by clicking the cart icons on the right side of the experiment summary.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/7140626c-386d-4bf1-be93-485b5f4a14ff/cpbi89-fig-0006-m.jpg</p>\nFigure 6\nThe Cart page. On the left are the file selectors. Although visually similar to the facet sidebar explored in the Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001], these file selectors only affect which files will be included in files.txt, introduced in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0002]. As selections are made, the number above the selectors, which reads \u201c137 files selected\u201d in this figure, will update dynamically. On the right is the list of experiments saved in the cart.\n7. The left side of the page contains the file selectors, which filter out the files included in the files.txt file. Locate the \u201cGenome Assembly\u201d file selector and click it to expand the options. Then, click the checkbox for \u201cGRCh38\u201d-aligned files. The number of files above the file selector will automatically update. As of September 2019, it should read \u201c60 files selected.\u201d\nThe file selectors are visually similar to the facet sidebar on the Search pages (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0003]), but these selectors only determine which files are included for download, and do not remove experiments from the cart.\n8. Locate the \u201cFile type\u201d file selector and expand it. Check the box for \u201cbed narrowPeak.\u201d The number of files is filtered to 15 bed narrowPeak files aligned to GRCh38.\nAs with the Search page facets, users can make multiple selections in a file selector and selections in more than one file selector.9. Click \u201cDownload\u201d in the upper right above the list of cart items to open the same pop-up seen in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0002], step 2, and then download files.txt as in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0002], step 3. The resulting files.txt file should contain only links to metadata.tsv and the 15 GRCh38-aligned bed narrowPeak files of the experiments in the cart.\n10. Continue following steps 4 to 6 of Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0002] to download the files.\nOnce downloaded, users can proceed to analyzing the data as they wish. One example is using BEDTools to find overlapping peaks in two of the files (see Current Protocols article: Quinlan, 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-bib-0013]).The ENCODE portal supports visualization of analysis results such as signal or peak data from one or multiple experiments at once using Ensembl (Aken et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-bib-0001]) or UCSC browsers (Kent et\u00a0al., 2002[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-bib-0010]). Visualization of supported HiC data is available through Juicebox browser (Durand et\u00a0al., 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-bib-0005]). The ENCODE portal has also recently introduced embedded visualization of tracks directly on individual experiment pages using Valis genome browser (https://valis.bio/[href=https://valis.bio/]).\nNecessary Resources\nHardware\nComputer with internet access\nSoftware\nUp-to-date web browser\nVisualize results from multiple experiments from a search page\n1. To visualize files from multiple experiments at once, first navigate to a search result page according to the Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001], steps 1 to 14.\nDue to limitations of the genome browsers, there must be fewer than 100 results for the search in order to visualize them. If more than 100 results are returned, the \u201cVisualize\u201d button will be disabled and will instead read \u201cFilter to 100 to visualize.\u201d\n2. Click the \u201cVisualize\u201d button at the top of the page. A pop-up appears with different options grouped by the available reference genome assemblies.\n3. In the \u201cGRCh38\u201d category, click \u201cUCSC\u201d to visualize the data from the experiments using the UCSC Genome Browser. Close the new window when done visualizing the tracks.\nVisualize results from a single experiment\n4. To visualize results from a single experiment, navigate to an experiment summary page (Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001], step 15). Then, scroll down to the Files section.\n5. Click on the \u201cGenome browser\u201d tab to visualize tracks using the embedded Valis genome browser.\nBy default, all visualizable tracks will be shown. If available for the given reference genome assembly, gene and/or genome tracks are also displayed.6. While hovering the mouse cursor over the tracks, click and drag or scroll to the left or right to move along the genome. Scroll up or down to change the zoom level of the tracks.\n7. The sidebar in the Files section contains filters to switch reference genome assemblies or select specific tracks. Below the \u201cOutput type\u201d header, select \u201csignal p-value\u201d to view three signal tracks. This produces the view shown in Figure 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-fig-0007].\nClick the left-facing arrow near the top of the sidebar to expand or collapse it. Click \u201cClear all filters\u201d to remove all selected filters.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e98d915e-2853-4ebd-b276-87e892f4e13f/cpbi89-fig-0007-m.jpg</p>\nFigure 7\nGenome browser tab in the Files section of an Experiment summary page. This tab contains the embedded Valis genome browser, which can be used to visualize signal and peaks tracks directly on the ENCODE portal. Filters to the left of the browser are used to select which tracks to visualize. Here, only the \u201csignal p-value\u201d tracks are selected for visualization. The small arrow above the words \u201cChoose an assembly\u201d can be used to collapse and expand the filter sidebar.\n8. A search box for genes, labeled \u201cSearch for a gene,\u201d is located above the tracks. Enter RBFOX1 and select the option that comes up, then click \u201cSubmit.\u201d The genome browser will automatically refresh and load the location of the requested gene.\nThe chromosome and coordinates are displayed below the search box. To directly edit the values, click the pencil icon to the right of the coordinates.9. The ENCODE portal also supports visualization of tracks using external browsers. Click on the File details tab of the Files section. In the upper right corner of the table are two drop-down menus, one for the assembly and the other for the genome browser, which can be UCSC, Ensembl, or Juicebox depending on the file formats of the available files.\n10. Click the \u201cVisualize\u201d button to the right of the two menus to open the external genome browser in a new window.The ENCODE portal is a user-friendly interface for querying the ENCODE database. Queries can also be sent directly to the database using the Representational State Transfer (REST) API, bypassing the portal's user interface. The REST API accepts URLs in a typical HTTP format and returns metadata records as JSON objects. Search methods described in the Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001] also make use of the REST API, but are \u201cwrapped\u201d in the portal's user interface for users who prefer to interact visually.\nFacets (see Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001], step 5) represent a subset of commonly used query parameters. However, any property of any object type can potentially be used as a query parameter. These properties and object types are listed comprehensively on the ENCODE portal at https://www.encodeproject.org/profiles/?[href=https://www.encodeproject.org/profiles/?].\nNecessary Resources\nHardware\nComputer with internet access\nSoftware\nUp-to-date web browser\nCommand line terminal\nCommand line utilities:\n               \ncurl: https://curl.haxx.se/[href=https://curl.haxx.se/]\nA text editor\n1. Open any text editor and write down the first part of a query, which begins with https://www.encodeproject.org/search/?[href=https://www.encodeproject.org/search/?].\nAt the end of this protocol, the completed query will be entered into a web browser or a terminal command.\n2. Add the parameter type=Biosample to the query. All parameters take the format property=value. This example uses the \u201ctype\u201d property with a value of \u201cBiosample.\u201d All objects share the \u201ctype\u201d property, which indicates what category the object belongs to. The full query is now: https://www.encodeproject.org/search/?type=Biosample[href=https://www.encodeproject.org/search/?type=Biosample].\n         \nThis query would return a list of all biosample objects on the portal.\nThe list of all object types is available on the Schemas page, which can be accessed by clicking the Materials & Methods drop-down menu in the top toolbar and then clicking Schemas.3. Add another parameter to the query by first appending \u201c&\u201d to the end of the query. Then, add the life_stage=adult parameter to the end. Add another parameter, biosample_ontology.organ_slims=eye, to filter by organ. All parameters will take the same format as the first one. The full query is now: https://www.encodeproject.org/search/?type=Biosample&life_stage=adult&biosample_ontology.organ_slims=eye[href=https://www.encodeproject.org/search/?type=Biosample&life_stage=adult&biosample_ontology.organ_slims=eye].\n         \nDifferent object types have different properties. From the Schemas page (see step 2), click on any of the objects to view a list of their properties.\nOther syntax tips and special parameters, as listed in Table 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-tbl-0003], exist which can also be utilized for querying.\nTable 3.\n                Syntax for Query Building\ntable:\n\ufeffSyntax,Parameter example,Description\n=,assay_title=TF ChIP-seq,The equal symbol (=) connects a property to its value\n&,assay_title=DNase-seq&assembly=mm10,The ampersand (&) symbol joins multiple parameters together\n!=,assembly!=hg19,\"!= represents \u201cnot equals\u201d and acts as a negation. In the example shown, the parameter would filter for objects with reference genome assemblies which are not hg19. This is equivalent to clicking the red exclusion icon in step 11 of the Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001].\"\n*,treatment=*,The wildcard (*) means the parameter can have any value\n.,biosample_ontology.term_name=HepG2,\"The data model of the ENCODE portal allows for certain objects to be embedded in others. Objects are able to access the properties of the objects embedded in them. The period joins properties and sub-properties to form the \u201cpath\u201d to an embedded property, akin to how the forward slash (/) is used in file directory paths. In this example, the biosample_ontology object is embedded in experiment objects, and this parameter accesses the term_name property of biosample_ontology.\"\nformat,format=json,format is a special property. Appending this to the query returns the page as a raw JSON object. The default value is HTML.frame,frame=embedded,\"frame is a special property indicating how the ENCODE database should return a requested object. Some examples of values include:  Object: No objects are embedded Embedded: All objects are embedded Raw: Object links are in UUID (Universally Unique Identifier) format, rather than the default @id format\"\n4. Request the query by entering the following command, which uses curl to fetch a URL, into the command line terminal: curl -H \u201cAccept: application/json\u201d https://www.encodeproject.org/search/?type=Biosample&life_stage=adult&biosample_ontology.organ_slims=eye\nAfter entering the command, the record for the biosample search should print directly to the terminal window. Like all objects on the portal, this record is a JSON object. Examples of terminal outputs are available at https://app.swaggerhub.com/apis-docs/encodeproject/api/basic_search#/[href=https://app.swaggerhub.com/apis-docs/encodeproject/api/basic_search#/].\nOther command-line utilities can also be used here. Some programming languages also have suitable libraries for this purpose.\n5. Alternatively, enter the query URL into a web browser to display the output as a list resembling the search page shown in Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.89#cpbi89-prot-0001], step 4.", "Step-by-step method details\nStep-by-step method details\n      Chromatin-associated RNA extraction with spike-in control\n    \nTiming: \u223c1\u20132\u00a0days\n    \n      Prepare chromatin-associated RNA according to protocol published\n      previously.13[href=https://www.wicell.org#bib13]\n        Pretreat mES cells with 1\u00a0\u03bcg/mL doxycycline for 12\u2009h, treat RPB3 or\n        dRPB3 degron mES cells with or without 500\u00a0\u03bcM indole-3-acetic acid\n        (auxin/IAA).\n      \n        Collect approximately 10\u02c67 cells and mix them with 10% Drosophila S2\n        cells as spike-in control.\n      \n        Lyse the cells in ice-cold NP-40 lysis buffer for 5\u00a0min (10\u00a0mM Tris-HCl\n        (pH 7.5), 150\u00a0mM NaCl, 0.05% NP-40, 1\u00a0\u00d7\u00a0proteinase[href=https://www.sciencedirect.com/topics/biochemistry-genetics-and-molecular-biology/proteases]\n        inhibitor cocktail).\n      \n        Cell lysates were gently put on the top of the sucrose cushion (24%\n        sucrose in NP-40 lysis buffer), centrifuged at 12,000 g for\n        10\u00a0min. Perform sucrose cushion precipitation to isolate nuclei. Discard\n        the supernatants representing the cytoplasm and resuspend the pellets in\n        0.5\u00a0mL glycerol buffer.\n      \n        Add 0.5\u00a0mL nuclei lysis buffer to lyse the nuclei and incubate on ice\n        for 2\u00a0min.\n      \n        Centrifuge the mixture and discard the supernatant representing the\n        nucleoplasm fraction.\n      \nDiscard the supernatant and washed twice with ice-cold PBS.\n        Extract the RNA from the chromatin pellet using TRIzol reagent according\n        to the manufacturer\u2019s manual. Troubleshooting 1[href=https://www.wicell.org#sec7.1].\n      \n        Send the RNA samples to Novogene for ribosomal RNA depletion,\n        strand-specific library construction, and sequencing.\n      \nAlternatives: Cells from other\n      species can also be used for normalization. We recommend using higher\n      percentage of spike-in controls, such as 10%\u201320%.\n    \nNote: In 1\u20139 steps, different samples\n      should be processed at the same time by the same reagents and equipment to\n      reduce batch effects.\n    \nQuality control and sequence alignment\nTiming: \u223c10 h\n      The workflow for ChAR-seq data processing is shown in\n      Figure\u00a02[href=https://www.wicell.org#fig2].\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2758-Fig2.jpg\n          Figure\u00a02. Overview of the ChAR-seq data-processing pipeline\n        \n      The minimum computational requirement: this pipeline requires at least 10\n      GB of RAM.Trim adapter and low-quality bases from the raw sequencing data using\n        the cutadapt tool (v2.10) with customized settings. Low-quality bases\n        with a Phred quality score of <15 will be trimmed and reads with a\n        final length of <25\u00a0bp will be discarded.\n      \n#!/bin/bash\n# Step 10: Trim adapters from paired-end FASTQ files.\n# Note: Make sure cutadapt is installed before running this\n          script.\n# Set variables for the example of dRPB3_IAA_0h_rep1\noutputdir=\"/path/to/output_directory\"\nforward_adapt=\"CCCCCCCCCAGATCGGAAGAGCACACGTCTGAACTCCAGTCAC\"\nreverse_adapt=\"AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT\"\nmin_length=25\nprefix=\"dRPB3_IAA_0h_rep1\"\nfastq1=\"dRPB3_IAA_0h_rep1_R1.fq.gz\"\nfastq2=\"dRPB3_IAA_0h_rep1_R2.fq.gz\"\n# Create directory for cutadapt\nmkdir -p \"${outputdir}/cutadapt\"\n# Run cutadapt\ncutadapt -a \"${forward_adapt}\" \u2216\n        \u00a0\u00a0\u00a0\u00a0-A \"${reverse_adapt}\" \u2216\n\u00a0\u00a0\u00a0\u00a0-q 15,15 \u2216\n\u00a0\u00a0\u00a0\u00a0--overlap 1 \u2216\n\u00a0\u00a0\u00a0\u00a0-m \"${min_length}\" \u2216\n        \u00a0\u00a0\u00a0\u00a0-o \"${outputdir}/cutadapt/${prefix}_1.fq\" \u2216\n        \u00a0\u00a0\u00a0\u00a0-p \"${outputdir}/cutadapt/${prefix}_2.fq\" \u2216\n        \u00a0\u00a0\u00a0\u00a0\"${fastq1}\" \"${fastq2}\"\nCheck the quality of the trimmed reads using fastqc (v0.3.0).\n# Step\n11: Run FastQC to check the quality of the trimmed reads\n# Note: Step\n11 is part of a large script with the previous step.\n# Variables are defined earlier in the same script.\n# create directory for cutadapt output and quality control\n          results\nmkdir -p \"${outputdir}/cutadapt/QC\"\n# run FastQC on the cutadapt output files\nfastqc \"${outputdir}/cutadapt/${Prefix}_1.fq\"\n          \"${outputdir}/cutadapt/${Prefix}_2.fq\" \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-o \"${outputdir}/cutadapt/QC\"\n        Download rRNA sequences for Mus musculus from GenBank with\n        accession number BK000964.3.\n      \n        Build an index and map the trimmed reads to the rRNA sequence using the\n        bowtie2 aligner (v2.3.5.1).\n      \n# Note: Steps 12 and 13 are parts of a large script with the previous\n          step.\n# Variables are defined earlier in the same script.\n# create directory for genome sequence\nmkdir -p \"${outputdir}/genome/ \"\n# Define variables\nbowtie2_index=\"${outputdir}/genome/mm10_rRNA_bowtie2_index\"\nrRNA_accession=\"BK000964.3\"\n# Step 12: Download the rRNA sequence from GenBank\n# Note: The 'efetch' command is used to download sequences\n          from GenBank based on the accession number.\nefetch -db nucleotide -id \"${rRNA_accession}\" -format\n          fasta\u00a0>\u00a0\"${outputdir}/genome/mm10_rRNA.fasta\"\n# Step13: Build the index and map trimmed reads to the rRNA sequence and\n          keep unmapped reads\nbowtie2-build \"${outputdir}/genome/mm10_rRNA.fasta\"\n          \"${bowtie2_index}\"\nbowtie2 -x \"${bowtie2_index}\" -U\n          \"${trimmed_reads}\" \u2216\n        \u00a0\u00a0\u00a0\u00a0--un \"${outputdir}/unmapped_reads.fastq\" \u2216\n        \u00a0\u00a0\u00a0\u00a0-S \"${outputdir}/mapped_reads.sam\"\n        Map the unmapped reads from the output of bowtie2 to the concatenated\n        reference genome comprised of mouse and Drosophila chromosomes (mm10\u00a0+\n        dm6) using bowtie2 with options (-very-sensitive-local).\n        \n            Download the mouse (GRCm38/mm10) and Drosophilareference genome\n            sequences (dm6) from the UCSC genome browser and saved as\n            \u201cmm10.fa.gz\u201d and \u201cdm6.fa.gz\u201d, respectively.\n          \nUncompress \u201cmm10.fa.gz\u201d and \u201cdm6.fa.gz\u201d.\n            Because both mouse and Drosophila genomes have chromosome X (named\n            \u201cchrX\u201d) and chromosome Y (named \u201cchrY\u201d), we need to modify the\n            Drosophila chromosome IDs to make them different from that of\n            the mouse. In this case, we add \u201cdm6_\u201d to each chromosome ID of the\n            Drosophila. For example, we change Drosophila\u2019s \u201cchrX\u201d\n            into \u201cdm6_chrX\u201d, \u201cchr2L\u201d into \u201cdm6_chr2L\u201d, and so forth.\n          \n            Concatenate the mouse (mm10) and Drosophilareference genome\n            sequences (mm10) into a single FASTA file\u00a0\u2013 the composite reference\n            genome named \u201cmm10_dm6.fa\u201d.\n          \nIndex the \u201cmm10_dm6.fa\u201d using the \u201cbowtie2-build\u201d command.\n            Map reads to the composite reference genome using Bowtie2. Using\n            Samtools, convert the output alignments into BAM format, sort the\n            BAM files by position, and then index them.\n          \n            Repeat step f for all biological replicates of ChAR-seq data in\n            RPB3- and dRPB3-degron cells before and after IAA treatment.\n          \n# Note: Steps\n14(a)-(e) are parts of a large script with the previous step.\n# Variables are defined earlier in the same script.\n# Define variables\nunmapped_reads=\"${outputdir}/unmapped_reads.fastq\"\nmouse_reference=\"mm10.fa\"\ndrosophila_reference=\"dm6.fa\"\ncomposite_reference=\"mm10_dm6.fa\"\nbowtie2_index=\"${outputdir}/mm10_dm6_bowtie2_index\"\n# Step\n14a: Download the mouse and Drosophila reference genome sequences\n          from the UCSC genome browser\nwget\n          http://hgdownload.soe.ucsc.edu/goldenPath/mm10/bigZips/${mouse_reference}.gz\n          -P \"${outputdir}\"\nwget\n          http://hgdownload.soe.ucsc.edu/goldenPath/dm6/bigZips/${drosophila_reference}.gz\n          -P \"${outputdir}\"\n# Step\n14b: Uncompress the reference genome sequences\ngunzip \"${outputdir}/${mouse_reference}.gz\"\ngunzip \"${outputdir}/${drosophila_reference}.gz\"\n# Step\n14c: Modify the Drosophila chromosome IDs\nsed -i 's/\u02c6>chr\u2216([0-9A-Za-z]\u2216)/>dm6_chr\u22161/'\n          \"${outputdir}/${drosophila_reference}\"\n# Step14d: Concatenate the mouse and Drosophila reference genome sequences\n          into a single FASTA file\ncat \"${outputdir}/${mouse_reference}\"\n          \"${outputdir}/${drosophila_reference}\"\u00a0>\u00a0\"${outputdir}/${composite_reference}\"\n# Step 5e: Index the composite reference genome using the\n          bowtie2-build command\nbowtie2-build \"${outputdir}/${composite_reference}\"\n          \"${outputdir}/${bowtie2_index}\"\n        Remove unmapped or unpaired reads and sort and generate indexes of\n        filtered bam files using sambamba (v0.7.0) and SAMtools (v0.1.19).\n      \n        Classify reads by splitting the \"composite\" BAM files into\n        mouse and Drosophila BAM files.\n      \n# Note: Steps 15 and\n16 are parts of a large script with the previous step.\n# Variables are defined earlier in the same script.\n# create directory for mapping results\nmkdir -p \"${outputdir}/mapping\"\n# map reads to reference and spike-in genome using Bowtie2\nbowtie2 -q -x \"${index}\" --very-sensitive-local --reorder\n          -1 \"${outputdir}/cutadapt/${Prefix}_1.fq\" -2\n          \"${outputdir}/cutadapt/${Prefix}_2.fq\" -p 15 -S\n          \"${outputdir}/mapping/${Prefix}.sam\" 2>\n          \"${outputdir}/mapping/${Prefix}_mapping.stat\"\n# Step 15: mark duplicates using sambamba\n# Note: Sambamba is used to mark duplicates in the mapped\n          reads.\n# The '-r' option removes secondary and supplementary\n          alignments, '-t' specifies the number of threads.\nsambamba markdup -r -t 10\n          \"${outputdir}/mapping/${Prefix}.bam\"\n          \"${outputdir}/mapping/${Prefix}_rmdup.bam\"\n# Step\n15: sort the SAM file and convert it to BAM format\n# Note: The SAM file is sorted and converted to BAM format using\n          samtools mentioned in Step\n15.\n# The '-@' option specifies the number of threads,\n          '-o' specifies the output BAM.\nsamtools sort -@ 10 -o \"${outputdir}/mapping/${Prefix}.bam\"\n          \"${outputdir}/mapping/${Prefix}.sam\"\n# extract unique and concordant pairs from the BAM file\necho \"Extracting unique and concordant pairs...\"\nsamtools view -@ 5 -hF 4\n          \"${outputdir}/mapping/${Prefix}_rmdup.bam\" | grep -v\n          \"XS:\" | samtools view -@ 5 -bS -o\n          \"${outputdir}/mapping/${Prefix}_unique.bam\"\nsamtools view -H\n          \"${outputdir}/mapping/${Prefix}_rmdup.bam\"\u00a0>\u00a0\"${outputdir}/mapping/header.txt\"\nsamtools view -@ 5 -hF 4\n          \"${outputdir}/mapping/${Prefix}_unique.bam\" | grep\n          \"YT:Z:CP\" | cat \"${outputdir}/mapping/header.txt\"\n          - | samtools view -@ 5 -bS -o\n          \"${outputdir}/mapping/${Prefix}_unique_concordant.bam\"\nsamtools index\n          \"${outputdir}/mapping/${Prefix}_unique_concordant.bam\"\n# Step\n16: split the \"composite\" BAM files into mouse andDrosophila BAM files\necho \"Filtering out reads that map to non-target\n          regions...\"\nsamtools view -h\n          \"${outputdir}/mapping/${Prefix}_unique_concordant.bam\"\n          $(echo dm6_chr{2L,2R,3L,3R,4,X,Y,M}) -o\n          \"${outputdir}/mapping/${Prefix}_unique_concordant_dm6.bam\"\nsamtools view -h\n          \"${outputdir}/mapping/${Prefix}_unique_concordant.bam\"\n          $(echo chr{{1..19},X,Y,M}) -o\n          \"${outputdir}/mapping/${Prefix}_unique_concordant_mm10.bam\"\n        Count reads originating from Drosophila S2 spike-in cells and calculate\n        the calibration factors (alpha=1e6/dm6_count) for reads that mapped to\n        the mouse genome.\n      \n>scale_factor=$(echo 1000000/${fly_num} | bc -l)\n        Assign plus and minus strands to uniquely mapped reads using SAMtools.\n      \n# Step\n18: Use SAMtools to assign plus and minus strands for uniquely mapped\n          reads\n# Note: Step\n18 is part of a large script with the previous step.\n# Variables are defined earlier in the same script.\n# Create output directory\nmkdir -p \"$output_dir/mapping/split_strand\"\n# Split bam file by strand\nsamtools view -@ 10 -hf 80 -F 32 \"$input_bam\" -o\n          \"$output_dir/mapping/split_strand/${prefix}_r1_rev.bam\"\nsamtools view -@ 10 -hf 96 -F 16 \"$input_bam\" -o\n          \"$output_dir/mapping/split_strand/${prefix}_r1_forw.bam\"\nsamtools view -@ 10 -hf 144 -F 32 \"$input_bam\" -o\n          \"$output_dir/mapping/split_strand/${prefix}_r2_rev.bam\"\nsamtools view -@ 10 -hf 160 -F 16 \"$input_bam\" -o\n          \"$output_dir/mapping/split_strand/${prefix}_r2_forw.bam\"\n# Merge and sort strand-specific bam files\nsamtools merge -f -@ 5\n          \"$output_dir/mapping/split_strand/${prefix}_plus.bam\"\n          \"$output_dir/mapping/split_strand/${prefix}_r1_rev.bam\"\n          \"$output_dir/mapping/split_strand/${prefix}_r2_forw.bam\"\nsamtools sort -@ 5 -o\n          \"$output_dir/mapping/split_strand/${prefix}_plus_sorted.bam\"\n          \"$output_dir/mapping/split_strand/${prefix}_plus.bam\"\nsamtools index\n          \"$output_dir/mapping/split_strand/${prefix}_plus_sorted.bam\"\nrm\n          \"$output_dir/mapping/split_strand/${prefix}_plus.bam\"\nsamtools merge -f -@ 5\n          \"$output_dir/mapping/split_strand/${prefix}_minus.bam\"\n          \"$output_dir/mapping/split_strand/${prefix}_r1_forw.bam\"\n          \"$output_dir/mapping/split_strand/${prefix}_r2_rev.bam\"\nsamtools sort -@ 5 -o\n          \"$output_dir/mapping/split_strand/${prefix}_minus_sorted.bam\"\n          \"$output_dir/mapping/split_strand/${prefix}_minus.bam\"\nsamtools index\n          \"$output_dir/mapping/split_strand/${prefix}_minus_sorted.bam\"\nrm\n          \"$output_dir/mapping/split_strand/${prefix}_minus.bam\"\n        Convert final BAM files to bigWig tracks in consecutive 10\u00a0bp throughout\n        the genome, separated by strand, and normalize to spike-in controls\n        using bamCoverage from deeptools.\n      \n# Step\n19: Generate bigWig files for plus and minus strand reads\n# Note: Step\n19 is part of a large script with the previous step.\n# Variables are defined earlier in the same script.\nbamCoverage --bam\n          ${outputdir}/mapping/split_strand/${Prefix}_plus_sorted.bam \u2216\n-o ${outputdir}/bigwig/${Prefix}_plus_spikein.bw \u2216\n--binSize 10 \u2216\n--normalizeUsing None \u2216\n--scaleFactor ${scale_factor} \u2216\n--effectiveGenomeSize 2652783500 \u2216\n-p 5\nbamCoverage --bam\n          ${outputdir}/mapping/split_strand/${Prefix}_minus_sorted.bam \u2216\n-o ${outputdir}/bigwig/${Prefix}_minus_spikein.bw \u2216\n--binSize 10 \u2216--normalizeUsing None \u2216\n--scaleFactor ${scale_factor} \u2216\n--effectiveGenomeSize 2652783500 \u2216\n-p 5\n        Merge the bigWig score tracks from two biological replicates to an\n        averaged signal for visualization using the mean operator from the\n        WiggleTools (v1.2) package.\n      \n# Note: Step\n20 is standalone and function independently.\n# Variables are defined within this script.\n#Define the list of samples, stages and strands as arrays\nsamples=(\"RPB3_IAA\" \"dRPB3_IAA\")\nstages=(\"0h\" \"1h\" \"3h\")\nstrands=(\"plus\" \"minus\")\ngenome_file=\"mm10_dm6.genome.txt\"\n# Use arrays to loop through the values\nfor sample in \"${samples[@]}\"\ndo\n\u00a0\u00a0for stage in \"${stages[@]}\"\n\u00a0\u00a0do\n        \u00a0\u00a0\u00a0\u00a0for strand in \"${strands[@]}\"\n\u00a0\u00a0\u00a0\u00a0do\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Use variables to make the command more readable\n          input_file_1=\"${output_dir}/bigwig/${sample}_${stage}_1_${strand}_spikein.bw\"\ninput_file_2=\"${output_dir}/bigwig/${sample}_${stage}_2_${strand}_spikein.bw\"\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0output_file=\"${sample}_${stage}_${strand}_spikein.bw\"\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Note: The 'wiggletools mean' calculates the mean of the\n          bigWig files, and the output is redirected to 'wigToBigWig'\n          along with the genome file and output file path. wigToBigWig\n          <(wiggletools mean \"$input_file_1\"\n          \"$input_file_2\") \"$genome_file\"\n          \"${output_dir}/bigwig/${output_file}\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0done\n\u00a0\u00a0\u00a0\u00a0done\ndone\nEvaluate ChAR-seq library with summary statistics\nTiming: \u223c4 h\n      Evaluate the ChAR-seq sequencing library with mapped reads distribution,\n      RNA fragment size and metagene profile (Figure\u00a03[href=https://www.wicell.org#fig3]).\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2758-Fig3.jpg\n          Figure\u00a03. Assessment of ChAR-seq library\n        \n          (A) Distribution of uniquely mapped read pairs over gene features\n          (5\u2032UTR, CDS, 3\u2032UTR, exon, and intron) across six merged ChAR-seq\n          samples.\n        \n          (B) Distribution of fragment sizes of different samples, inferred from\n          the separation of read pairs using RSeQC.\n        \n          (C) Metagene profiles of normalized ChAR-seq reads in gene bodies at\n          each time point after IAA treatment. The vertical dashed lines\n          indicate the magnified portions of TSS (transcription start site,\n          left) and TTS (transcription termination site, right) regions.\n          ChAR-seq densities in the sense and antisense directions are indicated\n          by solid and dashed lines, respectively.\n        \nFigure\u00a0adapted from Li et\u00a0al.2[href=https://www.wicell.org#bib2]\n        Inspect the distribution of mapped reads and read density over gene\n        features using the read_distribution.py module from RSeQC.\n      \n        Prioritize gene features as follows: CDS exons\u00a0>\u00a0UTRexons\u00a0>\u00a0Introns.\n      \n        Calculate the inner distance between read pairs to estimate the RNA\n        fragment size using the inner_distance.py module.\n      \n        Record the calculated mean, median, and standard deviation values for QC\n        plots.\n      \n# Note: Steps 21 and\n23 are standalone and function independently.\n# Variables are defined within this script.\n# Define the list of samples and stages as arrays\nsamples=(\"RPB3_IAA\" \"dRPB3_IAA\")\nstages=(\"0h\" \"1h\" \"3h\")\n# Loop through each sample and stage\nfor sample in \"${samples[@]}\"\ndo\n\u00a0\u00a0for stage in \"${stages[@]}\"\n\u00a0\u00a0do\n        \u00a0\u00a0\u00a0\u00a0# Define the input and output filenames\n        \u00a0\u00a0\u00a0\u00a0input_bam=\"${sample}_${stage}.bam\"\n        \u00a0\u00a0\u00a0\u00a0output_inner=\"${sample}_${stage}.inner_distance\"\n# Step\n21: Inspect the distribution of mapped reads and read density over\n          gene features\n# Note: The 'read_distribution.py' module from RSeQC is used\n          to analyze the distribution of mapped reads over gene features.\n        \u00a0\u00a0\u00a0\u00a0read_distribution.py -i \"$input_bam\" -r mm10_RefSeq.bed\n          >> read_distribution.txt\n# Step\n23: Calculate inner distance between read pairs to estimate RNA\n          fragment size\n# Note: The 'inner_distance.py' module from RSeQC is used to\n          calculate the inner distance between read pairs.\n        \u00a0\u00a0\u00a0\u00a0inner_distance.py -i \"$input_bam\" -o\n          \"$output_inner\" -q 0 -r mm10_RefSeq.bed\n\u00a0\u00a0done\ndone\n        Report the number of aligned reads and the alignment percentage as the\n        alignment statistics (Table 1[href=https://www.wicell.org#tbl1])\n        table:files/protocols_protocol_2758_5.csv\n# Step 25: Record alignment statistics\n# Note: Step\n25 is part of a large script along with the step\n19.\n# Variables are defined earlier in the same script.\n# Count the number of reads using the appropriate commands\ncutadapt=$(grep \"\u02c6@\" ${outputdir}/cutadapt/${Prefix}_1.fq |\n          wc -l)\nmapping=$(samtools view -F 4 ${outputdir}/mapping/${Prefix}.bam | wc\n          -l)\nrmdup=$(samtools view -F 4 ${outputdir}/mapping/${Prefix}_rmdup.bam |\n          wc -l)\nunique=$(samtools view -c\n          ${outputdir}/mapping/${Prefix}_unique.bam)\nconcordant=$(samtools view -c\n          ${outputdir}/mapping/${Prefix}_unique_concordant.bam)\nmm10_concord=$(samtools view -c\n          ${outputdir}/mapping/${Prefix}_unique_concordant_mm10.bam)\ndm6_concord=$(samtools view -c\n          ${outputdir}/mapping/${Prefix}_unique_concordant_dm6.bam)\n# Write the statistics to the stastic file\necho -e \"sample_name,total_reads,cutadapt,mapping reads,rm\n          duplicate,unique mapping,concordant pairs,mm10 concordant pairs,dm6\n          concordant pairs\" >>\n          ${outputdir}/${Prefix}_stastic.csv\necho -e\n          \"${Prefix},${total_reads},${cutadapt},${mapping},${rmdup},${unique},${concordant},${mm10_concord},${dm6_concord}\"\n          >> ${outputdir}/${Prefix}_stastic.csvGenerate strand-specific metagene, TSS and TTS profiles from a BAM file\n        using \"ngs.plot\".\n        \n            Set working directories, sample information and their corresponding\n            BAM file location.\n            \n#!/bin/bash\n# Note: Step 26a are standalone and function independently.\n# Variables are defined within this script.\n# Set working directory, temporary directory, and\n                  metaprofiles directory\nWORKDIR=\"/path/to/my/working_directory/\"\n# Note: $WORKDIR is where the BAM files to be plotted are\n                  stored.\nTMPDIR=\"${WORKDIR}tmp/\"\nPROFDIR=\"${WORKDIR}metaprofiles/\"\n# Create temporary and metaprofiles directories\nmkdir -p \"$TMPDIR\"\nmkdir -p \"$PROFDIR\"\n# Define the sample name and input BAM file\nSAMPLE=\"dRPB3_0h\"\nBAM=\"${WORKDIR}dRPB3_0h.bam\"\n# Define the output BAM file with mate pairs\nMATE1=\"${WORKDIR}dRPB3_0h.mate1.bam\"\n            Restrict to the first mate reads. If the BAM file contains paired\n            reads, create a new file containing only the first mate reads.\n          \n            Reformat the chromosome names in the BAM file to match those in the\n            ngs.plot database.\n            \n# Step\n26c: Reformat the chromosome names\n# Note: Step\n26c is part of a large script along with the step\n26a.\n# Variables are defined earlier in the same script.\n# Define output file names and the number of threads\nMATE1REHEADER=\"${WORKDIR}/dRPB3_0h.mate1.reheader.bam\"\nTHREADS=10\n# Convert BAM to SAM, filter by flag 64, and convert\n                  back\nsamtools view --threads $THREADS -h -b -f 64 $BAM -o\n                  $MATE1\n# Index the BAM file\nsamtools index $MATE1\n# Modify the header of the BAM file to replace chromosome\n                  numbers with \"chr\" prefix\nsamtools view --threads $THREADS -H ${MATE1} | sed -e\n                  's/SN:\u2216([0-9XY]\u2217\u2216)/SN:chr\u22161/' -e\n                  's/SN:MT/SN:chrM/' | samtools reheader -\n                  ${MATE1}\u00a0>\u00a0${MATE1REHEADER}\n# Index the reheadered BAM file\nsamtools index ${MATE1REHEADER}\n            Run ngs.plot to generate sense and anti-sense profiles for specific\n            regions of interest using the correct BAM file. Note that if the\n            libraries were produced such that mate2 represents the forward\n            strand, the sense and anti-sense profiles would be reversed.\n            \n# Note: Step26d is part of a large script along with the step\n26a and 26c.\n# Variables are defined earlier in the same script.\n## Genebody\nREGION=\"genebody\"\nfor STRAND in both same opposite\ndo\n                \u00a0\u00a0OUTPUT=\"${PROFDIR}${SAMPLE}.${REGION}.${STRAND}\"\nngs.plot.r -G mm10 -R $REGION -C ${MATE1REHEADER} -O $OUTPUT\n                  -P 10 -SS $STRAND -SE 1 -L 5000 -F charseq -GO total -RB 0.05\n                  -AL bin -CS 500 -FL 150 -D ensembl\ndone\n## TSS\nREGION=\"tss\"\nfor STRAND in both same opposite\ndo\n                \u00a0\u00a0OUTPUT=\"${PROFDIR}${SAMPLE}.${REGION}.${STRAND}\"\nngs.plot.r -G mm10 -R $REGION -C ${MATE1REHEADER} -O $OUTPUT\n                  -P 10 -SS $STRAND -SE 1 -L 5000 -F charseq -GO total -RB 0.05\n                  -AL bin -CS 500 -FL 150 -D ensembl\ndone\n## TTS\nREGION=\"tts\"\nfor STRAND in both same opposite\ndo\n                \u00a0\u00a0OUTPUT=\"${PROFDIR}${SAMPLE}.${REGION}.${STRAND}\"\nngs.plot.r -G mm10 -R $REGION -C ${MATE1REHEADER} -O $OUTPUT\n                  -P 10 -SS $STRAND -SE 1 -L 5000 -F charseq -GO total -RB 0.05\n                  -AL bin -CS 500 -FL 150 -D ensembl\ndone\n            Use R to combine the sense and anti-sense profiles on a single set\n            of axes by utilizing the ngs.plot output.\n          \n            Scale the profile by employing the calculated calibration factors in\n            step 17.\n            Edit the resulting \".cnt\" file to multiply the read count\n            by the calibration factor, and re-run ngs.plot using the same\n            parameters as before with the modified one.\n            \n# This R code refers to plots in\nFigure\u00a03[href=https://www.wicell.org#fig3]\n# Load required packages\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(ggpubr)\nlibrary(readxl)\n# Read the Excel file and remove missing values for\nFigure\u00a03[href=https://www.wicell.org#fig3]A\n              \ndf <- na.omit(read_excel(\"protocol_stats.xlsx\",\n                  sheet\u00a0= 1, trim_ws\u00a0= FALSE, range\u00a0= cell_cols(c(\"A\",\n                  \"E\"))))\n# Reshape the data using the melt function from the reshape2\n                  package\ndf_long <- reshape2::melt(df, id\u00a0= \"Samples\",\n                  value.name\u00a0= \"count\")\n# Round the count values to the nearest integer\ndf_long$count <- round(df_long$count, 0)\n# Create a grouped bar plot using ggbarplot from ggpubrp1\u00a0<- ggbarplot(data\u00a0= df_long, x\u00a0= \"Samples\",\n                  y\u00a0= \"count\",\n                \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0fill\u00a0= \"variable\", palette\u00a0= c('#3494BAFF',\n                  '#58B6C0FF', '#75BDA7FF',\n                  '#666666FF'),\n                \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0position\u00a0= position_dodge(0.7),\n                \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Set background and line colors\n                \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0background.color\u00a0= \"white\", color\u00a0= NA,\n                \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0xlab\u00a0= \"\", ylab\u00a0= \"FPKM\",\n                \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0remove\u00a0= \"all\",\n                \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0axis.line\u00a0= element_line(color\u00a0= \"black\")\n)\u00a0+ ggpubr::theme_pubr()\n# Save the plot as a PDF file\nggsave(p1, filename\u00a0=\n                  \"Figure\u00a03A_gene_features.pdf\", width\u00a0= 6, height\u00a0=\n                  4)\n# Read the Excel file and remove missing values for\nFigure\u00a03[href=https://www.wicell.org#fig3]B\n              \ndf <- na.omit(read_excel(\"protocol_stats.xlsx\",\n                  sheet\u00a0= 2, trim_ws\u00a0= FALSE, range\u00a0= cell_cols(c(\"F\",\n                  \"J\"))))\n# Create the second plot\np2\u00a0<- ggplot(df, aes(x\u00a0= Samples, y\u00a0= Mean, fill\u00a0=\n                  Group))\u00a0+\n                \u00a0\u00a0geom_boxplot(aes(lower\u00a0= Mean - SD, upper\u00a0= Mean\u00a0+ SD,\n                  middle\u00a0= Median, ymin\u00a0= Mean - 3 \u2217 SD, ymax\u00a0= Mean\u00a0+ 3 \u2217\n                  SD),\n                \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0stat\u00a0= \"identity\", width\u00a0= 0.8)\u00a0+\n                \u00a0\u00a0stat_summary(fun.y\u00a0= mean, geom\u00a0= \"point\", shape\u00a0=\n                  20, size\u00a0= 3, color\u00a0= \"pink\", fill\u00a0=\n                  \"pink\")\u00a0+\n                \u00a0\u00a0scale_fill_manual(values\u00a0=\n                  c(\"#E64B35FF\",\"#4DBBD5FF\",\"#00A087FF\"))+\n\u00a0\u00a0ggpubr::theme_pubr()\n# Save the box plot as a PDF file\nggsave(p2, filename\u00a0= \"Figure\u00a03B_insert size.pdf\",\n                  width\u00a0= 5, height\u00a0= 4)\n# The following R code refers to metagene plot for dRPB3\n                  in\nFigure\u00a03[href=https://www.wicell.org#fig3]C\n              \n# Note: similar code can be employed for RPB3.\n# Define input files for\nFigure\u00a03[href=https://www.wicell.org#fig3]C\n              \nsense_profile <- \"dRPB3_sense_avgprof.txt\"\nantisense_profile <-\n                  \"dRPB3_antisense_avgprof.txt\"\n# Read sense and anti-sense profiles from ngs.plot\n                  output\nsense <- read.table(sense_profile, header\u00a0= TRUE)\nantisense <- read.table(antisense_profile, header\u00a0=\n                  TRUE)\n# Plot the combined profiles\npdf(\"Figure\u00a03C_metagene_dRPB3.pdf\", 8,8)\nplot(sense$position, sense$IAA_0h, type\u00a0= \"l\",\n                  col\u00a0= \"#1B9E77\", lwd\u00a0= 2,\n                \u00a0\u00a0xlab\u00a0= \"Position\", ylab\u00a0= \"Value\", main\u00a0=\n                  \"Combined Profiles\")\nlines(sense$position, sense$IAA_1h, type\u00a0= \"l\",\n                  col\u00a0= \"#D95F02\", lwd\u00a0= 2)\nlines(sense$position, sense$IAA_3h, type\u00a0= \"l\",\n                  col\u00a0= \"#7570B3\", lwd\u00a0= 2)lines(antisense$position, antisense$IAA_0h, type\u00a0=\n                  \"l\", col\u00a0= \"#1B9E77\", lwd\u00a0= 2)\nlines(antisense$position, antisense$IAA_1h, type\u00a0=\n                  \"l\", col\u00a0= \"#D95F02\", lwd\u00a0= 2)\nlines(antisense$position, antisense$IAA_3h, type\u00a0=\n                  \"l\", col\u00a0= \"#7570B3\", lwd\u00a0= 2)\nlegend(\"topright\", legend\u00a0= c(\"Sense\n                  IAA_0h\", \"Sense IAA_1h\", \"Sense\n                  IAA_3h\",\n                \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Anti-sense IAA_0h\", \"Anti-sense IAA_1h\",\n                  \"Anti-sense IAA_3h\"),\n                \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0col\u00a0= c(\"#1B9E77\", \"#D95F02\",\n                  \"#7570B3\", \"#1B9E77\", \"#D95F02\",\n                  \"#7570B3\"), lty\u00a0= 1, lwd\u00a0= 2)\ndev.off()\n      Construct gene loci annotation and down-sample the resulting bam files\n    \nTiming: \u223c6 h\n        Download the mouse gene annotation GTF file from \u201cComprehensive gene\n        annotation\u201d in the \u201cGTF/GFF3 files\u201d table named\n        \u201cgencode.vM23.annotation.gtf.gz\u201d, which is the main annotation file for\n        most users.\n      \n> wget -c\nhttps://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M23/gencode.vM23.annotation.gtf.gz[href=https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_mouse/release_M23/gencode.vM23.annotation.gtf.gz]\nUnzip gene annotation files.\n> gunzip gencode.vM23.annotation.gtf.gz\n        Check the strandedness of comma-separated GTF files, and use it as input\n        to generate a gene loci annotation file.\n      \n> Get_loci_annotation -out ./gencode_mm10_vM23 -gtf\n          gencode.vM23.annotation.gtf\nNote: The output file is named\n      loci_annotation.bed, to be used as input for Get_DoGs\n    \n        Preprocess, sort and index the input bam files to keep their order\n        constant.\n      \n#!/bin/bash\n# Note: Step\n30 are standalone and function independently.\n# Variables are defined within this script.\n# Define the list of samples, stages, and repeats as arrays\nsamples=(\"RPB3_IAA\" \"dRPB3_IAA\")\nstages=(\"0h\" \"1h\" \"3h\")\nreps=(\"rep1\" \"rep2\")\n# Loop over samples, stages, and replicates to perform samtools\n          operations\nfor sample in \"${samples[@]}\"; do\n\u00a0\u00a0for stage in \"${stages[@]}\"; do\n        \u00a0\u00a0\u00a0\u00a0for rep in \"${reps[@]}\"; do\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Perform samtools operations on the current sample, stage, and\n          repeat\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0samtools view -@ 10 -hF 4 ${sample}_${stage}_${rep}.bam | grep\n          \"YT:Z:CP\" | cat common_header.txt - | samtools view -@ 10\n          -bS -o ${sample}_${stage}_${rep}_reheader.bam\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0samtools sort ${sample}_${stage}_${rep}_reheader.bam -o\n          ${sample}_${stage}_${rep}.bam\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0samtools index ${sample}_${stage}_${rep}.bam\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0done\n\u00a0\u00a0done\ndone\n        Downsample the ChAR-seq bam files to match the read depth of the\n        identified bam file with the lowest read depth.\n      \n> Pre_Process -Q 10 -bam dRPB3_IAA_0h_rep1.bam,\n          dRPB3_IAA_0h_rep2.bam, dRPB3_IAA_1h_rep1.bam, dRPB3_IAA_1h_rep2.bam,dRPB3_IAA_3h_rep1.bam, dRPB3_IAA_3h_rep3.bam, RPB3_IAA_0h_rep1.bam,\n          RPB3_IAA_0h_rep2.bam, RPB3_IAA_1h_rep1.bam, RPB3_IAA_1h_rep2.bam,\n          RPB3_IAA_3h_rep1.bam, RPB3_IAA_3h_rep3.bam -ref\n          ./gencode_mm10_vM23/loci_annotation.bed\nNote: Input sorted bam file of each sample\n      should be separated by comma, troubleshooting 3[href=https://www.wicell.org#sec7.5].\n    \nNote: This step may take a long time. Once\n      finished, the output raw and downsampled files will be located in the same\n      folder as the original bam files.\n    \nDetect readthrough gene candidates\nTiming: \u223c2 h\n        Use the downsampled bam files and a loci annotation bed file as input\n        for the DoGFinder tool to identify readthrough gene candidates.\n        \nRemove all genic reads from the bam file.\n            Limit gene boundaries by the location of the nearest 3\u2018 neighboring\n            gene in the genome, and discard genes with 3\u2019 nearest neighbor\n            closer than 4 kb.\n          \n            Run the Get_DoGs function with the following parameters to identify\n            readthrough gene candidates based on a minimum length of 4 kb and a\n            minimum coverage of 60% over the entire downstream length: -S\n            -minDoGLen 4000 -mode F -minDoGCov 0.6.\n          \n            Elongate the identified readthrough gene candidates to find their\n            putative endpoint.\n          \n#!/bin/bash\n# Note: Step\n32 are standalone and function independently.\n# Variables are defined within this script.\n# Usage: Get_DoGs -out /outdir -bam my_sorted_bam.sorted_DS.bam -a\n          loci_annotation.bed\n# This script executes the Get_DoGs command for multiple samples,\n          stages, and replicates.\n# Define variables\nsamples=(\"dRPB3_IAA\" \"RPB3_IAA\")\nstages=(\"0h\" \"1h\" \"3h\")\nreps=(\"rep1\" \"rep2\")\n# Loop over samples, stages, and replicates to call the Get_DoGs\n          command\nfor sample in \"${samples[@]}\"; do\n\u00a0\u00a0for stage in \"${stages[@]}\"; do\n        \u00a0\u00a0\u00a0\u00a0for rep in \"${reps[@]}\"; do\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0Get_DoGs -out ./Get_DoGs \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-bam\n\"${sample}_${stage}_${rep}.sorted_PE.sorted_DS.bam\" \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-suff \"${sample}_${stage}_${rep}\" \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-s -minDoGLen 4000 -minDoGCov 0.6 -w 200 -mode F \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-a ./gencode_mm10_vM23/loci_annotation.bed \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-max 100000 done\n\u00a0\u00a0\u00a0\u00a0done\ndone\n        Intersect readthrough gene candidates from different replicates.\n        \n            Gather all the annotation bed files of readthrough gene candidates\n            from different replicates that need to be intersected.Use the \" Common_DoGs_annotation\" function from DoGFinder\n            to get the list of readthrough candidate gene sets common to all\n            biological replicates.\n          \n            Execute the \"sortBed\" function from bedtools to sort the\n            intersected bed file by chromosome and coordinate.\n          \n            Choose the most downstream coordinate as the end coordinate for all\n            intersected readthrough gene candidates. Use the \"awk\"\n            command to extract the most downstream coordinate for each gene name\n            and save it in a new bed file.\n          \n#!/bin/bash\n# Note: Step\n33 are standalone and function independently.\n# Variables are defined within this script.\n# Usage: Common_DoGs_annotation -comm\n          Dog_annotation_replicate1.bed,Dog_annotation_replicate2.bed -out\n          /outdir\n# This script executes the Common_DoGs_annotation command for\n          multiple samples and stages.\n# Define variables\nsamples=(\"dRPB3_IAA\" \"RPB3_IAA\")\nstages=(\"0h\" \"1h\" \"3h\")\n# Loop over samples and stages to run the Common_DoGs_annotation\n          command\nfor sample in \"${samples[@]}\"; do\n\u00a0\u00a0for stage in \"${stages[@]}\"; do\n        \u00a0\u00a0\u00a0\u00a0Common_DoGs_annotation -comm\n          \"Dog_annotation_${sample}_${stage}_rep1.bed,\n          Dog_annotation_${sample}_${stage}_rep2.bed\" \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0./Final_Common_Dog_annotation/\n\u00a0\u00a0\u00a0\u00a0sortBed -\ni ./Final_Common_Dog_annotation/union_dog_annotation.bed \u2216\n>\n          ./Final_Common_Dog_annotation/sorted_union_dog_annotation.bed\ncat ./Final_Common_Dog_annotation/sorted_union_dog_annotation.bed\n          \u2216\n        \u00a0\u00a0\u00a0\u00a0| awk -F '\u2216t' -v OFS='\u2216t' '{if($4!=id) {print\n          chr, start, end, id, score, strand; id=$4; chr=$1; start=$2; end=$3;\n          score=$5; strand=$6;} else {if($3>end) end=$3;}} END {print chr,\n          start, end, id, score, strand;}' \u2216\n        \u00a0\u00a0\u00a0\u00a0>\n          ./Final_Common_Dog_annotation/Final_CommonDog_annotation_${sample}_${stage}_rep1.bed\n\u00a0\u00a0done\ndone\nNote: Assuming that the filenames for the\n      replicate 1 and replicate 2 outputs follow the same pattern as the inputs,\n    \n        Merge DoG annotations from different treatments or experiments.\n        \n            Collect all DoG annotation bed files to be merged for different\n            treatments.\n          \n            Use the \" Union_DoGs_annotation\" function from DoGFinder\n            to create a single DoG annotation bed file.\n          \n            For any DoG that appears in more than one input file, unify them\n            according to their maximal length.\n          \n            Execute the \"sortBed\" function from bedtools to sort the\n            merged bed file by chromosome and coordinate.\n          \n            Format the output bed file as a tab-separated gene annotation bed\n            file to be used as input for Get_DoGs\n          \n#!/bin/bash# Note: Step\n34 is standalone and function independently.\n# Variables are defined within this script.\n# Usage: Union_DoGs_annotation -dog\n          Final_Dog_annotation1.bed,Final_Dog_annotation2.bed -out /outdir\n# This script executes the Union_DoGs_annotation command for multiple\n          samples.\n# Define variables\nsamples=(\"dRPB3_IAA\" \"RPB3_IAA\")\n# Create directories and run the Union_DoGs_annotation command\nfor sample in \"${samples[@]}\"; do\n        \u00a0\u00a0mkdir -p \"./Final_UnionDog_annotation_${sample}\"\n        \u00a0\u00a0Union_DoGs_annotation -dog\n          \"./Final_Common_Dog_annotation/Final_CommonDog_annotation_${sample}_0h.bed,./Final_Common_Dog_annotation/Final_CommonDog_annotation_${sample}_1h.bed,./Final_Common_Dog_annotation/Final_CommonDog_annotation_${sample}_3h.bed\"\n          \u2216\n-out \"./Final_UnionDog_annotation_${sample}/\"\ncat\n          \"./Final_UnionDog_annotation_${sample}/union_dog_annotation.bed\"\n          | awk -F '\u2216t' -v OFS='\u2216t' '{print\n          $1,$2,$3,$6,$5,$4\n          }'\u00a0>\u00a0\"./Final_UnionDog_annotation/Final_UnionDog_annotation_${sample}.bed\"\ndone\n        Calculate the expression levels of readthrough gene candidates across\n        experiments.\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2758-Fig4.jpg\n              Figure\u00a04. Comparison of readthrough gene candidates in dRPB3 and\n              RPB3 degron cells treated with IAA\n            \n              Top: Venn diagram illustrating the union overlap between genes\n              that produce readthrough genes at different time points (0 h, 1 h,\n              and 3 h) after IAA treatment in dRPB3 (left) or RPB3 (right)\n              degron cells. Bottom: Bar plot showing the number of readthrough\n              gene candidates discovered by DoGFinder in each sample. The number\n              in parentheses represents the common genes found in two replicates\n              across six conditions.\n            \n#!/bin/bash\n# Note: Step\n35 is standalone and function independently.\n# Variables are defined within this script.\n# Usage: Get_DoGs_rpkm -out /outdir -bam my_sorted_bam_DS.bam -s -dog\n          Final_Dog_annotation.bed\n# This script calculates RPKM values for DoGs using the Get_DoGs_rpkm\n          command.\n# Define variables\nsamples=(\"dRPB3_IAA\" \"RPB3_IAA\")\nstages=(\"0h\" \"1h\" \"3h\")\nreps=(\"rep1\" \"rep2\")\noutput_dir=\"./Get_DoGs_rpkm\"\nunion_dog_file=\"./Final_UnionDog_annotation/Final_UnionDog_annotation_\"\n# Loop over samples, stages, and replicates to run the command\nfor sample in \"${samples[@]}\"; do\n\u00a0\u00a0for stage in \"${stages[@]}\"; do\n        \u00a0\u00a0\u00a0\u00a0for rep in \"${reps[@]}\"; do\ninput_bam=\"${sample}_${stage}_${rep}.sorted_PE.sorted_DS.bam\"\n        \u00a0\u00a0\u00a0\u00a0suffix=\"${sample}_${stage}_${rep}\"\n        \u00a0\u00a0\u00a0\u00a0output_file=\"${output_dir}/${suffix}_DoGs_rpkm.txt\"\n        \u00a0\u00a0\u00a0\u00a0dog_file=\"${union_dog_file}${sample}.bed\"\n        \u00a0\u00a0\u00a0\u00a0Get_DoGs_rpkm -out \"$output_dir\" \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-bam \"$input_bam\" \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-suff \"$suffix\" \u2216\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-s \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-dog \"$dog_file\" \u2216\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-g ./Get_DoGs/genome_file.txt\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Move output file to correct location and rename it\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0mv \"${output_dir}/${suffix}_DoGs_rpkm.txt\"\n          \"$output_file\"\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0done\n\u00a0\u00a0done\ndone\n# This R code refers to metagene plot for dRPB3 in\nFigure\u00a04[href=https://www.wicell.org#fig4]# Note: similar code can be employed for RPB3.\n# Load required packages\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(ggpubr)\nlibrary(readxl)\n# Read the Excel data into a dataframe, removing rows with missing\n          values\ndf <- na.omit(read_excel(\"protocol_stats.xlsx\", sheet\u00a0=\n          3, trim_ws\u00a0= TRUE, range\u00a0=\n          cell_cols(c(\"A\",\"D\"))))\n# Display the first few rows of the dataframe\nhead(df)\n# Convert the \"sample\" column to a factor with specified\n          levels\ndf$sample <- factor(df$sample, levels\u00a0= df$sample)\n# Create a bar plot using ggbarplot from ggpubr\np3\u00a0<- ggbarplot(df, x\u00a0= \"sample\", y\u00a0=\n          \"count\",\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0fill\u00a0= \"cond\", color\u00a0= \"cond\", palette\u00a0=\n          \"npg\",\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0label\u00a0= TRUE,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0position\u00a0= position_dodge(0.8))\u00a0+\n\u00a0\u00a0\u00a0\u00a0ggprism::theme_prism()\u00a0+\n        \u00a0\u00a0\u00a0\u00a0theme(axis.text.x\u00a0= element_text(angle\u00a0= 40, hjust\u00a0= 1))\n# Save the plot as a PDF file\nggsave(p3, filename\u00a0= \"Figure\u00a04_\n          Readthrough_gene_number.pdf\", width\u00a0= 10, height\u00a0= 6)\n# Create a PDF device for the Venn diagram\npdf(\"Figure\u00a04_Venn_Diagram.pdf\", 8, 8)\n# Set up the plotting layout\npar(mfrow\u00a0= c(1, 2), bty\u00a0= 'n')\n# Draw the Venn diagram for dRPB3 using the total number of\n          readthrough gene candidates from the bar plot, as well as determine\n          their overlaps\nvenn.dRPB3\u00a0<- draw.triple.venn(area1\u00a0= 10065,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0area2\u00a0= 10698,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0area3\u00a0= 10703,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0euler.d\u00a0= TRUE, scaled\u00a0= TRUE,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0n12\u00a0= 9552\u00a0+ 215, n23\u00a0= 9552\u00a0+ 598, n13\u00a0= 9552\u00a0+ 155, n123\u00a0=\n          9552,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0category\u00a0= c(\"IAA_0h\", \"IAA_1h\",\n          \"IAA_3h\"),\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0lty\u00a0= \"blank\",\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0fill\u00a0= c(\"#CF4C35\", \"#4DA8BE\",\n          \"#1A937D\"),\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0alpha\u00a0= 0.5,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cex\u00a0= 2, cat.cex\u00a0= 2, cat.pos\u00a0= c(-1, -1, 180))\n# Draw the Venn diagram for RPB3\nvenn.RPB3\u00a0<- draw.triple.venn(area1\u00a0= 7195,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0area2\u00a0= 7733,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0area3\u00a0= 8024,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0euler.d\u00a0= TRUE, scaled\u00a0= TRUE,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0n12\u00a0= 6651\u00a0+ 258, n23\u00a0= 6651\u00a0+ 592, n13\u00a0= 6651\u00a0+ 136, n123\u00a0=\n          6651,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0category\u00a0= c(\"IAA_0h\", \"IAA_1h\",\n          \"IAA_3h\"),\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0lty\u00a0= \"blank\",\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0fill\u00a0= c(\"#CF4C35\", \"#4DA8BE\",\"#1A937D\"),\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0alpha\u00a0= 0.5,\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cex\u00a0= 2, cat.cex\u00a0= 2, cat.pos\u00a0= c(-1, -1, 180))\n# Close the PDF device\ndev.off()\nDetermine readthrough and non-readthrough genes\nTiming: \u223c1 h\nDefine readthrough genes.\n        Exclude very short genes with a length of less than 2 kb from the\n        analysis.\n      \n        Generate a sub-list of active genes from our untreated PRO-seq data.\n      \nNote: Gene activity was determined based\n      on the ratio of N/L, where N corresponds to the count of coding-strand\n      Pro-seq reads from\u00a0+1kb (relative to the TSS) to the end of each gene, and\n      L denotes the number of mappable bases in this region. The statistical\n      significance of a given gene's activity level was estimated by\n      calculating the probability of observing a minimum of N reads within an\n      interval of length L, derived from a Poisson distribution of mean \u03bb\u00a0= 0.04\n      reads/kb. Genes with a probability of less than 0.01 were defined as\n      active, as described in our publication.2[href=https://www.wicell.org#bib2]\n        Refine readthrough gene candidates identified in step 34 that\n        overlap with active genes but do not overlap with readthrough regions\n        that correspond to neighboring genes on either strand using Bedtools\n        intersect analysis.\n        \n            Identify readthrough gene candidates that overlap with active genes.\n          \n            Find transcripts whose TSS or TTS region extended in 5\u2032 and 3\u2032\n            overlaps with any transcript from another gene.\n          \n            Remove readthrough gene candidates that overlap with neighboring\n            genes on either strand.\n          \nKeep each readthrough gene a unique name.\n#!/bin/bash\n# Note: Step 38\nis standalone and function independently.\n# Variables are defined within this script.\n# Define input files and directories\npromoter_size=1000\n# Find genes that overlap promoters\ngtftk overlapping -i ./gencode_mm10_vM23/loci_annotation.bed -c\n          mm10.chromInfo -t promoter -u $promoter_size -d $promoter_size | gtftk\n          select_by_key -k feature -v transcript | gtftk tabulate -k\n          gene_id,overlap_promoter_u${promoter_size}_d${promoter_size}\u00a0>\u00a0dist_to_overlapping_mm10.txt# Use the paste command to join the files by columns\npaste gencode.vM23.gene.bed dist_to_overlapping_mm10.txt \u2216\n        \u00a0\u00a0\u00a0\u00a0| awk -v FS='\u2216t' -v OFS='\u2216t' '{print\n          $5,$1,$2,$3,$4,$6}' \u2216\n\u00a0\u00a0\u00a0\u00a0| sort -k1,1 \u2216\n\u00a0\u00a0\u00a0\u00a0> overlapping_gene.bed\n# Use an array to store sample names\nsamples=(dRPB3_IAA RPB3_IAA)\n# Use a for loop to iterate over the sample names\nfor sample in \"${samples[@]}\"; do\n        \u00a0\u00a0bedtools intersect -s -a\n          ./Final_UnionDog_annotation/Final_UnionDog_annotation_${sample}.bed -b\n          gencode.vM23_active.bed \u2216\n        \u00a0\u00a0\u00a0\u00a0| sort -u\u00a0>\u00a0Final_UnionDog_annotation_active_${sample}.bed\n        \u00a0\u00a0bedtools intersect -s same -v -a\n          Final_UnionDog_annotation_active_${sample}.bed -b overlapping_gene.bed\n          \u2216\n        \u00a0\u00a0\u00a0\u00a0| sort -u\u00a0>\u00a0${sample}_readthrough.bed\n# Use a single awk command to filter for the longest transcript per\n          gene with unique name\n        \u00a0\u00a0awk '{key=$4; if ($3-$2\u00a0>\u00a0max[key]) {max[key]=$3-$2;\n          row[key]=$0}} END{for (k in row) print row[k]}'\n          ${sample}_readthrough.bed \u2216\n\u00a0\u00a0\u00a0\u00a0| sort -k4,4 -u \u2216\n        \u00a0\u00a0\u00a0\u00a0> Final_${sample}_readthrough.bed\nDone\nNote: The bedtools and gtftk tools have\n      already been installed in the system's PATH.\n    \nDefine non-readthrough genes.\n        Obtain the genomic coordinates for the active genes and readthrough\n        regions of neighboring genes.\n      \n        Identify overlap between the active genes and the readthrough regions\n        according to bedtools intersect analysis.\n        \n            Create a list of active genes that do not overlap with readthrough\n            regions of neighboring genes on either strand.\n          \n            Subtract the readthrough genes from this list of active genes to\n            create a new list of non-readthrough genes that fail to generate\n            readthrough transcripts.\n          \nEnsure that each non-readthrough is given a unique name.\nNote: The script in this part is similar\n      to that of defining readthrough genes.\n    \nQuantify the readthrough indices\nTiming: \u223c1\u20132 h\n        Consider genes with a minimum length of 2 kb to exclude very short genes\n        from the analysis. Troubleshooting 4[href=https://www.wicell.org#sec7.7].\n      \n        Define two sub-genic windows as the following: termination windows are\n        from 3 to 6 kb downstream of polyA sites; pre-polyA windows are 1 kb\n        upstream of respective polyA sites.Calculate the ChAR-seq read counts in two sub-genic windows.\n        Filter out genes with fewer than five reads in either termination or\n        pre-polyA windows from the analysis.\n      \n        Determine the readthrough index as the ratio of the ChAR-seq read\n        coverage in the termination window to that in the pre-polyA window.\n      \nOptional: Build high-quality 3\u2032UTR\n      polyadenylation sites (PASs) for mouse genome directly from GENCODE\n      annotation GTF files (M23). It may be helpful to refine genes that contain\n      polyadenylation sites.\n    \n# This R code refers to an option of step 45\n% if (!\"BiocManager\" %in%\n          rownames(installed.packages()))\ninstall.packages(\"BiocManager\")\nBiocManager::install(\"APAlyzer\")\n# Load required packages\nlibrary(APAlyzer)\nGTFfile=\"gencode.vM23.annotation.gtf.gz\"\nPASREFraw=PAS2GEF(GTFfile)\nrefUTRraw=PASREFraw$refUTRraw\nUTRdbraw=REF3UTR(refUTRraw)\nwrite.table(UTRdbraw, \"UTRdbraw.tsv\", append\u00a0= TRUE, sep\u00a0=\n          \"\u2216t\")\nNote: our quantification pipeline includes\n      a custom Perl (Calculate_readthrough_indices.pl) that (i)\u00a0use downsampled\n      reads from step 32 and the bed file from step 45 to divide\n      read coverages in pre-polyA window by termination window to generate\n      readthrough indices and (ii) make sure genes are at least 2 kb long.\n    \n#!/usr/bin/perl\nuse strict;\nuse warnings;\nuse File::Basename;\nuse File::Spec::Functions qw(catfile);\n# Get command line arguments\nmy $genes_file\u00a0= $ARGV[0];\nmy $frags_file\u00a0= $ARGV[1];\n# Check if files exist\ndie \"Error: Genes file $genes_file not found\u2216n\" unless -e\n          $genes_file;\ndie \"Error: Fragments file $frags_file not found\u2216n\" unless\n          -e $frags_file;\n# Define output file names\nmy $genes_over_2kb_file\u00a0= 'genes.over2kb.bed';\nmy $term_window_file\u00a0= 'term.bed';\nmy $poly_window_file\u00a0= 'poly.bed';\nmy $term_sorted_file\u00a0= 'term.srt.bed';\nmy $poly_sorted_file\u00a0= 'poly.srt.bed';\nmy $term_count_file\u00a0= 'term.count';\nmy $poly_count_file\u00a0= 'poly.count';\n# Make sure genes are at least 2 kb long\nsystem(\"awk '{if (\u2216$10-\u2216$9>2000) print \u2216$0}'\n          $genes_file\u00a0>\u00a0$genes_over_2kb_file\");\nprint \"Genes\u00a0<\u00a02000bp parsed out\u2216n\u2216n\";\n# Define new termination window from TTS+3000 to TTS+6000 (only genes\n          >= 2000\u00a0bp and not overlapping any other gene)\nopen $over2kb_fh, '<', $genes_over_2kb_file or die\n          \"Cannot open $genes_over_2kb_file: $!\";open my $term_fh, '>', $term_window_file or die\n          \"Cannot open $term_window_file: $!\";\nwhile (my $line\u00a0= <$over2kb_fh>) {\n\u00a0\u00a0chomp $line;\n\u00a0\u00a0my @fields\u00a0= split(/\u2216t/, $line);\n\u00a0\u00a0if ($fields[5] eq '+') {\n        \u00a0\u00a0\u00a0\u00a0print $term_fh join(\"\u2216t\", $fields[0], $fields[2]\u00a0+\n          3000,\n$fields[2]\u00a0+ 6000, @fields[3..5]), \"\u2216n\";\n\u00a0\u00a0} elsif ($fields[5] eq '-') {\n        \u00a0\u00a0\u00a0\u00a0print $term_fh join(\"\u2216t\", $fields[0], $fields[1] -\n          6000,\n$fields[1] - 3000, @fields[3..5]), \"\u2216n\";\n\u00a0\u00a0}\n}\nclose $over2kb_fh;\nclose $term_fh;\n# Define new pre-polyA window from TTS-1000 to TTS+0 (only genes\n          >= 2000\u00a0bp and not overlapping any other gene)\nopen $over2kb_fh, '<', $genes_over_2kb_file or die\n          \"Cannot open $genes_over_2kb_file: $!\";\nopen my $poly_fh, '>', $poly_window_file or die\n          \"Cannot open $poly_window_file: $!\";\nwhile (my $line\u00a0= <$over2kb_fh>) {\n\u00a0\u00a0chomp $line;\n\u00a0\u00a0my @fields\u00a0= split(/\u2216t/, $line);\n\u00a0\u00a0if ($fields[5] eq '+') {\n        \u00a0\u00a0\u00a0\u00a0print $poly_fh join(\"\u2216t\", $fields[0], $fields[2] - 1000,\n          $fields[2], @fields[3..5]), \"\u2216n\";\n\u00a0\u00a0} elsif ($fields[5] eq '-') {\n        \u00a0\u00a0\u00a0\u00a0print $poly_fh join(\"\u2216t\", $fields[0], $fields[1],\n          $fields[1]\u00a0+ 1000, @fields[3..5]), \"\u2216n\";\n\u00a0\u00a0}\n}\nclose $over2kb_fh;\nclose $poly_fh;\n# Sort termination and gene polyA window files\nsystem(\"sort -k1,1 -k2,2n\n          $term_window_file\u00a0>\u00a0$term_sorted_file\");\nsystem(\"sort -k1,1 -k2,2n\n          $poly_window_file\u00a0>\u00a0$poly_sorted_file\");\nprint \"Windowed files sorted\u2216n\u2216n\";\n# Calculate coverage over windows, run bedtools with -s option for\n          strandedness (ChAR-seq)\nsystem(\"bedtools coverage -s -a $term_sorted_file -b $frags_file\n          -counts\u00a0>\u00a0$term_count_file\")\u00a0== 0\n        \u00a0\u00a0or die \"bedtools coverage failed for\n          $term_sorted_file\u2216n\";\nsystem(\"bedtools coverage -s -a $poly_sorted_file -b $frags_file\n          -counts\u00a0>\u00a0$poly_count_file\")\u00a0== 0\n        \u00a0\u00a0or die \"bedtools coverage failed for\n          $poly_sorted_file\u2216n\";\nprint \"Read coverage over windows calculated\u2216n\u2216n\";\n# Open coverage files\nopen my $promcov, \"<\", \"term.count\" or die\n          \"Can't open term.count: $!\";\nopen my $genecov, \"<\", \"poly.count\" or die\n          \"Can't open poly.count: $!\";\nopen my $bed, \">\", \"readthrough_indices.txt\"\n          or die \"Can't open readthrough_indices.txt: $!\";\n# Print header to output file\nprint $bed\n          \"poly_chr\u2216tpoly_start\u2216tpoly_stop\u2216tpoly_strand\u2216tpoly_name\u2216tpoly_cov\u2216tterm_name\u2216tterm_start\u2216tterm_stop\u2216tterm_strand\u2216tterm_cov\u2216tri\u2216n\";\n# Read through coverage files\nmy %poly_cov;\nwhile (my $line1\u00a0= <$promcov>) {\n\u00a0\u00a0chomp $line1;my ($chr, $start, $stop, $name, $cigar, $strand, $cov)\u00a0= split /\u2216t/,\n          $line1;\n        \u00a0\u00a0$poly_cov{$name}\u00a0=\n          \"$chr&$start&$stop&$name&$cigar&$strand&$cov\";\n}\nwhile (my $line2\u00a0= <$genecov>) {\n\u00a0\u00a0chomp $line2;\n        \u00a0\u00a0my ($term_chr, $term_start, $term_stop, $term_name, $term_cigar,\n          $term_strand, $term_cov)\u00a0= split /\u2216t/, $line2;\n# Check if termination coverage file has matching name to polyA\n          window coverage file\n\u00a0\u00a0if (exists $poly_cov{$term_name}) {\n        \u00a0\u00a0\u00a0\u00a0my $storedvalue\u00a0= $poly_cov{$term_name};\n        \u00a0\u00a0\u00a0\u00a0my ($poly_chr, $poly_start, $poly_stop, $poly_name, $poly_cigar,\n          $poly_strand, $poly_cov)\u00a0= split /&/, $storedvalue;\n        \u00a0\u00a0\u00a0\u00a0# Calculate readthrough indices\n        \u00a0\u00a0\u00a0\u00a0if ($poly_cov\u00a0>\u00a05\u00a0&& $term_cov\u00a0>\u00a05) {\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0my $poly_density\u00a0= $poly_cov / 3000;\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0my $term_size\u00a0= $term_stop - $term_start;\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0my $term_density\u00a0= $term_cov / 1000;\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0my $ri\u00a0= $poly_density / $term_density;\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0print $bed\n          \"$poly_chr\u2216t$poly_start\u2216t$poly_stop\u2216t$poly_strand\u2216t$poly_name\u2216t$poly_cov\u2216t$term_name\u2216t$term_start\u2216t$term_stop\u2216t$term_strand\u2216t$term_cov\u2216t$ri\u2216n\";\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0}\n}\n# Close files\nclose $promcov;\nclose $genecov;\nclose $bed;\nexit;\n        Compute readthrough indices of readthrough genes and non-readthrough\n        genes for every bam file in step 32.\n#!/bin/bash\n# Note: Step 46\nis standalone and function independently.\n# Variables are defined within this script.\n# Loop through samples, stages, and genes to calculate readthrough\n          indices\nfor sample in dRPB3_IAA RPB3_IAA; do\n\u00a0\u00a0for stage in 0h 1h 3h; do\n        \u00a0\u00a0\u00a0\u00a0for gene in readthrough non_readthrough; do\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0BED_FILE=\"${sample}_${stage}_${gene}.bed\"\nBAM_FILE=\"${sample}_${stage}_merged.sorted_PE.sorted_DS.bam\"\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0OUT_FILE=\"${sample}_${stage}_${gene}.txt\"\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0# Run Calculate_readthrough_indices.pl command\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0perl Calculate_readthrough_indices.pl \"$BED_FILE\"\n          \"$BAM_FILE\"\u00a0>\u00a0\"$OUT_FILE\" || { echo\n          \"Calculate_readthrough_indices.pl failed for $BED_FILE and\n          $BAM_FILE\"; exit 1; }\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0done\n\u00a0\u00a0done\ndone\n      Compare the changes of 3\u2032end processing upon RPB3 and dRPB3 depletion\n    \nTiming: \u223c30\u00a0min\n        Apply the DESeq2 package to perform the differential 3\u2032end processing\n        analyses before (IAA-0 h) and after (IAA-3 h) dRPB3 or RPB3 depletion.\n      \nNote: The readthrough indices are float\n      numbers generated in step 45, which need to be rounded into an\n      integer.\n    \n        Organize the readthrough indices data into the matrix format required\n        for DESeq2 analysis.\n      \n        Select genes whose adjusted p-values are less than 0.05 and whoseabsolute fold changes are greater than 2 (Figure\u00a05[href=https://www.wicell.org#fig5]B).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2758-Fig5.jpg\n              Figure5. Results of ChAR-seq analysis\n            \n              (A) Metagene profiles of ChAR-seq signals at the polyadenylation\n              sites of readthrough and nonreadthrough transcripts at different\n              time points (0 h, 1 h, and 3 h) after IAA treatment in dRPB3\n              (left) or RPB3 (right) degron cells. Boxplots showing ChAR-seq\n              readthrough indices of gene sets identified by DoGFinder,\n              presented as the median in an interquartile range with whiskers\n              indicating 1.5-fold the interquartile range and confidence region\n              notches. A two-sided Wilcoxon rank-sum test was performed to\n              determine significance. The data were reported as the average of\n              two biological replicates for each treatment.\n            \n              (B) Volcano plots of induced 3\u2032 end-processing changes upon dRPB3\n              (left) or RPB3 (right) depletion, as determined by ChAR-seq.\n            \n              (C) Metagene analysis of spike-in\u2013normalized ChAR-seq reads around\n              annotated polyadenylation sites of ribosomal subunit genes before\n              and after depletion of RPB3 or dRPB3. The corresponding 3\u2032 end RNA\n              processing changes were quantified and displayed as boxplots. The\n              middle line, lower hinge and upper hinge in the boxplot indicate\n              the median, first quartile and third quartile, respectively.\n              Significance was determined by a two-tailed Wilcoxon rank-sum\n              test. \u2217P\u00a0<\u00a00.05, \u2217\u2217\u2217P\u00a0<\u00a00.001.\n            \n              Figure\u00a0reprinted and reorganized with permission from Li et\u00a0al.1[href=https://www.wicell.org#bib1]\n# This chunk of R code refers to steps 47\nand 49\n# Load required packages\nlibrary(DESeq2)\nlibrary(org.Mm.eg.db)\nlibrary(data.table)\n# read data\nrawcount <-\n          data.table::fread(\"dRPB3_combined_RIs.tsv\")\n# select columns of interest and set GeneID as row names\nct <- data.frame(rawcount[, c(\"dRPB3_IAA_0h_rep1\",\n          \"dRPB3_IAA_0h_rep2\", \"dRPB3_IAA_3h_rep1\",\n          \"dRPB3_IAA_3h_rep2\")])\nrow.names(ct) <- rawcount$GeneID\n# create column data\ncolData <- data.frame(\n        \u00a0\u00a0condition\u00a0= factor(rep(c(\"0h\", \"3h\"), each\u00a0=\n          2)),\n\u00a0\u00a0ind\u00a0= factor(c(1, 2, 1, 2)),\n\u00a0\u00a0row.names\u00a0= colnames(ct)\n)\n# create DESeqDataSet object\ndds <- DESeqDataSetFromMatrix(\n\u00a0\u00a0countData\u00a0= round(ct, 0),\n\u00a0\u00a0colData\u00a0= colData,\n\u00a0\u00a0design\u00a0= \u223c condition\u00a0+ ind\n)# filter genes with no counts\ndds <- dds[rowSums(counts(dds))\u00a0>\u00a00, ]\n# estimate size factors\ndds <- estimateSizeFactors(dds)\n# run DESeq2\ndds <- DESeq(dds)\nres <- results(dds, contrast\u00a0= c(\"condition\",\n          \"3h\", \"0h\"))\nsummary(res)\n# create differential expression table\ndiff <- data.frame(res@listData, row.names\u00a0= res@rownames)\nlfc_thresh <- 1\npadj_thresh <- 0.05\n# classify genes based on log2FoldChange and adjusted p-value\ndiff$reg <- \"unchanged\"\ndiff$reg[diff$log2FoldChange\u00a0<\u00a0-lfc_thresh\u00a0&\n          diff$padj\u00a0<\u00a0padj_thresh] <- \"down\"\ndiff$reg[diff$log2FoldChange\u00a0>\u00a0lfc_thresh\u00a0&\n          diff$padj\u00a0<\u00a0padj_thresh] <- \"up\"\n# display the number of genes in each category\ntable(diff$reg)\ndiff$ensembl_id <- rownames(diff)\n# Mapping Ensembl IDs to gene symbols using org.Mm.eg.db\ngene_name <- mapIds(x=org.Mm.eg.db, keys=diff$ensembl_id, column\u00a0=\n          \"SYMBOL\", keytype\u00a0= \"ENSEMBL\")\ngene_name <- data.frame(ensembl_id=names(gene_name),\n          gene=gene_name)\n# Joining 'gene_name' and 'diff' data frames\ndiff <- plyr::join(gene_name, diff, type=\"full\")\nhead(diff)\n# Creating a data frame of normalized counts\nnorm_ct <- as.data.frame(counts(dds, normalized\u00a0= T))\nnorm_ct$ensembl_id <- rownames(norm_ct)\n# Joining 'norm_ct' and 'diff' data frames\ndiff <- plyr::join(norm_ct, diff, type=\"right\")\nhead(diff)\nthis_tile <- paste0('Up :\n          ',nrow(diff[diff$reg\u00a0=='up',]) ,' | ','Down :\n          ',nrow(diff[diff$reg\u00a0=='down',]),' |\n          ','Unchange :\n          ',nrow(diff[diff$reg\u00a0=='unchanged',]))\n# Calculating the number of genes in each category\nndown <- table(diff$reg)[1]\nnunchange <- table(diff$reg)[2]\nnup <- table(diff$reg)[3]\n# Creating a scatterplot for dRPB3 as displayed in\nFigure\u00a05[href=https://www.wicell.org#fig5]B\n      \n# Note: similar code can be employed for RPB3.\np4\u00a0<- ggplot(data\u00a0= diff, aes(x\u00a0= log2FoldChange, y\u00a0= -log(padj),\n          color\u00a0= reg))\u00a0+\ngeom_point()\u00a0+\n        \u00a0\u00a0ggrepel::geom_text_repel(data\u00a0= filter(diff, gene %in% label_ribo),\n          aes(label\u00a0= gene), box.padding\u00a0= 0.5, max.overlaps\u00a0= Inf)\u00a0+\n        \u00a0\u00a0geom_hline(yintercept\u00a0= -log(0.05), linetype\u00a0= \"dashed\",\n          color\u00a0= \"grey30\")\u00a0+\n        \u00a0\u00a0geom_vline(xintercept\u00a0= -1, linetype\u00a0= \"dashed\", color\u00a0=\n          \"grey30\")\u00a0+\n        \u00a0\u00a0geom_vline(xintercept\u00a0= 1, linetype\u00a0= \"dashed\", color\u00a0=\n          \"grey30\")\u00a0+\n\u00a0\u00a0xlab(\"log2 (FC)\")\u00a0+\n\u00a0\u00a0ylab(\"-log10 (p-adj)\")\u00a0+\n        \u00a0\u00a0ggtitle(\"R3F 3h-VS-Dox Readthrough Index\")\u00a0+\n\u00a0\u00a0scale_x_continuous(limits\u00a0= c(-5, 5))\u00a0+\n\u00a0\u00a0scale_y_continuous(limits\u00a0= c(0, 30))\u00a0+scale_color_manual(values\u00a0= c(\"#005792\", \"grey\",\n          \"#ca3e47\"), name\u00a0= \"\",\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0label\u00a0= c(paste0(\"down (\", ndown, \")\"),\n          paste0(\"unchanged (\", nunchange, \")\"),\n          paste0(\"up (\", nup, \")\")))\u00a0+\n        \u00a0\u00a0theme(plot.title\u00a0= element_text(colour\u00a0= \"black\", size\u00a0=\n          20, hjust\u00a0= 0.5),\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0axis.line.x\u00a0= element_blank(), axis.line.y\u00a0= element_blank(),\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0legend.text\u00a0= element_text(size\u00a0= 15),\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0axis.text\u00a0= element_text(colour\u00a0= \"black\", size\u00a0= 17),\n          axis.title\u00a0= element_text(size\u00a0= 20),\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0panel.border\u00a0= element_rect(colour\u00a0= \"black\", fill\u00a0= NA,\n          size\u00a0= 0.5),\n        \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0panel.background\u00a0= element_blank())\n# Saving the scatterplot as a PDF file\nggsave(p4, filename\u00a0= \"Figure\u00a05B_dRPB3_scatterplot.pdf\",\n          width\u00a0= 5, height\u00a0= 4)\n        Conduct GO term analysis to show distinct gene ontologies of 3\u2032end\n        processing-affected genes linked to ribosomal subunit genes, as\n        described in our publication.1[href=https://www.wicell.org#bib1]\n        Perform a Wilcoxon test to assess the statistical significance of\n        differences in 3\u2032end processing before and after dRPB3 or RPB3 depletion\n        (Figure\u00a05[href=https://www.wicell.org#fig5]C).", "Step-By-Step Method Details\nStep-By-Step Method Details\nSample processing is divided into four main steps: DNA isolation, library construction by PCR, amplicon cleaning and dilution, and sequencing (Figure\u00a02[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/224-Fig2.jpg\nFigure\u00a02. .Workflow of 16S rRNA Gene Sequencing Preparation and Analysis\nSteps are structured into three sections: the sample collection and storage, the sample preparation and sequencing, and the sample preprocessing and data analysis. The given time for each step can be seen as a point of reference.\nDNA Isolation\nTiming: approx. 3\u00a0h for 24 samples\nDNA is isolated with a modification of the protocol by Godon etal. (1997)[href=https://www.wicell.org#bib6]. A blank sample, consisting of 600\u00a0\u03bcL DNA Stabilizer from Invitek, is processed in every second DNA isolation batch (i.e., one blank sample for each 47 samples).\nThaw fecal samples (ca. 2\u00a0g in 8\u00a0mL DNA stabilizer) for approximately 2\u00a0h at 20\u00b0C\u201322\u00b0C.\nVortex until the sample is fully homogenized and let stand for 3\u00a0min to sediment debris.\nFor each sample, a volume of 600\u00a0\u03bcL fecal slurry is transferred into a 2-mL screw cap tube containing 0.1\u00a0mm silica beads. Use autoclaved hand-cut blue tips that allow pipetting even in the presence of remaining debris. This aliquot is processed immediately.\nNote: The remaining sample is frozen at \u221280\u00b0C for long-term storage.\nAdd 250\u00a0\u03bcL 4\u00a0M guanidinium thiocyanate to the sample. This step is necessary to denature proteins.\nAdd 500\u00a0\u03bcL 5% N-lauroylsarcosine sodium salt, which is an ionic surfactant that separates all cellular components from each other.\nIncubate the samples for 60\u00a0min at 70\u00b0C while shaking at 700\u00a0rpm.Lyse remaining microbial cells by using a FastPrep-24 fitted with a CoolPrep adapter (filled with a handful of dry ice). The FastPrep instrument performs the lysis of biological samples by using an optimized motion to disrupt cells through beating of beads on the sample material.\nProgram: 5\nCycles: 40 s; 6.5\u00a0m/s\n3 rounds (add more dry ice between each round)\nAdd 15\u00a0mg polyvinylpyrrolidone (PVPP), a polymer used for removing phenolics and other fecal contaminants.\nAfter vortexing, centrifuge for 3\u00a0min at 15,000\u00a0\u00d7 g and 4\u00b0C.\n500\u00a0\u03bcL of the supernatant is transferred to a new 2-mL tube.\nAdd 5\u00a0\u03bcL RNase A and incubate for 20\u00a0min at 37\u00b0C while shaking at 700\u00a0rpm.\nThe DNA is then purified using a silica membrane-based approach following the manufacturer's instructions of the kit used (NucleoSpin gDNA Clean-up Kit, REF 740230.250 Machery-Nagel).\nAdd 1500\u00a0\u03bcL Binding Buffer and vortex for 5 s.\nTransfer each sample to one column: this is performed in three steps with each 650\u00a0\u03bcL. After each transfer, columns are centrifuged for 30\u00a0s (11,000\u00a0\u00d7 g); discard the flow-through.\nWash columns by adding 700\u00a0\u03bcL Washing Buffer. After 2\u00a0s vortex, columns are centrifuged for 30\u00a0s (11,000\u00a0\u00d7 g); discard the flow-through. Washing is performed three times.\nDry the silica membrane by centrifuging the columns for 1\u00a0min (11,000\u00a0\u00d7 g) and discard the collection tube.\nAdd 50\u00a0\u03bcL Buffer DE to elute the DNA. Incubate for 1\u00a0min and centrifuge for 1\u00a0min (as before). Repeat the elution step and pool the flow-through to obtain a final volume of 100\u00a0\u03bcL with the isolated DNA.\nAfter DNA purification, nucleic acid concentrations are measured by using a NanoDrop.Note: Use a DNA solution of known concentration and measure serial dilutions thereof to check for the accuracy of the NanoDrop.\nLibrary Construction by Polymerase Chain Reaction\nTiming: approx. 3\u00a0h for 96 samples\nDilute isolated DNA of each sample to a final concentration of 12\u00a0ng/\u03bcL in 20\u00a0\u03bcL water into a 96-well skirted plate.\nPrepare the Master Mix (Table 1[href=https://www.wicell.org#tbl1]) for the first (1st) PCR.\ntable:files/protocols_protocol_224_1.csv\nTransfer 27\u00a0\u03bcL of the prepared Master Mix (per well) and add 3\u00a0\u03bcL of the sample (per well) to a new 96-well skirted plate. The well plate with 30\u00a0\u03bcL sample per well is covered with a foil seal and is centrifuged for 30\u00a0s at low speed to collect the liquid at the bottom.\nPut the plate into the cycler (Biometra TAdvanced) and run the first (1st) PCR program for 15 cycles following the time and temperature settings shown in (Table 2[href=https://www.wicell.org#tbl2]).\ntable:files/protocols_protocol_224_2.csv\nPrepare the Master Mix (Table 3[href=https://www.wicell.org#tbl3]) for the second (2nd) PCR including forward index primer. For each 96-well plate, 6 different forward primer and 16 different reverse primer are used. The reverse primer is not included in the Master Mix, they are divided in strips which are placed in the robot working area as well. For each of the six forward primer one separate Master Mix is to be prepared.\ntable:files/protocols_protocol_224_3.csv\nAfter the first PCR the plate returns to the robot.\nMix 2\u00a0\u03bcL of the DNA from 1st PCR, 45.5\u00a0\u03bcL of the Master Mix (Table 3[href=https://www.wicell.org#tbl3]), and 2.5\u00a0\u03bcL of one reverse index primer. Primer are combined in order to insert a double index in each sample following the method introduced by Kozich et\u00a0al. (2013)[href=https://www.wicell.org#bib14]. It is possible to select from 38 forward and 60 reverse primer Table 4[href=https://www.wicell.org#tbl4].\ntable:files/protocols_protocol_224_4.csvThe plate is covered again with a PCR foil seal and is centrifuges for 30\u00a0s as before.\nThe second PCR starts by putting the covered plate into the cycler (Biometra TAdvanced). Run the program for ten cycles following the time and temperature settings shown in (Table\u00a05[href=https://www.wicell.org#tbl5]).\ntable:files/protocols_protocol_224_5.csv\nPause Point: After the second PCR, the plate can be stored at 4\u00b0C for 1\u00a0day.\nPool the final PCR products of both plates after the second PCR, which results in a total volume of 100\u00a0\u03bcL per sample.\nNote: Fifteen \u03bcL can be used for quality control issues (e.g., gel electrophoresis).\nLibrary Cleaning\nTiming: approx. 1\u00a0h 30\u00a0min for 96 samples\nPCR purification is performed with AGENCOURT AMPure XP Beads (Beckman Coulter) and again fully automatized using Beckman Coulter Biomek 4000 robot.\nPrior to the library cleaning\nRemove the AMPure XP beads from 4\u00b0C storage and let stand for at least 30\u00a0min to bring to 20\u00b0C\u201322\u00b0C.\nVortex the AMPure XP beads until they are well dispersed.\nAdd 1.8\u00a0\u03bcL AMPure XP beads per 1.0\u00a0\u03bcL PCR product. Using a P1000 multi-channel pipette,\u00a0the robot gently pipettes the entire volume up and down 10-times to mix thoroughly.\nNote: For stool samples, the standard settings are 85\u00a0\u03bcL PCR product and 153\u00a0\u03bcL AMPure XP beads resulting in a total volume of 238\u00a0\u03bcL.\nIncubate at 20\u00b0C\u201322\u00b0C for 5\u00a0min.\nPut the well plate in the magnetic rack and let stand at 20\u00b0C\u201322\u00b0C for 5\u00a0min or until the liquid becomes clear in appearance. The robot removes the all of the clear supernatant using a P1000 multi-channel pipette.\nThe fragment is bound to the beads and 200\u00a0\u03bcL freshly prepared 70% EtOH is added to each well using a P250 without barrier.Leave at 20\u00b0C\u201322\u00b0C for 30\u00a0s and discard the supernatant. Take extra care not to disturb the beads.\nSteps 31 and 32 are repeated once more, for a total of two 70% EtOH washes.\nLet the 96-well plate at 20\u00b0C\u201322\u00b0C for 4 - 5\u00a0min for drying, and then remove from the magnetic rack.\nRe-suspend the bead pellet in each well in 80\u00a0\u03bcL BE Elution (recommended volume of AMPure standard protocol). The robot gently pipettes the entire volume up and down 10-times to mix thoroughly using a P250 multi-channel pipette.\nCritical: The amount of added Elution Buffer depends on the DNA yield of the PCR product. Low amounts of PCR product, i.e., weak bands on the gel, should be re-suspend with amount at or below 20\u00a0\u03bcL BE Elution.\nIncubate the 96-well plate at 20\u00b0C\u201322\u00b0C for 2\u00a0min.\nPlace the 96-well plate on the magnetic rack at 20\u00b0C\u201322\u00b0C for 2\u00a0min or until the liquid becomes clear in appearance. Seventy \u03bcL of the clear supernatant from each well are transferred to an XP plate. Eight \u03bcL are transferred to a second plate for DNA measurements by fluorimetry (Qubit measurement according to the manufacturer\u2019s instructions).\nNote: If not enough volume is available, the total amount is transferred manually.\nSamples are diluted to a concentration of 2\u00a0nM and finally diluted to a concentration of 0.5\u00a0nM.\nFrom each sample of the 96-well plate, 5\u00a0\u03bcL are transferred to a low binding tube (pool of all samples of one plate).\nPause Point: After the library cleaning the plate can be stored at 4\u00b0C for 1\u00a0day.\nPrepare Samples for 16S rRNA Gene Sequencing\nCalculate molarity of each sample based on measured Qubit concentrations for a mean over four measurements: imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/224-Si1.jpg\ntable:files/protocols_protocol_224_6.csv\nNote: For V3V4, the average library size is 572\u00a0bp.Following steps are necessary to denature the DNA and set to a concentration of 20 pM.\nCreate a fresh 0.2\u00a0nM NaOH solution and a 0.2\u00a0nM Tris HCl solution.\nAdd 40\u00a0\u03bcL of the 0.5\u00a0nM DNA pool and 40\u00a0\u03bcL of the 0.2\u00a0N NaOH solution to a 1.5-mL tube.\nVortex the sample and centrifuge for 1\u00a0min (280\u00a0\u00d7 g). Leave it in a stand for 5\u00a0min at 20\u00b0C\u201322\u00b0C. Add 40\u00a0\u03bcL of the 0.2\u00a0nM Tris HCl solution. Vortex the mixture and centrifuge for 1\u00a0min (280\u00a0\u00d7 g).\nIncubate for 5\u00a0min at 95\u00b0C and for 5\u00a0min at 4\u00b0C.\nAdd 880\u00a0\u03bcL cooled HT1-Buffer to the denaturated DNA pool to generate a 20 pM library.\nDilute the DNA to get the final pM concentration of 10 pM final library concentration that was spiked-in using 20% (v/v) PhiX. PhiX DNA in a ready to sequence library (Illumina PhiX Control v3, FC-110-3001) is added in order to increase complexity for the first few bases sequenced. Otherwise, the sequencer miscalculates the amount of the dominating base and the sequencing fails.\nSix-hundred \u03bcL of the final pool is transferred to the Illumina MiSeq cartridge v3 with 600 cycles.", "Step-by-step method details\nStep-by-step method details\nIncorporate Drosophila S2 cell chromatin in ChIP samples of human prostate cancer PC-3 cells\nTiming: about 2\u00a0days\nThe information of pan-chromatin changes would be lost in the PCR-based amplification of DNA library. To retrieve the lost information, it is necessary to use the constant amount of Drosophila chromatin to precipitate alongside with the PC-3 cell chromatin.\n(Prepare the block solution) Pre-block and binding of antibody to magnetic beads.\nAdd 100\u00a0\u03bcL Dynal magnetic beads to microfuge tube. Add 1\u00a0mL block solution. Set up 1 tube per IP.\nCollect the beads using magnetic stand. Remove supernatant.\nWash beads in 1.5\u00a0mL block solution two more times.\nResuspend beads in 250\u00a0\u03bcL block solution and add 10\u00a0\u03bcg of antibody.\nIncubate for 16\u00a0h on a rotating platform at 4\u00b0C.\nNext day, wash beads as described above (3 times in 1\u00a0mL block solution).\nResuspend in 100\u00a0\u03bcL block solution.\nCombine Drosophila S2 cell and human PC-3 cell chromatin from Preparation 2.\nFor each spike-in H3K27-ac ChIP experiment (in either DMSO or SAHA treated cells), a 20:1 ratio of PC-3 versus Drosophila S2 cell chromatin was used (according to the DNA quantification results). Typically, we got \u223c300\u00a0\u03bcg DNA in sonicated chromatin from 5\u00d7107 PC3 cells (in a volume of \u223c1.5\u00a0mL). Therefore, the S2 cell chromatin containing 15\u00a0\u03bcg Drosophila DNA would be added.\nNote: Once Drosophila S2 and human cells were combined, the sample was treated as a single ChIP-seq sample throughout the experiment until completion of DNA sequencing.\nAdd 100\u00a0\u03bcL antibody/magnetic bead mix to 1.5\u00a0mL chromatin mixture produced above.\nGently mix for 16\u00a0h on rotator or rocker at 4\u00b0C.\n(Prepare Wash Buffer, TE Buffer and Elution Buffer, set thermoshaker to 65\u00b0C) Wash, elution, and cross-link reversal after ChIP.Pre-chill one 1.5-mL microfuge tube for each ChIP.\nTransfer half the volume of the ChIP material (steps 2b and 2c) to a pre-chilled tube.\nLet tubes sit in magnetic stand to collect the beads. Remove supernatant and add remaining IP. Let tubes sit again in magnetic stand to collect the beads.\nAdd 1\u00a0mL Wash Buffer to each tube. Remove tubes from magnetic stand and shake or agitate tube gently to resuspend beads. Replace tubes in magnetic stand to collect beads. Remove supernatant. Wash 7 more times.\nNote: Exact number of washes depends on quality of antibody and may need to be optimized for each antibody. For most antibody we used, washing 6 times or more is applicable.\nWash once with 1\u00a0mL TE buffer that contains 50\u00a0mM NaCl.\nSpin at 1,000\u00a0\u00d7 g for 3\u00a0min at 4\u00b0C and remove any residual TE buffer.\nAdd 210\u00a0\u03bcL of elution buffer.\nElute at 65\u00b0C for 15\u00a0min. Resuspend beads every 2\u00a0min with brief vortex.\nSpin down beads at 11,000\u00a0\u00d7 g for 1\u00a0min at 21\u00b0C.\nRemove 200\u00a0\u03bcL of supernatant and transfer to new tube. Reverse crosslink of this IP DNA by incubating at 65\u00b0C for 16 h.\nCritical: Steps 3a through 3e should be carried out in a 4\u00b0C cold room.\n(set thermoshaker to 55\u00b0C) Digestion of cellular protein/RNA and extraction of ChIP-DNA.\nAdd 200\u00a0\u03bcL of TE buffer to each tube of IP to dilute SDS in elution buffer.\nAdd 8\u00a0\u03bcL of 10\u00a0mg/mL RNaseA (0.2\u00a0\u03bcg/mL final concentration).\nMix and incubate at 37\u00b0C for 2 h.\nAdd 4\u00a0\u03bcL of 20\u00a0mg/mL proteinase K (0.2\u00a0\u03bcg/mL final concentration).\nMix and incubate at 55\u00b0C for 2 h.Add 400\u00a0\u03bcL phenol: chloroform: isoamyl alcohol (P:C: IA) and transfer the mixture to a new 2-mL tube. Then centrifuge the sample at 12,000\u00a0\u00d7 g for 10\u00a0min to separate phases.\nTransfer 400\u00a0\u03bcL aqueous layer to new centrifuge tube containing 16\u00a0\u03bcL of 5M NaCl (200\u00a0mM final concentration) and 1.5\u00a0\u03bcL of 20\u00a0\u03bcg/\u03bcL glycogen (30\u00a0\u03bcg total).\nAdd 800\u00a0\u03bcL EtOH. Incubate for 30\u00a0min at \u221280\u00b0C.\nSpin at 11,000\u00a0\u00d7 g for 10\u00a0min at 4\u00b0C to pellet DNA. Wash pellets with 500\u00a0\u03bcL of 80% EtOH.\nDry pellets and resuspend each in 70\u00a0\u03bcL of 10mM Tris-HCl, pH 8.0.\nSave 15\u00a0\u03bcL of ChIP product for future checkpoints or verification.\nMeasure DNA concentration of ChIP products with Qubit dsDNA quantification system. (Here is a Safe Stopping Point, DNA samples can be stored at \u221280\u00b0C for one month.)\nAnalyzing PC-3 cell ChIP-DNA with spike-in Drosophila DNA\nTiming: 7\u201314\u00a0days\nThe ChIP-sequencing generally takes 4\u201355 h, but the exact time can vary greatly depending on the workload of the sequencing core, the model of the sequencers (NextSeq, NovaSeq) used, the flow cells used, and the availability of the computational resources. The analysis time also varies significantly commensurate to personal experiences and different type of tertiary analyses (Figure\u00a02[href=https://www.wicell.org#fig2]). A walkthrough example to analyze spike-in ChIP-seq data are provided in our online documentation (https://spiker.readthedocs.io/en/latest/index.html[href=https://spiker.readthedocs.io/en/latest/index.html])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/829-Fig2.jpg\nFigure\u00a02. Workflow of spike-in ChIP-DNA sequencing and analysisSequencing. We use The NEB Next Ultra II DNA Library Preparation Kit to convert our ChIP DNA (containing spike-in Drosophila DNA) into libraries for next-generation sequencing on the Illumina platform. To be general, the dsDNA from step 4 is end repaired, 5\u2032 phosphorylated and added with a dA tail at the 3\u2032 ends. Then the processed DNA is ligated with NEBNext Adaptors (adaptor is 1:10 diluted) and U excision is performed. We do clean up of the products without size selection, following the kit instruction. We amplify these DNA to a library yield of \u223c1000\u00a0ng using PCR (We use unique dual index E6440S that has both forward and reverse primers, PCR cycles is 15). Libraries were sequenced at 30 million fragment reads per sample following Illumina\u2019s standard protocol using the Illumina cBot and HiSeq 3000/4000 PE Cluster Kit. Base-calling is performed using Illumina\u2019s RTA version 2.7.7. Raw reads in FASTQ format are usually delivered from the sequencing core. If pre-aligned BAM files are delivered, we convert them back into FASTQ files using Samtools (e.g., samtools fastq -1 read_1.fq -2 read_2.fq -0 /dev/null -s /dev/null -n input.bam).\nPreprocessing and mapping. After QC, reads are mapped to the composite reference genome comprised of human and Drosophila chromosomes (human/hg38\u00a0+ Drosophila/dm6).\nDownload the human reference genome sequences (the current version is GRCh38 or hg38) and Drosophila reference genome sequences (the current version is \u201cBDGP Release 6\u00a0+ ISO1 MT\u201d or dm6) reference genome sequences and saved as \u201chg38.fa.gz\u201d and \u201cdm6.fa.gz\u201d, respectively.\nUncompress \u201chg38.fa.gz\u201d and \u201cdm6.fa.gz\u201d.Because both human and Drosophila genomes have the chromosome X (named \u201cchrX\u201d) and chromosome Y (named \u201cchrY\u201d), we need to modify the Drosophila chromosome IDs to make them different from humans. In this case, we add \u201cdm6_\u201d to each chromosome ID of the Drosophila. For example, we change Drosophila\u2019s \u201cchrX\u201d into \u201cdm6_chrX\u201d, \u201cchr2L\u201d into \u201cdm6_chr2L\u201d, and so forth. Although changing the human chromosome IDs is technically equivalent at this step (for example, changing human \u201cchr1\u201d into \u201chg38_chr1\u201d), doing this brings inconvenience even problems present in downstream analyses.\nConcatenate the human reference genome sequences (hg38) and Drosophila reference genome sequences (dm6) into a single FASTA file \u2013 the composite reference genome named \u201chg38_dm6.fa\u201d.\nIndex the \u201chg38_dm6.fa\u201d using the \u201cbowtie2-build\u201d command: bowtie2-build hg38_dm6.fa hg38_dm6\nMap reads to the composite reference genome using Bowtie2. Using Samtools, convert the output alignments into BAM format, sort the BAM files by position, and then index them. Using other short reads aligners such as BWA is fine, as long as the composite reference genome is indexed accordingly.\nRepeat this step for all biological replicates of ChIP and control samples.\nA step-by-step implementation is given below. Assuming \u2018wget\u2019, \u2018gunzip\u2019, \u2018sed\u2019, \u2018bowtie2\u2019, \u2018bowtie2-build\u2019 and \u2018samtools\u2019 commands are already available in a Linux/Unix environment.\nQuantify Drosophila chromatin. The amount of Drosophila chromatin in the sequencing library can be measured by the number of reads that are aligned to the Drosophila genome. Reads in each BAM file can be classified into four categories including:\n\u201cHuman reads\u201d: reads can only map to the human reference genome. These reads are used for peak calling.\n\u201cDrosophila reads\u201d: reads can only map to the Drosophila genome. These reads are used to calculate the scaling factors.\n\u201cBoth\u201d: reads can be mapped to both human and the Drosophila genomes. These reads are tossed out.\u201cNeither (or unmapped)\u201d: reads cannot reliably map to the composite reference genome. Reads with mapping quality (MAPQ)\u00a0< 30 are also included in this category.\n# Step (a): Download reference genome sequences.\n$ wget http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz[href=http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz]\n$ wget http://hgdownload.soe.ucsc.edu/goldenPath/dm6/bigZips/dm6.fa.gz[href=http://hgdownload.soe.ucsc.edu/goldenPath/dm6/bigZips/dm6.fa.gz]\n# Step (b): Uncompress the reference genome files.\n$ gunzip hg38.fa.gz\n$ gunzip dm6.fa.gz\n# Step (c): Change Drosophila chromosome IDs.\n$ sed 's/chr/dm6_chr/g' dm6.fa > dm6_renameID.fa\n# Step (d): Concatenate the human and Drosophila genome files.\n$ cat hg38.fa dm6_renameID.fa > hg38_dm6.fa\n# Step (e): Run \"bowtie2-build\" command to build index files.\n# \"hg38_dm6\" is the prefix of index files, which is needed in the next step.\n$ bowtie2-build hg38_dm6.fa hg38_dm6\n# Step (f): Use \"bowtie2\" to map reads to the composite reference genome (i.e., hg38_dm6.fa); save the alignments in BAM format; sort and index the BAM file using samtools.\n# For paired-end reads\n$ bowtie2 --threads 4 -x GRCh38_dm6 -1 read_1.fastq.gz -2 read_2. fastq.gz | samtools view -Sbh - > output.bam\n# For single-end reads\n$ bowtie2 --threads 4 -x GRCh38_dm6 -U read.fastq.gz | samtools view -Sbh - > output.bam\n# sort the BAM file using samtools\n$ samtools sort -@ 4 output.bam > output.sorted.bam\n# index the BAM file using samtools\n$ samtools index -@ 4 output.sorted.bam\n# Step (g) : Repeat steps (a-f) for other samples.\nRun \u201csplit_bam.py\u201d in the SPIKER package (https://spiker.readthedocs.io/en/latest/index.html[href=https://spiker.readthedocs.io/en/latest/index.html]) to classify reads and split the \u201ccomposite\u201d BAM files into Human and Drosophila BAM files.Calculate scaling factors. For both peak calling (i.e., comparing ChIP to control samples) and quantitative analysis (i.e., comparing different ChIP samples), the number of \u201cDrosophila reads\u201d calculated from different samples need to normalize to the same level. For example, if we want to normalize the number of \u201cDrosophila reads\u201d to 1 million, the scaling factor for a particular sample with 800,000 Drosophila reads is 1000000/800000\u00a0= 1.25.\nPeak calling. The peak calling procedure is described in https://spiker.readthedocs.io/en/latest/usage.html[href=https://spiker.readthedocs.io/en/latest/usage.html]. Depending on the histone marker, ENCODE narrowPeak or gappedPeak are generated. bedGraph and bigwig files are also generated for visualization on UCSC genome browser. A demo dataset and step-by-step walkthrough is also available: https://spiker.readthedocs.io/en/latest/walkthrough.html[href=https://spiker.readthedocs.io/en/latest/walkthrough.html]", "Step-by-step method details\nStep-by-step method details\nThe following are detailed instructions on how to how to implement the biomedical knowledge discovery pipeline. We show examples of each step in a tutorial Jupyter Notebook project called \u201cKnowledge_Discovery_Pipeline.ipynb\u201d, which can be found in our GitHub repository.\niBKH data preprocessing\nTiming: 30\u00a0min\nNote: This section introduces steps for preprocessing the iBKH BKG data.\nThis protocol uses a comprehensive BKG we built, termed iBKH.1[href=https://www.wicell.org#bib1] Figure\u00a02[href=https://www.wicell.org#fig2] illustrates the schema of iBKH. Currently, iBKH includes 11 entity types (including anatomy, disease, drug, gene, molecule, symptom, pathway, side effect, dietary supplement ingredient [DSI], dietary supplement product [DSP], and dietary\u2019s therapeutic class [TC]) and 45 relation types within different entity pairs such as Drug- Disease (DDi), Drug-Drug (DD), Drug-Gene (DG), etc.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3067-Fig2.jpg\nFigure\u00a02. Schema of iBKH knowledge graph\nEach circle denotes an entity type, and each link denotes a meta relation between a pair of entities. Of note, a meta relation can represent multiple types of relations between a specific pair of entities. For example, five potential relations including \u2018Associates\u2019, \u2018Downregulates\u2019, \u2018Upregulates\u2019, \u2018Inferred_Relation\u2019, \u2018Text_Semantic_Relation\u2019 can exist between a pair of disease and gene entities.\nIn a BKG, like the iBKH, a triplet is the smallest unit for storing information. Typically, a triplet can be formulated as   (  h , r , t  )  , where   h   and   t   are the head and tail entities, and   r   is the relation linking   h   to   t  . This section describes the steps for iBKH KG data preprocessing, i.e., extracting and formatting triplets from the iBKH, which will be used to train knowledge graph embedding models.\nOpen the Anaconda-Navigator and launch the Jupyter Notebook.\nIn the Jupyter Notebook interface, run the following codes to import required packages.\n>import pandas as pd\n>import numpy as np\n>import pickle>import torch as th\n>import torch.nn.functional as fn\n>import os\n>import sys\n>sys.path.append('.')\n>import funcs.KG_processing as KG_processing\nExtract triplets from raw data of iBKH.\nSet up the input and output file paths.\n> /\u2217 Input iBKH-KG data path \u2217/\n>kg_folder\u00a0= 'Data/iBKH/'\n> /\u2217 Output path \u2217/\n>triplet_path\u00a0= 'Data/triplets/'\n>if not os.path.exists(triplet_path):\n> os.makedirs(triplet_path)\n> /\u2217 Output data file path \u2217/\n>output_path\u00a0= 'Data/dataset/'\n>if not os.path.exists(output_path):\n> os.makedirs(output_path)\nExtract triplets of different entity pair types by running following codes.\n>KG_processing.DDi_triplets(kg_folder, triplet_path)\n>KG_processing.DG_triplets(kg_folder, triplet_path)\n>KG_processing.DPwy_triplets(kg_folder, triplet_path)\n>KG_processing.DSE_triplets(kg_folder, triplet_path)\n>KG_processing.DiDi_triplets(kg_folder, triplet_path)\n>KG_processing.DiG_triplets(kg_folder, triplet_path)\n>KG_processing.DiPwy_triplets(kg_folder, triplet_path)\n>KG_processing.DiSy_triplets(kg_folder, triplet_path)\n>KG_processing.GG_triplets(kg_folder, triplet_path)\n>KG_processing.GPwy_triplets(kg_folder, triplet_path)\n>KG_processing.DD_triplets(kg_folder, triplet_path)\nNote: This will result in a set of CSV files in the \u201ciBKH-KD-protocol/data/triplets/\u201d, storing triplets regarding each entity pair type.\nCombine the triplets to generate a TSV file based on the DGL-KE input requirement.\n> /\u2217 Specify triplet types you want to use. \u2217/\n>included_pair_type\u00a0= ['DDi', 'DG', 'DPwy', 'DSE', 'DiDi', 'DiG',\n'DiPwy', 'DiSy', 'GG', 'GPwy', 'DD']\n> /\u2217 Combine triplets \u2217/\n>KG_processing.generate_triplet_set(triplet_path=triplet_path)\n> /\u2217 Generate DGL-KE required input triplet file \u2217/\n>KG_processing.generate_DGL_training_set(triplet_path=triplet_path,\u2216\noutput_path=output_path)\nNote: The variable \u201cincluded_pair_type\u201d specifies a list of triplet types that we plan to use for analysis. The generated data files can be found in the folder \u201ciBKH-KD-protocol/data/dataset/\u201d, including \u201ctraining_triplets.txt\u201d, \u201cvalidation_triplets.tsv\u201d, and \u201ctesting_triplets.tsv\u201d, which will be used for training and evaluating the knowledge graph embedding models, as well as \u201cwhole_triplets.tsv\u201d, which will be used for training the final models.\nKnowledge Graph Embedding Learning\nTiming: variable depending on hardware, approximately 8\u201324 h\nNote: This section introduces steps for learning embedding vectors for entities and relations in the iBKH.Knowledge graph embedding aims to learn machine-readable embedding vectors for entities and relations in a BKH (e.g., the iBKH) while preserving the graph structure.9[href=https://www.wicell.org#bib9],10[href=https://www.wicell.org#bib10] We engage four deep learning-based knowledge graph embedding algorithms implemented in the DGL-KE, including TransE,11[href=https://www.wicell.org#bib11] TransR,12[href=https://www.wicell.org#bib12] ComplEx,13[href=https://www.wicell.org#bib13] and DistMult.14[href=https://www.wicell.org#bib14] This section describes the steps for training the models.\nThis step trains each knowledge graph embedding model (TransE, TransR, ComplEx, and DistMult) using the iBKH.\nOpen command line (Windows OS and UNIX OS) or terminal (MAC OS) and change directory to the project as below.\n>cd [your file path]/iBKH-KD-protocol\nTrain and evaluate the knowledge graph embedding model using below command:\n> DGLBACKEND=pytorch \u2216\ndglke_train --dataset iBKH --data_path ./data/dataset \u2216\n--data_files training_triplets.tsv \u2216\nvalidation_triplets.tsv \u2216\ntesting_triplets.tsv \u2216\n--format raw_udd_hrt --model_name [model name] \u2216\n--batch_size [batch size] --hidden_dim [hidden dim] \u2216\n--neg_sample_size [neg sample size] --gamma [gamma] \u2216\n--lr [learning rate] --max_step [max step] \u2216\n--log_interval [log interval] \u2216\n--batch_size_eval [batch size eval] \u2216\n-adv --regularization_coef [regularization coef] \u2216\n--num_thread [num thread] --num_proc [num proc] \u2216\n--neg_sample_size_eval [neg sample size eval] \u2216\n--save_path ./data/embeddings --test\nNote: We use multiple measurements to evaluate model performances including: HITS@k, the average number of times the positive triplet is among the k highest ranked triplets; Mean Rank (MR), the average rank of the positive triplets; and Mean Reciprocal Rank (MRR), the average reciprocal rank of the positive instances. Higher values of HITS@k and MRR and a lower value of MR indicate good performance, and vice versa. Some useful arguments of the DGL-KE command are listed in Table\u00a01[href=https://www.wicell.org#tbl1]. Detailed instructions for the DGL-KE commands can be found at: https://dglke.dgl.ai/doc/train.html[href=https://dglke.dgl.ai/doc/train.html].\ntable:files/protocols_protocol_3067_1.csv\nOnce the model can achieve a desirable performance in the testing set, we can re-train the model using the whole dataset by running:\n> DGLBACKEND=pytorch \u2216\ndglke_train --dataset iBKH --data_path ./data/dataset \u2216--data_files whole_triplets.tsv \u2216\n--format raw_udd_hrt --model_name [model name] \u2216\n--batch_size [batch size] --hidden_dim [hidden dim] \u2216\n--neg_sample_size [neg sample size] --gamma [gamma] \u2216\n--lr [learning rate] --max_step [max step] \u2216\n--log_interval [log interval] \u2216\n-adv --regularization_coef [regularization coef] \u2216\n--num_thread [num thread] --num_proc [num proc] \u2216\n--save_path ./data/embeddings\nNote: This will generate two output files for each model: \u201ciBKH_[model name]_entity.npy\u201d, containing the low dimension embeddings of entities in iBKH and \u201ciBKH_[model name]_relation.npy\u201d, containing the low dimension embeddings of relations in iBKH. These embeddings can be used in downstream BKD tasks.\nWe run above procedures based on TransE, TransR, ComplEx, and DistMult, respectively, to gain embedding vectors of entities and relations in the iBKH.\nNote: The user may repeat the Step 4b multiple times to find the optimal hyperparameters of each model. Here, we share the optimal hyperparameter values we found in our experiments as listed in Table\u00a02[href=https://www.wicell.org#tbl2]. For simplicity, the user can directly use the suggested hyperparameter values to train the models. In addition, running time of the knowledge graph embedding procedure varies, depending on hardware used. For our experiments, we used a machine equipped with an Intel i7-7800X CPU, boasting 6 cores and 12 threads, with a fundamental clock speed of 3.5 GHz, coupled with 62 GB of RAM. Training the four knowledge graph embedding models within the dataset took approximately 8 hours in our experiment. The required running time could extend to 24 hours or even more if a user expects to tune the models to find the optimal hyperparameters for enhancing model performance.\ntable:files/protocols_protocol_3067_2.csv\nBiomedical knowledge discovery\u00a0\u2013 biomedical hypothesis generation\nTiming: 30\u00a0min\nNote: This section introduces the implementation of BKD based on knowledge graph embeddings learned from iBKH.\nHere, we showcase a case study of drug repurposing hypothesis generation for Parkinson\u2019s disease (PD).Turn to the Jupyter Notebook interface and run the following script to import required package packages.\n>from funcs.KG_link_pred import generate_hypothesis,\u2216\ngenerate_hypothesis_ensemble_model\nDefine the PD entity using\n> PD\u00a0= [\"parkinson's disease\", \"late onset parkinson's disease\"]\nNote: Here we collect a list of PD terms. These PD terms can be obtained in the entity vocabularies in the \u201cdata/iBKH/entity\u201d folder.\nThe task is to predict drug entities that don\u2019t have \u201ctreats\u201d and \u201cpalliates\u201d relationships with PD in the iBKH but can potentially treat or palliate PD. Therefore, we define a relation type list:\n> r_type\u00a0= [\"Treats_DDi\", \"Palliates_DDi\"]\nNote: More relation types can be found in the \u201cdata/iBKH/relation\u201d folder.\nPredict repurposable drug candidates for PD (in this example, we use embedding vectors based on the TransE model for prediction):\n> proposed_df\u00a0= generate_hypothesis(target_entity=PD,\ncandidate_entity_type='drug',\nrelation_type=r_type,\nembedding_folder='data/embeddings',\nmethod='transE_l2',\nkg_folder\u00a0= 'data/iBKH',\ntriplet_folder\u00a0= 'data/triplets',\ntopK=100, save_path='output',\nsave=True, without_any_rel=False)\nRunning the above code will result in an output CSV file within the \u201coutput\u201d folder, which stores top-100 ranked repurposable drug candidates for PD based on the TransE model.\nNote: Please refer to Table\u00a03[href=https://www.wicell.org#tbl3] for detailed information regarding arguments of the function.\ntable:files/protocols_protocol_3067_3.csv\nUsing the code in Step 8 can make predictions based on a single knowledge graph embedding model (TransE in the example). To enhance prediction performance, we also proposed an ensemble model, which combines the four embedding algorithms to make predictions. In our preliminary work, we have demonstrated that the ensemble model can improve knowledge discovery performance in iBKH.1[href=https://www.wicell.org#bib1] The following code introduces the usage of the ensemble model to predict repurposable drug candidates for PD.\n> proposed_df\u00a0= generate_hypothesis_ensemble_model (target_entity=PD,\ncandidate_entity_type='drug',\nrelation_type=r_type,\nembedding_folder='data/embeddings',\nkg_folder\u00a0= 'data/iBKH',\ntriplet_folder\u00a0= 'data/triplets',\ntopK=100, save_path='output',\nsave=True, without_any_rel=False)Running the above code will result in an output CSV file within the \u201coutput\u201d folder, which stores top-100 ranked repurposable drug candidates for PD based on the ensemble model.\nNote: Please refer to Table\u00a03[href=https://www.wicell.org#tbl3] for detailed information regarding arguments of the function.\nPrediction result interpretation\nTiming: 30\u00a0min\nNote: This section introduces the procedure of interpreting the prediction results based on the iBKH.\nWe extract the shortest paths that connect the target entity (e.g., PD) with the predicted entities (e.g., the predicted repurposing drug candidates of PD) to generate a contextual subnetwork.\nTaking the PD drug repurposing task as an example, we can generate the contextual subnetwork surrounding PD and some predicted repurposing drug candidates as below.\nImport required package.\n>from funcs.knowledge_visualization as kv\nSpecify the predicted drug candidates to interpret. Here, we focus on the top four candidates predicted using the ensemble model, including Glutathione, Clioquinol, Steroids, and Taurine.\n> drug_list\u00a0= ['Glutathione', 'Clioquinol', 'Steroids', 'Taurine']\nCreate a contextual subnetwork linking PD and the drug candidates.\n> kv.subgraph_visualization(target_type='Disease',\ntarget_list=PD,\npredicted_type='Drug',\npredicted_list=drug_list,\nneo4j_url\u00a0= \"neo4j://54.210.251.104:7687\",\nusername\u00a0= \"neo4j\", password\u00a0= \"password\",\nalpha=1.5, k=0.8, figsize=(15, 10),\nsave=True)\nThis will result in a figure saved as a PDF file in the \u201coutput\u201d folder. Please refer to Table\u00a04[href=https://www.wicell.org#tbl4] for detailed information regarding arguments of the function.\ntable:files/protocols_protocol_3067_4.csv\nNote: The shortest path query is based on iBKH deployed using the Neo4j, an efficient graph database. Please refer to https://github.com/wcm-wanglab/iBKH#neo4j-deployment[href=https://github.com/wcm-wanglab/iBKH] for detailed information if you want to create your own iBKH Neo4j instance.", "Step-by-step method details\nStep-by-step method details\nPre-imaging preparations\nTo generate reproducible results, especially in vivo live transport assay in C.\u00a0elegans, the pre-imaging preparations are as critical as subsequent steps, including microscopy imaging and post-imaging analysis. For example, the age of the worm affects the frequency of kinesin and dynein-driven IFT motility along cilia in C.\u00a0elegans (Zhang et\u00a0al., 2021[href=https://www.wicell.org#bib9]). Furthermore, because unfavorable environmental conditions, including starvation and contamination (mold, fungi, etc.) may impact the health of C.\u00a0elegans, they likely influence a variety of cellular processes, especially IFT. Consistent with this logic, starvation increases the speed of anterograde IFT (from the base of cilia to the tip of cilia) in C.\u00a0elegans (Kimura et\u00a0al., 2018[href=https://www.wicell.org#bib4]). We recommend having contamination-free, well-fed, and synchronized young worms (1 and 2\u00a0days old) for the microscopy analysis. For reproducible results, cultivating healthy C.\u00a0elegans in a sterile Petri dish with contamination-free NGM (Nematode Growth Media) spread with OP50 strain at well-controlled temperatures (20\u00b0C) is critical.\nPreparation of next steps for in vivo live microscopy analysis\nTiming: 1\u00a0day\nCulture worms for at least one generation without starving and any contaminations at 20\u00b0C.\nThe day before microscopy analysis, transfer 100 L4s (fourth-stage larvae, the last stage of larvae before becoming adults) into a new plate, and the microscopy analysis can be done on a 1-day adult.\nPrepare 3% of agarose in a 250\u00a0mL bottle and keep it at 65\u00b0C in the water bath.\nCritical: We do not advise allowing agarose to re-solidify since it necessitates re-melting with the microwave, which changes the percentage of agarose in each microwave heating.Note: We do not recommend using the freshly prepared 3% of agarose on the day of preparation, wait for 1\u00a0day, because when making an agarose pad in the subsequent days, agarose pads become less brittle, giving better agarose pads. You can use the 3% of agarose up to 7\u00a0days.\nFor immobilization of the worms for in\u00a0vivo live microscopy analysis, 10\u00a0mM is working concentration for levamisole, and prepare 10\u00a0mM levamisole (an anesthetizing reagent) for 100\u00a0\u03bcL from a 500\u00a0mM stock solution by diluting in M9 buffer.\nCritical: There are several anesthetics that can be utilized, including sodium azide, levamisole, tricaine, and 1-Phenoxy-2-propanol. Because of its effect on ATP, sodium azide has an effect on IFT motility and is not recommended for the IFT motility experiment.\nCritical: Imaging living C.\u00a0elegans is always challenging and requires a lot of effort as C.\u00a0elegans tend to display movements during live imaging. Levamisole alone will not effectively immobilize the worms, and in our experience, there will be zero movements in 5 of the 25 videos collected for in\u00a0vivo live microscopy analysis when only anesthetic levamisole was used, thus increasing the overall effort of microscopy analysis. A microfluidic device or polystyrene beads, or a higher concentration of agar pads are alternative ways of immobilizing worms (Kim et\u00a0al., 2013[href=https://www.wicell.org#bib3]; Clark et\u00a0al., 2021[href=https://www.wicell.org#bib2]). They can be used in conjunction with levamisole.\nPreparations for the time of image acquisition\nTiming: 4\u20135 hBefore beginning image acquisition, ensure that the imaging room\u2019s temperature is steady (21\u00b0C\u201322\u00b0C) and not fluctuating. Check worms on the day of microscopy imaging to make sure that worms are healthy. We employ a variety of parameters to check the health of worms: 1) They are moving in the plate. 2) There is no bacterial or fungal contamination in the plate. 3) We also make sure they have adequate food on their plate.\nBegin by turning on the computer and fluorescent microscope (in our example, a Leica DM6 B upright fluorescence microscope), then the camera (Andor iXon Ultra 897 EMCCD Camera), and last the controlling software (Andor iQ 3.6.2 Software). It is worth noting that inverted scope can also be employed here. Find fluorescence signal on the slide with a 20\u00d7 objective lens, then apply a small amount of immersion oil before rotating a 100\u00d7 objective lens onto the slide to begin taking a live time-lapse movie (HCX PL APO 100\u00d7, 1.40 oil microscope objective).\nCritical: To execute the software and subsequent image analysis, a high-performance computer system is required. However, image processing should be performed on a separate computer so that the computer connected to the microscope is not occupied. A high-quality microscope is necessary, but without high-quality cameras and suitable software, it is not possible to acquire high-quality time-lapse recordings for further analysis. Before purchasing a microscopy complete system for live time-lapse movie analysis, we recommend trying the microscope, camera, and software together.A running protocol must first be set up to take a live time-lapse movie. We therefore spent time finding a suitable exposure time and the gain setting on a fluorescence microscope for an IFT component (IFT-74::GFP; human IFT74) in C.\u00a0elegans. We set the exposure time to 300.0\u00a0ms and the gain to 7. We next set up a running protocol of 333.0\u00a0ms per second (3 frames per second (s)) with a total of 100 frames on the Andor iQ 3.6.2 Software.\nCritical: The longer camera exposure time produces a sharper image with higher pixel intensity while also photobleaching i.e., the reduction of the sample's fluorescence signal during image capture. The sample will be exposed to light 3 times per second because the protocol is designed to take 3 images per second, resulting in a decrease in the fluorescent signal. A live time-lapse movie, on the other hand, necessitates a particular amount of time, during which light is gathered from the chosen object. The main issue is that the beginning part of the time-lapse movie may have brighter pictures, but the brightness of the images decreases in the later part of the time-lapse movie, which may damage the overall quality of the time-lapse movie. For better quality of time-lapse movies, we recommend reducing the length of the time-lapse movie with a longer exposure time.\nCritical: Because of the nature of endogenously tagged fluorescent proteins, their expression levels might vary and some may be inadequate to be detected by the microscope. Playing with the exposure time and gain will not assist any more until the fluorescent expression is detectable by the microscope system. Make sure to start with a strain containing fluorescent-tagged protein that is well expressed.Use Pasteur pipette to take a little amount of 3% (0.3\u00a0mL) agarose (stored at 65\u00b0C in the water bath). Drop the agarose liquid onto the 18\u00a0mm\u00a0\u00d7\u00a018\u00a0mm area of the microscope slide, then place the second slide over the agarose to create a smooth and thin layer. Gently press and wait 1\u00a0min for the agarose to solidify. Slowly separate slides from each other without disrupting the agarose pad (Methods video S1[href=https://www.wicell.org#mmc1]: Slide preparation, related to steps 1\u20134).\nCritical: Agarose pads must be freshly made at the time of image capturing, and once prepared, they must be utilized within 1\u00a0minute.\nAdd 2\u00a0\u03bcL of 10\u00a0mM levamisole over the agarose pad under a stereomicroscope (ZEISS Stemi 508), and then transfer 10\u201315 1-day adult worms using a platinum wire pick onto agar pads. Put cover slide above the worms and gently press. After mounting, we immediately transfer the slide to the fluorescent compound scope for subsequent microscopy analysis (Methods video S1[href=https://www.wicell.org#mmc1]: Worm transformation, related to steps 5\u20139).\nCritical: Do not harm the worm during transferring; for example, the worm's skin may be injured as a result of the pick's tip, resulting in subsequent bursting while transferring, hence it may affect the result of the IFT assay.\nPlace the slide under the microscope, dab a drop of immersion oil on the cover slide, and look for a worm tail to film. Focus on the GFP signal in the cilia of two phasmid sensory neurons (PHA and PHB) in the tail of worms and start running the protocol that was already set up (Methods video S1[href=https://www.wicell.org#mmc1]: Focusing fluorescent signal, related to steps 12 and 13).Critical: When considering the movement of worm parts and the presence of ciliated cells, recording a worm from the tail is less problematic than filming a worm from the head.\nCritical: Putting mouse pointer over the cilia or a part of the cilia can help to understand movement during the recording.\nCritical: We recommend finishing the time-lapse movie within 30\u00a0minutes after mounting the slide. In our experience, we notice that after some time, the speed of IFT particles slows down, which might result in inconsistent results.\nSave all time-lapse series of images in TIFF format taken from the image list section to distinct files for later use with kymograph, speed, and frequency analysis.\nCritical: Discard the time-lapse series of images where worms move. Alternatively, the ImageJ plugin for drift correction can be used to help correct movements in time-lapse movies. The following plugin can be downloaded for manual drift correction: https://imagej.net/plugins/manual-drift-correction#:\u223c:text=Manual%20Drift%20Correction%20plugin%20allows,by%20interpolating%20between%20key%20landmarks[href=https://imagej.net/plugins/manual-drift-correction#:%7E:text=Manual%20Drift%20Correction%20plugin%20allows,by%20interpolating%20between%20key%20landmarks].\nCritical: The image file size for each time-lapse TIFF file of 100 frames is 200.0 megabytes (MB). Make sure you have enough storage for all microscope analyses, and that you keep the original files. The main challenge, however, is the long-term storage of each time-lapse image. The external hard drives are easy to fail. Cloud storage is another option for long-term data storage, although it needs ongoing subscriptions.\n    Your browser does not support HTML5 video.\n  \nMethods video S1. Preparations for image acquisition, related to steps 1\u201313\nPost-imaging analyses\nTiming: 1\u20133 h\nFor generating Kymographs and speed analysis, we use two software: ImageJ, a freely available image analysis software, and KymographDirect and KymographClear, automated packages for kymograph analysis, from https://imagej.nih.gov/ij/[href=https://imagej.nih.gov/ij/] and https://sites.google.com/site/kymographanalysis[href=https://sites.google.com/site/kymographanalysis], respectively (Mangeol et\u00a0al., 2016[href=https://www.wicell.org#bib5]; Schindelin et\u00a0al., 2012[href=https://www.wicell.org#bib7]). Download and install both of them.Once KymographDirect and KymographClear related files are downloaded from the website mentioned above, copy and paste the \u201cKymographClear 2.0a.ijm\u201d, \u201cKymographDirect2.1.aliases\u201d, and \u201cKymographDirect2.1.ini\u201d to the \u201ctoolsets\u201d file in the \u201cmacro\u201d file in the ImageJ file. Open ImageJ and, install \u201cKymographClear 2.0a.ijm\u201d using the install button on Plugins. Additional buttons will appear on ImageJ which allow users to run KymographDirect, and KymographClear on ImageJ. The user must install the kymograph generation-related plugins to the ImageJ panel each time opens.\nNote: KymographDirect and KymographClear website provides step-by-step instructions about how to install and use these tools: https://sites.google.com/site/kymographanalysis[href=https://sites.google.com/site/kymographanalysis].\nCopy and paste your TIFF files into their dedicated new files created for each TIFF at a particular location on your computer.\nCritical: Dedicated files for each time-lapse TIFF will help avoid confusion. Kymographs are created automatically from each TIFF file during TIFF file analysis; thus, no kymographs will be mixed up when they have their dedicated files.\nDetermining the frequency of anterograde and retrograde IFT\nTiming: 1\u20132\u00a0days\nOpen a single TIFF file with the Kymograph Open Sequence plugin (KymographClear plugin) (yellow file figure in the panel) previously installed on ImageJ. Choose the first of two options: \u201copen a sequence and compute average image\u201d or \u201copen a sequence and compute max intensity image\u201d. After you upload the TIFF file, the \u201cSequence options\u201d tab will appear. In the \u201cSequence options\u201d tab, type 1 in the section of \u201cnumber of images\u201d, then click OK. The time-lapse image will appear and zoom on cilia (Methods video S2[href=https://www.wicell.org#mmc2]: Opening files with ImageJ, related to steps 1 and 2).\nNote: Fiji is an ImageJ2-based image analysis software option, however, subsequent analysis needs kymographs created using KymographDirect and KymographClear, both of which are incompatible with Fiji.Critical: ImageJ and Fiji have alternative kymograph generation plugins, but they require manual drawing of lines in order to produce the kymograph. With manual drawing, two lab members separately did an internal study using the same movies and determined IFT speeds, and their findings were slightly different. The discrepancy was most likely caused by differences in kymograph drawing lines (the slope of line drawings differed). Furthermore, we compared manual kymograph analysis findings to those obtained by KymographDirect and KymographClear using the same videos and discovered that the average IFT speed of manual kymograph analysis was slower than that of KymographDirect and KymographClear.\nMethods video S2[href=https://www.wicell.org#mmc2]: Time-lapse microscopy images of yeast cells, related to step 1.\nUsing ImageJ\u2019s panel, draw a segmented line above the cilia from the base to the end (entire cilia). The line delineates the area where the kymograph will be built and measurements (IFT speed and frequency) will be taken (Methods video S2[href=https://www.wicell.org#mmc2]: Determining cilia parts, related to step 3).\nNext, in the KymographDirect toolbar, select \u201cKymograph generation action tool\u201d and set the line width to 5 to produce kymographs for anterograde and retrograde motilities. Kymographs (Anterograde, retrograde, and merged) and speed-related files are automatically saved in the same folder as the TIFF file (Methods video S2[href=https://www.wicell.org#mmc2]: Kymograph generation, related to step 4).Launch the KymographDirect tool and upload kymograph from the file. In the KymographDirect tool, the subpanel is located on the left panel, and the kymograph is in the middle. Anterograde particles appear on a kymograph when the \u201cforward\u201d option is selected, whereas retrograde particles appear when the \u201cbackward\u201d option is selected. The quantity of anterograde and retrograde particles may be observed in the subpanel\u2019s \u201cSelect a line\u201d option (Methods video S2[href=https://www.wicell.org#mmc2]: Counting particles, related to step 6). After noting the number of anterograde and retrograde particles in the time-lapse videos examined, you can proceed to the next time-lapse movies.\nCritical: Intensity threshold and brightness & contrast settings help to reduce background noise during the counting process if you want to check lines as manually. Playing with the threshold on the subpanel will change the number of particles.\nCreate an excel file to save all particle numbers with the experiment name and directions for further analysis (Methods video S2[href=https://www.wicell.org#mmc2]: Counting particles, related to step 6).\nTo calculate the average frequency of anterograde or retrograde transport events for each strain, use at least 10 kymograph findings.\n    Your browser does not support HTML5 video.\n  \nMethods video S2. Post-imaging analyses, related to steps 1\u201311\nDetermining the average speed of anterograde and retrograde IFT\nTiming: 1\u20132\u00a0daysIn C.\u00a0elegans, ciliary cargos and signaling molecules are distributed across cilia by kinesin and dynein driven IFT complex. Anterograde IFT, mediated by the kinesin motor, refers to transport happenings from the ciliary base to the ciliary tip, whereas retrograde IFT, mediated by cytoplasmic dynein, refers to transport events that occur from the ciliary tip back to the ciliary base. C.\u00a0elegans channel cilia have a middle segment (\u223c4\u00a0\u03bcm long, microtubule doublets) and a distal segment (\u223c2.5\u00a0\u03bcm long, microtubule singlets). In the middle segment of channel cilia, Kinesin II and OSM-3 (human KIF17) motors move at a slower rate (the mean velocity of 0.7\u00a0\u03bcm/s) carrying IFT components, IFT cargos, and signaling molecules, and all of these cargos are handed over to OSM-3, which travels at a quicker rate (the mean velocity of 1.2\u00a0\u03bcm/s). The retrograde transport is achieved by cytoplasmic dynein at a velocity of 1.10\u00a0\u03bcm/s. In the following protocol, we will go over kymograph-based IFT speed measurements using the KymographDirect tool.\nTo measure the average speed of the anterograde IFT in the middle segment, draw a segmented line above the cilia from the base to the middle segment (4\u00a0\u03bcm), and repeat the step 25 (Figures\u00a02[href=https://www.wicell.org#fig2]A and 2B). In a brief, use the KymographDirect toolbar to generate kymograph for anterograde and retrograde transport events. System automatically calculates the average anterograde IFT and retrograde IFT speeds.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1790-Fig2.jpg\nFigure 2. IFT velocity for anterograde and retrograde directions for IFT-74::GFP(A) The wild type phasmid sensory cilia (PHA and PHB cilia) are shown with GFP tagged endogenous IFT-74. IFT-74 (green) stains the whole cilium, including the basal body (BB), middle segment (MS), and distal segment (DS). A white arrow indicates the tip of the cilia. The red and yellow arrows represent the locations where the MS and DS kymographs were created, respectively. Scale Bar: 1 \u03bcm.\n(B) The kymographs for the middle and distal segments are created with the ImageJ software\u2019s KymographClear2.0 plugin. The red and green lines on the merged kymographs for each kymographs depict particles traveling anterograde and retrograde directions, respectively. Separate anterograde and retrograde kymographs (grayscale) are exhibited under combined kymographs with representative drawing panels of particles. The yellow and red arrow marks on the combined kymographs reflect the kymograph\u2019s orientation for MS and DS, respectively. The x-axis denotes distance (m), while the y-axis denotes time (30 s). Scale bar for distance and time is shown on the merged kymographs.\n(C) The average anterograde and retrograde velocities of IFT-74::GFP in the middle and distal segments are displayed. The number of particles (n) and worms (N) examined is shown in the table. SD stands for standard deviation.\nUsers should hit \u201cSave\u201d on the upper save button on the KymographDirect tool to save files to an existing file called \u201cResults.\u201d. The file named \u201cvelocity vs. time\u201d files will be used to calculate average anterograde IFT velocities in the middle segment, and we have supplied a walkthrough video to help you calculate average IFT speeds (Methods video S2[href=https://www.wicell.org#mmc2]: Speed, related to step 7).To determine the average velocity of the anterograde IFT in the distal segment, draw a 2.5\u00a0\u03bcm segmented line from the ciliary tip and repeat the step 16 and rest of the steps mentioned above (Figures\u00a02[href=https://www.wicell.org#fig2]A and 2B). This procedure may be repeated to calculate the average IFT frequency for retrograde, with kymographs obtained to calculate the average IFT frequency.\nNext, open the \u201cResults\u201d file and choose the \u201cvelocity vs. time\u201d text file. Copy-paste all results from this text file to an excel file to find all results for further analysis (Methods video S2[href=https://www.wicell.org#mmc2]: Saving velocity results, related to steps 8 and 9).\nAverage all velocity and time results for the whole lines in the excel file, separately (Methods video S2[href=https://www.wicell.org#mmc2]: Average speed calculation, related to steps 10 and 11).\nAdd more analysis until achieving at least 100 particles for each strain. Then, calculate the average velocity in the overall experiments. We always undertake two independent analyses and then increase the number of IFT particles whenever we observe statistically significant differences in IFT rates between the wild type and mutant strains examined.\nOptional: A more visually appealing display of merged kymograph images would aid readers in estimating the number of moving particles. KymographClear2.0 generates a merged kymograph in the kymograph file called color-coded direction. This kymograph has anterograde and retrograde directions on a static blue backdrop. To create a visually appealing kymograph during the post-imaging process split the color-coded kymograph using the \"Color\" option from ImageJ's Image subpanel, and close the blue TIFF file while keeping red and green TIFF files. Arrange the green and red fluorescent signals using brightness/contrast parameters, then merge them using the \"Color\" option, \"Merge Channels.\" The newly generated kymograph is more aesthetically evident than the color-coded kymograph (Methods video S3[href=https://www.wicell.org#mmc3]).Your browser does not support HTML5 video.\n  \nMethods video S3. Kymograph preparation", "Step-by-step method details\nStep-by-step method details\nDownload our package and install the prerequisites\nTiming: \u00a0< 10\u00a0min\nDownload the latest version of FusionAI into your preferred directory. The running will be executed inside of this directory (a, b, c, and d are required for running FusionAI. e, f and g are required to draw feature landscape images for the chosen fusion genes):\nDownload FusionAI_pred.py from https://compbio.uth.edu/FusionGDB2/FusionAI/FusionAI_pred.py[href=https://compbio.uth.edu/FusionGDB2/FusionAI/FusionAI_pred.py]\nDownload FusionAI model (newdat_newmod_jj.h5) from https://compbio.uth.edu/FusionGDB2/FusionAI/newdat_newmod_jj.h5[href=https://compbio.uth.edu/FusionGDB2/FusionAI/newdat_newmod_jj.h5]\nDownload preprocessing script (pre_processing_for_FusionAI_from_tab_delim.py) from https://compbio.uth.edu/FusionGDB2/FusionAI/pre_processing_for_FusionAI_from_tab_delim.py[href=https://compbio.uth.edu/FusionGDB2/FusionAI/pre_processing_for_FusionAI_from_tab_delim.py]\nDownload example fusion gene file (k562_starfusion.txt) https://compbio.uth.edu/FusionGDB2/FusionAI/k562_starfusion.txt[href=https://compbio.uth.edu/FusionGDB2/FusionAI/k562_starfusion.txt]\nDownload 44 human genomic feature information files (features.tar.gz, features_info.txt, and chromosome_size.txt) from https://compbio.uth.edu/FusionGDB2/FusionAI/features.tar.gz[href=https://compbio.uth.edu/FusionGDB2/FusionAI/features.tar.gz], https://compbio.uth.edu/FusionGDB2/FusionAI/features_info.txt[href=https://compbio.uth.edu/FusionGDB2/FusionAI/features_info.txt], and https://compbio.uth.edu/FusionGDB2/FusionAI/chromosome_size.txt[href=https://compbio.uth.edu/FusionGDB2/FusionAI/chromosome_size.txt]\nDownload human gene structure file and nib files (gencode_hg19v19_.txt and nib_files_hg19.tar.gz) from https://compbio.uth.edu/FusionGDB2/FusionAI/gencode_hg19v19_.txt[href=https://compbio.uth.edu/FusionGDB2/FusionAI/gencode_hg19v19_.txt] and https://compbio.uth.edu/FusionGDB2/FusionAI/nib_files_hg19.tar.gz[href=https://compbio.uth.edu/FusionGDB2/FusionAI/nib_files_hg19.tar.gz]\nInstall R packages using the command install.packages(). Input the individual R package name into the parenthesis like install.packages(\u2018devtools\u2019). These\nPrepare input data of 20 Kb DNA sequence of fusion genes\nTiming: \u00a0< 1\u00a0min\nFusionAI takes the input data of fusion gene breakpoint information, which is given by other fusion gene prediction tools or known fusion gene information (k562_starfusion.txt and Table 2[href=https://www.wicell.org#tbl2]). The preprocessing script will make 20 Kb DNA sequences for individual fusion genes, which is the combined sequence of\u00a0+/-5 Kb flanking sequence from the two breakpoints\u2019 genomic position for individual fusion partner genes (Figure\u00a01[href=https://www.wicell.org#fig1] and Table 3[href=https://www.wicell.org#tbl3]).\ntable:files/protocols_protocol_1463_2.csv\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1463-Fig1.jpg\nFigure\u00a01. Make input data of FusionAI\ntable:files/protocols_protocol_1463_3.csv\nRun preprocessing script to make a 20 Kb DNA sequence from the given fusion gene information. The fusion gene information should include the following information in tab-delimited format: Hgene, Hchr, Hbp, Hstrand, Tgene, Tchr, Tbp, Tstrand. The command is shown below. Here the $ INPUT_FILE is the output file after checking the junction position of the fusion breakpoints in step 2.\n>python pre_processing_for_FusionAI_from_tab_delim.py [INPUT_FILE]\n>python pre_processing_for_FusionAI_from_tab_delim.py k562_starfusion.txtCritical: The timing is based on the number of fusion genes of the input file.\nRun FusionAI\nTiming: \u00a0< 2\u00a0s (depending on your data)\nFusionAI takes the 20 Kb DNA sequence of fusion genes from the previous step and outputs two probabilities as not being used and being used as the fusion gene breakpoints (Figure\u00a02[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1463-Fig2.jpg\nFigure\u00a02. Diagram of fusion gene breakpoints classification by FusionAI\nRun FusionAI prediction script to predict the fusion breakpoint tendency from the FusionAI model. Here the $ INPUT_FILE is the output file after making the 20 Kb DNA sequence in the previous step. $COLA and $COLB are the DNA sequences of 5\u2032 and 3\u2032 fusion partner genes that were created from the previous step. If the user wants to run for one specific fusion gene, then set $INDEX_OF_FUSION as row index of interested line in the input file.\n>python FusionAI_pred.py [-h] -f [INPUT_FILE] -m [MODEL, default: newdat_newmod_jj.h5] -o [OUTPUT_FILE] -A [COLA] -B [COLB] -I [INDEX_OF_FUSION]\n>python FusionAI_pred.py -f k562_starfusion.FusionAI.input -o k562_starfusion.FusionAI .output -m newdat_newmod_jj.h5\nSelect high scored fusion genes (or interested fusion genes) from FusionAI output\nTiming: \u00a0< 5 sFrom the output scores of FusionAI for the fusion candidates that were predicted from other tools, the users can select high scored or interested fusion genes. This can be done by the user in a text editor or another appropriate tool of choice. The users can stop the pipeline at this step if they do not need to do further analyses including feature importance analysis or drawing a landscape image of human genomic features in fusion genes, which take relatively long. With the output scores of FusionAI, still uses can reduce the false positives. For better understanding, Table 4[href=https://www.wicell.org#tbl4] shows the comparison results among different cutoff of FusionAI scores, other prediction tools, and experimentally validated fusion genes. Table 5[href=https://www.wicell.org#tbl5] shows the accuracy comparisons. When we used a higher threshold of FusionAI output scores, we could reduce the false positives efficiently.\ntable:files/protocols_protocol_1463_4.csv\ntable:files/protocols_protocol_1463_5.csv\nSort the FusionAI prediction output based on the FusionAI scores of individual fusion genes and select high-scored fusion genes. The users can choose the cutoff score, which should be larger than 0.5. Table 4[href=https://www.wicell.org#tbl4] below shows the examples that were chosen with different cutoffs like 0.5 or 0.95. Then, the selected fusion genes will be used for further analyses such as screening of the feature importance scores and landscaping the human genomic features across 20 Kb fusion DNA sequence in the following steps.\nCalculate the feature importance scores across 20 Kb DNA sequence\nTiming: \u00a0< 33\u00a0minAfter selecting the reliable fusion gene candidates, the users can check the distribution of the feature importance scores of individual fusion genes across the 20 Kb fusion DNA sequence. To calculate the feature importance score (FIS), we masked 20\u00a0bp each time by setting all the 20 values to zero and measured the change of prediction outcome upon this masking. We slide this 20\u00a0bp window 20 nucleotides each time along the whole 20K input sequence and repeated the procedure to obtain the FIS for all the 20\u00a0bp segments. In this way, we got 20,000/20\u00a0= 1,000 FIS for each input sequence.\nRun FusionAI feature importance score script to get the feature importance scores across the 20 Kb fusion DNA sequence. Here the $ INPUT_FILE is the output file after making 20 Kb DNA sequence in step 3. $COLA and $COLB are the DNA sequences of 5\u2032 and 3\u2032 fusion partner genes that were created from step 3. If the user wants to run for one specific fusion gene, then set $INDEX_OF_FUSION, the row indexes of interested lines in the input file. If the user can use multiple GPUs, then the user can control the number of GPUs using the parameter of NGPUS. However, the GPU is not necessary (Figure 3[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1463-Fig3.jpg\nFigure\u00a03. Calculate the feature importance scores across 20 Kb fusion DNA sequence\n>python FusionAI_FIS.py [-h] -f FILENAME [-m MODEL, default: newdat_newmod_jj.h5] [-o OUTPUT] [-A COLA] [-B COLB] [-I ROWI] [-N NGPUS]\n>python FusionAI_FIS.py -f k562_starfusion.FusionAI.output -o k562_starfusion.FusionAI.output.FIS\nVisualize 44 human genomic features across 20 Kb DNA sequence\nTiming: \u00a0< 1\u00a0h 10\u00a0min and\u00a0< 20\u00a0min for step 6 and 7, respectivelyAfter getting reliable fusion gene candidates and feature importance scores, it is important to interpret the aspect of human genomic features. From our original work, we integrated 44 human genomic features across five important cellular mechanism categories such as integration site category of 6 viruses, 13 types of repeat category, 5 types of structural variant category, 15 different types of chromatin state category, and 5 gene expression regulatory category (Kim et\u00a0al., 2021a[href=https://www.wicell.org#bib10], 2021b[href=https://www.wicell.org#bib11]). From this step, the users can create two figures on the landscape of the fusion gene breakpoint-related genomic features across the 20 Kb fusion DNA sequence. Each script will create separate figures of individual fusion genes that have the FIS values from the previous step. All figures will be created under the user defined directory. The first figure is the overlap between the 20 Kb fusion DNA sequence and 44 genomic features and the second figure is the overlap between the top 1% FIS regions and 44 genomic features (Figure 4[href=https://www.wicell.org#fig4]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1463-Fig4.jpg\nFigure\u00a04. Left - distribution of 44 human genomic features across 20 Kb fusion DNA sequence\nRight - overlap between the top 1% FIS regions and 44 different types of human genomic features across 20 Kb fusion DNA sequence.\nVisualize 44 human genomic features across a 20 Kb DNA sequence. Run FusionAI genomic feature analysis script to make a landscape image of overlap between fusion breakpoints area (+/- 5 Kb) and 44 human genomic features.\n>Rscript FusionAI_genomic_features.R -g [FUSION_GENE_FILE] -f [FEATURE_PATH] -s [CHROMOSOME_SIZE_FILE] -i [FEATURE_INFO_FILE] -o [OUTPUT_FILE_PATH]\n>Rscript FusionAI_genomic_features.r -g K562_STARfusion.FusionAI.output.FIS -f ./features/ -s chromosome_size.txt -i features_info.txt -o ./K562/whole_features/Visualize the overlaps between the top 1% FIS regions and 44 human genomic features across 20 Kb DNA sequence. Run FusionAI genomic feature analysis script to have the landscape of overlap between high-FIS regions of fusion genes and 44 human genomic features.\n>Rscript FusionAI_genomic_features2.R -g [FUSION_GENE_FILE] -f [FEATURE_PATH] -s [CHROMOSOME_SIZE_FILE] -i [FEATURE_INFO_FILE] -o [OUTPUT_FILE_PATH]\n>Rscript FusionAI_genomic_features2.r -g K562_STARfusion.FusionAI.output.FIS -f ./features/ -s chromosome_size.txt -i features_info.txt -o ./K562/top1pct_features/", "This Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-prot-0001] describes how to run ntEdit+Sealer with the protocol Makefile to polish and correct errors in a long-read genome assembly using short reads. The protocol involves populating several Bloom filters with short-read k-mers for ntEdit and Sealer using several k-mer sizes. After the Bloom filters are created, ntEdit is run iteratively with long to short k values to correct base errors and flag unfixable and problematic regions in the assembly by soft-masking them. Unmasked sequences that are shorter than the lowest k value and flanked by soft-masked regions are further soft-masked after the ntEdit runs. Finally, Sealer is run with the same decreasing k values to fill in the erroneous soft-masked regions and existing assembly gaps by traversing an implicit Bloom filter de Bruijn graph.\nThe pipeline is invoked with a single Makefile command. We illustrate the steps with an E. coli strain NDM5 dataset consisting of a long-read Shasta (Shafin et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-bib-0012]) assembly generated from ONT MinION reads and paired-end Illumina MiSeq reads for assembly finishing. Additionally, we demonstrate how to analyze the draft and finished assemblies with QUAST (Gurevich, Saveliev, Vyahhi, & Tesler, 2013[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-bib-0005]).\nNecessary Resources\nHardware\nA server or machine running a 64-bit Linux or Mac operating system with a sufficient amount of disk space and RAM (see Support Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-prot-0002] for more details).\nSoftware\nThe following packages and their dependencies must be installed and be referenced to in your PATH environment variable:\n         \nsra-tools v2.9.1+ (https://github.com/ncbi/sra-tools[href=https://github.com/ncbi/sra-tools])\nntHits v0.0.1+ (https://github.com/bcgsc/nthits[href=https://github.com/bcgsc/nthits])\nntEdit v1.3.5+ (https://github.com/bcgsc/ntEdit[href=https://github.com/bcgsc/ntEdit])\nABySS v2.3.2+ (https://github.com/bcgsc/abyss[href=https://github.com/bcgsc/abyss])\nntEdit+Sealer protocol v1.0.0+ (https://github.com/bcgsc/ntedit_sealer_protocol[href=https://github.com/bcgsc/ntedit_sealer_protocol])\nQUAST v5.0.0+ (https://github.com/ablab/quast[href=https://github.com/ablab/quast])\nFiles\nShort sequencing reads (paired- or single-end) can be provided in compressed or uncompressed FASTQ format. Paired-end reads do not need to be interleaved. The long-read draft genome assembly can be provided as either a multi- or single-line FASTA file.\nSample dataThe example E. coli strain NDM5 draft genome assembly is included in the ntEdit+Sealer Protocol Github Repository under the \u201cdemo\u201d subdirectory (https://github.com/bcgsc/ntedit_sealer_protocol/blob/main/demo/ecoli_shasta.fa[href=https://github.com/bcgsc/ntedit_sealer_protocol/blob/main/demo/ecoli_shasta.fa]). The corresponding Illumina short reads will be used as demonstration for polishing and can be obtained from the NCBI Sequencing Read Archive (Accession: SRX12150405). Additionally, we will use the E. coli strain K-12 substr. MG1655 reference genome assembly (Accession: GCF_000005845.2) to assess the assemblies with QUAST.\nProtocol steps\n1. Install ntHits, ntEdit, ABySS, and the ntEdit+Sealer repository as outlined in the Strategic Planning section and add all binaries to your PATH environment variable.\n2. Install protocol-specific dependencies fasterq-dump (part of sra-tools) and QUAST via Conda or manually. Ensure sra-tools v2.9.1 or newer is installed in order to obtain fasterq-dump, a more performant, multi-threaded version of fastq-dump. If the correct version cannot be installed, fastq-dump can be used as a replacement (see step 3 below for details). If installing the tools manually, the executables must be added to your PATH environment variable.\nOption A: Installation using the Conda package manager\nIf Option A of the Strategic Planning section was used to install ntHits, ntEdit, and ABySS, dependencies may be installed into the same environment. Otherwise, a new Conda environment should be created. The Conda environment must be activated (using conda activate <env name>) prior to installing the tools.\nconda install -c bioconda \"sra-tools>=2.9.1\" \"quast>=5.0.0\"\nOption B: Manual installation\nsra-tools\nIdentify the correct version of the SRA Toolkit for your operating system from https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit[href=https://github.com/ncbi/sra-tools/wiki/01.-Downloading-SRA-Toolkit], and replace the URL if necessary.\nwget https://ftp-trace.ncbi.nlm.nih.gov/sra/sdk/3.0.0/sratoolkit.3.0.0-centos_linux64.tar.gz\ntar -xzf sratoolkit.3.0.0-centos_linux64.tar.gz\nexport PATH=/path/to/sratoolkit.3.0.0-centos_linux64/bin:$PATH\nQUAST\nwget https://github.com/ablab/quast/releases/download/quast_5.0.2/quast-5.0.2.tar.gz\ntar -xzf quast-5.0.2.tar.gz\nexport PATH=/path/to/quast:$PATH\n3. Create a new directory for running the ntEdit+Sealer protocol. Enter the new directory, soft-link the draft genome assembly from the ntEdit+Sealer repository and download the reference genome assembly and short reads.\n         \nmkdir ecoli_democd ecoli_demo\nln -s /path/to/ntedit_sealer_protocol/demo/ecoli_shasta.fa\nwget https://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/005/845/GCF_000005845.2_ASM584v2/GCF_000005845.2_ASM584v2_genomic.fna.gz\nfasterq-dump SRR15859208\nThe fasterq-dump command will download the short reads into two separate FASTQ files. These files are named according the SRA run accession number (i.e., SRR15859208_1.fastq and SRR15859208_2.fastq for the forward and reverse reads, respectively). If, for some reason, the correct version of sra-tools and fasterq-dump cannot be installed, fastq-dump may be used instead with the following command: fastq-dump SRR15859208 --split-3 --skip-technical.\n4. Run the ntEdit+Sealer Makefile with the \u201cfinish\u201d command to polish and fill the draft assembly ecoli_shasta.fa with the two reads files SRR15859208_1.fastq and SRR15859208_2.fastq. Specify k-mer lengths of 80, 65 and 50 with the k parameter, and a Bloom filter size of 200 MB with the b parameter. The protocol should take approximately 5 min to complete and requires under 550 MB of RAM, so can easily be run on a modern laptop or desktop computer.\n         \nntedit-sealer finish seqs=ecoli_shasta.fa \\\nreads=\"SRR15859208_1.fastq SRR15859208_2.fastq\" \\\nk=\"80 65 50\" b=200M\nEnsure that quotation marks are used to enclose lists of parameter values (i.e., read files and k-mer lengths), and that individual items in lists are space-separated. k-mer lengths must be passed in decreasing order. The command will run (1) ntHits and ABySS-bloom to populate Bloom filters from the short reads, (2) a Bash script to call ntEdit iteratively with decreasing k, (3) a Python script to consolidate (soft-mask) sequences shorter than the lowest k that are flanked by soft-masked regions, and finally (4) Sealer.\n5. Ensure that the ntEdit+Sealer run completes successfully. Successful completion will result in the Makefile reporting \u201cntEdit and Sealer polishing steps complete! Polished assembly can be found in: ecoli_shasta.ntedit_edited.prepd.sealer_scaffold.fa\u201d.\n6. Run QUAST to analyze the draft and ntEdit+Sealer-finished genome assemblies.\n         \nquast --fast -r GCF_000005845.2_ASM584v2_genomic.fna.gz \\\n-o ecoli_quast ecoli_shasta.fa \\\necoli_shasta.ntedit_edited.prepd.sealer_scaffold.faAll QUAST output files will be printed to the ecoli_quast directory, specified by the -o parameter. A summary report will be printed in tab-separated format to a file named report.tsv, where each column describes one of the input assemblies. The \u201c# mismatches per 100 kbp\u201d metric in this summary describes the base accuracy of the draft and finished assemblies compared to the reference. The \u201c# indels per 100 kbp\u201d metric describes the average proportion of insertions or deletions of either assembly compared to the reference. The expected values for these metrics are shown in Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-tbl-0001]. If QUAST was installed manually, the executable will be quast.py.\nTable 1.\n                Number of Mismatches and Indels per 100 kbp for E. coli Assembly Before and After Finishing with ntEdit+Sealera[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-tbl1-note-0001_22]\ntable:\n\ufeffUnnamed: 0,Draft assembly,After ntEdit+Sealer\n# mismatches per 100 kbp,371.33,345.86\n# indels per 100 kbp,122.38,7.22\na Running ntEdit+Sealer assembly finishing protocol decreases the proportion of mismatched bases and the proportion of indels in the E. coli genome assembly.Both ntEdit and Sealer employ a k-sweep approach, iterating from long to short k-mer lengths. This approach is beneficial because different k-mer lengths can provide resolution at different scales. Larger k-mers can disambiguate repeats as they span longer regions, while shorter k-mers are useful when the local read coverage is low and for assemblies with lower base quality. The same sequence of k values is used for both tools. k=40 is the practical lower limit for Sealer, as shorter k values cause its runtime to increase sharply. We find that k=80 generally performs well for a variety of datasets and suggest decreasing in intervals of 10-15. Generally speaking, there is no strict upper limit for k (apart from the read length), so a wide range of k-mer lengths can be used to achieve the best polishing results. Time and memory restrictions will be the limiting factors in these cases.\nntHits will automatically select the optimal Bloom filter size for ntEdit by calculating the k-mer distribution for the input reads, but ABySS-bloom requires the desired Bloom filter size to be specified. This parameter is controlled by the b parameter when invoking the protocol Makefile.\nThe optimal size of a Bloom filter depends on several factors, namely the desired false positive rate (FPR), number of hash functions used for insertion, and number of elements that will be inserted. For large Bloom filters, the FPR can be approximated (Equation 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-disp-0001]):\n         \nformula:\n\\begin{equation} f = {\\left( {1 - {e^{\\frac{{ - hn}}{m}}}} \\right)^h} \\end{equation}where f is the FPR, m is the size of the filter in bits, h is the number of hash functions, and n is the number of elements (Broder & Mitzenmacher, 2004[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-bib-0003]). By default, ABySS-Bloom uses one hash function for insertion. Using this relationship and asserting h=1, we can approximate the optimal m for a given dataset and desired FPR (Equation 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-disp-0002]):\n         \nformula:\n\\begin{equation} m = Ceil\\left( {\\frac{{ - n}}{{\\ln \\left( {1 - f} \\right)}}} \\right) \\end{equation}\nThe following Bloom filter sizes (RAM) generally perform well for common model organisms:\n         \nHomo sapiens (3 Gbp genome): 100 GB\nCaenorhabditis elegans (100 Mbp genome): 2.5 GB\nEscherichia coli (5 Mbp genome): 200 MB\nThe optimal b value for other genome sizes can be interpolated from these guidelines, or can be estimated using ntCard (Mohamadi, Khan, & Birol, 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-bib-0009]). ntCard is a streaming algorithm for estimating k-mer frequencies in genomic datasets and can be used to determine the number of unique k-mers in a set of short reads. ABySS-Bloom creates a 2-level cascading Bloom filter (Salikhov, Sacomoto, & Kucherov, 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-bib-0011]) from short-read k-mers; this means that only k-mers appearing two or more times are accounted for. Therefore, only k-mers with multiplicity of 2 or more should be considered when estimating optimal Bloom filter size.\nThe following steps in this Support Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-prot-0002] describe how ntCard should be used to calculate the optimal Bloom filter size for a dataset, ensuring a false positive rate (FPR) of \u223c0.005. The same E. coli short reads from the Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-prot-0001] will be used to demonstrate this protocol.\nNecessary Resources\nHardware\nA server or machine running a 64-bit Linux or Mac operating system capable of running ntCard.\nSoftware\nntCard v1.2.2+ (https://github.com/bcgsc/ntCard[href=https://github.com/bcgsc/ntCard])ntCard is available on the Conda and Homebrew package managers and can also be cloned and compiled from Github.\nFiles\nThe short sequencing reads that will be used as input to ntEdit+Sealer will be analyzed here. The reads can be provided in compressed or uncompressed FASTQ format and paired-end reads do not need to be interleaved.\n1. Install ntCard.\nOption A: Installation using the Conda package manager\nconda create -n ntcard\nconda activate ntcard\nconda install -c bioconda \"ntcard>=1.2.2\"\nOption B: Installation using the Homebrew package manager\nbrew install brewsci/bio/ntcard\nOption C: Manual installation from Github\ngit clone https://github.com/bcgsc/ntCard.git\n./autogen.sh\n./configure --prefix=/path/to/ntCard\nmake\nmake install\nThe --prefix parameter for the configure script installs ntCard to the provided path. This parameter can be excluded if you have sudo privileges and wish to install the tool into /usr/local. If using Conda, activate the environment that ntCard was installed to with the command conda activate environment_name. If manually installing ntCard to a specific directory, ensure that the path supplied to the --prefix parameter is on your PATH.\n2. Run ntCard on the read files, providing all k values you are planning on using for ntEdit+Sealer. We will use k=80, k=65, and k=50.\n         \nntcard -k80,65,50 -p freq \\\nSRR15859208_1.fastq SRR15859208_2.fastq\nThis command will generate a k-mer distribution histogram for each k provided. Each histogram will be printed to a two-column, tab-separated file with the prefix freq, for example, freq_k80.hist, where the first column represents an Fk metric or multiplicity and the second column contains the number of corresponding k-mers.\n3. Inspect the k-mer frequency histograms. Only the first three lines are necessary for our purposes.\n         \nhead -n 3 freq_k*.histThe first two lines of each histogram contain Fk metrics, which describe statistics for the input dataset. F0 is the number of distinct k-mers in the reads, and F1 denotes the total number of k-mers in the dataset. The third line contains the number of k-mers appearing once in the reads. See Table 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-tbl-0002] for the expected values of these metrics.\nTable 2.\n                F1, F0 and Number of Multiplicity 1 k-mers for E. coli Illumina Readsa[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-tbl2-note-0001_31]\ntable:\n\ufeffk,F1,F0,1\n50,322869315,47853592,41155307\n65,300812763,50900032,44179082\n80,278838102,52274328,45577189\na Table values correspond to the first three rows of ntCard output histograms for 50-mer, 65-mer, and 80-mer cardinality estimation of E. coli strain NDM5 Illumina reads.\n4. Estimate the number of k-mers that appear two or more times in the reads. Subtract the number of k-mers that appear only once from F0 with the following one-liner:\n         \nfor k in 80 65 50; do distinct=$(grep \u201cF0\u201d freq_k$k.hist | awk '{print $2}'); once=$(grep \u201c\u02c41\\b\u201d freq_k$k.hist | awk '{print $2}'); echo k=$k: n=$((${distinct} - ${once})); done\nThis command will calculate and print the number of k-mers appearing two or more times for each k. See Table 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-tbl-0003] for the expected values for each k.\nTable 3.\n                Number of k-mers Appearing at Least Twice in E. coli Illumina Readsa[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-tbl3-note-0001_33]\ntable:\n\ufeffk,n\n50,6697139\n65,6720950\n80,6698285\na Number of k-mers with multiplicity of 2 or more calculated from ntCard output. For each k, n is equal to the number of unique k-mers in the dataset (F0) minus the number of k-mers with multiplicity of one.5. Calculate the optimal Bloom filter size from the k-mer distributions using the largest n calculated in step 4 (k=65). This will maximize the number of unique k-mers considered, and therefore produce the lowest FPR for the range of k, a method analogous to that employed by Kmergenie (Chikhi & Medvedev, 2013[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-bib-0004]). Using the relationship between FPR and Bloom filter density, the optimal value of m given an FPR (f) of 0.005 is (Equation 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-disp-0003]):\n         \nformula:\n\\begin{equation} m = Ceil\\left( {\\frac{{ - n}}{{\\ln \\left( {1 - f} \\right)}}}\\right) = Ceil\\left( {\\frac{{ - 6720950}}{{\\ln \\left( {0.995} \\right)}}} \\right) = 1340826718{\\rm{\\;bits}} \\end{equation}\nThis equates to approximately 168 MB. We rounded this estimate up to 200 MB for the example in the Basic Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.442#cpz1442-prot-0001].", "Step-by-step method details\nStep-by-step method details\nWe have created a github that features all of the necessary files, scripts, and example outputs for the method presented herein. The github can be found at:\nhttps://github.com/jinlab-washu/de-novo-wes-star-protocol[href=https://github.com/jinlab-washu/de-novo-wes-star-protocol]\nUsers are advised to refer to github to download example trios (in VCF format) and pedigree (PED) files if they wish to replicate the step-by-step instructions reported here exactly.\nRun DeNovo Analysis for each trio\nTiming: hours to days\nIn this step users will perform DeNovo analysis using TriodenovoRearrange_pythonAuto.R, which outputs a filtered tsv (tab-separated values) file of calls for each trio in your cohort.\nGenerate the commands for each trio, replacing TRIO_NAME with the name of each trio: Rscript TriodenovoRearrange_pythonAuto.R exome_calls_chr4_2trios.vcf Trios.ped OUTPUT_FOLDER TRIO_NAME. This should be done in the command line, and if this is sufficient then users should continue to step 2. Otherwise, users should execute the following sub-steps in R (recommended version 4.0.2).\nCreate a new directory for the cohort\nsystem(paste(\"mkdir \",CohortName,sep\u00a0= ''),intern\u00a0= F)\nRead in the pedigree file\nFam\u00a0= read.table(file=ped,header=TRUE,stringsAsFactors=FALSE)\nSee Trios.ped for an example pedigree format\nChange your working directory to the cohort directory\nsetwd(CohortName)\nCreate a directory for the sample and change to id\ncommand0=paste(\"mkdir -p \",Familyno,sep=\"\")\nsystem(command0,intern=F)\nsetwd(Familyno)\nGenerate a pedigree file for just one trio\nindex <- which(Fam$FamID\u00a0== Familyno)\nwrite.table(Fam[index,],file\u00a0= paste(\"Trio_\",Familyno,\".ped\",sep\u00a0= \"\"),col.names=FALSE,row.names=FALSE,sep=\"\u2216t\",quote=FALSE)\nFam\u00a0= Fam[index,]\nindex\u00a0= which(Fam$Father != 0)\nProband_ID\u00a0= Fam[index,2]\nFather_ID\u00a0= Fam[index,3]\nMother_ID\u00a0= Fam[index,4]\nGenerate a VCF file for each trio, removing all records with missing genotype, only extracting lines where AC != 0\ncommand1=paste(\"java -Xmx64g -jar GenomeAnalysisTK_3.5.jar -nt 16 -R /ref_data/h_sapiens/1000genomes/2.5/b37/human_g1k_v37_decoy.fasta -T SelectVariants --variant \",Input,\" -o Trio_\",Familyno,\".vcf -env -sn \",Proband_ID,' -sn ',Father_ID,' -sn ',Mother_ID,sep=\"\")\n-nt 16: use 16 threads\n-R: path to reference fasta\n-T SelectVariants --variant: select a subset of variants from a VCF-o: output file path\n-env\n-sn: specify a sample name from which to include genotypes\nsystem(command1,intern=F)\nRegenotype\nsystem(paste('java -Xmx64g -jar GenomeAnalysisTK_3.5.jar -nt 16 -R /ref_data/h_sapiens/1000genomes/2.5/b37/human_g1k_v37_decoy.fasta -T RegenotypeVariants --variant Trio_',Familyno,'.vcf -o Trio_',Familyno,'_reGT.vcf',sep\u00a0= ''),intern\u00a0= F)\n-nt 16: use 16 threads\n-R: path to reference fasta\n-T RegenotypeVariants --variant: Regenotypes the variants from a VCF\n-o: output file path\nSplit multi-allelic sites\nsystem(paste('bcftools norm -m-both -o Trio_',Familyno,'_reGT_step1.vcf Trio_',Familyno,'_reGT.vcf',sep\u00a0= ''),intern\u00a0= F)\nnorm: normalize indels\n-m-both\n-o: output file path\nLeft normalization\nsystem(paste('bcftools norm -f /ref_data/h_sapiens/1000genomes/2.5/b37/human_g1k_v37.fasta -o Trio_',Familyno,'_reGT_step2.vcf Trio_',Familyno,'_reGT_step1.vcf',sep\u00a0= ''),intern\u00a0= F)\nnorm: normalize indels\n-f: path to reference fasta\n-o: output file path\nRemove extra information (PID, PGT)\nsystem(paste('vcfkeepgeno Trio_',Familyno,'_reGT_step2.vcf GT AD DP GQ PL > Trio_',Familyno,'_reGT_step2_modified.vcf',sep\u00a0= ''),intern\u00a0= F)\nRemove ./., unfavored PL, and AC != 0\nsystem(paste('python ParseTrioVCF.py ',Familyno,sep\u00a0= ''),intern\u00a0= F)\nRequires ParseTrioVCF.py\nAnnotate the updated VCF with annovar\ncommand7=paste(\"perl table_annovar.pl --vcfinput Trio_\",Familyno,\"_updated.vcf /programs/annovar/humandb/ -buildver hg19 -out Trio_\",Familyno,\" -remove -protocol refGene,genomicSuperDups,snp138,dbnsfp33a,esp6500siv2_all,1000g2015aug_all,exac03,gnomad_exome,gnomad_genome,bravo -operation g,r,f,f,f,f,f,f,f,f -nastring '.'\",sep\u00a0= \"\")\n--vcfinput: specify the input vcf file\n-buildver: genome build version\n-out: specify the output file\n-remove: remove all temporary files\n-protocol: comma-delimited string specifying database protocol\nIn this case we are using these protocols: refGene,genomicSuperDups,snp138,dbnsfp33a,esp6500siv2_all,1000g2015aug_all,exac03,gnomad_exome,gnomad_genome\n-operation: comma-delimited string specifying type of operation\n-nastring: string to display when a score is not available\nsystem(command7,intern=F)\nRun triodenovo\ncommand6=paste(\"triodenovo --ped Trio_\",Familyno,\".ped --in_vcf Trio_\",Familyno,\"_updated.vcf --out Trio_\",Familyno,\".denovo.Bayfilter.vcf --mixed_vcf_records\",sep\u00a0= \"\")\n--ped: specifies the pedigree file to use\n--in_vcf: specifies the vcf file to use\n--out: specifies the output file\n--mixed_vcf_records\nsystem(command6,intern=F)\nDelete intermediate files\nsystem(paste(\"rm Trio_\",Familyno,\".avinput\",sep\u00a0= \"\"),intern\u00a0= F)\nsystem(paste(\"rm Trio_\",Familyno,\".hg19_multianno.txt\",sep\u00a0= \"\"),intern\u00a0= F)\nRearrangement\nsystem(paste('python PrepareMerge.py ',Familyno,sep\u00a0= ''),intern\u00a0= F)\nRequires PrepareMerge.py\nDetermine the order of members in the VCF\nOrder\u00a0= unlist(strsplit(try(system(paste(\"grep -w '#CHROM' Trio_\",Familyno,\"_updated.vcf\",sep\u00a0= \"\"),intern\u00a0= T)),'\u2216t'))\ncol15\u00a0= Order[10]\ncol16\u00a0= Order[11]col17\u00a0= Order[12]\nProcess the triodenovo output\nBayfilter=readLines(paste(\"Trio_\",Familyno,\".denovo.Bayfilter.content.txt\",sep\u00a0= \"\"))\nBayfilter=sapply(1:length(Bayfilter),function(i) unlist(strsplit(Bayfilter[i],\"\u2216t\")))\nBayfilter=data.frame(t(Bayfilter),stringsAsFactors\u00a0= F)\ncolnames(Bayfilter)=c(\"CHROM\",\"POS\",\"ID\",\"REF\",\"ALT\",\"QUAL\",\"FILTER\",\"INFO\",\"FORMAT\",Father_ID,Mother_ID,Proband_ID)\nBayfilter$POSITION=paste(Bayfilter$CHROM,Bayfilter$POS,Bayfilter$REF,Bayfilter$ALT,sep=\":\")\nProcess the annovar output\nAnno=readLines(paste(\"Trio_\",Familyno,\".hg19_multianno.content.txt\",sep\u00a0= \"\"))\nAnno=sapply(1:length(Anno),function(i) unlist(strsplit(Anno[i],\"\u2216t\")))\nAnno=data.frame(t(Anno),stringsAsFactors\u00a0= F)\ncolnames(Anno)=c(\"CHROM\",\"POS\",\"ID\",\"REF\",\"ALT\",\"QUAL\",\"FILTER\",\"Info\",\"Format\",paste(\"Anno.\",col15,sep\u00a0= \"\"),paste(\"Anno.\",col16,sep\u00a0= \"\"),paste(\"Anno.\",col17,sep\u00a0= \"\"))\nAnno$POSITION=paste(Anno$CHROM,Anno$POS,Anno$REF,Anno$ALT,sep=\":\")\nDelete intermediate files\nsystem(paste(\"rm Trio_\",Familyno,\".hg19_multianno.content.txt\",sep\u00a0= \"\"),intern\u00a0= F)\nsystem(paste(\"rm Trio_\",Familyno,\".denovo.Bayfilter.content.txt\",sep\u00a0= \"\"),intern\u00a0= F)\nMerge based on the triodenovo file\nAnnoBayfilter=merge(Bayfilter,Anno,by=\"POSITION\",all.x=TRUE)\nAnnoBayfilter=AnnoBayfilter[,c(1:8,10:13,21:25)]\ncolnames(AnnoBayfilter)=c(\"POSITION\",\"CHROM\",\"POS\",\"ID\",\"REF\",\"ALT\",\"QUAL\",\"FILTER\",\"FORMAT\",Father_ID,Mother_ID,Proband_ID,\"INFO\",\"AnnoFormat\",paste(\"Anno.\",col15,sep\u00a0= \"\"),paste(\"Anno.\",col16,sep\u00a0= \"\"),paste(\"Anno.\",col17,sep\u00a0= \"\"))\nWrite the final output to a file\nRun the list of commands generated in step 1\nConcatenate the output files into one final file, in the command line\nhead -1 TRIO_NAME/\u2217DenovoM > Trios.BayesianFilter.DenovoM\nfor file in TRIO_PREFIX\u2217/\u2217DenovoM;do sed '1d' $file >>Trios.BayesianFilter.DenovoM;done\nOutput: Trios.BayesianFilter.DenovoM\nFilter the concatenated file using Python 3.7.3 These commands should be executed in the command line with paths to the Python scripts.\npython ParseBayOutput_SciencePaper.py Trios.BayesianFilter.DenovoM Trios.BayesianFilter.DenovoM.filtered\nRequires ParseBayOutput_SciencePaper.py\nOutput: Trios.BayesianFilter.DenovoM.filtered\nDenovolyze preparation and visualization of candidate DNMs\nThe DeNovo Analysis output will now be manually filtered and annotated to prepare for the Denovolyze step.\nAfter compiling a list of candidate DNMs, all calls must be verified manually using the integrative genomics viewer (IGV) (Robinson et\u00a0al., 2011[href=https://www.wicell.org#bib10]). IGV can be accessed at igv.org[href=http://igv.org] and manual visualization of DNM calls can be accomplished as follows:\nOpen IGV and load in a BAM, CRAM (and any associated index file), or other supported file format from the first subject you wish to analyze.\nSelect the appropriate reference sequence for your analysis and enter the chromosome and position of the called variant to the search bar at the top.\nZoom in to the user-determined threshold window to visualize all recorded reads.\nExport the image by clicking \"Save Image\" under the \"File\" menu and saving to your desired folder.Repeat steps for all candidate DNMs in the call list for the specific subject file loaded into IGV.\nEnd the current session and create a new session loading the BAM or CRAM file of the next subject you wish to analyze.\nRepeat the above steps until all candidate DNM calls have been manually analyzed in IGV.\nTroubleshooting (Problem 2)[href=https://www.wicell.org#troubleshooting]\nOpen the output in your preferred spreadsheet software and classify the mutations into different categories in the column denovo_metasvm_cadd30\nUse the ExonicFunc.refGene column to aid in this process\nThe MetaSVM and CADD columns may also be helpful\nExample: A nonsynonymous_SNV with a CADD (v1.3) score \u2265 30 or a MetaSVM prediction of \u201cD\u201d would be classified as misD (Kircher et\u00a0al., 2014[href=https://www.wicell.org#bib5])\nOutput: Trios.BayesianFilter.DenovoM.filtered.PlotReads.xlsx\nEnrichment analysis\nThe enrichment analysis relies on the denovolyzeR library and will output mutability or enrichment tables depending on the script. The sample data was generated by the DeNovo Analysis step and prepared for denovolyzeR in the previous step. The gene lists used by Dong et\u00a0al. are available in the github. Users will need curate their own gene lists based upon published literature for the disease they are studying.\nRun the scripts\nRscript denovolyzR.script.R\nRequires: TN_n70_Input.txt, 0819_hs37d5_coding_idt_med_v2_spikein_padded_Mar2018_adj_modified.txt\nOutput: Trios_ObserveExpect_metasvm_cadd30.txt", "Step-by-step method details\nStep-by-step method details\nFor first-time users, we recommend first running the protocol on toy data following steps 1, 2, 5, and 6. For those who wish to learn how to transform typical raw scRNA-seq data into a normalized and annotated gene-by-cell matrix, please follow steps 1, 3, and 7. For users who would like to try this protocol on scRNA-seq data from a species other than Arabidopsis thaliana, (e.g., human PBMC data), please see steps 4 and 8.\nAlign raw reads to genome via scKB\nTiming: 2\u00a0min for toy data. For the full datasets demonstrated in this protocol, the running time ranged between 25\u00a0min to 120\u00a0min, depending on data size and species\nThis step aligns raw reads to the designated genome and calls transcript UMI counts for each cell barcode and gene. The expected output of this step includes gene-by-cell matrices of spliced and unspliced UMI counts of transcripts, reads/aligning stats of the sample, cell barcodes, and gene ID files. These outputs are COPILOT-compatible and can serve as direct inputs to COPILOT for quality filtering and generation of the summary file.\nExtract intron information with the R package BUSpaRse and make kallisto index. This step is important for kallisto to distinguish spliced and unspliced reads when mapping against the designated genome.Note: Extraction of intron information is done by running BUSpaRse::get_velocity_files(). Notice that argument \u201cX\u201d should point to the directory of your annotation file. Argument \u201cL\u201d specifies the length of reads, which varies with single-cell technologies. For example, 10x Genomics v1: 98 nucleotides, 10x v2: 98 nucleotides, 10x v3:91 nucleotides, Drop-seq: 50 nucleotides. In this example, we set \u201cL\u201d to 91 since our toy data was generated with 10x v3 chemistry. We should set the argument \u201cstyle\u201d to \u201cEnsembl\u201d since our gene annotation (.gtf) file is downloaded from Ensembl (https://plants.ensembl.org/index.html[href=https://plants.ensembl.org/index.html]). For details related to get_velocity_files() arguments, please refer to the BUSpaRse manual (https://bioconductor.org/packages/release/bioc/manuals/BUSpaRse/man/BUSpaRse.pdf[href=https://bioconductor.org/packages/release/bioc/manuals/BUSpaRse/man/BUSpaRse.pdf]).\nCritical: After running get_velocity_files(), the working directory should have files \u201ccDNA_introns.fa\u201d, \u201ccDNA_tx_to_capture.txt\u201d, \u201cintrons_tx_to_capture.txt\u201d, and \u201ctr2g.tsv\u201d. Preparing intron files for the Arabidopsis thaliana genome takes about 1\u00a0min.\n# In R\n>setwd(\"\u223c/to/where/you/clone/the/repo/scKB\")\n# Load libraries and genome\n>library(BUSpaRse)\n>library(BSgenome.Athaliana.TAIR.TAIR9)\n# Extract intron information\n>get_velocity_files(X\u00a0= \"./Arabidopsis_thaliana.TAIR10.43.gtf\", L\u00a0= 91, Genome\u00a0= BSgenome.Athaliana.TAIR.TAIR9, out_path\u00a0= \"./\", isoform_action\u00a0= \"separate\", chrs_only=FALSE, style=\"Ensembl\")\n# Index the intron file with kallisto and exit R\n>system(\"kallisto index -i ./cDNA_introns_10xv3.idx ./cDNA_introns.fa\")\n>quit()\nRun scKB.\nNote: Name the output directory as your sample name and set the number of computational threads/cores available accordingly with argument \u201ct\u201d.\nNote: After running scKB, the output directory (in this case \u201c./col0_toy\u201d) should have 8 files: a \u201cinspect.json\u201d, a \u201crun_info.json\u201d, a \u201cspliced.barcodes.txt\u201d, a \u201cspliced.genes.txt\u201d, a \u201cspliced.mtx\u201d, a\u201dunspliced.barcodes.txt\u201d, a \u201cunspliced.genes.txt\u201d and a \u201cunspliced.mtx\u201d.\nNote: The json files document the sample stats of reads and alignment; the mtx files represent gene-by-cell matrices; the txt files contain cell barcodes and gene IDs information.\nNote: The toy data contain 1 million reads and the run takes about 30 s.\n# In bash\n>cd \u223c/to/where/you/clone/the/repo/scKB\n# Run bash script scKB. Please name the output directory as your sample name using \"-n\" flag>./scKB -f ./toy_data -i ./cDNA_introns_10xv3.idx -d ./ -s 10xv3 -t 16 -w ./10xv3_whitelist.txt -n ./col0_toy\nRun scKB on Arabidopsis thaliana wild-type Columbia-0 (\u201ccol0\u201d) full data.\nNote: The col0 full data contains 386 million reads and downloading can take 2\u20133 h.\nNote: Here, we store the fastq files for the full dataset under the folder named \"col0_data\".\nNote: Notice that if containers were applied, it is recommended to set the output directories of sra files and the fastqs files under the mounted directory \u201c/data/\u201d to avoid potential data loss (see steps 7b and c in the section \u201cinstall scKB and COPILOT on your machine[href=https://www.wicell.org#sec1.1]\u201d).\nNote: The scKB run takes 22\u00a0min.\n# In bash\n>cd \u223c/to/where/you/clone/the/repo/scKB\n# Install sra-tools and download sra from sample GSM4626009 (col0 full data)\n>conda install -c bioconda sra-tools\n>prefetch -v SRR12046119 SRR12046120\n# Convert sra to fastqs (sras are downloaded to /home/[USER]/ncbi/public/sra/ by default)\n>fastq-dump --outdir ./col0_data/ --split-files --gzip /home/[USER]/ncbi/public/sra/SRR12046119.sra\n>fastq-dump --outdir ./col0_data/ --split-files --gzip /home/[USER]/ncbi/public/sra/SRR12046120.sra\n#Rename fastq files for scKB compatibility\n>rename 's/_1.fastq.gz/_R1_001.fastq.gz/' ./col0_data/\u2217.fastq.gz\n>rename 's/_2.fastq.gz/_R2_001.fastq.gz/' ./col0_data/\u2217.fastq.gz\n>rename 's/_3.fastq.gz/_I1_001.fastq.gz/' ./col0_data/\u2217.fastq.gz\n# Run scKB on the full data col0 from which the toy_data is subsetted\n>./scKB -f ./col0_data -i ./cDNA_introns_10xv3.idx -d ./ -s 10xv3 -t 16 -w ./10xv3_whitelist.txt -n ./col0\nRun scKB on PBMC dataset generated with 10x v2 technology.\nNote: The codes cover data download, intron file extraction, kallisto index building, and scKB execution.\nNote: Notice that if containers were applied, the data downloaded and the output directory\u00a0are recommended to be put under the mounted directory \u201c/data/\u201d to avoid potential data loss (see steps 7b and c in the section \u201cinstall scKB and COPILOT on your machine[href=https://www.wicell.org#sec1.1]\u201d).\nNote: The whole process takes about 120\u00a0min (preparing genome and intron files take 85\u00a0min, scKB takes 45\u00a0min).\n# In bash\n>cd \u223c/to/where/you/clone/the/repo/scKB# Download PBMC dataset from 10x Genomics website\n>mkdir pbmc\n>cd pbmc\n>curl -O https://cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_1k_v2/pbmc_1k_v2_fastqs.tar[href=https://cf.10xgenomics.com/samples/cell-exp/3.0.0/pbmc_1k_v2/pbmc_1k_v2_fastqs.tar]\n>tar xvf pbmc_1k_v2_fastqs.tar\n>cd ../\n# Download gene annotation file from 10x Genomics website\n>curl -O https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-GRCh38-2020-A.tar.gz[href=https://cf.10xgenomics.com/supp/cell-exp/refdata-gex-GRCh38-2020-A.tar.gz]\n>tar zxvf refdata-gex-GRCh38-2020-A.tar.gz\n# Activate R and set working directory in R\n>R\n>setwd(\"\u223c/to/where/you/clone/the/repo/scKB\")\n# Install Human BSgenome object\n>BiocManager::install(\"BSgenome.Hsapiens.UCSC.hg38\")\n# Load libraries and genome\n>library(BUSpaRse)\n>library(BSgenome.Hsapiens.UCSC.hg38)\n# Extract intron information\n>get_velocity_files(X\u00a0= \"./refdata-gex-GRCh38-2020-A/genes/genes.gtf\", L\u00a0= 98, Genome\u00a0= BSgenome.Hsapiens.UCSC.hg38, out_path\u00a0= \"./\", isoform_action\u00a0= \"separate\", chrs_only=TRUE, style=\"Ensembl\")\n# Index the intron file with kallisto\n>system(\"kallisto index -i ./cDNA_introns_10xv2.idx ./cDNA_introns.fa\")\n>quit()\n# In bash\n# Run scKB\n>./scKB -f ./pbmc/pbmc_1k_v2_fastqs -i ./cDNA_introns_10xv2.idx -d ./ -s 10xv2 -t 16 -w ./10xv2_whitelist.txt -n ./pbmc_1k_v2\nRun COPILOT for quality filtering\nTiming: between 30\u00a0s to 50\u00a0min, depending on data size and whether downstream analysis with Seurat is performed\nThis step includes quality filtering on cells, optional doublet removal, and cell type/developmental stage annotation (optional). Notice that if containers were used, the data downloaded and the output directory are recommended to be put under the mounted directory \u201c/data/\u201d to avoid potential data loss (see steps 7b and c under the section \u201cinstall scKB and COPILOT on your machine[href=https://www.wicell.org#sec1.1]\u201d).\nRun COPILOT for non-iterative quality filtering without doing downstream analysis with Seurat.\nNote: The toy data contain only 1 million reads. Therefore, the argument \u201cmin.UMI.low.quality\u201d and \u201cmin.UMI.high.quality\u201d are adjusted accordingly for expected low sequencing depth.\nNote: The non-iterative filtering is achieved by setting the argument \u201cfiltering.ratio\u201d to 1.\nNote: The run takes about 30 s.\n# In R\n>setwd(\"\u223c/to/where/you/clone/the/repo/scKB\")\n# Load COPILOT\n>library(COPILOT)\n# Run COPILOT\n>copilot(sample.name = \"col0_toy\", species.name = \"Arabidopsis thaliana\", transcriptome.name = \"TAIR10\", sample.stats = NULL, mt.pattern = \"ATMG\",mt.threshold = 5, cp.pattern = \"ATCG\", remove.doublet = FALSE, do.seurat = FALSE, do.annotation = FALSE, unwanted.genes = NULL, filtering.ratio\u00a0= 1, min.UMI.low.quality = 1, min.UMI.high.quality = 3)\nRun COPILOT for iterative quality filtering without downstream analysis.\nNote: The iterative filtering is achieved by setting the argument \u201cfiltering.ratio\u201d to 0, which will run until there are no cells more similar to the low-quality cell expression profile than the high-quality cell expression profile.\nNote: The run takes about 2\u00a0min.\n# Run COPILOT\n>copilot(sample.name = \"col0_toy\", species.name = \"Arabidopsis thaliana\", transcriptome.name = \"TAIR10\", sample.stats = NULL, mt.pattern = \"ATMG\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0mt.threshold = 5, cp.pattern = \"ATCG\", remove.doublet = FALSE, do.seurat = FALSE, do.annotation = FALSE, unwanted.genes = NULL, filtering.ratio\u00a0= 0, min.UMI.low.quality = 1, min.UMI.high.quality = 3)\nRun COPILOT for non-iterative quality filtering with downstream analysis (do.seurat\u00a0= TRUE), including doublet removal (remove.doublet\u00a0= TRUE) and annotation of cell types and developmental stages based on bulk-RNAseq and microarray data (do.annotation\u00a0= TRUE).\nNote: If users wish to annotate cells based on an established reference profile as described in Shahan et\u00a0al. (2022)[href=https://www.wicell.org#bib25], example code for label transfer is also provided.\nNote: Notice that the code for label transfer is suitable only for reference objects created by Seurat version 4. Label transfer demonstrated here requires users to have sufficient amount of space storage and memory (128 GB) to host and load the reference object.Note: Here we also demonstrate how to remove genes from downstream analysis with the argument \u201cunwanted.genes\u201d. In this case, the protoplasting-induced genes are removed from the Arabidopsis data. Protoplasting refers to a process of removing cell walls from plant cells, which alters the expression of a subset of genes (Denyer et al., 2019[href=https://www.wicell.org#bib28]). Therefore, to correct for artificial factors in downstream analysis, genes known to be induced by protoplasting during library preparation are removed. The run takes about 50\u00a0min without label transfer and 60\u00a0min with label transfer.\n# In R\n>setwd(\"\u223c/to/where/you/clone/the/repo/scKB\")\n# Load COPILOT\n>library(COPILOT)\n# Load unwanted genes (optional)\n>pp.genes <-as.character(read.table(\"./supp_data/Protoplasting_DEgene_FC2_list.txt\", header=F)$V1)\n# Run COPILOT\n>copilot(sample.name = \"col0\", species.name = \"Arabidopsis thaliana\", transcriptome.name = \"TAIR10\", sample.stats = NULL, mt.pattern = \"ATMG\",\nmt.threshold = 5, cp.pattern = \"ATCG\", remove.doublet = TRUE, do.seurat = TRUE, do.annotation = TRUE, unwanted.genes = pp.genes, filtering.ratio\u00a0= 1, dir_to_color_scheme\u00a0= \"./supp_data/color_scheme_at.RData\", dir_to_bulk\u00a0= \"./supp_data/Root_bulk_arabidopsis_curated.RD\")\n# Label transfer from Shahan et\u00a0al. (2022)[href=https://www.wicell.org#bib25]\n# After downloading supplementary file Root_Atlas_seu4.rds.gz directly from GEO GSE152766, decompress Root_Atlas_seu4.rds.gz in bash\n>gunzip Root_Atlas_seu4.rds.gz\n# Load Seurat, reference atlas and query data in R\n>R\n>library(Seurat)\n>rc.integrated <- readRDS(\"./Root_Atlas_seu4.rds\")\n>seu <- readRDS(\"./col0/col0_COPILOT.rds\")\n# Find anchors and transfer annotation from reference in Shahan et\u00a0al.,2022\n>lt.anchors <- FindTransferAnchors(reference\u00a0= rc.integrated, query\u00a0= seu, normalization.method\u00a0= \"SCT\", npcs\u00a0= 50, dims\u00a0= 1:50)\n>predictions <- TransferData(anchorset\u00a0= lt.anchors, refdata\u00a0= rc.integrated$celltype.anno, dims\u00a0= 1:50, weight.reduction\u00a0= \"pcaproject\")\n>seu <- AddMetaData(seu, metadata\u00a0= predictions)\n>seu@meta.data$celltype.anno <- seu@meta.data$predicted.id\n>predictions <- TransferData(anchorset\u00a0= lt.anchors, refdata\u00a0= rc.integrated$time.anno, dims\u00a0= 1:50, weight.reduction\u00a0= \"pcaproject\")\n>seu <- AddMetaData(seu, metadata\u00a0= predictions)\n>seu@meta.data$time.anno <- seu@meta.data$predicted.id\n# Visualize the transferred labels and save them as plots (optional)\n>pdf(\"col0_transferred_celltype_anno.pdf\", height=8, width=8)\n>DimPlot(seu, reduction\u00a0= \"umap\", group.by\u00a0= \"celltype.anno\")\n>dev.off()\n>pdf(\"col0_transferred_time_anno.pdf\", height=8, width=8)\n>DimPlot(seu, reduction\u00a0= \"umap\", group.by\u00a0= \"time.anno\")>dev.off()\n# Save the query data with transferred labels\n>saveRDS(seu, file\u00a0= \"./col0/col0_COPILOT.rds\")\nRun COPILOT on the PBMC dataset generated with 10x v2 technology for non-iterative quality filtering without downstream analysis.\nNote: The run takes about 15 s.\n# In bash\n# Extract mitochondrial gene names from genome annotation file\n>grep chrM ./refdata-gex-GRCh38-2020-A/genes/genes.gtf | awk '{print $10}' | uniq | sed -e 's/\"//g' -e 's/;//g'\u00a0>\u00a0mt_genes.txt\n# Activate R and set working directory in R\n>R\n>setwd(\"\u223c/to/where/you/clone/the/repo/scKB\")\n# Load COPILOT\n>library(COPILOT)\n# Run COPILOT\n>copilot(sample.name = \"pbmc_1k_v2\", species.name = \"Homo sapiens\", transcriptome.name = \"hg38\", sample.stats = NULL, mt.pattern = read.table(\"mt_genes.txt\")$V1, mt.threshold = 5, cp.pattern = NULL, remove.doublet = FALSE, do.seurat = FALSE, do.annotation = FALSE, unwanted.genes = NULL, filtering.ratio = 1, legend.position=c(0.2,0.8))", "Step-by-step method details\nStep-by-step method details\ncfDNA extraction\nTiming: 60\u00a0min\nHere, cfDNA is isolated from CSF supernatant using the NucleoSnap cfDNA kit (Macherey-Nagel) according to the manufacturer\u2019s instructions (Protocol Version November 2019: https://www.mn-net.com/media/pdf/56/35/fa/Instruction-NucleoSnap-cfDNA.pdf[href=https://www.mn-net.com/media/pdf/56/35/fa/Instruction-NucleoSnap-cfDNA.pdf]). Other cfDNA extraction kits used by the authors include the QIAamp Circulating Nucleic Acid Kit (Qiagen), QIAamp MinElute ccfDNA Mini Kit (Qiagen), and Maxwell RSC ccfDNA Plasma Kit (RSC). The NucleoSnap kit is chosen due to a combination of factors including flexibility of input volume, high cfDNA yield (Maass et\u00a0al., 2021[href=https://www.wicell.org#bib4]), and simplicity of protocol.\nAvoid exogenous DNA contamination by working in a pre-PCR area with designated equipment, regularly decontaminating pipettes and surfaces with diluted bleach, and wearing clean gloves and lab coats at start of experiment.\nThaw CSF supernatant (if necessary) at room temperature (15\u00b0C\u201325\u00b0C) and note volume for each sample.\nPrepare sample by centrifugation at 4,500\u00a0g for 10\u00a0min at room temperature, transfer supernatant to a new 50\u00a0mL conical tube.\nAdd 15\u00a0\u03bcL Proteinase K per mL of sample, close lid and mix carefully by swirling tube without moistening lid, incubate at room temperature for 5\u00a0min.\nAdd 1\u00a0mL of Buffer VL per mL of sample, mix by vortexing, incubate tube at 56\u00b0C for 5\u00a0min in bead or water bath.\nAdd 1\u00a0mL of 96%\u2013100% ethanol per mL of sample, mix by vortexing.\nSet up the NucleoSnap cfDNA columns (one per sample) by connecting them to a vacuum manifold.\nPrepare the column by adding 500\u00a0\u03bcL of Buffer CC to the column and applying vacuum (0.4\u20130.6 bar) until the solution has passed through the column. Perform this step 1\u20135\u00a0min before the sample lysate is to be added according to the next step.Decant the sample lysate into the column and apply vacuum (0.4\u20130.6 bar) until the lysate has passed through the column.\nWash membrane by adding 1\u00a0mL of Buffer VW1 to column, apply vacuum (0.2\u20130.4 bar) until buffer has passed through the column.\nWash membrane by adding 0.5\u00a0mL of Buffer WB to column, apply vacuum (0.2\u20130.4 bar) until buffer has passed through the column.\nRemove the column from the vacuum manifold and insert it into a 2\u00a0mL collection tube, break off the upper part of the column and discard it.\nCentrifuge the lower part of the column in the collection tube at >11,000\u00a0g for 3\u00a0min at room temperature, discard collection tube with flow through and insert column into a new 1.5\u00a0mL collection tube.\nAdd 50\u00a0\u03bcL of Elution Buffer, incubate at room temperature for 3\u00a0min, and centrifuge at 11,000\u00a0g for 1\u00a0min.\nOptional: When connecting NucleoSnap cfDNA columns to the vacuum manifold, the use of disposable NucleoVac Mini Adapter is recommended to avoid cross-contamination, and the addition of NucleoVac Valves or equivalent stopcocks can be used if samples with different volumes are expected.\nPause point: If not proceeding with quantification and fragment size determination, extracted cfDNA can be stored at 4\u00b0C for a short duration or at -20\u00b0C for longer term, freeze-thaw cycles should be avoided.\nQuantification and fragment size determination\nTiming: 3 h\nAs part of quality control, extracted cfDNA is subjected to quantification and fragment size determination. In view of the expected low cfDNA concentration, a quantitative PCR (qPCR)-based assay targeting a repeat sequence (such as ALU or LINE-1) is adopted to maximize sensitivity (Rago et\u00a0al., 2007[href=https://www.wicell.org#bib5]), although fluorometric quantification such as using the Qubit fluorometer and dsDNA HS Assay Kit can be considered.cfDNA is quantified by qPCR by comparison against standards of genomic DNA (gDNA) with\u00a0known\u00a0concentrations using primers targeting the ALU sequences (ALU115-F\u2019: CCTGAGGTCAGGAGTTCGAG, ALU115-R\u2019: CCCGAGTAGCTGGGATTACA) (Umetani et\u00a0al., 2006[href=https://www.wicell.org#bib6]).\nUsing gDNA at known concentration (e.g., Promega G1521), prepare serially diluted gDNA samples at (10\u00a0ng/\u03bcL, 1\u00a0ng/\u03bcL, 100 pg/\u03bcL, 10 pg/\u03bcL, 1 pg/\u03bcL) as qPCR standards.\nRun qPCR according to the following setup with gDNA standards (1 pg/\u03bcL \u2013 10\u00a0ng/\u03bcL) and samples in triplicates (1\u00a0\u03bcL of standard or extracted cfDNA per well) to obtain the mean concentration of samples relative to the standard curve.\ntable:files/protocols_protocol_1555_1.csv\ntable:files/protocols_protocol_1555_2.csv\nFragment size is determined using the Agilent 4150/4200 TapeStation instruments.\nThe High Sensitivity D1000 ScreenTape and Reagents allow sizing of DNA at the cell-free range and has a sensitivity down to 5 pg/\u03bcL (requires 2\u00a0\u03bcL of extracted cfDNA).\nAlternatively, the Cell-free DNA ScreenTape and Reagents allow sizing of DNA within the cell-free range (150-200\u00a0bp) and at the same time determines the proportion of high-molecular weight gDNA contamination (sensitivity down to 20 pg/\u03bcL, requires 2\u00a0\u03bcL of extracted cfDNA).\nPause point: If not proceeding with library preparation, extracted cfDNA should be stored at \u221220\u00b0C and freeze-thaw cycles avoided.\nNote: Absence of a DNA peak within the cell-free range (150-200\u00a0bp) on a TapeStation trace does not preclude downstream processing of a sample as concentrations of tumor-derived cfDNA may be lower than the limit of detection for the TapeStation assay.\nLibrary preparation\nTiming: 6 hLibrary preparation for lcWGS using the 2S Hyb DNA Library Kit (Integrated DNA Technologies) and the 2S MID Adapter (S1-4, Integrated DNA Technologies). Other indexing options are also available from the manufacturer. Dephosphorylation, end repair, and ligation of adapters are performed according to protocol (Indexing by Ligation and Direct Sequencing workflow, Protocol PRT-037 Version 1 by Swift Biosciences, Document S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1555-Mmc1.pdf]), with the following specifications and modifications.\nIn our experience, starting material in the range of 50 pg-5\u00a0ng of cfDNA was compatible with library preparation (ideally determined by DNA concentration from qPCR in conjunction with proportion of cfDNA based on Cell-free DNA ScreenTape). Top off sample with low-EDTA TE to 40\u00a0\u03bcL for Repair I.\nAfter completion of Ligation II, proceed with pilot amplification and library amplification according to the next section.\nPause point: If not proceeding with library amplification, prepared library should be stored at 4\u00b0C for short term or \u221220\u00b0C for long term, with freeze-thaw cycles avoided.\nOptional: Suggest to always include a negative control with 40\u00a0\u03bcL of low-EDTA TE as the pipeline can generate library even from negligible amounts of contaminating DNA.\nLibrary amplification\nTiming: 3 h\nLibrary amplification according to cfDNA quantity may not be effective due to the low cfDNA concentration and frequent inability to determine the proportion of cfDNA versus high-molecular weight gDNA. A pilot qPCR run is first performed to offer guidance on the PCR cycles required for actual library amplification.\nLibrary prepared after completion of Ligation II is used for a qPCR run as follows:\ntable:files/protocols_protocol_1555_3.csv\nSet up and run qPCR according to the following protocol (1 well per library):\ntable:files/protocols_protocol_1555_4.csv\nCT value to be determined from the amplification curve for each library.\nAmplify library with number of PCR cycles being (CT value\u00a0+ 2), detailed as follows:\ntable:files/protocols_protocol_1555_5.csv\ntable:files/protocols_protocol_1555_6.csvClean up PCR reaction using SPRIselect beads at ratio of 0.85 (sample volume\u00a0= 50\u00a0\u03bcL, bead volume\u00a0= 42.5\u00a0\u03bcL). Elute in 20\u00a0\u03bcL of Tris HCl (pH 8.5) or water.\nAssess size profile and concentration of amplified and cleaned-up library using the D5000 ScreenTape and Reagents (Agilent).\nIf needed, libraries with low concentration (<4\u00a0nM) can be further amplified for 2\u20134 more cycles.\nPause point: If not proceeding with sequencing, amplified library should be stored at 4\u00b0C for short term or \u221220\u00b0C for long term, with freeze-thaw cycles avoided.\nNote: Further amplification beyond 4 cycles is unlikely to yield libraries useful for sequencing.\nWhole-genome sequencing\nTiming: 2\u00a0days\nDNA libraries are then subjected to further quality assessment and sequencing to achieve 3\u00d7 genome-wide coverage.\nAnalyze for library size distribution using a 4200 TapeStation D5000 ScreenTape assay (Agilent).\nQuantify libraries using the Quant-iT PicoGreen ds DNA assay (ThermoFisher).\nPaired-end, 100\u00a0bp sequencing is then performed on a NovaSeq 6000 (Illumina), targeting approximately 50 million read pairs.\nAfter sequencing is complete, run bcl2fastq to translate and demultiplex BCL files into FASTQ files.\nComputational analysis\nTiming: 3 h\nComplete this section to detect focal and large-scale CNVs and confirm the presence of tumor-derived cfDNA in CSF based on lcWGS.\nProcess lcWGS reads (recommended QC thresholds are indicated):\nRun FastQC on FASTQ files.\nCheck that the average sequence quality is >25.\nCheck that there is not an over-representation of k-mers that would be indicative of adapter sequences.\nCheck GC content is within the expected normal distribution.\nRun ddupk on FASTQ files to remove adaptors.\nRun BWA-mem to map FASTQs to human reference genome (hg19).\nCheck >80% of reads mapped.\nRun Picard de-duplication to remove predicted duplicates.\nThe number of duplicates can vary and will be larger for lower input samples.Optionally, if unique molecular identifiers (UMIs) were included then fgbio (https://github.com/fulcrumgenomics/fgbio[href=https://github.com/fulcrumgenomics/fgbio]) can be used to collapse UMI families and remove duplicates.\nCalculate tumor purity and CNVs.\ncfdna python package: see quantification and statistical analysis[href=https://www.wicell.org#quantification-and-statistical-analysis].", "Step-by-step method details\nStep-by-step method details\nDensity gradient centrifugation\nTiming: 60\u2013100\u00a0min\nThis section describes how to isolate mononuclear cells from BMA. Lymphoprep/Ficoll with a density of 1.077g/mL is used in this protocol (Figure\u00a02[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/831-Fig2.jpg\nFigure\u00a02. Lymophoprep/Ficoll separation demonstration and expected layering after centrifugation\n(A) Illustration of layering of Lymophoprep/Ficoll and diluted bone marrow before and after centrifugation in a SepMate tube.\n(B) Picture of separated layers of bone marrow after centrifugation.\nAdd 22\u00a0mL Lymphoprep/Ficoll to the 50\u00a0mL SepMate Tube by carefully pipetting it through the central hole of the SepMate insert without creating bubbles.\nDilute BMA with an equal volume of 2% FBS/DPBS and mix gently with wide-bore pipette.\nNote: The maximum volume of diluted BMA is 8\u00a0mL for SepMate-15 and 34\u00a0mL for SepMate-50.\nOverlay diluted BMA to Lymphoprep/Ficoll in the SepMate tube by slowly pipetting it down the side of the tube slightly above liquid level.\nCritical: Mixing diluted BMA and density gradient may result in incomplete separation or loss of cells and recovery will decrease.\nCentrifuge tube at 1200\u00a0\u00d7 g for 20\u00a0min at room temperature (20\u00b0C\u201322\u00b0C), with brake off.\nCritical: It is important that the brake is off otherwise the buffy layer of mono-nuclear cells (MNCs) will be disrupted and recovery will decrease.\nCritical: Polycythemia vera samples with high red blood cell (RBC) counts may require additional RBC lysis. The enriched cell layer (layer above the SepMate barrier) should be poured off into a new tube before lysing the RBCs using eBioscience 1X RBC Lysis Buffer.\u00a0The manufacturer\u2019s protocol can be found here: https://www.thermofisher.com/document-connect/document-connect.html?url=https%3A%2F%2Fassets.thermofisher.com%2FTFS-Assets%2FLSG%2Fmanuals%2F00-4333.pdf&title=VGVjaG5pY2FsIERhdGEgU2hlZXQ6IDFYIFJCQyBMeXNpcyBCdWZmZXI=[href=https://www.thermofisher.com/document-connect/document-connect.html?url=https%3A%2F%2Fassets.thermofisher.com%2FTFS-Assets%2FLSG%2Fmanuals%2F00-4333.pdf&title=VGVjaG5pY2FsIERhdGEgU2hlZXQ6IDFYIFJCQyBMeXNpcyBCdWZmZXI=]\nRemove the top layer of plasma/platelet and move the buffer layer containing MNCs to a new 50\u00a0mL tube.Top up the MNCs to 45\u00a0mL with 2% FBS/PBS and mix well with a wide-bore pipette.\nCentrifuge MNCs at 300\u00a0\u00d7 g for 12\u00a0min at room temperature (20\u00b0C\u201322\u00b0C), with low brake.\nRemove and discard supernatant.\nTop up the MNCs again until 45\u00a0mL with 2% FBS/PBS and mix well with a wide-bore pipette.\nCentrifuge MNCs at 120\u00a0\u00d7 g for 12\u00a0min at room temperature (20\u00b0C\u201322\u00b0C), with no brake\nCritical: If excess platelets remain (common when samples are from patients with essential thrombocythemia), repeat steps 9 and 10 once. Excess platelets can be identified by visual inspection of the supernatant after step 10. If the supernatant appears cloudy, we identify the sample as having excess platelets.\nDiscard supernatant and resuspend MNC in 1\u00a0mL of EasySep buffer on ice, mix with a wide-bore pipette.\nMeasure cell concentration with a hemocytometer and an automatic cell counter.\nMagnetic-beads cell enrichment\nTiming: 45\u00a0min\nThe subsequent steps are performed to obtain a single-cell suspension of CD34+ cells.\nAdd MNCs (at concentration of >108 cells/mL) to 5\u00a0mL polystyrene round-bottom tube.\nAdd 100\u00a0\u03bcL EasySep Human CD34 Positive Selection Cocktail to the sample.\nCritical: If sample volume is greater than 1\u00a0mL, add the selection cocktail at a ratio of 100\u00a0\u03bcL per 1\u00a0mL of sample.\nIncubate the sample at room temperature (20\u00b0C\u201322\u00b0C) for 10\u00a0min.\nVortex EasySep Dextran RapidSpheres for 30\u00a0s immediately before use.\nAdd 75\u00a0\u03bcL of EasySep Dextran RapidSpheres to the sample (Van Egeren et\u00a0al., 2021[href=https://www.wicell.org#bib1]).\nCritical: If sample volume is greater than 1\u00a0mL, add the RapidSpheres at a ratio of 75\u00a0\u03bcL per 1mL of sample.\nMix the sample and incubate at room temperature (20\u00b0C\u201322\u00b0C) for 5\u00a0min.\nTop up the sample to 2.5\u00a0mL with EasySep Buffer.Place the tube containing the sample in an EasySep magnet and incubate at room temperature (20\u00b0C\u201322\u00b0C) for 3\u00a0min.\nDiscard supernatant by inverting the magnet with the tube inside.\nRinse the tube with EasySep buffer.\nNote: Rinse the side of the tube to achieve maximum recovery.\nRepeat steps 19 to 22 four more times.\nResuspend MNC in 1\u00a0mL of EasySep buffer on ice, mix with a wide-bore pipette.\nMeasure cell concentration and cell viability with a hemocytometer and an automatic cell counter. The target final concentration is 700\u20131,200 cells/\u03bcL. The typical percent CD34+ cells viability obtained by following this procedure ranges from 85%\u201395% based on trypan blue staining.\nNote: Measuring cell concentration with both a manual hemocytomter and an automatic cell counter is recommended to minimize the counting error.\nKeep isolated cells on ice and proceed immediately to the 10\u00d7 Genomic Chromium Single Cell protocol.\nChromium single cell 3\u2032 GEM cDNA and library construction\nTiming: 45\u00a0min\nGenerate the full transcriptomic libraries according to 10\u00d7 Genomic Chromium Single Cell v3 manufacturer\u2019s protocol. The manufacturer\u2019s protocol can be found at https://support.10xgenomics.com/single-cell-gene-expression/library-prep/doc/user-guide-chromium-single-cell-3-reagent-kits-user-guide-v3-chemistry[href=https://support.10xgenomics.com/single-cell-gene-expression/library-prep/doc/user-guide-chromium-single-cell-3-reagent-kits-user-guide-v3-chemistry].\nPause point: The cDNA can be stored in a freezer at \u221220\u00b0C for 1\u20132\u00a0months until the following procedures.\nLocus-specific single-cell amplicon libraries\nTiming: 4\u20138 h\nThe five-step locus-specific PCR amplification first amplifies the somatic mutation of interest (PCR 1\u20133) in a nested fashion. Illumina sequencing adaptors are subsequently added to the amplified products by PCR (PCR 4\u20135). Libraries are then quantified on an Agilent Tapestation or Bioanalyzer. (Tapestation traces can be found in Figure\u00a03[href=https://www.wicell.org#fig3])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/831-Fig3.jpg\nFigure\u00a03. Step-by-step tapestation traces for locus-specific amplicon library construction\n(A\u2013E) Step by step tapestation traces from locus-specific PCR 1 to SI PCR.\n(F) Collapsed view of tapestation of PCR steps 1\u20135.\nNested locus specific PCR 1.Prepare master mix in a 0.2\u00a0mL thin-wall PCR tube.\ntable:files/protocols_protocol_831_2.csv\nNote: Internal_forward (5\u2032-AATGATACGGCGACCACCGAGATCTACAC-TCTTTCCCTACACGACGCTC) completes Illumina Read 1 and adds partial Illumina P5 sequence to the 5\u2032\u00a0end of the cDNA library.\nMix by pipetting 10 times or gently vortexing, centrifuge briefly.\nInsert tubes into a thermocycler and incubator with the following protocol.\ntable:files/protocols_protocol_831_3.csv\nRemove primer-dimers using SPRIselect beads.\nCritical: SPRIselect beads should be acclimated to room temperature (20\u00b0C\u201322\u00b0C) before continuing.\nAdd 25\u00a0\u03bcL nuclease-free water to each sample.\nVortex to resuspend the SPRIselect reagent. Add 40\u00a0\u03bcL SPRIselect reagent (0.8\u00d7) to each sample and pipette mix (15\u00d7) or gently vortex mix.\nIncubate 5\u00a0min at room temperature (20\u00b0C\u201322\u00b0C).\nPlace on the 10\u00d7 magnetic separator (high orientation) until the solution clears.\nNote: The high orientation is used when the tube contains more than 50\u00a0\u03bcL of liquid and the low orientation is used when the tube contains less than 50\u00a0\u03bcL of liquid for easy liquid handling.\nRemove the supernatant.\nAdd 200\u00a0\u03bcL 80% ethanol to the pellet. Wait 30 s.\nCritical: 80% ethanol should be prepared fresh for best wash performance and yields.\nRemove the ethanol.\nRepeat steps f and g for a total of 2 washes.\nCentrifuge briefly and place on the magnet (low orientation).\nRemove any remaining ethanol and air dry for 2\u00a0min to avoid ethanol carryover.\nAdd 26\u00a0\u03bcL nuclease-free water. Pipette mix (15\u00d7) or gently vortex mix.\nIncubate 2\u00a0min at room temperature (20\u00b0C\u201322\u00b0C).\nPlace the tube strip on the magnet (low orientation) until the solution clears.\nTransfer 25\u00a0\u03bcL sample to a new tube strip.\nStore at 4\u00b0C for up to 72\u00a0h or at \u221220\u00b0C for up to 4\u00a0weeks, or proceed to the next step immediately.Pause point: The PCR product can be stored in a freezer at \u221220\u00b0C for 1\u20132\u00a0months or in a fridge at 4\u00b0C for 24\u00a0h until the following procedures.\nNested locus specific PCR 2.\nPrepare master mix in a 0.2\u00a0mL thin-wall PCR tube.\nMix by pipetting 10 times or gently vortexing, centrifuge briefly.\nInsert tubes into a thermocycler and incubator with the following protocol.\ntable:files/protocols_protocol_831_4.csv\nNote: Short_Int_For consists of partial Illumina P5 (5\u2032-AATGATACGGCGACCACCGAGATCT).\ntable:files/protocols_protocol_831_5.csv\nRepeat step 30 to remove primer-dimers.\nPause point: The PCR product can be stored in a freezer at \u221220\u00b0C for 1\u20132\u00a0months or in a fridge at 4\u00b0C for 24\u00a0h until the following procedures.\nNested locus specific PCR 3.\nPrepare master mix in a 0.2\u00a0mL thin-wall PCR tube.\nMix by pipetting 10 times or gently vortexing, centrifuge briefly.\nInsert tubes into a thermocycler and incubator with the following protocol.\ntable:files/protocols_protocol_831_6.csv\ntable:files/protocols_protocol_831_7.csv\nRepeat step 30 to remove primer-dimers.\nPause point: The PCR product can be stored in a freezer at \u221220\u00b0C for 1\u20132\u00a0months or in a fridge at 4\u00b0C for 24\u00a0h until the following procedures.\nNested locus specific PCR 4.\nPrepare master mix in a 0.2\u00a0mL thin-wall PCR tube.\nMix by pipetting 10 times or gently vortexing, centrifuge briefly.\nInsert tubes into a thermocycler and incubator with the following protocol.\ntable:files/protocols_protocol_831_8.csv\nNote: Locus-Specific Primer 4 contains Illumina Read 2 (GTGACTGGAGTTCAGACGTGTGCTCTTCCGATCT) and 18\u201322 nucleotides from the 5\u2032 end of locus-specific primer\u00a03.\ntable:files/protocols_protocol_831_9.csv\nRepeat step 30 to remove primer-dimers.\nSample Index PCR.\nPrepare master mix in a 0.2\u00a0mL thin-wall PCR tube.\nMix by pipetting 10 times or gently vortexing, centrifuge briefly.\nInsert tubes into a thermocycler and incubator with the following protocol.\ntable:files/protocols_protocol_831_10.csvNote: SI Primer, Chromium i7 Sample Index, and 10\u00d7 Amp Mix are included in the 10\u00d7 Chromium 3\u2032 kit. Record the i7 Sample Index for each library for downstream analysis.\ntable:files/protocols_protocol_831_11.csv\nRepeat step 30 to remove primer-dimers.\nQuantify the locus-specific library using Tapestation.\nPause point: The PCR product can be stored in a freezer at \u221220\u00b0C for 1\u20132\u00a0months or in a fridge at 4\u00b0C for 24\u00a0h until the following procedures.\nNote: High Sensitivity D5000 ScreenTapes and reagents are recommended dur to the library size. However, other Agilent ScreenTapes or Agilent Bioanalyzer reagents can be\u00a0used for quantifying the libraries. If a TapeStation or Bioanalyzer is not available, quantification can be done by running a 2%\u20133% agarose gel with appropriate ladder. Quantification with NanoDrop and Qubit is not sufficient due to the lack of the fragment size information.\nSequence the locus-specific library using the following cycles:\nRead 1 28 cycles\nI7 Index 8 cycles\nI5 index 0 cycles\nRead 2 91 cycles\nNote: The locus-specific library can be pooled with the transcriptome library for sequencing or be sequenced alone. We sequence the pooled transcriptome and locus-specific libraries on a NovaSeq SP Flowcell (800 million reads) and the locus specific libraries are usually allocated with 2% of the total reads (16 million reads). The libraries can also be sequenced on MiSeq Flowcells for testing and troubleshooting. Please take into account the multiple peaks in fragment size distribution when calculating library concentration.", "Step-by-step method details\nStep-by-step method details\nBuild the DFT calculation database\nTiming: 6\u20137\u00a0days\nIn this section, we conduct DFT calculations to build the database containing mechanical and bond properties of ceramics for further machine-learning model training.\nNote: The time for DFT calculations largely depends on sizes of the models, the number of cases involved in the database, and the hardware. This calculation time in this protocol is obtained using 64 CPU cores for 438 cases having 8 atoms in each unit cell, which is a reference of time arrangement for users. In most cases, the model size is the main factor influencing computational cost, so small models such as the unit cell containing 8 atoms shown in Figure\u00a01[href=https://www.wicell.org#fig1] are recommended for the DFT calculation database construction.\nDetermine the type of ceramic for mechanical property prediction. In this protocol, the DFT calculation database contains properties of 438 rock-salt carbides, nitrides and carbonitrides, which is available on the project GitHub.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1837-Fig1.jpg\nFigure\u00a01. Structure of the rock-salt ceramic\nThe data in the left red box are the template structure file in POSCAR format, the atomic structure on the right is the corresponding 3D model visualized in VESTA.\nGeometry optimization calculations.\nPrepare input structure \u201cPOSCAR\u201d files. Use the structure file of VC as the template \u201cPOSCAR\u201d file for all rock-salt ceramics as shown in Figure\u00a01[href=https://www.wicell.org#fig1], replace m and n by metal and nonmetal element names, respectively.\nPrepare input pseudopotential \u201cPOTCAR\u201d files. Taking the structure in Figure\u00a01[href=https://www.wicell.org#fig1] as an example, create the \u201cPOTCAR\u201d file for this ceramic using the command:\n>cat m1 m2 m3 m4 n1 n2 n3 n4\u00a0>> POTCARNote: \u201cm1\u201d, \u201cm2\u201d, \u201cm3\u201d, \u201cm4\u201d, \u201cn1\u201d, \u201cn2\u201d, \u201cn3\u201d and \u201cn4\u201d are names of pseudopotential files (provided by VASP) of elements m1, m2, m3, m4, n1, n2, n3 and n4, respectively.\nPrepare input \u201cKPOINTS\u201d files. Use the \u201cKPOINTS\u201d file as follows:\n>Automatic mesh\n>0\n>Gamma\n>11 11 11\n>0 0 0\nPrepare input \u201cINCAR\u201d files. Use the \u201cINCAR\u201d file as follows:\n>ENCUT\u00a0= 600\n>ISTART\u00a0= 0\n>ICHARG\u00a0= 2\n>ISMEAR\u00a0= -5\n>SIGMA\u00a0= 0.05\n>NSW\u00a0= 200\n>IBRION\u00a0= 2\n>ISIF\u00a0= 3\n>SYMPREC\u00a0= 1E-5\n>ispin\u00a0=2\n>POTIM\u00a0= 0.2\n>EDIFF\u00a0= 1E-5\n>EDIFFG\u00a0= -1E-2\n>PREC\u00a0= Accurate\nPut the four input files, namely \u201cPOSCAR\u201d, \u201cPOTCAR\u201d, \u201cKPONITS\u201d and \u201cINCAR\u201d, in the same dictionary, and then run the calculation using the command:\n> mpirun -n number_of_cores vasp_std >& log\nBond property calculations.\nPrepare input \u201cPOSCAR\u201d files. Rename the relaxed structure file \u201cCONTCAR\u201d obtained by the geometry optimization calculation by \u201cPOSCAR\u201d.\nPrepare input \u201cPOTCAR\u201d and \u201cKPONITS\u201d files. Use the same input files with those used in the optimization calculation.\nPrepare input \u201cINCAR\u201d files. Use the \u201cINCAR\u201d file as follows:\n>ENCUT\u00a0= 600\n>ISTART\u00a0= 0\n>ICHARG\u00a0= 2\n>ISMEAR\u00a0= 0\n>SIGMA\u00a0= 0.05\n>LREAL\u00a0= .FALSE.\n>SYMPREC\u00a0= 1E-5\n>ispin\u00a0= 2\n>IBRION\u00a0= -1\n>ISIF\u00a0= 3\n>NSW\u00a0= 0\n>POTIM\u00a0= 0.2\n>EDIFF\u00a0= 1E-5\n>PREC\u00a0= Accurate\n>LORBIT\u00a0= 11\n>LAECHG\u00a0= .TRUE.\n>PREC\u00a0= Accurate\nPut the four input files, namely \u201cPOSCAR\u201d, \u201cPOTCAR\u201d, \u201cKPONITS\u201d and \u201cINCAR\u201d, in the same dictionary, and then run the calculation using the command:\n> mpirun -n number_of_cores vasp_std >& logPut the bond property DFT calculation generated files \u201cAECCAR0\u201d, \u201cAECCAR2\u201d and \u201cCHGCAR\u201d and the corresponding \u201cPOTCAR\u201d file, together with the file \u201cjob_control.txt\u201d (provided by Chargemol) and the Chargemol executable file \u201cChargemol_09_26_2017_linux_serial\u201d in the same directory, and then run the density derived electrostatic and chemical (DDEC) calculation using the command:\n>./Chargemol_09_26_2017_linux_serial\nCalculate the bonding characteristic: net charge (NETM).\nNote: The file \u201cDDEC6_even_tempered_net_atomic_charges.xyz\u201d generated after the DDEC calculation contains the value of net charge on each atom. Taking the DDEC calculation results for VC as an example, Figure\u00a02[href=https://www.wicell.org#fig2] shows the first few lines of the \u201cDDEC6_even_tempered_net_atomic_charges.xyz\u201d file, and we can have the average value for metallic (V)\u00a0atoms in the red box as the net charge for the ceramic VC.\nCalculate the bonding characteristic: sum of bond order (SBO).\nNote: The file \u201cDDEC6_even_tempered_bond_orders.xyz\u201d generated after the DDEC calculation contains the information of sum of bond order of each atom. Taking the DDEC calculation results for VC as an example, Figure\u00a03[href=https://www.wicell.org#fig3] shows the first few lines of the \u201cDDEC6_even_tempered_bond_orders.xyz\u201d file, and we can use the sum of average values respectively for V and C atoms in the red boxes as the value of sum of bond order for the ceramic VC.\nCalculate the bonding characteristic: bond length (BL).\nNote: The average bond length of the relaxed structure equals to     V \u2215 8  3   , where V is the volume of the relaxed unit cell, which is given by VESTA as shown in Figure\u00a04[href=https://www.wicell.org#fig4] after reading the \u201cCONTCAR\u201d file obtained by the geometry optimization calculation.\nMechanical property calculations.\nPrepare input \u201cPOSCAR\u201d, \u201cPOTCAR\u201d and \u201cKPONITS\u201d files. Use the same input files with those used in the optimization calculation.\nPrepare input \u201cINCAR\u201d files. Use the \u201cINCAR\u201d file as follows:\n>ENCUT\u00a0= 600\n>ISTART\u00a0= 0\n>ICHARG\u00a0= 2\n>SYMPREC\u00a0= 1E-5>ispin\u00a0= 2\n>ISMEAR\u00a0= 0\n>SIGMA\u00a0= 0.05\n>POTIM\u00a0= 0.05\n>LCHARG\u00a0= .TRUE.\n>LWAVE\u00a0= .TRUE.\n>LREAL\u00a0= .FALSE.\n>IBRION\u00a0= 6\n>ISIF\u00a0= 3\n>NSW\u00a0= 1\n>NFREE\u00a0= 4\n>EDIFF\u00a0= 1E-5\n>EDIFFG\u00a0= -0.01\n>PREC\u00a0= Accurate\nPut the four input files, namely \u201cPOSCAR\u201d, \u201cPOTCAR\u201d, \u201cKPONITS\u201d and \u201cINCAR\u201d, in the same dictionary, and then run the calculation using the command:\n> mpirun -n number_of_cores vasp_std >& log\nCalculate mechanical properties: bulk (B), shear (G) and Young\u2019s (E) moduli.\nNote: The \u201cOUTCAR\u201d file obtained by the mechanical property DFT calculation contains the elastic constants, based on which we can calculate the bulk, shear and Young\u2019s moduli using the Voigt-Reuss-Hill approximation (Hill, 1952[href=https://www.wicell.org#bib2]).\nAlternatives: The commands in steps 1\u20134 are for case-by-case calculations. If many cases are involved in a dataset, we can prepare all input files for a case in one dictionary and run a series of jobs for all cases automatically using the script \u201cjob.sh\u201d by:\u00a0>\u00a0sh job.sh, as is shown in Figure\u00a05[href=https://www.wicell.org#fig5].\nAlternatives: The command \u201cmpirun\u201d is used to start the parallel version of VASP compiled with MPI. To run a serial version of VASP, it is needed to replace the \u201cmpirun\u201d line by \u201c> vasp_std\u201d. The parallel version of VASP is recommended for the higher calculation efficiency.\nNote: In this example, 438 cases are involved in the dataset. For the practical application, a small dataset containing information on simple ceramics is sufficient to build reliable machine-learning models. E.g., involving information on rock-salt ScC, TiC, VC, CrC, MnC, FeC, CoC, NiC, ZnC, YC, ZrC, NbC, MoC, HfC, TaC, WC carbides into the DFT dataset is sufficient to build reliable machine-learning models for complex rock-salt (ScTiVCrMnFeCoNiZnYZrNbMoHfTaW)C carbides.\nNote: The commands and calculations are implemented under Linux environment.Note: The parameters in \u201cINCAR\u201d and \u201cKPOINTS\u201d files are the same for different cases for the same kind of DFT calculations.\nNote: To build new DFT datasets for other rock-salt ceramics like oxides, sulfides, chlorides and fluorides, no significant modifications are required, simply replacing m and n in steps 2b and 2c by specific elements will work.\nNote: To build new DFT datasets for WC-type carbides, modifications of input files and the calculation method for average bond length are needed.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1837-Fig2.jpg\nFigure\u00a02. Results of the net charge calculation\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1837-Fig3.jpg\nFigure\u00a03. Results of the sum of calculated bond order\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1837-Fig4.jpg\nFigure\u00a04. Results of calculated average bond length\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1837-Fig5.jpg\nFigure\u00a05. File preparation and the script to run a series of jobs automatically\nTrain machine-learning prediction models\nTiming: \u223c30\u00a0min\nIn this section, we train the machine-learning models to learn correlations between bonding characteristics and mechanical properties based on the DFT calculation database. The dataset for machine-learning training, the training codes and the obtained prediction models are all available on the project GitHub.\nPrepare the dataset for machine-learning model training. Store the data in a .csv file as shown in Figure\u00a06[href=https://www.wicell.org#fig6] and then load the .csv file to MATLAB with the name \u201ctrainingData\u201d.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1837-Fig6.jpg\nFigure\u00a06. DFT dataset stored in a .csv file for machine learning\nPrepare the training code. Taking the training of machine-learning prediction model for bulk modulus as an example, use the training code saved in the file \u201ctrainModelB.m\u201d as follows:\n>function [ModelB, validationRMSE]\u00a0= trainModelB(trainingData)\n>inputTable\u00a0= trainingData;\n>predictorNames\u00a0= {'SBO', 'NETM', 'BL'};\n>predictors\u00a0= inputTable(:, predictorNames);\n>response\u00a0= inputTable.B;\n>isCategoricalPredictor\u00a0= [false, false, false];\n>\u00a0\u00a0predictors, ...\n>\u00a0\u00a0response, ...\n>\u00a0\u00a0'BasisFunction', 'constant', ...\n>\u00a0\u00a0'KernelFunction', 'exponential', ...\n>\u00a0\u00a0'Standardize', true);\n>predictorExtractionFcn\u00a0= @(t) t(:, predictorNames);\n>gpPredictFcn\u00a0= @(x) predict(regressionGP, x);>ModelB.predictFcn\u00a0= @(x) gpPredictFcn(predictorExtractionFcn(x));\n>ModelB.RequiredVariables\u00a0= {'BL', 'NETM', 'SBO'};\n>ModelB.RegressionGP\u00a0= regressionGP;\n>inputTable\u00a0= trainingData;\n>predictorNames\u00a0= {'SBO', 'NETM', 'BL'};\n>predictors\u00a0= inputTable(:, predictorNames);\n>response\u00a0= inputTable.B;\n>isCategoricalPredictor\u00a0= [false, false, false];\n>partitionedModel\u00a0= crossval(ModelB.RegressionGP, 'KFold', 10);\n>validationPredictions\u00a0= kfoldPredict(partitionedModel);\n>validationRMSE\u00a0= sqrt(kfoldLoss(partitionedModel, 'LossFun', 'mse'));\nPerform the machine-learning training. Taking the training of machine-learning prediction model for bulk modulus as an example, put the training code file \u201ctrainModelB.m\u201d and the training dataset file \u201ctrainingData\u201d in the same workspace, and then run the command in MATLAB as follows to obtain the machine-learning prediction model \u201cModelB\u201d:\n>[ModelB, validationRMSE]\u00a0= trainModelB(trainingData)\nNote: As the brittleness and hardness estimated using empirical models have intrinsic analytic relations with the shear and bulk moduli, here only mechanical properties of bulk, shear and Young\u2019s moduli are set as targeted outputs.\nNote: Machine-learning prediction models for shear and Young\u2019s modulus can be obtained by respectively replacing the keyword \u201cB\u201d in the code by \u201cG\u201d and \u201cE\u201d in steps 6 and 7.\nNote: Detailed code annotations are available in the training code files provided on the project GitHub.\nNote: In step 7, only the root mean squared error (RMSE) of a 10-fold cross-validation is given as a reference for the validation of the reliability. This is because the rock-salt ceramics and WC-type carbides have been systematically investigated and the effectiveness of this protocol and the reliability of machine-learning models have been examined for the two systems. If using this protocol for ceramics other than rock-salt ceramics or WC-type carbides, more parameters should be included for the validation.\nPrepare the inputs for prediction\nTiming: \u223c30\u00a0minBonding characteristics of a multi-element ceramic are the inputs to predict its mechanical properties based on machine-learning prediction models. In this section, we calculate bond parameters of multi-element ceramics based on those of simple ceramics from the DFT dataset.\nCalculate bonding characteristics of multi-element ceramics.\nNote: Bonding characteristics of multi-element carbides, nitrides and carbonitrides equals to the weighted value of those of involved constituents (mono-carbides and nitrides) according to their atomic concentrations (Tang et\u00a0al., 2021[href=https://www.wicell.org#bib7]). For instance, for the multi-element rock-salt carbonitride (VNbTaW) (CN), its bonding characteristics are average values of corresponding bonding characteristics of VC, NbC, TaC, WC, VN, NbN, TaN and WN.\nCollect bonding characteristics of all concerned multi-element rock-salt carbides, nitrides and\u00a0carbonitrides in a .csv file and then load the .csv file into MATLAB by the name of \u201cpredictioninputs\u201d.\nNote: The sum of bond order, net charge and bond length is respectively labeled as \u201cSBO\u201d, \u201cNETM\u201d and \u201cBL\u201d. The prediction inputs dataset is available on the project GitHub.\nBuild the machine-learning prediction database\nTiming: \u223c10\u00a0min\nUsing machine-learning prediction models and empirical models, we predict mechanical properties of multi-element ceramics based on bonding characteristics to build the mechanical property database for all potential multi-element ceramics.\nPredict mechanical properties: bulk, shear and Young\u2019s moduli. Put the prediction inputs file \u201cpredictioninputs\u201d and the prediction model files \u201cModelB\u201d, \u201cModelG\u201d and \u201cModelE\u201d in the same workspace, and then run the commands as follows in MATLAB to get bulk, shear and Young\u2019s moduli stored in \u201cfitB\u201d, \u201cfitG\u201d and \u201cfitE\u201d, respectively:\n>fitB\u00a0= ModelB.predictFcn(predictioninputs)\n>fitG\u00a0= ModelG.predictFcn(predictioninputs)\n>fitE\u00a0= ModelE.predictFcn(predictioninputs)\nCalculate mechanical properties: brittleness and hardness.Note: These two properties are estimated by empirical models based on predicted bulk and shear moduli. Specifically, the brittleness equals to G/B (Pugh, 1954[href=https://www.wicell.org#bib6]), the hardness equals to   2   (   k 2  G  )  0.585  \u2212 3   (Chen et\u00a0al., 2011[href=https://www.wicell.org#bib1]).\nCollect information on rock-salt carbides, nitrides and carbonitrides, including their names and predicted bulk, shear and Young\u2019s moduli, brittleness and hardness, in one Excel file to form the machine-learning prediction database.", "Step-by-step method details\nStep-by-step method details\nFor the standard cell differentiation protocol, follow steps 1-20. For the alternative barcoding protocol, please follow steps 1-3 and then skip to step 21.\nCell sorting- Day 1\nTiming: 1\u00a0day\nIn this step your population of interest is sorted as single cells in 96-well plates.\nHarvest BM.\nIn the morning, euthanize mouse according to your ethical permit.\nDissect BM containing bone such as pelvis, femur, tibia-fibula, radius-ulna, humerus, spine (vertebra), scapula, and ribs from mice and collect these in a 50\u00a0mL tube containing cold (4\u00b0C) FACS-buffer.\nNote: In our experience tibia, femur and pelvis contain the highest number of BM cells.\nCrush bones in ice-cold FACS-buffer with a mortar and pestle and make single-cell suspensions by gently pipetting up and down using a 1\u00a0mL pipette tips.\nFilter through a 40\u00a0\u03bcm cell strainer. Count the cells and centrifuge at 350\u00a0\u00d7\u00a0g for 5\u00a0min at 4\u00b0C.\nRemove the supernatant and resuspend cells in FACS-buffer, 1\u00a0mL/250\u00a0\u00d7\u00a0106 cells.\nSet aside around (10k) cells for \u201cFMO APC\u201d and \u201cUnstained control\u201d. The remaining cells will be used for C-Kit-enrichment.\nNote: This step is important to enrich for LSK population and reduces sorting time. Lineage depletion also can replace C-kit enrichment in this step.\nC-Kit-enrichment.\nAdd 2\u00a0\u03bcL c-Kit-APC-antibody per 1\u00a0mL cell suspension.\nIncubate samples at 4\u00b0C in darkness and gentle shaking for 30\u00a0min.\nWash cells by filling up the tube with FACS-buffer and spin 350\u00a0g for 8\u00a0min.\nRe-suspend the cell pellet in 500\u00a0\u03bcL FACS-buffer with 25\u00a0\u03bcL anti-APC beads/250\u00a0\u00d7\u00a0106 cells.\nIncubate dark at 4\u00b0C for 30\u00a0min. Shaking is not required.\nWash cells by filling up the tube with FACS-buffer and spin 350\u00a0g for 8\u00a0min.\nRe-suspend cells in 500\u00a0\u03bcL FACS buffer.For C-Kit-enrichment insert MACS column into the MACS separator magnet and put a 50\u00a0\u03bcm cup filter on top of each column.\nEquilibrate the columns by running 3\u00a0mL FACS buffer through the column and discard the liquid flow through.\nPlace labeled collection-tubes underneath each column and pipette the 500\u00a0\u03bcL sample through the filter.\nRinse the sample-tube with 500\u00a0\u03bcL of buffer and add to column.\nAdd 3\u00a0mL of buffer to the column to rinse off the unbound cells, wait until the buffer run through then repeat this step two more times.\nSeparate the column from the separator magnet and place it on top of a new collection-tube.\nAdd 5\u00a0mL buffer to the column and flush out the cells by firmly pushing the supplied plunger.\nCell staining.\nCount the cells using an Automated Cell Counter.\nSet aside around (200k) cells for the remaining \u201cFMOs\u201d.\nSpin the cell suspensions at 4\u00b0C 350\u00a0g for 8\u00a0min; remove the supernatant.\nResuspend the cell pellet in FACS buffer containing FC-block (1:100) at the volume of 100\u00a0\u03bcL/107 c-Kit enriched cells and incubate for 5\u00a0min in 4\u00b0C.\nAdd antibody staining cocktail according to the table in the \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d section.\nStain the sample and FMO controls and incubate dark for 45\u00a0min at 4\u00b0C and gentle shaking.\nWash cells in FACS buffer, spin at 350\u00a0g for 8\u00a0min at 4\u00b0C, remove supernatant.\nRe-suspend pellet in FACS-buffer containing 7AAD (1:200), and re-suspend FMO PECy5 and unstained-control in FACS-buffer without 7AAD.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2329-Fig2.jpg\nFigure\u00a02. FACS cell sorting gating strategy for hematopoietic stem and progenitor populations\n(A) Representative FACS gating strategy for LSK, LTHSCs and LMPP.\n(B and C) Purity check and FMOs have been shown in (B)\u00a0for LTHSCs, LMPPs and (C)\u00a0for LSK.Note: We sorted on an AriaIII instrument, according to configuration designed in \u201cFACS cell sorter and analyser\u201d in \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d section.\nCell sorting.\nSort single cells (LT-HSCs) into 96-well plate containing the OP9-layer according to the gating strategy in (Figures\u00a02[href=https://www.wicell.org#fig2]A and 2B).\nOptional: Activate index sorting in the FACS sorting software to save the immunophenotypic profile for each single cell.\nNote: In this study we used 70\u00a0\u03bcm nozzle in AriaIII with the sort of precision mode 4-way purity (0-32-0). The sorting purity is recommended to be greater than 95% of viable cells (Figure\u00a02[href=https://www.wicell.org#fig2]B).\nAfter sorting, wipe the surface of the plate with 70% ethanol and keep it in a closed container together with a petri culture dish containing autoclaved water.\nStore in incubator at 37\u00b0C and 5% CO2 for 4\u00a0days.\nPreparation for switch culture- Day 3\nTiming: 2\u20133 h\nIn this step the growing single-cell cultures will be split equally to Erythroid or B cells conditions according to the experimental design (Figure\u00a03[href=https://www.wicell.org#fig3]A). The 96 well plates used in this step should be with low evaporation lid.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2329-Fig3.jpg\nFigure\u00a03. Methodology and FACS read out for the switch culture protocol\n(A) Experimental strategy for clonal switch-culture.\n(B and C) Representative FACS gating strategy of (B) LTHSCs-derived Myeloid (My), Macrophage (M\u03a6) and B clones from B condition at day 21 co-cultures, and (C) Myeloid (My), Macrophage (M\u03a6) and Erythoid (ER) clones for ER culture system at day 21.\n(D) Lineage distribution of clones derived from single cells from the indicated sub-populations (LT-HSCs and LMPP). Error bars indicate standard deviation.\n(E and F) Representative May-Gruwald-Giemsa staining of cytospin slides from the cells of readout in (E) Erythroid or (F) B condition at day 21. Scale-bare represents 20\u00a0\u03bcm.For B-cell differentiation culture, 2000 OP9 cells are seeded into a new flat bottom 96 well culture plate containing 50\u00a0\u03bcL complete OptiMEM with cytokine.\nNote: The cytokine concentration should be appropriate for final volume of 200\u00a0\u03bcL according to the table \u201cAlternative 1-cytokines B conditions\u201d.\nFill the wells surrounding the seeded cells with autoclaved water and incubate at 37\u00b0C and 5% CO2 preferably overnight or at least 6\u00a0h before sorting.\nFor Erythroid (ER) differentiation culture, prepare a new flat bottom 96-well plate with 50\u00a0\u03bcL complete SFEM with cytokines.\nNote: The cytokine concentration should be appropriate for a final volume of 200\u00a0\u03bcL according to the table \u201cAlternative 1- cytokines Erytrhoid condition\u201d.\nFill the wells surrounding the SFEM wells with autoclaved water, keep them in 4\u00b0C overnight in a sterile container.\nNote: This step can be prepared at day 4 as well.\nSwitch culture- Day 4\nTiming: 0.5\u00a0day\nSterilize the tissue culture hood and microscope according to step 1.\nCheck the plate with sorted cells after 4\u00a0days under the microscope to verify the growth of clones.\nNote: At this stage the earliest colonies from certain progenitors such as LMPP may be visible; however, colonies from LT-HSCs will appear later. As a positive control seed a well with 10 cells while sorting.\nPlace the plate with sorted cells as well as the plates prepared in steps 1 and 3 in the hood.\nHomogenize the cells in each well (100\u00a0\u03bcL) by pipetting with a Multichannel pipette.\nDivide 50\u00a0\u03bcL into B cell differentiation culture and 50\u00a0\u03bcL in ER cell differentiation culture (Figure\u00a03[href=https://www.wicell.org#fig3]A). Make sure to plate the cells in the same plate-position as in the (index) sorted plate.Rinse each well two times with 50\u00a0\u03bcL complete SFEM medium or OptiMEM medium without cytokines to recover all the cells.\nDivide the cells equally to each differentiation condition (the total volume in each condition should be 200\u00a0\u03bcL/well).\nCheck the sorted plate under the microscope to ensure that no cells remain in the wells.\nNote: If cells remain, rinse again with complete SFEM or OptiMEM medium and add to the respective well of each condition. Adjust cytokine concentration accordingly.\nKeep plates in a sterile container including a petri dish with autoclaved water in 37\u00b0C and 5% CO2 incubator until readout.\nChange half the medium every 7-day.\nVerify cell growth by microscope analysis.\nCentrifuge the plate for 5\u00a0min in 350\u00a0g at 4\u00b0C.\nRemove 100\u00a0\u03bcL medium from top carefully without interfering with the growing clone.\nAdd 100\u00a0\u03bcL fresh medium with cytokine concentration appropriate for 200\u00a0\u03bcL and incubate in 37\u00b0C and 5% CO2.\nNote: If the cells are confluent they should be moved to a larger-sized plate. In this case, homogenize the cells in a 96-well plate by pipetting the cells and transfer (200\u00a0\u03bcL/well) into the newly prepared 48-well plates, for B condition (containing OP9 cells) or erythroid condition, as previously explained. Rinse the 96-well plates two times with 100\u00a0\u03bcL medium supplemented with appropriate cytokine concentrations for a total of 400\u00a0\u03bcL, and transfer to the 48-well plate (in total 400\u00a0\u03bcL/well).\nRead out-Day 21\nTiming: 1\u00a0day\nThe kinetics of progenitors and lineage differentiation varies. It is important to optimize the time point for read out for the test population beforehand at bulk or single cell level.\nNote: In our experience, the kinetics of bulk cells is slightly faster compared to single cells; therefore we recommend setting up the experiment at single cell level.FACS analysis.\nHomogenize the cells by pipetting up and down (using 8- Multichannel pipette), transfer cells to V- bottom 96 well plates.\nRinse each well once more with 100\u00a0\u03bcL of FACS buffer and add to the respective well in the V- bottom 96 well plates.\nCentrifuge plates for 5\u00a0min in 350\u00a0g and discard supernatant without disturbing the cell pellet.\nRe-suspend the pellets in 50\u00a0\u03bcL of FACS buffer containing FC-block (1:100) and incubate for 5\u00a0min at 4\u00b0C.\nFor Erythroid or B cell differentiation culture, respectively, stain cells according to the table \u201cAlternative 1- antibody cocktail\u201d and incubate for 45\u00a0min, in dark, 4\u00b0C on slow shaker.\nRinse stained cells with FACS buffer, centrifuge at 350\u00a0g for 5\u00a0min at 4\u00b0C.\nRemove supernatant and re-suspended cells in 200\u00a0\u03bcL FACS buffer containing 7AAD (1:200).\nSet up staining compensation matrix using single stained controls (beads) on LSR-Fortessa (BD).\nSet gates as shown for Erythroid cultures in (Figure\u00a03[href=https://www.wicell.org#fig3]B) or B-cell cultures in (Figure\u00a03[href=https://www.wicell.org#fig3]C).\nRun samples and FMOs using plate or tube reader.\nNote: If a plate reader is used; the \u201cmaximum wash\u201d setting should be applied between each sample according to the specific FACS instrument\u2019s settings.\nFollowing FACS, lineage output is analyzed in FlowJo for the presence of Erythroid cells (CD45-, F4-80-, Mac1-, Gr1-, TER119+), B cells (CD45+, CD19+, Mac1-) and myeloid cells (either CD45+, Mac1+, Gr1+, or CD45+, Mac1+, F-480+).\nNote: We set the cut-off to ensure that we accurately capture the population. For clones to be considered positive, the scatter profile must contain at least 50-gated events. Each lineage clone's population should be distinct in back gating, and more than 20 events must be recorded within the scatter profile for the analysis to score as positive.\nVerify cell type\nTiming: 0.5\u00a0dayAs an alternative the morphology of cells could be confirmed by May-Gr\u00fcnwald/Giemsa-staining (Figures\u00a03[href=https://www.wicell.org#fig3]E and 3F).\nCytospin and May-Gr\u00fcnwald/Giemsa-staining in brief.\nCells were Cytospinned using Shandon Cytospin 3, at 550 r.p.m with low acceleration for 3\u00a0min.\nNote: When spinning less than 1000 cells, pre-wet the filter with 50\u00a0\u03bcL PBS, spin at 550 r.p.m with low acceleration for 3\u00a0min, and load 100\u00a0\u03bcL of cell suspension.\nStain air-dried slides in May-Gr\u00fcnwald solution for 5\u00a0min.\nAfter a brief wash with water, stain the slides in Giemsa solution for 10\u00a0min.\nWash the slide with dH2O and air dry before microscopic analysis.\nNote: In our hand, it is critical to dilute Giemsa with dH2O or miliQ water (1:10) and cover cuvette with a lid to reduce evaporation. Otherwise, a higher concentration of Giemsa will confound results.\nAlternative protocol 2 (barcoding protocol)\nIn this protocol, we have optimized the differentiation culture system and included barcoding, allowing for higher throughput and bulk culture. This protocol also allows for the differentiation and growth of all lineages without a switch in culture conditions (Figure\u00a04[href=https://www.wicell.org#fig4]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2329-Fig4.jpg\nFigure\u00a04. Summary of the barcoding protocol\u2019s experimental workflow\nCellular barcoding is emerging as a powerful tool for addressing individual cell fates on a large scale by tagging individual cells of interest with unique heritable identifiers or barcodes. The generation of lentiviral barcode libraries, the significance of complexity and size of libraries, as well as analytical challenges, has all been carefully reviewed before.2[href=https://www.wicell.org#bib3],3[href=https://www.wicell.org#bib4],4[href=https://www.wicell.org#bib5] Prior planning is necessary to ensure an appropriate experimental design. The barcoding protocol used here adopts technology from Kristiansen et\u00a0al.5[href=https://www.wicell.org#bib2] to track the multilineage differentiation of LSK cells in-vitro. However, this protocol is also useful for assaying committed progenitors as exemplified by the LMPP data in Figure\u00a06[href=https://www.wicell.org#fig6]B.\nPreparationThe preparation step is performed as described above. Only steps 4c and 4d are different from switch culture and marked as Alternative protocol 2.\nPlease perform steps 1-3 above before continuing with step 21.\nCell sorting and transduction\nTiming: 1\u00a0day\nCell sorting. We used a BDAria III instrument (configuration listed in \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d section).\nSort at least 2500 LSK (Lin-, Sca1+, C-kit+) or LMPP (LSK, CD34+, Flt3+) cells according to the gating strategy in Figure\u00a02[href=https://www.wicell.org#fig2]A, into prepared collection tubes containing 150\u00a0\u03bcL cold complete OptiMEM with cytokines (step 4d in preparation).\nRecord the cell counts from the sort and store tubes on ice.\nIn a sterile tissue culture hood, seed the 150\u00a0\u03bcL sorted cells in one well from a 48-wells\u00a0plate\u00a0(HSCs-plate) containing OP9 cells from step 4c in preparation (in total 300\u00a0\u03bcL per well).\nVirus Transduction.\nCritical: Before performing the experiment, it is important to titrate your barcoding-containing lentivirus for the population of choice and determine the amount of virus required to achieve transduction efficiency that ensures the number of transduced cells per recipient does not exceed 10% of the complexity of the lentiviral barcode library, for an estimated barcode complexity of 80k as previously shown.4[href=https://www.wicell.org#bib5],6[href=https://www.wicell.org#bib6] Here, we titrated the virus using a dilution series and a multiplicity of infection (MOI) to achieve a transduction efficiency of 15\u201330% (Figure\u00a05[href=https://www.wicell.org#fig5]), to reduce the risk of multiple barcode integration, as previously described in Kristiansen et\u00a0al.5[href=https://www.wicell.org#bib2]\nThis step should be performed in an adequate cell culture hood using procedures approved for lentiviral transduction and adhering to the appropriate local safety and ethical protocols.\nAdd the previously optimized amount of lentivirus to each well except for the un-transduced controls.\nGently rock the plate to disperse the virus.Incubate in humidified incubator at 37\u00b0C and 5% CO2 incubator overnight.\nWash the cells in the morning.\nCentrifuge plate at 350\u00a0g for 5\u00a0min at 4\u00b0C.\nRemove supernatant and wash the cells with 300\u00a0\u03bcL of FACS buffer, spin at 350\u00a0g for 5\u00a0min at 4\u00b0C,\nRemove supernatant and repeat the wash one more time. Centrifuge the plate at 350\u00a0g for 5\u00a0min, 4\u00b0C.\nRemove the supernatant. Add 300\u00a0\u03bcL of complete OptiMEM with cytokine and incubate in humidified, 37\u00b0C and 5% CO2 incubator.\nNote: Make sure to adhere to your specific safety rules regarding the handling of virus-transduced cells according to your local safety and ethical protocols.\nMeasure transduction efficiency (Day 3).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2329-Fig5.jpg\nFigure\u00a05. Example of GFP transduction efficiency readout on day 3\nViable single LSK and LMPP cells are shown from un-transduced control (left) and transduced LSK and LMPP samples (middle and right respectively).\nCollect 50\u00a0\u03bcL of the transduced cells and un-transduced controls into individual FACS tubes with 200\u00a0\u03bcL of FACs buffer.\nCentrifuge samples at 350\u00a0\u00d7\u00a0g for 5\u00a0min at 4\u00b0C and remove supernatant.\nRe-suspend each sample in 150\u00a0\u03bcL staining buffer containing 7AAD.\nAnalyze GFP+ cells by FACS among live (7AAD-Neg) cells (Figure\u00a05[href=https://www.wicell.org#fig5]).\nTransfer cells to larger plate when confluent (approximately day 7\u201311).\nSeed 50k OP9 cells in 6-well plates (Op9-plate) with 2\u00a0mL of complete optiMEM at the day of, or (>6 h) before transferring cells.\nThe following day, remove 1.3\u00a0mL of medium in Op9-plate (700\u00a0\u03bcL left in Op9-plate).\nHomogenize the cells in the HSCs-plate by pipetting, and transfer cells (300\u00a0\u03bcL/well) to the Op9-plate from step 24b (in total 1\u00a0mL/well).Rinse each well from the HSCs-plate 4 times with 250\u00a0\u03bcL of fresh complete optiMEM with cytokine concentration appropriate for a total volume of 2\u00a0mL, and transfer to Op9-plate. The final volume after transfer to Op9-plate is 2\u00a0mL per well.\nChange half of the medium every 7\u00a0days to fresh complete optiMEM with cytokine.\nNote: The cytokine concentration should appropriate for a total volume of 2\u00a0mL according to the table \u201cAlternative 2- cytokine\u201d in the \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d section, without disturbing the cell layers.\nBarcode extraction, day 21 post transduction\nIn this step we recover the cell-specific barcodes. All steps are previously described in Kristiansen et\u00a0al.5[href=https://www.wicell.org#bib2] Differentiation kinetics is unique for each progenitor and can additionally be affected by the virus transduction. It is recommended to optimize the readout time based on the specific population assayed since readout time point can vary. Here, we sort LSK output cells at day 16 and day 21 and LMPP at day 8 and day 16 to ensure discovery of clonal output into multi-lineages with different differentiation kinetics.\nCell sorting and lyses.\nTransfer cells into 3\u00a0mL FACS tubes and centrifuge at 350\u00a0\u00d7\u00a0g for 5\u00a0min at 4\u00b0C.\nResuspend cells in FACS buffer containing FC-block (1:100) and incubate for 5\u00a0min in 4\u00b0C.\nStain cells according to the table \u201cAlternative 2-antibody cocktail\u201d in the materials and equipment, incubate 45\u00a0min in darkness at 4\u00b0C and gentle shaking.\nWash cells in FACS-buffer at 350 g, 8\u00a0min.\nRe-suspend cells in FACS-buffer containing 7AAD (1:200).\nNote: Our staining suggestion was sorted on AriaIII according to configuration in \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d.\nThe FACS gating strategy for sorting of Megakaryocyte, Erythroid, myeloid (granulocyte and Monocyte/Macrophage) and Lymphoid (B)\u00a0cells are shown in (Figure\u00a06[href=https://www.wicell.org#fig6]).Note: For small number of cells, sort directly into lysis buffer.\nSort GFP+ cells into \u201cnon-stick tubes \u201c(low-binding surface) containing 500\u00a0\u03bcL of FACS buffer. Store at \u221280\u00b0C as cell pellet for a maximum of 7\u00a0days before lysing.\nExtraction of genomic DNA.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2329-Fig6.jpg\nFigure\u00a06. Representative FACS sorting strategy at read out of barcoding cultures\n(A and B) (A) Sort purity check and Representative May-Gruwald-Giemsa staining of cytospine slides for Myeloid (My), Macrophage (M\u03a6), B, Megakaryocyte (MK) and Erythroid (ER) form lineage output of LSK cells after 21\u00a0day or (B) LMPP cells after 8\u00a0day or 16\u00a0day of co-culture, Scale-bare represent 5\u00a0\u03bcm.\nPrepare lysis buffer according to table lysis buffer in the \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d.\nCell pellets are lysed to extract genomic DNA in 100\u00a0\u03bcL cell lysis buffer for 2\u00a0h at 56\u00b0C, followed by 10\u00a0min at 95\u00b0C to inactivate proteinase K. Store at \u221220\u00b0C or continue with step 28.\nLibrary preparation and barcode sequencing.\nPurify genomic DNA using Ampure XP beads in 1:1 ratio according to manufacturer's protocol (Beckman Coulter).\nMeasure genomic DNA concentration using a high sensitivity method e.g., the Qubit dsDNAhs Assay Kit according to manufacturer's protocol (Thermo Fisher Scientific).\nDivide up to 40\u00a0ng DNA from each sample into two technical replicate PCR reactions (20\u00a0ng each).\nNote: Use the appropriate primers and a high-fidelity DNA Taq polymerase (e.g., Q5 Taq DNA polymerase) for amplification of the barcode fragment (Figure\u00a04[href=https://www.wicell.org#fig4]). PCR reaction master Mix and PCR cycling is described in \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d. Following this step, the DNA can be stored at \u221220\u00b0C.\nPurify the amplified fragments (155\u00a0bp) using Ampure XP beads 1:1.5\u00d7 ratio according to the manufacturer\u2019s recommendations (Beckman Coulter).\nNote: Here the fragments can be stored at \u221220\u00b0C.Measure DNA concentration using a high sensitivity method e.g., the Qubit Fluorometer and the Qubit dsDNA HS Assay Kit.\nOptional: A second step of PCR amplification can be performed to add multiplexing adaptors, depending on the design of the PCR primers and the sequencing platform used.\nRun second PCR according to the manufacturer\u2019s recommendations (Thermo Fisher Scientific, InvitrogenTM).\nNote: PCR reaction master Mix and PCR cycling condition are described in \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d. The PCR product can be stored at \u221220\u00b0C.\nPurify the amplified fragments (225\u00a0bp) using Ampure XP beads 1:1.5\u00d7 ratio as previously described in \u201c11d\u201d. The fragments can be stored at \u221220\u00b0C.\nQuantify amplicons (size and concentration) using the Bioanalyzer HS DNA Analysis Kit.\nNote: A product of the correct size without contaminating primer dimer is critical for successful sequencing. The presence of primer dimers can be removed through an additional round of Ampure XP bead purification.\nPool equivalent amounts of amplified and indexed barcode libraries for sequencing according to the instructions for the chosen method for sequencing.\nNote: Here we used the Illumina MiSeq instrument and the v3 high output kit (150 cycles).\nStatistical analysis\nThe aim of the data analysis is to generate a list of reliable barcodes for each sorted sample, which will be combined with barcode read frequency, reflecting the abundance of each clone within the population. To enable rapid analysis of the data generated using our protocol, we have developed a Python package called Bartide. The package contains modules tailored to streamline three key challenges viz., barcode extraction, sequencing error correction and cross-sample comparison.\nTo install Bartide you need to have Python version 3.9 or upward. Use the following command to install Bartide.\n>pip install bartide==0.3.2Barcodes are extracted from the sequencing reads using Bartide\u2019s BarcodeExtractor class. This class is provided two input files, one for each end of paired-end sequencing, in FASTQ format.\nNote: This class is designed to automatically extract the barcodes assuming that the barcodes are of the same lengths, they span the same position in the reads, and the flanking sequence is constant. This is achieved by first summarizing the nucleotide composition at each position for all the reads (or a sample of reads). The flanking sequence will be dominated by a single nucleotide while the barcode should have variable base composition. This pattern is used to identify the position of barcodes and the flanking primer sequence. This behaviour can be overridden by providing the barcode length and the flanking primer sequence to BarcodeExtractor. Below we show an example of how to call the BarcodeExtractor class and perform automatic flank identification.\n>import bartide\n>extractor\u00a0= bartide.BarcodeExtractor(\n\u00a0\u00a0\u2018sample1_read1.fastq.gz\u2019,\n\u00a0\u00a0\u2018sample1_read2.fastq.gz\u2019\n)\n>extractor.identify_flanks()\nAlternatively, users can provide the flanking sequence and the barcode lengths manually as shown below:\n>extractor\u00a0= bartide.BarcodeExtractor(\n\u00a0\u00a0\u2018sample1_read1.fastq.gz\u2019,\n\u00a0\u00a0\u2018sample1_read2.fastq.gz\u2019,\n\u00a0\u00a0left_flank='GTAGCC',\n\u00a0\u00a0right_flank='AGATCG',\n\u00a0\u00a0barcode_length=27\n)\nOnce the flank sequences and barcode length are determined, they are stored as extractor.leftFlank, extractor.rightFlank and extractor.barcodeLength. Now the barcodes can be extracted, and their frequency counted.\nNote: The BarcodeExtractor class will compare the barcode sequence and its reverse complementary sequence from the other pair of the sequenced read. By default, there should not be more than 3 mismatches between the two sequences otherwise the extraction fails for that read. Users can change the maximum allowed mismatch value by using the max_dist parameter when calling BarcodeExtractor.\nThe actual extraction of barcodes is triggered by the following command:\n>extractor.count_barcodes()\nUsers can access these uncorrected barcodes with the following command:\n>print (extractor.rawCounts)This prints the barcodes and their frequencies as shown below:\nTTGTAGGGGTGTGTTCTACCGGTAATT\u00a0\u00a0\u00a0\u00a02843\nGTGCTGGTAATGTGGGCGACGGTGGGG\u00a0\u00a0\u00a0\u00a0913\nTTGGTGAAGCATAGTTCCGTGATTGAA\u00a0\u00a0\u00a0\u00a0909\nTTCCATGACGTTAAATACCTCCTTATA\u00a0\u00a0\u00a0\u00a0723\nATCTGGCGTCCAGCAGATATTAGTTTT\u00a0\u00a0\u00a0\u00a0717\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0...\nAAGTTACATGCCGCAAAGGGTTCTTTG\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01\nAAGGATGAATGACAAGGTGCTAGCCAT\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01\nGGTACAAGGCGGGATTACCATGCATTG\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01\nGTGCTGGAAATGTGGGCGACGGTGGGG\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01\nAAGTCACATGCCGCAAAGTGTCCATTG\u00a0\u00a0\u00a0\u00a0\u00a0\u00a01\nNext, the list obtained above may still contain barcodes that harbor sequencing errors. We assume that a barcode contains error(s) if it has less than three nucleotide differences with a higher abundance barcode. Since the pairwise comparison of all the barcodes can be computationally prohibitive, we use the approximate nearest neighbor detection library \u2018nmslib\u2019 to efficiently identify similar barcodes.\nNote: If an erroneous barcode is found, its frequency is added to the barcode with the nearest match. This functionality is implemented in the SeqCorrect class. Users can obtain the corrected barcode list (in the same format as the uncorrected list above), by running the following command:\n>corrector\u00a0= bartide.SeqCorrect()\n>corrector.run(extractor.rawCounts)\nThe corrected list of barcodes is stored under corrector.correctedCounts. These barcodes can then be saved in CSV format table as shown below:\n>corrector.save_to_csv(\u2018barcodes_freq_sample1.csv\u2019)\nThe SeqCorrect class will by default, remove any barcode with a frequency of less than 20, as suggested previously 5. This behavior can be overridden by changing the value of the min_counts parameter when calling SeqCorrect.\nOnce the corrected barcode frequencies are saved for all the samples, they can be compared using the BarcodeAnalyzer class.\nNote: This class is initialized by providing the name, with full path, of the directory wherein all the CSV files were saved. the following command illustrates this:\n>analyzer\u00a0= bartide.BarcodeAnalyzer(\u2018barcodes_dir\u2019)\nThis will lead to aggregation of all the barcodes across all the samples that can be accessed from a single table stored under analyzer.barcodes. Users can perform all the custom downstream analysis using this table as the starting point.Bartide provides four essential plots that allow users to easily identify how the barcodes are shared across the sample.\nNote: The first plot is the \u2018Upset\u2019 that shows all the combinations of samples and the number of barcodes that overlap between them. This plot, compared to Venn diagrams, allows easy visualization of the overlaps and non-overlaps of barcodes. Please note that when using upset plots, we only look at the unique number of barcodes found in each of the samples and not their frequencies.\n>analyzer.plot_upset()\nSometimes due to large differences in the number of barcodes captured, it might be difficult to easily identify the similarity or differences between the samples. To solve this, rather than using the absolute number of barcodes in a sample, the percentage overlap of barcodes from a sample with all other samples is used. This allows the barcodes from a sample to be defined in terms of proportions and may allow insights into sample similarity that is otherwise difficult to discern using absolute frequencies. The following command shows the proportions in the form of a stacked barplot:\n>analyzer.plot_stacked()\nAn alternative way to deal with the situation wherein the absolute number of unique barcodes is quite different across the samples is to perform normalization by dividing the overlap value by the sum of the total barcodes from the two samples. The resulting normalized values can be visualized in the form of a heatmap.\n>analyzer.plot_overlap_heatmap()\nIn all the above three plotting functions, we do not use the frequencies of the barcodes, which are indicative of how dominant a particular barcode is in the samples. A weighted overlap of barcodes is calculated between two samples as following:\n   \u2211 b B    (   S b i  \u2212  S b j   )  2Wherein, S is a column-sum normalized matrix of samples (columns) and barcodes (rows) containing barcode frequencies, i and j are two samples, b is a barcode in a set of barcodes B that are present in either of the two samples or both. These overlap values are then plotted in the form of a heatmap using the following function:\n>analyzer.plot_weighted_heatmap()\nTo save the images generated by the functions above, users can pass a value (path and name of the file where to save) to the save name parameter.", "Step-by-step method details\nStep-by-step method details\nCell plating and treatments\nTiming: 2\u00a0days\nThese steps are required to induce the expression of the POI fused to BirA\u2217 in the stable Flp-In T-REx cell line. To robustly biotinylate proximity proteins, BirA\u2217 requires exogenous biotin as a substrate.\nSeed the Flp-In T-REx stable cell line in a 15\u00a0cm dish cultured in DMEM supplemented with 10% TET-free FBS and allow them to adhere and proliferate overnight (about 16 h). The number of cells can vary from one cell line to another and should be optimized in each lab.\nWhen the confluency reaches 75%\u201380% treat the cells.\nAspirate the media.\nGently replace it with fresh media supplemented with 10% of FBS, 1 ug/mL of tetracycline and 50\u00a0\u03bcM of biotin for 24 h.\nNote: The treatments can otherwise be added directly to the cells.\nCell harvesting\nTiming: 40\u00a0min\nThese steps describe how to properly harvest the cells prior to performing the BioID.\nNote: The following steps are done on ice.\nAspirate the media and add 5\u00a0mL of ice-cold PBS to the dish.\nAgitate on an orbital shaker at 4\u00b0C for 10\u00a0min.\nScrape the cells using a cell scraper and transfer them to a 15\u00a0cm tube.\nAdd 7\u00a0mL of cold PBS to the dish.\nCollect the remaining cells and transfer into the same 15\u00a0cm tube.\nPellet the cells by centrifuging at 500\u00a0g for 5\u00a0min at 4\u00b0C.\nAspirate the supernatant.\nWash the pellet with 10\u00a0mL of cold PBS.\nRepeat steps 8\u201310.\nPellet the cells by centrifuging at 500\u00a0g for 5\u00a0min at 4\u00b0C.\nAspirate the supernatant.\nPause point: At this point, you can freeze the cell pellets at \u221280\u00b0C for several months. This can be useful to synchronize the processing of samples from different experiments.Affinity capture of biotinylated proteins\nTiming: 5\u20137\u00a0h depending on the number of samples\nThese steps explain how to lyse the cells and how to capture the proteins that were biotinylated by BirA\u2217 in cellulo using streptavidin Sepharose\u00ae beads.\nNote: Keep the samples on ice.\nThaw frozen cell pellets in 1,5\u00a0mL of ice-cold RIPA lysis buffer supplemented with inhibitors (DTT, PMSF and protease cocktail inhibitor).\nResuspend by pipetting up and down\nTransfer to a 2\u00a0mL Eppendorf tube.\nAdd 1\u00a0\u03bcL of benzonase (250\u00a0U/\u03bcL) to each sample to digest nucleic acids, prior to the following step.\nTo complete cell lysis, sonicate samples for 30\u00a0s at 30% amplitude. Perform 3 rounds of 10\u00a0s with 2\u00a0s pause in between.\nNote: Ideally, the sonication performed at step 16 should be done on ice. If not possible, the sample can be taken from the ice and replaced back after.\nCentrifuge the lysates for 30\u00a0min at 12,000\u00a0g at 4\u00b0C to pellet the cell debris.\nMeanwhile wash 70\u00a0\u03bcL of streptavidin Sepharose\u00ae beads slurry in a 2\u00a0mL Eppendorf tube.\nAdd 1\u00a0mL of RIPA buffer (with or without inhibitors) to the 70\u00a0\u03bcL of beads.\nSpin for 1\u00a0min at 375\u00a0g at 4\u00b0C to pellet the beads.\nAspirate the wash buffer.\nRepeat step 18 two more times.\nCollect the clarified cell lysates of step 17 and transfer it to the 2\u00a0mL Eppendorf tube containing 70 \u03bcL of the pre-washed beads.\nNote: A 20\u00a0\u03bcl or 40\u00a0\u03bcL aliquot of the supernatant should ideally be stored at \u221280\u00b0C to monitor protein expression, lysis efficiency and solubility.\nSecure tubes with parafilm to prevent leaking.\nIncubate with rotation at 4\u00b0C for 3 h.\nSpin down the beads by centrifuging them for 1\u00a0min at 375\u00a0g at 4\u00b0C.Remove the supernatant.\nNote: You can keep the supernatant to monitor unbound bait as a control and store it at \u221280\u00b0C.\nAdd 1\u00a0mL of RIPA lysis buffer without inhibitors to the beads and transfer to a new 2\u00a0mL Eppendorf tube to remove unwanted material that adhered to the tube walls.\nSpin down the beads by centrifuging them for 1\u00a0min at 375\u00a0g at 4\u00b0C and discard the supernatant.\nWash with 1\u00a0mL of RIPA lysis buffer.\nRepeat steps 25\u201326 one more time.\nCentrifuge 1\u00a0min at 375\u00a0g at 4\u00b0C and discard the supernatant.\nWash the beads with 1\u00a0mL of 50\u00a0mM cold ABC which provides an optimal pH for the trypsin digestion and is also compatible with the downstream MS analysis.\nRepeat step 28\u201329 two times and discard the supernatant.\nResuspend beads in 100\u00a0\u03bcL of 50\u00a0mM cold ABC.\nBiotinylated proteins digestion\nTiming: 18 h\nThese steps allow the trypsin-mediated digestion of the bound biotinylated proteins. It is essential to work in a clean environment for steps 32\u201346.\nResuspend trypsin stock in 200\u00a0\u03bcL of 20\u00a0mM Tris-HCl pH 8,0 by vortexing for around one minute and make sure it is well dissolved.\nCritical: Wear a mask to perform this step, trypsin powder can be harmful to the respiratory tract.\nAdd 10\u00a0\u03bcL (1\u00a0\u03bcg) of resuspended trypsin to each sample.\nSecure tube with parafilm to prevent leaking.\nIncubate overnight (about 15 h) at 37\u00b0C with rotation to digest peptides. Make sure all samples rotate correctly.\nThe next day, resuspend a new tube of trypsin stock in 200\u00a0\u03bcL of 20\u00a0mM Tris-HCl pH 8,0.\nNote: It is important to use a new tube of trypsin each day. Trypsin stored at 4\u00b0C overnight can lose its activity.Add 10\u00a0\u03bcL (1\u00a0\u03bcg) of resuspended trypsin to each sample.\nSecure tube with parafilm to prevent leaking.\nIncubate for 2\u00a0h at 37\u00b0C with rotation to make sure proteins are maximally digested.\nPeptide recovery\nTiming: 4\u20135\u00a0h depending on the number of samples\nThese steps describe how to collect the released peptides from the digestion and how to concentrate them.\nPellet the beads by centrifugation for 1\u00a0min at 375\u00a0g at room temperature (20\u00b0C\u201325\u00b0C).\nTransfer 100\u00a0\u03bcL of supernatant which contains peptides to a 1,5\u00a0mL Eppendorf tube.\nNote: From now on be careful not to carry over any beads. To avoid disturbing the bead pellet, we recommend limiting movement by staying close to the centrifuge and manipulating the samples the least possible.\nRinse beads with 100\u00a0\u03bcL of HPLC grade water.\nCentrifuge the beads for 1\u00a0min at 375\u00a0g at room temperature (20\u00b0C\u201325\u00b0C).\nCollect the supernatant containing the remaining peptides in the same tube as step 41 to pool all peptides.\nRepeat steps 42\u201344 one more time.\nAdd 30\u00a0\u03bcL of 50% formic acid stock solution for a final concentration of 5% formic acid to the 1,5\u00a0mL Eppendorf tube containing peptide to end digestion and mix by pipetting up and down.\nCentrifuge for 10\u00a0min at 16300\u00a0g at room temperature (20\u00b0C\u201325\u00b0C) to pellet remaining beads.\nTransfer supernatant to a new 1.5\u00a0mL Eppendorf tube making sure not to carry over any beads by leaving a few \u03bcL (5\u201315) in the tube.\nRepeat steps 47 and 48 again.\nPlace samples in a SpeedVac for 3\u00a0h at maximum speed and 35\u00b0C to dry completely.\nFreeze samples at \u221280\u00b0C.\nPause point: Frozen samples can be stored for several months.Before MS injection, resuspend the pellet in 15\u00a0\u03bcL of 5% formic acid by pipetting up and down.\nMS injection\nTiming: 145\u00a0min/sample\nThese steps of the protocol describe how to inject the samples prepared above in the LC-MS/MS system.\nFor each series of samples (duplicates or triplicates) perform an LC-MS/MS analysis using a short concentration gradient on at least one diluted sample (1:20) prior to the full gradient injection.\nCritical: This run is important as it allows to determine the highest volume to be injected for the full gradient analysis. The goal is to reach the maximum sensitivity without saturating the LC-MS/MS system.\nLoad the samples at the previously determined dilution into a 75\u00a0\u03bcm i.d. \u00d7 150\u00a0mm Self-Pack C18 column installed on the Easy-nLC II or the Easy-nLC 1000 system.\nElute the peptides with a two-slope gradient at a flow rate of 250 nL/min (The HPLC system is either coupled to a Q Exactive or the LTQ Orbitrap Velos mass spectrometer through a Nanospray Flex Ion Source.)\nSet the Nanospray and S-lens voltages to 1.3\u20131.7 kV and 50\u201360 V, respectively.\nSet the capillary temperature to 250\u00b0C.\nIncrease solvent B from 2 to 36% in 100\u00a0min and then from 36 to 82% B in 10\u00a0min.\nAcquire full scan MS survey spectra (m/z 360\u20132000) in profile mode in the Orbitrap with a resolution of 60,000 and the AGC target at 1E6.\nDepending on the machine available follow the steps (a) or (b) below:\nOn the Q Exactive\nFragment the 16 most intense peptide ions in the collision cell at a normalized collision energy of 27.\nAnalyze MS/MS spectra in the Orbitrap with a resolution of 17,500, the AGC target at 1E5, and the dynamical exclusion set to 7 s.\nOn the LTQ Orbitrap VelosFragment the 11 most intense peptide ions in the high-pressure cell at a normalized collision energy of 35.\nAnalyze MS/MS spectra in the linear trap with the AGC target at 1E4 and the dynamical exclusion set to 30 s.", "Step-by-step method details\nStep-by-step method details\nStep 1: Load differentially expressed genes (DEGs) between the different sample groups and set a common filter based on Adjusted p value (padj)\nTiming: 5\u00a0min\nThis step loads the DEGs between the different contrasts for downstream analysis. DEGs with an padj\u00a0< 0.1 based on sublayer differences for control and epileptic condition are retained. See bulk gene expression signatures from highly heterogeneous tissue in cell-type composition[href=https://www.wicell.org#sec1.1] in the \u201cbefore you begin[href=https://www.wicell.org#before-you-begin]\u201d section.\nLoad the DEGs tables into dataframes.\nSet the working directory path to the RNA-seq data folder.\n> path\u00a0= \"\u223c/Differentially_expressed_gene_tables/\"\n> setwd(path)\nRead the RNA-seq data from disk.\n>ctrl_DEGs<- read.delim(\"Sup_deep_diff_in_controls_FullTable.tsv\", stringAsFactors = FALSE)\n>epil_DEGs<- read.delim(\"Sup_deep_diff_in_epilepsy_FullTable.tsv\", stringAsFactors = FALSE)\nNote: The DEGs between CA1 sublayers in control and epileptic condition are available as Data Table 1 and Data Table 2 (see \u201cbefore you begin[href=https://www.wicell.org#before-you-begin]\u201d section and \u201ckey resources table[href=https://www.wicell.org#key-resources-table]\u201d). These data are also available in the Resource web application for the LCM-RNA-seq data from Cid et\u00a0al. (2021)[href=https://www.wicell.org#bib7]: http://lopezatalayalab.in.umh-csic.es/CA1_Sublayers_&_Epilepsy[href=http://lopezatalayalab.in.umh-csic.es/CA1_Sublayers_%26_Epilepsy].\nRemove DEGs table rows with empty gene symbols.\n> ctrl_DEGs <- ctrl_DEGs[!is.na(ctrl_DEGs$Gene_symbol),]\n> epil_DEGs <- epil_DEGs[!is.na(epil_DEGs$Gene_symbol),]\nFilter the DEGs tables by significance (padj\u00a0< 0.1).\n> ctrl_DEGs <- ctrl_DEGs[ctrl_DEGs$padj\u00a0< 0.1,]\n> epil_DEGs <- epil_DEGs[epil_DEGs$padj\u00a0< 0.1,]\nRank the DEGs tables by fold-change.\n> ctrl_DEGs <- ctrl_DEGs[order(-ctrl_DEGs$log2FoldChange),]\n> epil_DEGs <- epil_DEGs[order(-epil_DEGs$log2FoldChange),]\nSubset DEGs tables based on the layer they are enriched in controls and in epileptics.\n> ctrl_DEGs_sup <- ctrl_DEGs[ctrl_DEGs$log2FoldChange > 0,]\n> ctrl_DEGs_deep <- ctrl_DEGs[ctrl_DEGs$log2FoldChange\u00a0< 0,]\n> epil_DEGs_sup <- epil_DEGs[epil_DEGs$log2FoldChange > 0,]\n> epil_DEGs_deep <- epil_DEGs[epil_DEGs$log2FoldChange\u00a0< 0,]\nGet DEGs lists, extracting gene symbols from DEGs tables.\n> ctrl_DEGs_sup <- ctrl_DEGs_sup$Gene_symbol\n> ctrl_DEGs_deep <- ctrl_DEGs_deep$Gene_symbol\n> epil_DEGs_sup <- epil_DEGs_sup$Gene_symbol\n> epil_DEGs_deep <- epil_DEGs_deep$Gene_symbol\nStep 2: Load the single-cell expression dataset and build the single-cell dataset object\nTiming: 20\u00a0minThis step loads the original scRNA-seq dataset and builds the single-cell dataset object. See reference dataset of single-cell type-specific expression profiles[href=https://www.wicell.org#sec1.2] in the \u201cbefore you begin[href=https://www.wicell.org#before-you-begin]\u201d section.\nLoad the single-cell gene expression matrix and cell metadata.\nSet the working directory path to a single-cell RNA-seq data folder.\n> path\u00a0= \"\u223c/AllenBrainMap_MouseCortexAndHippo_SMART-seq/\"\n> setwd(path)\nRead the single-cell RNA-seq metadata from disk.\n> metadata <- read.csv(file\u00a0= \"metadata.csv\", row.names\u00a0= 1)\nLoad the data.table library, to access the fread() function, a very efficient tool to read regular delimited files from disk.\n> library(data.table)\nRead the single-cell RNA-seq counts from disk.\n> counts <- fread(\"matrix.csv\", data.table\u00a0= FALSE)\nSet the cell barcodes as rownames.\n> rownames(counts) <- counts$sample_name\nRemove the column sample_name to have a count matrix.\n> counts <- counts[,-!names(counts) %in% c(\"sample_name\")]\nSet the count matrix as a matrix.\n> counts <- as.matrix(counts)\nTranspose the counts matrix to get the correct format to be used as input in Seurat.\n> transposed_counts <- t(counts)\nRemove old count objects to free space.\n> rm(counts)\nFree memory through garbage collection.\n> gc()\nCritical: The gene-barcode matrix from the scRNA-seq dataset must be arranged so that each of the columns represents a barcode and each of the rows represents a gene (related to point 7.h.).\nBuild the Seurat single-cell dataset object.\nLoad the Seurat library.\n> library(Seurat)\nCreate the Seurat object with the transposed raw counts and cell metadata without filtering any cell or genes.\n>sc_data<- CreateSeuratObject(counts = transposed_counts,\n\u00a0\u00a0meta.data = metadata, min.cells = 0, min.features = 0,\n\u00a0\u00a0project =\"AllenBrainMap_MouseCortexHippo_SMART-seq-\n\u00a0\u00a02019_with_10x_SMART-seq-2020-taxonomy\")\nClean unneeded data.\nRemove transposed_counts and metadata objects to free space.\n> rm(transposed_counts)\n> rm(metadata)\nFree memory through garbage collection.\n> gc()\nStep 3: Subset scRNA-seq reference dataset to match cell-type populations in the bulk-tissue RNA-seq\nTiming: 10\u00a0minThis step sets a subset of the scRNA-seq dataset to match the cell types present in the tissue processed for bulk RNA-seq.\nExplore the different cell metadata categories in the scRNA-seq object to select the categories more related to bulk-tissue RNA-seq data. The categories selected to perform the subset are \u201cregion_label\u201d and \u201csubclass_label\u201d. See cell numbers in Tables 1[href=https://www.wicell.org#tbl1] and 2[href=https://www.wicell.org#tbl2].\ntable:files/protocols_protocol_1386_1.csv\ntable:files/protocols_protocol_1386_2.csv\n> str(sc_data@meta.data)\nNote: The number of cells can be consulted at any moment using the function table() with the associated category/metadata as argument.\n> table(sc_data$region_label)\n> table(sc_data$subclass_label)\nNote: Consider removing potential confounding factors (e.g. conditions or treatments) that are unrelated to the bulk-tissue RNA-seq from the initial scRNA-seq dataset.\nFilter out any cell population from tissue areas, treatments or other groups that are unrelated to the bulk-tissue RNA-seq data. Troubleshooting 1[href=https://www.wicell.org#sec7.1].\nWe first generate a scRNA-seq reference of the cells-types in the hippocampal CA1 region of the brain. The 74,973 cells in the initial scRNA-seq dataset are filtered to subset cells from hippocampal region and non-neuronal subclasses (Astro, Micro-PVM, Endo, Oligo or VLMC). Then, excitatory cells from CA2, CA3, DG and also a group of cells in a subclass without label ([No label]) are removed. This initial filtering extracts a subset of 5,506 cells.\n>sc_data<-subset(x = sc_data,\n\u00a0\u00a0subset = region_label == \"HIP\" | subclass_label %in% c(\"Astro\",\"Micro-PVM\",\"Endo\",\"Oligo\",\"VLMC\"))\n>sc_data<-subset(x = sc_data,\n\u00a0\u00a0cells = colnames(sc_data)[!sc_data$subclass_label %in% c(\"\",\"CA2\",\"CA3\",\"DG\")])\nOf these, cells labeled by subclass as CA1-ProS, Astro, Lamp5, Vip, Sncg, Sst, Oligo, Endo, Micro-PVM, VLMC, or Pvalb are retained for further analysis, remaining a total of 5,429 cells. See cell numbers in Tables 3[href=https://www.wicell.org#tbl3] and 4[href=https://www.wicell.org#tbl4]. Troubleshooting 2[href=https://www.wicell.org#sec7.3].\n>sc_data<-subset(x = sc_data,\n\u00a0\u00a0cells = colnames(sc_data)[!sc_data$subclass_label %in%\n\u00a0\u00a0\u00a0\u00a0c(\"L2 IT RHP\",\"L2/3 IT CTX-1\",\"L2/3 IT CTX-2\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"L2/3 IT ENTl\",\"L2/3 IT PPP\",\"L5 IT TPE-ENT\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"L6 CT CTX\",\"L6b CTX\",\"Meis2\",\"NP SUB\",\"Sst Chodl\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"SUB-ProS\")])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1386-Fig1.jpgFigure\u00a01. Major cell types present in the reference scRNA-seq dataset\nCell populations in scRNA-seq data from the Allen Brain Map portal (Mouse Whole Cortex and Hippocampus SMART-seq (2019) with 10x-SMART-seq taxonomy (2020) (Yao et\u00a0al., 2021[href=https://www.wicell.org#bib25])) identified as non-neuronal or from CA1 (from \u201chippocampus\u201d but not NA, \u201cCA2\u201d, \u201cCA3\u201d or \u201cDG\u201d) were subset and grouped in their corresponding major cell populations as follows: Astrocytes (Astro) (976 cells), Endothelial (Endo) (213 cells), Interneurons (Lamp5, Pvalb, Sncg, Sst, Vip) (2077 cells), Microglia (Micro-PVM) (176 cells), Mural (VLMC) (159 cells), Oligodendrocytes (Oligo) (236 cells) and Pyramidal (CA1-ProS) (1,592 cells). Left, Principal component analysis (PCA); Center, t-distributed stochastic neighbor embedding (tSNE); Right, uniform manifold approximation and projection (UMAP).\ntable:files/protocols_protocol_1386_3.csv\ntable:files/protocols_protocol_1386_4.csv\nRename and group the cells in their corresponding major cell populations.\nRemove unused levels (labels without associated cells). See cell numbers in Table 5[href=https://www.wicell.org#tbl5].\n>sc_data$subclass_label<- droplevels(x = sc_data$subclass_label)\ntable:files/protocols_protocol_1386_5.csv\nSet the \u201csubclass_label\u201d metadata as the \u201cactive.ident\u201d (default identity class) for easy customization.\n> Idents(sc_data) <- \"subclass_label\"\nRename identity classes to the corresponding major cell population. The remaining 5,429\u00a0cells are grouped in their corresponding major cell populations as follows: Astrocytes (Astro) (976 cells), Endothelial (Endo) (213 cells), Interneurons (Lamp5, Pvalb, Sncg, Sst, Vip) (2,077 cells), Microglia (Micro-PVM) (176 cells), Mural (VLMC) (159 cells), Oligodendrocytes (Oligo) (236 cells) and Pyramidal (CA1-ProS) (1,592 cells). See Tables 5[href=https://www.wicell.org#tbl5] and 6[href=https://www.wicell.org#tbl6].\ntable:files/protocols_protocol_1386_6.csv\n>sc_data<-RenameIdents(sc_data,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Astro\" = \"Astrocytes\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"CA1-ProS\" = \"Pyramidal\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Endo\" = \"Endothelial\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Lamp5\" = \"Interneurons\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Micro-PVM\" = \"Microglia\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Oligo\" = \"Oligodendrocytes\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Pvalb\" = \"Interneurons\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Sncg\" = \"Interneurons\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Sst\" = \"Interneurons\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Vip\" = \"Interneurons\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"VLMC\" = \"Mural\")\nReorder alphabetically the identity classes. See cell numbers in Table 6[href=https://www.wicell.org#tbl6].\n>Idents(sc_data)<-factor(Idents(sc_data), levels = sort(levels(sc_data)))\nNormalize gene counts.\n> sc_data <- NormalizeData(sc_data, normalization.method\u00a0= \"LogNormalize\", scale.factor\u00a0= 10000)\nIdentify genes that are outliers on a 'mean variability plot'.> sc_data <- FindVariableFeatures(sc_data, selection.method\u00a0= \"vst\", nfeatures\u00a0= 2000)\nScale and center variable genes.\n> sc_data <- ScaleData(sc_data)\nPerform dimensionality reduction to summarize and visualize the cells in the low-dimensional space.\nPerform linear dimensionality reduction by PCA over the variable genes (default option).\n> sc_data <- RunPCA(sc_data)\nEstimate the number of principal components (PCs) that are biologically informative by plotting the standard deviations of the PCs for easy identification of an elbow in the graph (Elbow plot). This elbow often corresponds well with the significant dimensions that capture the majority of the variation in the data. Here, the elbow was identified at the 20th PC.\n> ElbowPlot(sc_data, ndims\u00a0= 50)\nPerform non-linear dimensionality reduction over biologically informative dimensions from linear dimensionality reduction using stochastic nearest neighbors (tSNE) (van der Maaten and Hinton, 2009[href=https://www.wicell.org#bib15]) and uniform manifold approximation and projection (UMAP) (McInnes et\u00a0al., 2018[href=https://www.wicell.org#bib16]) state-of-the-art techniques.\n> sc_data <- RunTSNE(sc_data, dims\u00a0= 1:20)\n> sc_data <- RunUMAP(sc_data, dims\u00a0= 1:20)\nPause point: Save the scRNA-seq object and resume the analysis in future R sessions without repeating the previous steps:\n> saveRDS(sc_data, \"Single-cell_custom_subset.rds\")\nTo reload the scRNA-seq object saved in a previous R session, use the following command:\n> sc_data <- readRDS(\"Single-cell_custom_subset.rds\")\nCreate a figure showing dimensionality reduction of major cell types (Figure\u00a01[href=https://www.wicell.org#fig1]). Use custom labels from the scRNA-seq and the dimensionality reduction techniques applied to the data.\nLoad the cowplot library, to access the get_legend() function.\n> library(cowplot)\nLoad the patchwork library, to access the area() function.\n> library(patchwork)\nLoad the ggplot2 library, to access the ggsave() function.\n> library(ggplot2)\nGenerate the dimensionality reduction plots.\n>cols <-c(\"limegreen\", #Astrocytes\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"steelblue\", #Endothelial\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"mediumorchid4\", #Interneurons\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"firebricks2\", #Microglia\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"magenta\", #Mural\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"gray52\", #Oligodendrocytes\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"tan1\") #Pyramidal\n>pca_plot<-DimPlot(sc_data, reduction = \"pca\", pt.size = 0.1, label = T, cols = cols)>tsne_plot<-DimPlot(sc_data, reduction = \"tsne\", pt.size = 0.1, label = T, cols = cols)\n>umap_plot<-DimPlot(sc_data, reduction = \"umap\", pt.size = 0.1, label = T, cols = cols)\n>legend<-get_legend(umap_plot)\nDefine the figure layout.\n>layout<-c (area(1, 1, 1, 2),#PCA\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0area(1, 3, 1, 4),#tSNE\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0area(1, 5, 1, 6),#UMAP\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0area(1, 7))#Legend\nBuild the figure.\n> fig1\u00a0<- pca_plot\u00a0+ tsne_plot\u00a0+ umap_plot\u00a0+ legend\u00a0+ plot_layout(design\u00a0= layout) & NoLegend()\nSave the figure to disk.\n> ggsave(filename\u00a0= \"Fig_1.png\", plot\u00a0= fig1, width\u00a0= 13, height\u00a0= 3.75, dpi\u00a0= 300)\nStep 4: Perform the deconvolution of cell-type-specific signal in bulk-tissue RNA-seq top DEGs using single-cell RNA-seq reference\nTiming: 10\u00a0min\nThis step subsets top DEGs lists of equal size for the groups of interest and imputes bulk signal to cell-type leveraging on the scRNA-seq subset from step 3.\nGet the list of genes names from the single-cell object.\n> gene_names <- rownames(sc_data)\nIntersect the DEGs lists with the list of genes names from the single-cell reference object, preserving the fold change order from point 4.\n> ctrl_DEGs_sup <- ctrl_DEGs_sup[ctrl_DEGs_sup %in% gene_names]\n> ctrl_DEGs_deep <- ctrl_DEGs_deep[ctrl_DEGs_deep %in% gene_names]\n> epil_DEGs_sup <- epil_DEGs_sup[epil_DEGs_sup %in% gene_names]\n> epil_DEGs_deep <- epil_DEGs_deep[epil_DEGs_deep %in% gene_names]\nOptional: In the case that the bulk-tissue RNA-seq and reference scRNA-seq dataset are from different species (e.g. rat vs. mouse), ortholog conversion should be performed. Troubleshooting 3[href=https://www.wicell.org#sec7.5].\nCapture comparable convoluted gene signatures by using gene sets of equal size that are differentially regulated between the experimental conditions. Subset top 250 differentially regulated genes for each sublayer and condition for further analysis, with the genes sorted in descending order by magnitude of change. Troubleshooting 4[href=https://www.wicell.org#sec7.7].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1386-Fig2.jpg\nFigure\u00a02. Deconvolution of gene signatures from bulk-tissue RNA-seq reveals strong presence of reactive microglia in superficial CA1 sublayer of the hippocampus in epilepsyPatterns of cell-type-specific gene expression were identified by deconvolution of bulk-tissue transcriptome profiling of deep and superficial hippocampal CA1 sublayers of control and epileptic animals. This analysis reveals strong presence of reactive microglia in superficial CA1 sublayer in epilepsy. Gene sets were the top 250 DEGs between superficial and deep CA1 sublayers in epileptic and control rats identified in bulk-tissue RNA-seq. For the selected genes, normalized expression in single cells was retrieved from publicly available scRNA-seq data from the Allen Brain Map portal (Mouse Whole Cortex and Hippocampus SMART-seq (2019) with 10x-SMART-seq taxonomy (2020)) (Yao et\u00a0al., 2021[href=https://www.wicell.org#bib25]) and single-cells were summarized by linear dimensionality reduction using PCA (top panels). Cells are colored by population membership (step 2). Hierarchical clustering heatmaps of pairwise correlations for all individual cells using scRNA-seq expression data for selected gene sets identified in differential expression analysis with bulk-tissue RNA-seq (top 250 DEGs) (bottom panels). Bona fide markers of distinct cell types are present in clusters of highly correlated genes representing cell-type gene signatures convoluted in the bulk-tissue RNA-seq. Patterns of cell-type-specific gene expression present in the bulk-tissue RNA-seq gene signatures of deep and superficial CA1 sublayers of control and epileptic animals lead to segregation of the individual cells in the corresponding PCAs and in the heatmap of the pairwise correlation matrices. Note the different distribution of distinct cell types in the deep and superficial (Sup) sublayers of the CA1 region in control and epileptic rats. Also note the presence of a strong gene signature of Micro in the Sup CA1 sublayer of epileptic rats (arrowhead). Names of highly correlated genes enriched in Micro are shown (Zeisel et\u00a0al., 2018[href=https://www.wicell.org#bib27]). Pyr, pyramidal cells; Inter, interneurons; ODC, oligodendrocytes; Astro, astrocytes; Endo, endothelial cells; Micro, microglia; Mural, mural cells.Reprinted from Cid et\u00a0al. (2021)[href=https://www.wicell.org#bib7], with permission from Elsevier.\n> ctrl_DEGs_sup <- head(ctrl_DEGs_sup,250)\n> ctrl_DEGs_deep <- rev(tail(ctrl_DEGs_deep,250))\n> epil_DEGs_sup <- head(epil_DEGs_sup,250)\n> epil_DEGs_deep <- rev(tail(epil_DEGs_deep,250))\nNote: In our case, we have 4 top DEGs lists that should be iteratively renamed to \u201ctopDEGs_list\u201d in point 21 to generate the plots in Figure\u00a02[href=https://www.wicell.org#fig2]. Our top DEGs lists are comparable in size, as they contain top 250 DEG genes, with the exception of \u201cepil_DEGs_sup\u201d that contains 230 genes.\nSet \u201ctopDEGs_list\u201d as any of the previous 4 DEGs lists and repeat the following steps (21\u201325) for all the other lists, renaming the plot output names in points 24 and 25 to avoid overwriting the plots in the different iterations.\n>topDEGs_list<- ctrl_DEGs_sup\u00a0\u00a0\u00a0\u00a0#Repeat steps 21-25 for each DEG list.\nScale the normalized scRNA-seq data for the top DEGs list.\n> sc_data <- ScaleData(sc_data, features\u00a0= topDEGs_list)\nPerform dimensionality reduction of the gene list to summarize and visualize the cells in the low-dimensional space. Troubleshooting 5[href=https://www.wicell.org#sec7.9].\nPerform linear dimensionality reduction by PCA over the top DEGs.\n> sc_data <- RunPCA(sc_data, features\u00a0= topDEGs_list)\nRetain genes with variance. This gene list will be used to perform pairwise Pearson correlation coefficients (related to point 25.e.).\n>topDEGs_list<- rownames(sc_data@reductions[[\"pca\"]]@feature.loadings)\nGenerate the dimensionality reduction plots and save them to disk. For Figure\u00a02[href=https://www.wicell.org#fig2] we use the PCA plots with labels set to false, to maximize visualization.\n>pca_plot<- Dimplot(sc_data, reduction = \"pca\", pt.size = 0.1, label = F, cols = cols)\n>legend<- get_legend(pca_plot)\n>pca_plot<- pca_plot & NoLegend()\n>ggsave(filename = \"Fig_2_pca_plot.png\", plot = pca_plot, width = 3.75, height = 3.75, dpi = 300)\n>ggsave(filename = \"Fig_2_legend.png\", plot = legend, width = 2, height = 3.75, dpi = 300)Perform pairwise correlations and hierarchical clustering for these gene sets (top DEGs) across all cells in the scRNA-seq subset, to capture cell-type-specific gene signatures that were assigned to major cell types in CA1 by using previously identified cell markers.\nDefine the correlation plot palette.\nCreate a matrix of 50x10 random values within the range [\u22121,\u00a0+1].\n> random.matrix <- matrix(runif(500, min\u00a0= -1, max\u00a0= 1), nrow\u00a0= 50)\nProduce the sample quantiles corresponding to the given probabilities.\n> quantile.range <- quantile(random.matrix, probs\u00a0= seq(0, 1, 0.01))\nDefine the quantiles where the minimum and maximum correlation values were set to the lowest and highest color. In our case, it was set empirically to 35% and 83% respectively, as these values maximized the contrast for the distribution of values.\n> palette.breaks <- seq(quantile.range[\"35%\"], quantile.range[\"83%\"], 0.06)\nCreate a color ramp that maps the previous interval.\n> color.palette <- colorRampPalette(c(\"#0571b0\",\"#f7f7f7\",\"#ca0020\"))(length(palette.breaks)-1)\nImport the library gplots to access the enhanced heatmap function heatmap.2().\n> library(gplots)\nDefine the hierarchical cluster analysis function, with Pearson correlation coefficient as distance matrix and average as agglomeration method.\n>clustFunction<-function(x)\n\u00a0\u00a0\u00a0\u00a0hclust(as.dist(1-cor(t(as.matrix(x)), method = \"pearson\")), method = \"average\")\nDefine the heatmap correlation function, with Pearson correlation coefficient as numeric matrix and the color palette, breaks and clustering function set above.\n>heatmapPearson<- function(correlations)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0heatmap.2(x = correlations,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0col = color.palette,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0breaks = palette.breaks,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0trace = \"none\", symm = T,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0hclustfun = clustFunction)\nCompute the Pearson correlation coefficient on the logarithm of normalized expression of genes from \u201ctopDEGs_list\u201d in all cells from scRNA-seq subset.\n> correlations_DEGs_log <- cor(method\u00a0= \"pearson\",\nlog2(t(as.matrix(sc_data@assays[[\"RNA\"]]@data[topDEGs_list,]))+1))\nGenerate the gene-gene correlation plot and save it to disk.\n> pdf(file\u00a0= \"Fig_2_corr_plot.pdf\", width\u00a0= 25, height\u00a0= 25)\n> heatmapPearson(correlations_DEGs_log)\n> dev.off()Figure\u00a02[href=https://www.wicell.org#fig2] shows PCA plots and heatmaps of pairwise correlations generated from the 4 lists of top DEGs in our study.\nNote: Points 24 and 25 generate the plots shown in Figure\u00a02[href=https://www.wicell.org#fig2]. Rename the output files in points 24 and 25 to avoid overwriting of the plots when different iterations of Step 4 are performed.\nNote: A R script with the code of this protocol is publicly available at Mendeley Data repository: https://doi.org/10.17632/nkrfxtbrmc[href=https://doi.org/10.17632/nkrfxtbrmc].", "Step-by-step method details\nStep-by-step method details\nGenerating bait DSB\nTiming: 2\u00a0days\nThis step is aimed to introduce a bait DSB into cells and allow the formation of DSB repair products. The bait DSB can be induced by multiple approaches, including transfection, viral transduction, and nucleofection. Here, we describe the procedure of polyethylenimine (PEI)-mediated transfection with a plasmid containing SpCas9-MYC1 in HEK-293T cells.\nNote: If a bait DSB has been generated and then it underwent DSB repair in cells already, please skip this part and proceed with the genomic DNA isolation.\nCulture HEK-293T cells in DMEM/10% FBS medium to a 60%\u201380% coverage in the 6-cm dish. Two dishes of cells should be prepared. One is set as the control that is without the bait DSB, the other one is for bait DSB generation.\nPEI-mediated plasmid transfection.\nBefore transfection, bring all reagents to room temperature.\nRemove the cell culture medium by aspiration, and add 4\u20135\u00a0mL pre-warmed (37\u00b0C) DMEM/10% FBS medium.\nPrepare the transfection mixture.\nAdd 200\u00a0\u03bcL opti-MEM and 5\u00a0\u03bcg plasmid containing SpCas9-MYC1 (inducing bait DSB by pX330-MYC1) or GFP (control, pX330-GFP) in a 1.5\u00a0mL tube. Mix well with gentle flaps.\nPrepare two 1.5\u00a0mL tubes, each containing 200\u00a0\u03bcL opti-MEM and 10\u201315\u00a0\u03bcL PEI (1\u00a0mg/mL). Mix well by vortexing or pipetting.\nAdd one diluted PEI (from ii) to one diluted DNA (from i), and the total volume should be less than 450\u00a0\u03bcL. Mix immediately with gentle flaps and incubate at room temperature for 15\u00a0min.\nDropwise add the DNA/PEI mixture to the cell culture by a pipette and put cells into a CO2 incubator for 8\u201312 h.\nRemove the medium by aspiration and add 5\u00a0mL pre-warmed (37\u00b0C) DMEM/10% FBS medium.Harvest transfected cells at 48\u201372\u00a0h post-transfection. Typically, over 0.1 million live cells are required for one PEM-seq library, and 1\u20133 million cells are recommended.\nAspirate the cell culture medium, wash cells with 3\u00a0mL 1\u00d7 PBS, and aspirate the PBS.\nAdd 400\u00a0\u03bcL 0.05% trypsin to cover all cells and incubate at 37\u00b0C for 2\u00a0min to allow cell detachment and separation.\nAdd 400\u00a0\u03bcL DMEM/10% FBS medium to quench trypsin and collect cells into a 1.5\u00a0mL tube.\nSpin at 300 g, 4\u00b0C, for 5\u00a0min and remove supernatant.\nWash once with 1\u00a0mL 1\u00d7 PBS, spin, and discard the supernatant.\nIsolating genomic DNA\nTiming: 1\u00a0day\nThe genomic DNA for PEM-seq analysis is isolated and purified in this major step.\nNote: Besides the procedures provided below, genomic DNA can be also isolated by any mainstream commercial kits, such as the PureLink Genomic DNA Purification Kit (Thermo), GenElute Mammalian Genomic DNA Miniprep Kits (Sigma-Aldrich), Monarch Genomic DNA Purification Kit (NEB), etc.\nPrepare cell lysis master mixture for each sample by mixing 495\u00a0\u03bcL of cell lysis buffer and 5\u00a0\u03bcL of proteinase K (20\u00a0mg/mL).\nLoose the cell pellet with flaps, add 500\u00a0\u03bcL of cell lysis master mixture, and incubate them in a thermomixer at 56\u00b0C, 500\u00a0rpm, for overnight (10\u201316 h).\nAdd 500\u00a0\u03bcL of Isopropanol, mix immediately and thoroughly by inverting and shaking the microtube until the white pellet forms. Troubleshooting 1[href=https://www.wicell.org#sec7.1]\nPick up the DNA pellet with a pipette tip, transfer it to a new 1.5\u00a0mL microtube containing 1\u00a0mL 70% (vol/vol) ethanol, and mix thoroughly by inverting and shaking.\nCentrifuge it at 13,000\u00a0g for 5\u00a0min at 4\u00b0C.\nRemove the supernatant completely, and air-dry the DNA pellet for 2\u20135\u00a0min.Dissolve the DNA pellet in 150\u00a0\u03bcL TE buffer and incubate at 56\u00b0C, 500\u00a0rpm, for at least 4 h.\nCritical: The DNA must be dissolved completely.\nDetermine the concentration of 1\u00a0\u03bcL of the isolated genomic DNA with a spectrophotometer; the value of A260/280 should be 1.8\u20132.0.\nPause point: Purified genomic DNA can be stored at \u221220\u00b0C for months or 4\u00b0C for a week.\nSonication\nTiming: 2 h\nIn this section, the isolated genomic DNA should be sheared to small fragments with a peak length of 300\u2013700\u00a0bp.\nTurn on the Covaris M220 focused-ultrasonicator and pre-cool it to 4\u00b0C.\nTransfer 20\u201350\u00a0\u03bcg DNA to a microtube-130 and adjust the final volume to 130\u00a0\u03bcL with nuclease-free water.\nSet the Covaris M220 by following the manufacturer\u2019s instructions (https://www.covaris.com/wp/wp-content/uploads/resources_pdf/pn_010252.pdf[href=https://www.covaris.com/wp/wp-content/uploads/resources_pdf/pn_010252.pdf]), and fragment the DNA to a target peak at 300\u2013700\u00a0bp:\ntable:files/protocols_protocol_1357_13.csv\nCritical: The performance of sonication varies with different samples. Carry out a time course based on the above settings. In our lab, we usually set the treatment time at 60 or 50\u00a0s for human or mouse genomic DNA, respectively.\nRun 1\u00a0\u03bcL of the sonicated DNA on a 1% (wt/vol) agarose gel in 1\u00d7 TAE buffer; the size of DNA should be with a target peak at 300\u2013700\u00a0bp. Troubleshooting 2[href=https://www.wicell.org#sec7.3]\nCritical: The length distribution of DNA fragments is important, as the too short or too long DNA fragments will be lost in PEM-seq libraries. Of note, if the library DNA of PEM-seq is sequenced with a 2\u00a0\u00d7 250\u00a0bp read, the size of fragmented DNA can be larger, such as with a peak at 500\u20131000\u00a0bp. The longer sequencing reads are more precise to identify outcomes, however, a 2\u00a0\u00d7 150\u00a0bp read is good enough for most experiments and is much cheaper.Pause point: DNA fragments can be stored at \u221220\u00b0C for months or 4\u00b0C for a week.\nPrimer extension\nTiming: 1.5 h\nDNA fragments containing the complementary sequence of the biotinylated primer are tagged with biotin at their 5\u2032 end by a one-round primer extension.\nPrepare the primer anneal mixture on ice for each sample as below:\ntable:files/protocols_protocol_1357_14.csv\nCritical: The total amount of sonicated DNA for each sample should be 1\u201340\u00a0\u03bcg with an optimal amount of about 20\u00a0\u03bcg. If a larger amount of DNA is needed, scale up the mixture.\nAliquot each of the primer anneal mixtures into 4 PCR tubes.\nPerform the primer anneal reaction with the following settings:\ntable:files/protocols_protocol_1357_15.csv\nCritical: The annealing temperature should be changed according to the melting temperature of biotinylated primer(s).\nSet up the primer extension mixture on ice for each sample as below:\ntable:files/protocols_protocol_1357_16.csv\nCritical: The amount of Bst 3.0 DNA polymerase is approximately 2\u00a0U/\u03bcg DNA. If the amount of DNA is more than 20\u00a0\u03bcg, scale up the polymerase.\nAdd 10\u00a0\u03bcL aliquots of the primer extension mixture to each PCR tubes in step 18, mix thoroughly by vortex.\nSet the primer extension reaction as follows:\ntable:files/protocols_protocol_1357_17.csv\nNote: A 15-min of primer extension is sufficient to amplify DNA fragments with a peak length at 500\u20131000\u00a0bp.\nPrimer removal\nTiming: 40\u00a0min\nThe exceeded biotinylated primers are removed in this major step, as the residual will be captured by streptavidin beads as well.\nPlace the AMpure XP beads at room temperature for at least 30\u00a0min before use.\nAdd 50\u00a0\u03bcL of pre-warmed AMpure XP beads to each PCR tube with primer extension product by pipetting up and down 15\u201320 times and then incubate at room temperature for 5\u00a0min.Critical: The volume of AMpure XP beads, every new batch or after a long-time storage, should be tested before use by following the manufacturer\u2019s instructions (https://www.beckman.com/reagents/genomic/cleanup-and-size-selection/pcr/a63880[href=https://www.beckman.com/reagents/genomic/cleanup-and-size-selection/pcr/a63880]). Principally, all of the free bio-primer should be removed and all DNA fragments larger than 200\u00a0bp should be kept. Typically, we use the beads ratio at 1.0\u00d7, which means that 50\u00a0\u03bcL of the AMpure beads are mixed with 50\u00a0\u03bcL of the primer extension product.\nPlace PCR tubes on the magnetic stand (DynaMa-PCR Magnet) at room temperature for 5\u00a0min.\nRemove and discard the supernatant completely.\nCritical: Hold the PCR tubes on the magnetic stand and do not disturb the AMpure beads. Moreover, during the whole process of primer removal, the AMpure beads cannot be over air-dry.\nAdd 200\u00a0\u03bcL of 70% ethanol to each tube and remove the supernatant completely.\nCritical: Leave the PCR tubes on the magnetic stand and do not disturb the AMpure beads.\nRepeat step 26.\nAdd 50\u00a0\u03bcL of 10\u00a0mM Tris-HCl, pH 7.5, and mix thoroughly by pipetting up and down 15\u201320 times.\nIncubate at room temperature for 2\u00a0min.\nPlace PCR tubes on the magnetic stand at room temperature for 2\u00a0min.\nTransfer 50\u00a0\u03bcL of the clear supernatant from each PCR tube and pool the same sample together in a 1.5\u00a0mL microtube.\nCritical: Do not disturb or transfer any of the AMpure beads.\nCheck the concentration of the supernatant with a spectrophotometer; the total amount of the recovered DNA should be closed to the initial amount. Troubleshooting 3[href=https://www.wicell.org#sec7.5]\nPause point: DNA fragments can be stored at \u221220\u00b0C for months.\nStreptavidin purification\nTiming: 5 h\nIn this section, the biotinylated single-stranded DNA is captured by the streptavidin beads.Incubate the purified DNA at 95\u00b0C for 5\u00a0min and immediately chilled on ice for 3\u00a0min.\nPrepare the streptavidin binding mixture:\ntable:files/protocols_protocol_1357_18.csv\nPrepare the streptavidin C1 beads.\nTransfer 20\u00a0\u03bcL of streptavidin C1 beads for each sample to a new 1.5\u00a0mL microtube.\nNote: If multiple samples are handled, scale up the amount of C1 beads.\nAdd 400\u00a0\u03bcL of 1\u00d7 B&W buffer, mix well by pipetting.\nPlace it on a DynaMag-2 holder at room temperature for 1\u00a0min.\nRemove and discard the supernatant completely.\nRepeat b-d for twice.\nSuspend the C1 beads with 20\u00a0\u03bcL of 1\u00d7 B&W buffer.\nAdd 20\u00a0\u03bcL of washed streptavidin C1 beads to the binding mixture from step 34 and incubate on a rotator at room temperature for 4 h.\nCritical: A 2\u00a0h incubation is sufficient to capture most of the biotinylated products, however, a 4\u00a0h incubation is recommended.\nPause point: The incubation can be sustained overnight (12\u201316 h).\nSpin the mixture at 200\u00a0g for 5 s.\nCritical: The centrifuge speed must be lower than 3000\u00a0rpm (or 1000 x g), otherwise, the C1 beads will be broken, resulting in the loss of biotinylated products. Regarding this, a mini benchtop microcentrifuge is not recommended.\nPlace the mixture on a DynaMag-2 holder at room temperature for 1\u00a0min and remove the supernatant completely.\nSuspend the C1 beads with 400\u00a0\u03bcL of 1\u00d7 B&W buffer, capture the beads on the magnet stand for 1\u00a0min and remove the supernatant completely.\nRepeat step 39.\nResuspend the beads with 400\u00a0\u03bcL of 10\u00a0mM NaOH, immediately place the mixture on the magnet stand for 1\u00a0min, and discard the supernatant completely.\nCritical: The total incubation time with 10\u00a0mM NaOH must be less than 2\u00a0min.Suspend the C1 beads with 400\u00a0\u03bcL of 10\u00a0mM Tris-HCl, pH 7.5, capture the beads on the magnet stand for 1\u00a0min and discard the supernatant completely.\nRepeat step 42.\nSuspend the C1 beads with 42.4\u00a0\u03bcL of 10\u00a0mM Tris-HCl, pH 7.5.\nOn-beads ligation\nTiming: 5 h\nThe 3\u2032 end of single-stranded DNA, on the streptavidin beads, is ligated with the bridge adapter containing a 14-bp RMB.\nPrepare the bridge-adapter mixture in a PCR tube:\ntable:files/protocols_protocol_1357_19.csv\nSet the bridge adapter assembly program:\ntable:files/protocols_protocol_1357_20.csv\nCritical: The ramp rate must be set at 0.1\u00b0C/s during the annealing processes.\nAdd 120\u00a0\u03bcL of nuclease-free water to bring the final concentration of annealed bridge adapter to 50\u00a0\u03bcM, prepare 80\u00a0\u03bcL aliquots and store them at \u221220\u00b0C for up to 1 year.\nSet up an 80\u00a0\u03bcL ligation reaction as below:\ntable:files/protocols_protocol_1357_21.csv\nCritical: Mix well all the reagents except PEG8000 by flaps, then add the PEG8000 with a cut-off P200 pipette tip and mix gently but thoroughly by pipetting up and down 10\u201315 times.\nIncubate the ligation reaction at room temperature for 4\u00a0h or overnight (12\u201316 h) on a rotator with a rotation rate at 8\u00a0rpm.\nCritical: Do not spin the mixture. It is recommended to resuspend the mixture every 2\u00a0h during the incubation, however, the resuspension can be skipped during overnight ligation.\nAdd 320\u00a0\u03bcL of 1\u00d7 B&W buffer and mix thoroughly, capture the beads on the magnet stand for 1\u00a0min and remove the supernatant completely.\nResuspend the beads with 400\u00a0\u03bcL of 1\u00d7 B&W buffer, place the mixture on the magnet stand for 1\u00a0min, and discard the supernatant completely.\nRepeat step 51.Resuspend the beads with 400\u00a0\u03bcL of 10\u00a0mM Tris-HCl, pH 7.5, place the mixture on the magnet stand for 1\u00a0min, and discard the supernatant completely.\nRepeat step 53.\nResuspend the on-beads ligated DNA with 73\u00a0\u03bcL of 10\u00a0mM Tris-HCl, pH 7.5.\nNested PCR\nTiming: 1.5 h\nThe nested PCR is used to enrich DNA fragments around the bait DSB. Moreover, it also introduces Illumina indexes to distinguish multiple samples that will be sequenced in the same lane.\nSet up a 100\u00a0\u03bcL nested PCR mixture as below:\ntable:files/protocols_protocol_1357_22.csv\nMix thoroughly, aliquot the nested PCR mixture into 2 PCR tubes and perform amplification with the following program:\ntable:files/protocols_protocol_1357_23.csv\nCritical: DNA polymerase used in this step should be without the nuclease-dependent proofreading activity, as it will digest the beads-bound ssDNA. Moreover, do not spin the PCR mixture as it will greatly reduce the amplification efficiency.\nSize selection\nTiming: 40\u00a0min\nDNA fragments larger than 300\u00a0bp are kept during the size selection to achieve the best performance of PEM-seq analysis.\nPlace the AMpure XP beads at room temperature for at least 30\u00a0min before use.\nAdd 40\u00a0\u03bcL of pre-warmed AMpure XP beads to each PCR tube after the nested PCR by pipetting up and down 15\u201320 times and then incubate at room temperature for 5\u00a0min.\nCritical: The volume of AMpure XP beads should be tested before use. Principally, all DNA fragments larger than 300\u00a0bp should be kept. Typically, we use 40\u00a0\u03bcL of the AMpure beads for 50\u00a0\u03bcL of the PCR product.\nPlace PCR tubes on the magnetic stand (DynaMa-PCR Magnet) at room temperature for 5\u00a0min.\nRemove and discard the supernatant completely.Critical: Hold the PCR tubes on the magnetic stand and do not disturb the AMpure beads. Moreover, the AMpure beads can be not air-dry.\nAdd 200\u00a0\u03bcL of 70% ethanol to each tube and remove the supernatant completely.\nCritical: Leave the PCR tubes on the magnetic stand and do not disturb the AMpure beads.\nRepeat step 62.\nAdd 35\u00a0\u03bcL of 10\u00a0mM Tris-HCl, pH 7.5, and mix thoroughly by pipetting up and down 15\u201320 times.\nIncubate at room temperature for 2\u00a0min.\nPlace PCR tubes on the magnetic stand at room temperature for 2\u00a0min.\nTransfer 33\u00a0\u03bcL of the clear supernatant from each PCR tube and pool the same sample together in a 1.5\u00a0mL microtube.\nCritical: Do not disturb or transfer the AMpure beads, as the AMpure beads greatly reduce the amplification efficiency of tagged PCR.\nCheck the concentration of 1\u00a0\u03bcL of the supernatant with a spectrophotometer; the concentration of the recovered DNA is less than 4\u00a0ng/\u03bcL, commonly ranging from 1 to 3\u00a0ng/\u03bcL. Troubleshooting 4[href=https://www.wicell.org#sec7.5]\nTagged PCR\nTiming: 1 h\nIn this section, PCR products are tagged by the Illumina adapter sequences with different indexes.\nSet up a 100\u00a0\u03bcL tagged PCR mixture as below:\ntable:files/protocols_protocol_1357_24.csv\nMix thoroughly, aliquot the tagged PCR mixture into 2 PCR tubes and perform PCR with the following program:\ntable:files/protocols_protocol_1357_25.csv\nCritical: To enhance the compatibility to different PCR machines and the efficiency of PCR, two 50\u00a0\u03bcL of aliquots are recommended. Moreover, the cycle number of PCR depends on the concentration of DNA in step 68. If the concentration is less than 2\u00a0ng/\u03bcL, perform 15 cycles; if the concentration is 2\u20134\u00a0ng/\u03bcL, perform 11\u201313 cycles; if the concentration is higher than 4\u00a0ng/\u03bcL, perform 10 cycles.\nPause point: The PCR products can be stored at \u221220\u00b0C for months.Size selection\nTiming: 3 h\nPCR products ranging from 300 to 700\u00a0bp are kept for sequencing.\nNote: Besides the procedures provided below, library DNA can be also size selected by AMpure XP beads (https://www.beckman.com/reagents/genomic/cleanup-and-size-selection/pcr/a63880[href=https://www.beckman.com/reagents/genomic/cleanup-and-size-selection/pcr/a63880]) or SPRIselect beads (https://www.mybeckman.cn/reagents/genomic/cleanup-and-size-selection/size-selection/b23317[href=https://www.mybeckman.cn/reagents/genomic/cleanup-and-size-selection/size-selection/b23317]) by following the manufacturer\u2019s instructions. Principally, DNA fragments ranging from 300 to 700\u00a0bp should be kept. Moreover, any mainstream gel extraction kit for DNA should work as well as the GeneJET Gel Extraction Kit.\nPool the same sample together, add 20\u00a0\u03bcL of 6\u00d7 DNA loading buffer, mix well, and run the PCR products on a 1.5% (wt/vol) agarose gel in 1\u00d7 TAE buffer.\nCritical: The time should be long enough to separate desired DNA products from by-products. Typically, the running time is over 1 h.\nExcise gel with DNA fragments ranging from 300\u00a0bp to 700\u00a0bp, cut into small pieces, and transfer them to a 15\u00a0mL tube. Troubleshooting 5[href=https://www.wicell.org#sec7.9]\nAdd 2\u00a0mL binding buffer from the GeneJET Gel Extraction Kit, mix thoroughly, and incubate at 56\u00b0C until all gel is dissolved.\nSpin the mixture through a column, included in the GeneJET Gel Extraction Kit, at 10,000\u00a0g for 1\u00a0min, and then discard the flow-through.\nAdd 800\u00a0\u03bcL of wash buffer (from the gel extraction kit) to wash the column, spin it at 10,000\u00a0g for 1\u00a0min, and then discard the flow-through.\nSpin the column at 10,000\u00a0g for 2\u00a0min, and transfer it to a new 1.5\u00a0mL microtube.\nAdd 50\u00a0\u03bcL of nuclease-free water to the column, incubate at room temperature for 2\u00a0min.\nSpin the column at 10,000\u00a0g for 2\u00a0min and collect the eluted fraction.Check the concentration of 1\u00a0\u03bcL of the library DNA with a spectrophotometer. The total amount of DNA should be 0.3\u20131\u00a0\u03bcg, with a concentration at 6\u201320\u00a0ng/\u03bcL. Troubleshooting 6[href=https://www.wicell.org#sec7.11]\nPause point: The library DNA can be stored at \u221280\u00b0C for months.\nHigh-throughput sequencing\nTiming: 3\u00a0days\nIn this section, pooled libraries with compatible indexes are sequenced on Illumina platforms.\nPool a proper number of PEM-seq libraries equally, and apply the mixed library DNA to be sequenced on Illumina sequencers.\nCritical: Make sure the pooled libraries have compatible indexes. Moreover, an optimal sequencing platform also has to be decided on, typically, we use Hi-seq 2500, with a 2\u00a0\u00d7 150\u00a0bp. For each library, 3\u20135 million raw reads, with coverage at 1.5\u20132.5\u00d7 as 1\u20132 million cells are recommended for one PEM-seq library, are sufficient for the following analysis.\nDemultiplex each library based on its index(es), typically, this can be done with the bcl2fastq2 conversion software by following the manufacturer\u2019s instruction (https://support.illumina.com/content/dam/illumina-support/documents/documentation/software_documentation/bcl2fastq/bcl2fastq2-v2-20-software-guide-15051736-03.pdf[href=https://support.illumina.com/content/dam/illumina-support/documents/documentation/software_documentation/bcl2fastq/bcl2fastq2-v2-20-software-guide-15051736-03.pdf]).\nSequence reads processing\nTiming: 3 h\nReads are processed by PEM-Q, which is available with a test sample datasheet and detailed instructions (https://github.com/liumz93/PEM-Q[href=https://github.com/liumz93/PEM-Q]). Please install the pipeline following the instructions on the website.\nPrepare information in the following order for each library:\ntable:files/protocols_protocol_1357_26.csv\nCritical: \u201cGenome\u201d is the reference assembly, e.g. hg38, mm10 for alignment. \u201cName\u201d stands for the basename of the input .fastq files, e.g. \u201c/path/LY001_R1.fastq\u201d should be \u201cLY001\u201d. \u201cCut-site\u201d is the breakpoint of the bait DSB on the strand of nested primer resident. \u201cP-start\u201d, \u201cP-end\u201d, \u201cP-strand\u201d, and \u201cP-sequence\u201d are the starting point, endpoint, resident strand, and sequence of the nested primer. Of note, \u201cP-sequence\u201d must be in Upper characters, while lower characters are for the rest items except for the \u201cName\u201d. Information for Myc6, MYC1 (+/\u2013), and MYC-YH have been provided in the following table.\ntable:files/protocols_protocol_1357_27.csvOptional: The following information is only used for the vector integration analysis. Skip the following table, if no vector is used in your experiments.\ntable:files/protocols_protocol_1357_28.csv\nNote: The \u201cName\u201d, \u201cGenome\u201d, \u201cChromosome\u201d, and \u201cP-strand\u201d are the same as the essential information for PEM-Q processing. The \u201cName of vector sequence file\u201d is the full title of the .fa file, e.g., \u201cSpCas9_pX330.fa\u201d. \u201csgRNA-start\u201d and \u201csgRNA-end\u201d are the start and end point of the single guide (sg) RNA sequence on the vector. Examples are provided in the online instruction of PEM-Q.\nMake a folder for each sample.\nMove the sequence files to the indicated file folder and then execute step 85 under this folder.\nRun the PEM-Q pipeline with the following command.\n>PEM-Q.py \u201cGenome\u201d \u201cName\u201d \u201cCut-site\u201d \u201cChromosome\u201d \u201cP-start\u201d \u201cP-end\u201d\n\u201cP-strand\u201d \u201cP-sequence\u201d\nCritical: The command must be executed in the folder containing the sequence files.\nOptional: Execute the following command to identify vector integrations:\n>vector_analyze.py \u201cName\u201d \u201cName of the vector sequence file\u201d \u201cGenome\u201d\n\u201cChromosome\u201d \u201cP-strand\u201d \u201csgRNA-start\u201d \u201csgRNA-end\u201d\nAfter that, the output contains two types of files in the \u201cresults\u201d folder, which will be used for the following analysis (Table below). The statistics file contains the number of each kind of editing event and the editing efficiency. Each .tab file is a kind of editing event, e.g., deletions, insertions, and intra- or inter-chromosomal translocations. Use the separated files for the analysis presented in the Quantification and statistical analysis section.\ntable:files/protocols_protocol_1357_29.csv\nCritical: Each line in the .tab file represents an editing event. The detailed description of output files is presented in the documentation of PEM-Q (https://github.com/liumz93/PEM-Q[href=https://github.com/liumz93/PEM-Q]).\nConvert the .tab files to .bdg files with commands listed below, and visualize the .bdg files in IGV.\n>tab2bdg_PEMQ.py \u201cFull name of the .tab file\u201d \u201cGenome\u201d\nOptional: Execute the following command to visualize reads aligned to the vector sequence.>vectorTab2bdg.py \u201cFull name of the .tab file containing vector\u201d \u201cthe\npath/ Name of the vector sequence file (.fa)\u201d", "Step-by-step method details\nStep-by-step method details\nPreparation of the data\nTiming: 4 h\nAs a first step, we choose the classification method and prepare the data for classification. Correct pre-processing and the choice of the classification method have an important influence on the results and the quality of analysis. These choices should be led by the scientific question to be answered, and by considering the dataset constraints.\nNote that the indicated Timings are a rough estimation of the computational time needed to perform computations on a server with 10 cores. Timings do not include the time needed for implementation of the source code.\nChoose the classification method.\nOur choice of the classifier is the linear Support Vector Machine (SVM). The SVM is used for binary classification and regression problems on multivariate data (i.e., data with multiple variables). The SVM is particularly efficient on datasets with many variables and a limited number of samples, since it has optimal generalization performance, and achieves good performance also on non-Gaussian distributions (Belousov et al., 2002[href=https://www.wicell.org#bib3]; Meyer et al., 2003[href=https://www.wicell.org#bib20]). These are precisely the properties of neural data that we typically obtain from in-vivo experiments. In in-vivo experiments, the number of samples (corresponding to the number of trials) in a recording session is typically low (<100) and the number of variables (corresponding to the number of neurons recorded in parallel) can be moderate to high, typically on the order of tens to hundreds of units. Moreover, distributions of spike counts across neurons are often not Gaussian. Another reason why the linear SVM is an attractive choice for the analysis of electrophysiological data is its ability to associate the activity of each neuron with its weight in the classification task in a straightforward manner.Note, however, that the choice of a linear model is not always appropriate. A linear classification model is searching for a linear separation of data points belonging to classes A and B, and a linear separation might not always be optimal. Troubleshooting 1[href=https://www.wicell.org#sec6.1]\nChoose which conditions to decode.\nWe decode conditions match vs. non-match, using only trials with correct behavioral performance. Since these conditions differ in both the stimulus presented and the subsequent choice of the animal, we are decoding a mixed variable \u201cstimulus+choice\u201d (see Koren et\u00a0al., 2020a[href=https://www.wicell.org#bib15]).\nChoose a relevant time window for averaging neural responses.\nIn the dataset considered here, the information about matching and non-matching of the target and the test stimuli only becomes available when the test stimulus is shown. We therefore expect that the neural activity contains class-related information during and after the presentation of the test stimulus, and not before. We use a time window of 0\u2013500\u00a0ms after the onset of the test stimulus. This time window covers the entire presentation of the test stimulus (0\u2013300\u00a0ms), and, in addition, 200\u00a0ms after the offset of the test stimulus, when potential choice-related information is expected to unfold. Troubleshooting 2[href=https://www.wicell.org#sec6.3]\nSet binary labels for the two classes.\nWe can choose arbitrary labels, for example yj\u00a0= 1 for trials in condition match and yj\u00a0= \u22121 for trials in condition non-match.\nDecide for a cross-validation (CV) method and split the data samples and the class labels into training and validation sets. We use Monte-Carlo CV, where the data is randomly split into non-overlapping training and test sets. Troubleshooting 3[href=https://www.wicell.org#sec6.5]\nFor each Monte-Carlo CV set, the order of trials is randomly permuted, without repetition. The new trial order is applied to data samples    x j    and to labels    y j   .Trials are then split into the training set (80%) and the test set (the remaining 20%). This is again relevant to both data samples and labels.\nSteps in 6.a and 6.b are iterated NCV-times. We use NCV=100 cross-validations in every recording session.\nCompute z-scored spike counts in the chosen time window. Troubleshooting 4[href=https://www.wicell.org#sec6.7]\nCompute the spike count in the chosen time window. For the neuron   n   in trial   j  , the spike count is the following:\n   S  n j   =  \u2211 k    f  n j    (  t k  )    \nwhere    f  n j    (  t k  )    is a binary vector of zeros and ones (the spike train) of the neuron n in trial j.\nCompute the Z-scored spike counts for each neuron individually. For each neuron, compute the mean and the standard deviation of the spike count across trials, using only trials from the training set. Then, z-score the spike counts in the training as well as in the test set as follows:\n   x  n j   =    S  n j   \u2212   S \u00af  n    S T D  (  S n  )     \nIf N is the number of neurons in the recording session, the vector of N z-scored spike counts in trial   j   is one sample for the classifier (Figure\u00a01[href=https://www.wicell.org#fig1]A) and can be written as follows:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/925-Fig1.jpg\nFigure\u00a01. Preparing the data for binary classification\n(A) Schema of a dataset from one recording session. We measure the z-scored spike count of multiple neurons in many trials. Each trial belongs to one of the two binary classes, \u201cmatch\u201d or \u201cnon-match\u201d. For the classifier, trials are samples, and neurons are features.(B) Schema of the division of the dataset in the training set and the test set (gray rectangles). Within the training set, another division of the data is done for the selection of the C-parameter (red rectangles).\n   x j  =  [   x  1 j   ,  x  2 j   , \u2026  x  N j    ]   \nwhere   T  denotes the transpose.\nCritical: For a control, it is important to repeat the analysis for additional time windows where we expect that no significant effect occurs, for example the target and/or the delay period. This increases the validity of the chosen method and the credibility of results.\nNote: The purpose of the decoding model here is to extract from the data information that is useful for classification, and not to build a generative model of the neural activity. See Part 4 and Troubleshooting 6[href=https://www.wicell.org#sec6.11] for further information and discussion on this topic.\nNote: As we use all the available trials in the experiment, the number of trials in each condition is imbalanced. The cross-validation does not change this since it merely permutes the order of trials. However, the imbalance in the number of trials, unless extreme, is not a problem if we use a performance measure that takes into account this bias (see Part 2).\nAlternatives: There are many alternative classification models that can be applied to neuroscience data, such as logistic regression (Alkan et\u00a0al., 2005[href=https://www.wicell.org#bib1]) or Generalized Linear Models (GLMs, see Pillow et\u00a0al., 2011[href=https://www.wicell.org#bib26]). For an introduction into explainable machine learning methods and some more examples, see Molnar (2019)[href=https://www.wicell.org#bib21].\nComputation of the classification model and its performance\nTiming: 6 hWhen working with linear SVMs, we utilize neurons as features and trials as samples for the classifier (Figure\u00a01[href=https://www.wicell.org#fig1]A). While features can be correlated, samples are required to be independent and therefore uncorrelated. Troubleshooting 5[href=https://www.wicell.org#sec6.9]\nCompute the parameters of the classification model on the training set.\nThe linear SVM has one hyperparameter, the regularizer, also commonly referred to as the C-parameter. The C-parameter must be optimized, utilizing exclusively the training set (Figure\u00a01[href=https://www.wicell.org#fig1]B). First, define a range of C parameters to be tested, for example, C\u00a0= [0.001, 0.01, 0.1, 1, 10, 100, 1000]. If most of the data is best fit with one of suggested C-parameters, we may refine the range around plausible values. However, once the range is defined, it cannot be changed anymore. In particular, the same range should be used for all recording sessions.\nChoose the optimal C-parameter with 10-fold cross-validation (CV). Split the training data into 10 folds. 9 folds are used to compute the classification model while the remaining fold (the validation set for the C-parameter) is used to compute the performance of the model (Figure\u00a01[href=https://www.wicell.org#fig1]B). Iterate the procedure such that every fold is the validation set, and average across iterations. By testing the entire range of C-parameters, we can choose the C-parameter that gives the highest performance (optimal C-parameter).\nNow, all the training data is gathered to compute a new classification model, utilizing the optimal C-parameter.\nTest the performance of the model on the test set.\nAfter the classification model has been trained, we assess its performance on yet unseen samples, the test set. As a performance measure, we use the balanced accuracy (BAC), which corrects for the imbalance in the number of trials (samples) across the two categories (\u201cmatch\u201d and \u201cnon-match\u201d). The balanced accuracy is defined as follows:B A C =  1 2   (    T P   T P + F N   +   T N   T N + F P    )   \n where TP, TN, FP and FN are the number of true positive, true negative, false positive and false negative test samples, respectively. Let us think of the condition \u201cmatch\u201d as \u201cpositive\u201d and \u201cnon-match\u201d as a \u201cnegative\u201d. We can see that the balanced accuracy computes the proportion of correct classifications of the condition \u201cmatch\u201d among all classifications that are \u201cmatch\u201d (first term in the parenthesis on the right-hand-side), and the proportion of correct classifications of \u201cnon-match\u201d among all classifications that are \u201cnon-match\u201d (second term on the right-hand side).\nBalanced accuracy, which is a single number between 0 and 1, is calculated in each cross-validation run. After collecting results across cross-validation runs, we report the balanced accuracy that is averaged across cross-validations.\nCritical: It is crucial that the model is tested on yet unseen data. Testing the performance on the training set will usually give much higher performance than on a held-out test set. Testing on a held-out test set is putting to the test the capability of the classification model to generalize to yet unseen samples.\nNote: In a binary classification task, the balanced accuracy of 0.5 is the chance level performance, meaning that test samples have been classified correctly as often as incorrectly. The balanced accuracy of 1, on the other hand, indicates that all test samples have been classified correctly.\nAlternatives: We opted for the 10-fold CV method for determining the C-parameter because it is faster than the Monte-Carlo CV. Computation of the optimal C-parameter is nested in the training set/validation set CV and is for this reason computationally expensive. With small datasets, however, Monte-Carlo CV is an alternative method for the selection of the C-parameter.Alternatives: We chose to measure the classification performance with balanced accuracy, a measure that accounts for the imbalance for the data classes. As an alternative, one can balance the classes with stratified k-fold cross-validation, a method that ensures equal participation of classes in each fold (Zeng and Martinez, 2000[href=https://www.wicell.org#bib33]).\nAssessment of significance of classification performance\nTiming: 24 h\nSignificance of classification performance is assessed with a permutation test. The permutation test is a non-parametric test that does not make any assumption on data, but instead creates the distribution of results of random models from the data itself, against which the true result is tested.\nCompute the performance of models trained on randomly permuted class labels.\nGather class labels of the training data in a vector   y  . Randomly permute the entries of the vector   y  , without repetition.\nTrain and test the classification model as in steps 7 and 8 but utilizing permuted class labels. The permutation of class labels is iterated Nperm-times, giving Nperm values for the balanced accuracy (BACperm). We used Nperm=1000.\nCompute the p-value by ranking the balanced accuracy among the distribution of balanced accuracies of models with permuted class labels.\nThe p-value is the proportion of the BACperm that are bigger than the BAC. Note that this is a one-sided test. If the p-value is smaller than the significance level \u03b1, the BAC is significant. We used a significance level \u03b1=0.05.\nWhen performing more than one permutation test, the significance level must be corrected for multiple testing. We used the Bonferroni correction, where the significance level is divided by the number of tests, \u03b1 corrected=\u03b1/Ntest.Critical: The precision of the p-value depends on the number of permutations we use. If we use Nperm=1000 permutations, and if one instance of BACperm is larger than the BAC, our result is significant with the p-value of p=0.001. If no instance of BACperm is larger than the BAC, our p-value is p<0.001. The precision of the p-value is therefore limited by the number of permutations, more precisely, with the p-value of p<1/ Nperm. Even if all BACperm are larger than BAC, stating that the p-value is zero is incorrect.\nNote: As we randomly permute class labels, the classification model cannot find the underlying structure of the data, since the association between data samples and class label has been randomized by the permutation. However, due to the limited number of samples, the BACperm will not be exactly 0.5, but will take values around 0.5 in different permutation cycles (Figure\u00a02[href=https://www.wicell.org#fig2]A).\nAlternatives: The permutation test is a very rigorous way of testing the significance of an effect. On the down side, the permutation test takes a long time to run, since it requires that the entire model is re-computed and re-evaluated Nperm - times. An alternative significance test can be performed with a parametric test, where one needs to be careful that the criteria of the parametric test are met.\nDecoding weights and functionally relevant subgroups\nTiming: 6 hIf the balanced accuracy is significant, we can proceed to study the role of single neurons in the classification task. With the linear SVM, neurons are features of the model (Figure\u00a01[href=https://www.wicell.org#fig1]A), and the activity of each neuron is associated with one feature weight. Considering feature weights of N neurons observed in parallel, we define a weight vector   w  . The weight vector determines the orientation of the separating boundary that separates data points in conditions A and B (Figure\u00a03[href=https://www.wicell.org#fig3]). Troubleshooting 6[href=https://www.wicell.org#sec6.11]\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/925-Fig2.jpg\nFigure\u00a02. Balanced accuracy of models with permuted class labels\n(A) Left: BAC of models with permutation of class labels (BACperm) in one recording session. We used 1000 random permutations. Right: Distribution of BAC shown on the left. The magenta line marks the mean of the distribution.\n(B) Left: Distribution of BACperm (black) and the BAC of the regular model (red) during target. The p-value is p=0.418. Right: Same as on the left, but during test. The p-value is p<0.001, since none of the BACperm is bigger than the BAC.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/925-Fig3.jpg\nFigure\u00a03. Schema of the separating boundary in a toy model with two neurons\nThe separating boundary is an (N-1)-dimensional plane (a hyperplane) in the space of inputs of N neurons. In case of N=2 neurons, the separating boundary is a line. The separating boundary optimally divides the space of inputs from category A and B (yellow and blue circles). The separating boundary is fully determined by the offset from the origin (here the offset is 0), and the weight vector w that determines its orientation. The weight vector has N entries, one for each neuron. The change in sign of the weight of a particular neuron changes the orientation of the separating boundary.The weight vector can easily be calculated numerically from the classification model. To calculate the weight vector, we utilize a subset of data points that are the most informative for the classification task, the so-called \u201csupport vectors\u201d (hence the name of the classifier). A support vector is a data point in the N-dimensional data space (in a particular trial) that the classifier has used for determining the separation boundary. Support vectors are data points that lie on or close to the separation boundary.\nGather all data and compute the classification model.\nWhen estimating feature weights of the model, no training/validation is required. Apply steps 6 and 7 on the complete dataset.\nCompute feature weights of the model.\nAs we compute the classification model, the classification function has several available outputs. We call the following outputs from the classification function: support vectors    \u03b1 j   , indices of support vectors, and Lagrange multipliers    \u03bb j   .\nCompute weights according to the following expression:\n  w =  \u2211  j = 1  Q    \u03bb j   y j   x j    \nwhere    y j    is the class label in trial   j   and    x j    is the vector of z-scored spike counts in trial   j  .   Q   is the number of support vectors, and it is smaller than the number of trials.\nRepeat the procedure for every cross-validation run. Average weights across cross-validations.\nNormalize weight vectors and gather results across recording sessions.\nNormalize the weight vector in every recording session as follows:\n   w \u02dc  =  w /  \u2016 w \u2016    \nwhere    \u2016 w \u2016  =    w 1 2  +  w 2 2  + \u2026 ,  w N 2      is the L2 norm.\nGather results across recording sessions.\nUsing properties of weights, define functional subgroups.An important property of the weight is its sign. Neurons with positive and negative weights have the opposite effect on the separation boundary, as they are pulling the separation boundary in opposite directions (Figure\u00a03[href=https://www.wicell.org#fig3]). Another important property is the amplitude of the weight. The larger the amplitude of the weight of a particular neuron, the higher the importance of the neuron for classification is.\nNote: The range of weights depends on the C-parameter, and the C-parameter differs across recording sessions. If we want to compare the amplitude of weights across neurons from different recording sessions, we are required to normalize the weight vector.\nNote: If all samples are correctly separated by the separation boundary, we say that the data is linearly separable. However, the model can function even in the absence of linear separability. In neural recordings, linear separability is unlikely. even samples in the training data can sometimes lie on the wrong side of the separating boundary. Such samples are called \u201cslack points.\" The C-parameter determines how strongly slack points are considered in the computation of the separating boundary (see Belousov et al., 2002[href=https://www.wicell.org#bib3]; Vapnik and Vapnik, 1998[href=https://www.wicell.org#bib27]).Alternatives: There are alternative ways of determining effects of a particular feature, one example of which is a Shapely value (see Molnar, 2019[href=https://www.wicell.org#bib21]). The Shapley value is the average marginal contribution of a feature (or set of features) across all possible sets of features. Unfortunately, the computation of the Shapley value is computationally expensive, to the point that it is only feasible if the number of simultaneously recorded units is small. We need to account for all possible subpopulations of neurons, which quickly leads to a combinatorial explosion. In the case of our data with up to 17 parallel units, systematic computation of Shapley values for single neurons was not feasible. However, computing the Shapley value might be feasible in datasets with a small number of parallel units (e.g., 5 units).\nThe effect of heterogeneity across neurons on performance\nTiming: 12 h\nIf all neurons contribute the same quantity of information to the classifier and respond to the stimulus class the same way (e.g., by increasing in the firing rate for the stimulus \u201cmatch\u201d), we can call the observed population homogeneous. Within such a population, and assuming no correlations in the data, all neurons would have the same decoding weight. A homogeneous network is an abstract concept that would hardly ever happen in the real-world scenarios, where we rather expect that neurons differ in the way they activate for a given class (e.g., some neurons increase the firing rate for the class \u201cmatch\u201d while others decrease the firing rate for the class \u201cnon-match\u201d). We also expect that neurons differ in the quantity of information that they convey to the classifier. In such a case, every neuron has a different weight, and we call the population heterogeneous.Here, we inquire how the heterogeneity across neurons contributes to the performance of the classification model. To this end, we compute the performance of the model that is homogeneous across neurons (Figure\u00a04[href=https://www.wicell.org#fig4]A) and compare it to the performance of the regular model. We remove the heterogeneity across neurons by permuting the activity across neurons, independently in each trial. Such a procedure destroys the activation patterns that are a source of information for the classifier, and we therefore expect a decrease in classification performance.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/925-Fig4.jpg\nFigure\u00a04. Creating homogeneous neural ensembles\n(A) By swapping the activity across all the neurons from the population (left) or between neurons with the same sign of the weight (right), we create homogeneous neural ensembles. Figure\u00a0reprinted with permission from Koren et\u00a0al., 2020a[href=https://www.wicell.org#bib15].\n(B) Homogeneous ensembles are created by randomly permuting the neural activity across neurons in the same group, independently in each trial.\nAssess the effect of heterogeneity across neurons.\nRandomly permute, without repetition, the elements of the vector of activations    x j    across all simultaneously recorded neurons. The vector of activations is permuted independently in every trial (Figure\u00a04[href=https://www.wicell.org#fig4]B), so that the identity of neurons is mixed differently across trials. Note that the class labels do not change.\nRepeat steps 5\u20138. In step 8, we get the balanced accuracy of the homogeneous model, BACh.\nCompare BACh with BAC by computing the difference, \u0394h\u00a0= BAC-BACh, in every recording session.\nAssess the effect of heterogeneity within functional subpopulations. A population is defined by the same sign of the weight.\nRandomly permute, without repetition, the vector of activations    x j   , but only across units with the same sign of the weight. As in step 15.a, the permutation is done on every trial independently, while labels stay untouched.Repeat steps 5\u20138. In step 8, we get the balanced accuracy of the model that is homogeneous within groups, BACh.groups.\nCompare BACh,groups with BAC by computing the difference,\n\u0394h, groups\u00a0= BAC-BACh, groups, in every recording session.\nCritical: It is crucial that all the steps are performed on the data from the same recording session. Gathering data from different recording sessions before we start with the step 14 will create artificial assemblies with artificial covariance and will give misleading results. However, we can evaluate the general effect of heterogeneity by gathering across recording sessions final results, \u0394h and \u0394h, groups..\nNote: Comparing the performance of the homogeneous models with the regular model, we assess how much the heterogeneity across neurons contributes to the performance of the regular model. Note that removing heterogeneity is always expected to decrease the performance of the model. However, it is interesting to consider how much of the prediction accuracy of the regular model is due to heterogeneity across neurons, and to compare results from steps 15 and 16 (see Expected Outcomes).", "Step-by-step method details\nStep-by-step method details\nTarget identification\nSelection of cell system and nuclei isolation\n      An adequate cell system or cell lines of interest should be initially\n      selected. Any cell line that can be expanded in\u00a0vitro is in\n      principle suited for this protocol. Our laboratory mainly focuses on\n      colorectal cancer and we, therefore, established this procedure using a\n      panel of CRC cell lines (e.g., Colo320HSR, LS180, HT55 and LS411N). Each\n      cell line is then profiled using GRO-seq. To capture nascent transcripts,\n      millions of cell nuclei are isolated and transcription is reinitiated in\n      presence of an analog of UTP (Br-UTP). Nascent transcripts with\n      incorporated Br-UTP can be captured with an agarose-conjugated anti-BrdU\n      antibody (Santa Cruz, IIB5). This section describes how to isolate cell\n      nuclei, perform and analyze GRO-seq. Steps related to the GRO-Seq protocol\n      are illustrated in Figure\u00a02[href=https://www.wicell.org#fig2]A.\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3032-Fig2.jpg\nFigure\u00a02. Global run-on sequencing\n          (A) Scheme highlighting key steps required to perform global run-on\n          sequencing.\n        \n          (B) Example of nuclei obtained from the CRC cell line HCT116,\n          following the procedure illustrated in A1.\n        \n          (C) Analysis of two purified run-on reactions (S1 and S2) using a\n          Bioanalyzer RNA 6000 Pico Chip. The size distribution is indicated by\n          a ladder (L).\n        \n          (D) Electropherograms of samples presented in B. Fragment sizes\n          (nucleotides, nt) are plotted against arbitrary fluorescence units\n          (FU).\n        \nCritical: RNA is sensitive to\n      temperature and enzymatic digestion by RNases, which are normally present\n      on skin, surfaces, etc. Keep all surfaces and equipment clean while\n      performing this protocol. We also recommend to wear gloves and to keep\n      samples on ice whenever indicated in the protocol. Only use RNase-free\n      materials and chemicals.\n    \nNote: Here we describe the procedure from\n      the day cells are harvested for nuclei isolation. We do not consider\n      seeding density, expansion time and possible treatments. However, if youhave different treatments or conditions in your experimental setup, these\n      factors should be carefully considered in the overall planning. For cancer\n      cell lines, we generally harvest 20\u00a0\u00d7\u00a0106 cells per replicate\n      and combine two run-on reactions (each with 5\u00a0\u00d7\u00a0106 nuclei) in\n      order to obtain sufficient material for the preparation of a sequencing\n      library. However, if the transcriptional output of your cells of interest\n      is low, you may have to increase the number of pooled run-on reactions per\n      replicate.\n    \nDay 1\n        Prepare the swelling, lysis, and freezing buffers and pre-cool them on\n        ice.\n      \nCool down a centrifuge to 4\u00b0C and pre-warm the trypsin.\n        Trypsinize the cells until they become rounded and floating.\n        \n            Neutralize the trypsin with 2 volumes of 10% FBS-supplemented medium\n            and transfer each sample to a 50\u00a0mL tube. Make sure that cells are\n            properly singularized.\n          \n            Centrifuge samples for 5\u00a0min at 300\u00a0\u00d7\u00a0g, and discard the\n            supernatants.\n          \n            From here on, keep the pellets on ice unless indicated otherwise.\n          \n        Resuspend each pellet in 30\u00a0mL of swelling buffer.\n        \n            Incubate samples 10\u00a0min on ice with occasional inverting of the\n            tubes.\n          \nCentrifuge samples for 5\u00a0min at 500\u00a0\u00d7\u00a0g (4\u00b0C).\n            Carefully remove the supernatant without disturbing the pellet.\n          \n        Resuspend each pellet in 10\u00a0mL of lysis buffer and transfer the\n        suspension into a 15\u00a0mL tube.\n        \n            Invert the tube 30 times and spin down for 5\u00a0min at 500\u00a0\u00d7\u00a0g\n            (4\u00b0C).\n          \nRemove the supernatant.\nRepeat step 5.\nNote: If a large number of cells (>\n      5\u00a0\u00d7\u00a0107) is collected per sample, you may split them in several\n      tubes to obtain a better swelling and lysis reactions. You can visually\n      inspect your sample under the microscope (after step 6), and confirm the\n      presence of clean nuclei (see Figure\u00a02[href=https://www.wicell.org#fig2]B). If not, youcan repeat step 5, before resuspending the sample in freezing buffer.\n    \n        Resuspend the pellet in 1\u00a0mL of freezing buffer and transfer the\n        suspension to a 1.5\u00a0mL tube.\n        \nCount nuclei using trypan blue.\nSpin down samples for 5\u00a0min at 500\u00a0\u00d7\u00a0g (4\u00b0C).\n            Resuspend the nuclei in freezing buffer to reach a concentration of\n            5\u00a0\u00d7\u00a0106 nuclei per 100\u00a0\u03bcL of buffer.\n          \nAliquot 100\u00a0\u03bcL into 1.5\u00a0mL tubes and keep them on ice.\nPause point: Here you can STOP and\n      freeze nuclei aliquots at \u221280\u00b0C OR proceed to the run-on reactions.\n    \nNote: For each replicate, we recommend to\n      perform two run-on reactions (each with 5\u00a0\u00d7\u00a0106 nuclei). If\n      your harvest did not yield enough nuclei for two run-on reactions, you can\n      repeat the previous steps or control the efficacy of your nuclei isolation\n      by taking along cell lines with high nuclei isolation yield such as HCT116\n      and HEK293.\n    \nRun-on reactions\nTiming: 1 h\n      Here we describe the use of global run-on sequencing to identify expressed\n      or regulated lncRNAs in a cell line of interest. Compared to conventional\n      RNA-Seq, GRO-Seq captures nascent transcripts, which improves the\n      detection of less stable and lowly transcribed RNAs (including non-polyA\n      RNAs).9[href=https://www.wicell.org#bib3],10[href=https://www.wicell.org#bib4] In addition, this method\n      allows the precise mapping of transcription start sites (TSSs). This\n      feature is particularly important since current CRISPRi tools achieve best\n      transcriptional regulation when gRNAs are targeted to these regions.\n    \nPreheat a thermo-mixer to 30\u00b0C.\n        Prepare the reaction buffer according to Table\u00a02[href=https://www.wicell.org#tbl2] and\n        preheat to 30\u00b0C.\n        table:files/protocols_protocol_3032_2.csv\nComposition of buffers for nuclei isolation.\ntable:files/protocols_protocol_3032_3.csv\nComposition of buffers for run-on reactions.\nIf you have stored the nuclei at \u221280\u00b0C, thaw them on ice.\n        Add 100\u00a0\u03bcL of pre-warmed reaction buffer (1\u00d7 volume) to each reaction\n        tube with 100\u00a0\u03bcL of nuclei and keep tubes at 20\u00b0C.Cut off the tip of a 200\u00a0\u03bcL pipette tip (the mixture is very\n            viscous) and pipette up and down each reaction mixture. Change the\n            tip between each sample to avoid contamination. Pipette carefully to\n            avoid air bubbles.\n          \nIncubate samples for 5\u00a0min at 30\u00b0C.\n            Pipette the reaction mixtures up and down (using cutoff tips) after\n            2\u20133\u00a0min of incubation, while keeping the samples at 30\u00b0C. Pipette\n            gently and avoid making air bubbles.\n          \n        Add 600\u00a0\u03bcL (3\u00d7 volume) of Trizol LS to each reaction tube and mix\n        samples by vortexing.\n        \nIncubate samples for 5\u00a0min at 20\u00b0C.\nPause point: You can STOP here and\n      store samples at \u221280\u00b0C, according to the manufacturer\u2019s recommendations OR\n      proceed with the RNA purification.\n    \nRNA purification and capture of nascent transcripts\nTiming: 7\u201312\u00a0h over 3\u00a0days\n    \nDay 1\nTiming: 2\u20133 h\n      Here we detail the methods used to purify nascent RNAs and prepare\n      sequencing libraries.\n    \nPre-cool a tabletop centrifuge to 4\u00b0C.\n        Add 160\u00a0\u03bcL of chloroform to each sample.\n        \nVortex samples for 15\u00a0s and incubate for 3\u00a0min at 20\u00b0C.\nCentrifuge samples for 15\u00a0min at 12000\u00a0\u00d7\u00a0g (4\u00b0C).\n            Transfer 400\u00a0\u03bcL of the aqueous phase (top) into a 1.5\u00a0mL collection\n            tube. We recommend transferring 2\u00a0\u00d7\u00a0200\u00a0\u03bcl (using 200\u00a0\u03bcl tips) to\n            avoid potential contamination with the interphase.\n          \nDiscard tubes with the bottom phase.\n        Add 400\u00a0\u03bcL (1\u00d7 volume) of acid phenol-chloroform to each sample.\n        \nVortex samples for 30 s.\nCentrifuge the tubes for 10\u00a0min at 12000\u00a0\u00d7\u00a0g (4\u00b0C).\n            Transfer 300\u00a0\u03bcL of the aqueous phase (top) to a new collection tube.\n          \nDiscard the tubes with the bottom phase.\n        Add 300\u00a0\u03bcL (1\u00d7 volume) of chloroform to each sample.\n        \nVortex samples for 30 s.\nCentrifuge samples for 10\u00a0min at 12000\u00a0\u00d7\u00a0g (4\u00b0C).Transfer 250\u00a0\u03bcL of the aqueous phase (top) to a new collection tube.\n          \n            Combine 2 run-on reactions per sample into one tube (total of\n            500\u00a0\u03bcL).\n          \nDiscard tubes with the bottom phase.\n        Add 2\u00a0\u03bcL Glyco Blue to each sample.\n        \nVortex vigorously for 10 s.\n        Add 350\u00a0\u03bcL (0.7 x volume) of isopropanol to each tube.\n        \nVortex vigorously for 15 s.\nIncubate samples 14\u201318\u00a0h at \u221220\u00b0C.\n        Prepare buffers as detailed in Table\u00a03[href=https://www.wicell.org#tbl3].\n        table:files/protocols_protocol_3032_4.csv\n          Composition of buffers to prepare anti-BrdU beads.\n        \n        To prepare anti-BrdU beads, transfer 60\u00a0\u03bcL (for each sample) of the bead\n        solution into a 1.5\u00a0mL tube.\n        \nSpin down the beads for 30\u00a0s at 500\u00a0\u00d7\u00a0g.\nCarefully remove the supernatant.\nAdd 800\u00a0\u03bcL of bead blocking buffer per tube.\n            Spin down the beads for 30\u00a0s at 500\u00a0\u00d7 g and remove the supernatant.\n          \nRepeat step 21.\n        Resuspend the bead pellet in 1\u00a0mL of bead blocking buffer.\n        \nAdd BSA to a final concentration of 1\u00a0mg/mL.\nIncubate the beads on a rotator for 14\u201318\u00a0h (4\u00b0C).\nDay 2\nTiming: 3\u20135 h\nNote: To minimize the loss of RNA, use low\n      binding 1.5\u00a0mL collection tubes for the following steps.\n    \nPre-cool a tabletop centrifuge to 4\u00b0C.\n        Spin down the BSA-blocked beads for 30\u00a0s at 500\u00a0\u00d7\u00a0g.\n        \nRemove the supernatant.\nResuspend each bead pellet in 500\u00a0\u03bcL of bead binding buffer.\n        Centrifuge RNA samples for 30\u00a0min at maximum speed (4\u00b0C). A small blue\n        pellet should be visible after the centrifugation step.\n        \nRemove the supernatant without disturbing the pellet.\n        Add 800\u00a0\u03bcL of 75% ethanol to each RNA pellet and vortex the samples for\n        5 s.\n        \nSpin down samples for 5\u00a0min at maximum speed (4\u00b0C).\n            Remove the supernatant and let the pellets dry with an open lid on\n            the bench.Remove residual traces of liquid by carefully flicking the tube.\n          \nNote: Make sure that the pellet is\n      completely dry before adding H2O.\n    \n        Add 100\u00a0\u03bcL of H2O (supplemented with RNase inhibitor) to each\n        pellet and put the samples on ice.\n      \nNote: We use SUPERaseIN RNase inhibitor at\n      a concentration of 20\u00a0U in 10\u00a0mL of H2O. You can use similar\n      products at a comparable concentration.\n    \nPreheat a thermo-mixer to 70\u00b0C.\n        Dissolve RNA pellets for 5\u00a0min on ice, and carefully pipette up and down\n        each sample.\n      \nIncubate samples for 5\u00a0min at 70\u00b0C.\n        Quickly transfer samples on ice and incubate for 2\u00a0min.\n        \n            Spin down samples for 5\u201310\u00a0s to collect all liquid at the bottom.\n          \n        Transfer each RNA sample (100\u00a0\u03bcL) to previously prepared tube containing\n        blocked anti-BrdU beads, and mix carefully by pipetting (total volume\n        \u223c600\u00a0\u03bcL).\n        \nIncubate samples on a rotor for 2\u00a0h at 4\u00b0C.\n        During this incubation time, prepare the Low-salt, High-salt,\n        Tris-EDTA-Tween and Elution buffer (without DTT), as specified in\n        Table\u00a04[href=https://www.wicell.org#tbl4].\n        table:files/protocols_protocol_3032_5.csv\n          Composition of buffers to purify and capture nascent RNA transcripts.\n        \n        Pre-chill the Low-salt, High-salt and Tris-EDTA-Tween buffers on ice.\n      \nKeep the Elution buffer at 20\u00b0C and add DTT just before use.\nNote: You can prepare buffers before\n      performing this protocol. However, Tween20-containing buffers should be\n      kept in the dark, and RNase-inhibitors as well as DTT should be freshly\n      added before use.\n    \n        Spin down the beads for 30\u00a0s at 500\u00a0\u00d7\u00a0g.\n        \n            Carefully remove the supernatant without disturbing the pellet.\n          \n        Start the washing steps by adding 500\u00a0\u03bcL of Low-salt buffer per sample,\n        and place tubes on a rotator for 5\u00a0min at 20\u00b0C.\n        \n            Spin down samples for 30\u00a0s at 500\u00a0\u00d7\u00a0g and remove the\n            supernatant.\n          \nRepeat step 38.Add 500\u00a0\u03bcL of High salt buffer per sample, and place tubes on a rotator\n        for 5\u00a0min at 20\u00b0C.\n        \n            Spin down samples for 30\u00a0s at 500\u00a0\u00d7\u00a0g and remove the\n            supernatant.\n          \nRepeat step 39.\n        Add 500\u00a0\u03bcL of Tris-EDTA-Tween buffer per sample, and place tubes on a\n        rotator for 5\u00a0min at 20\u00b0C.\n        \n            Spin down samples for 30\u00a0s at 500\u00a0\u00d7\u00a0g and remove the\n            supernatant.\n          \nRepeat step 40.\nAdd DTT to the Elution buffer and keep at 20\u00b0C.\nPrepare low binding tubes for the elution step.\n        Add 150\u00a0\u03bcL of Elution buffer to each sample and incubate on a rotator\n        for 5\u00a0min at 20\u00b0C.\n        \nSpin down samples for 30\u00a0s at 500\u00a0\u00d7\u00a0g.\nTransfer each eluate to a low binding tube.\n            Repeat the elution (step 43) two more times and combine eluates of\n            the same sample (total volume \u223c450\u00a0\u03bcL).\n          \nPre-chill a tabletop centrifuge to 4\u00b0C.\n        Add 500\u00a0\u03bcL of acid phenol-chloroform to each sample.\n        \nVortex samples for 30 s.\nCentrifuge samples for 5\u00a0min at 12000\u00a0\u00d7\u00a0g (4\u00b0C).\n            Transfer 400\u00a0\u03bcL of the aqueous phase (top) to a new low binding\n            collection tube.\n          \nDiscard tubes containing the bottom phase.\n        Add 400\u00a0\u03bcL (1\u00d7 volume) of chloroform.\n        \nVortex samples for 30 s.\nCentrifuge samples for 5\u00a0min at 12000\u00a0\u00d7\u00a0g (4\u00b0C).\n            Transfer 400\u00a0\u03bcL of the aqueous phase (top) to a new low binding\n            collection tube.\n          \nDiscard tubes containing the bottom phase.\n        Add 1\u00a0\u03bcL of Glyco Blue to each sample.\n        \nVortex vigorously for 10 s.\nNote: Here we use less Glycogen as\n      compared to the previous steps to avoid inhibition of the reverse\n      transcriptase.\n    \n        Add 1100\u00a0\u03bcL (2\u20133\u00d7 volume) of 100% ethanol per sample.\n        \nVortex vigorously for 15 s.\nIncubate samples for 14\u201318\u00a0h at \u221220\u00b0C.\nDay 3Timing: 2\u20134 h\nPre-chill a tabletop centrifuge to 4\u00b0C.\n        Spin samples for 30\u00a0min at maximum speed (4\u00b0C).\n        \nRemove the supernatant.\nDry pellets and remove residual liquid around the pellet.\nNote: You should see a small, light blue\n      pellet. If there is no pellet visible, the yield is probably too low to\n      continue.\n    \n        Carefully add 6\u00a0\u03bcL of H2O (supplemented with 1\u00a0\u03bcL of\n        SUPERaseIN per 10\u00a0mL) to each pellet and incubate tubes for 2\u00a0min on\n        ice.\n        \n            Vortex samples and spin down to collect the liquid at the bottom.\n          \nKeep samples on ice.\nNote: It is recommended to run a small\n      amount of each RNA sample (0.8\u00a0\u03bcL) on a Bioanalyzer instrument (Agilent)\n      to visually inspect the quality of the run-on, before proceeding to the\n      sequencing library preparation. An example of two run-on reactions\n      analyzed with a Bioanalyzer (Eukaryote Total RNA, RNA 6000 Pico Chip) are\n      shown in Figures\u00a02[href=https://www.wicell.org#fig2]C and 2D. To avoid\u00a0RNA degradation,\n      we recommend proceeding immediately to the 1st strand synthesis\n      step.\n    \n        Proceed with the preparation of the sequencing library according to the\n        manufacturer\u2019s manual.\n      \nNote: We generated the libraries for\n      sequencing using the TruSeq Stranded mRNA Library Prep Kit (Illumina).\n      Using a stranded sequencing kit is essential to determine strand-specific\n      transcriptional activities. Moreover, we quantified libraries with the\n      NEBNext Library Quant Kit (New England Biolabs). Barcoded samples were\n      pooled equimolarly and sequenced on the Illumina HiSeq4000 platform\n      (single-end 50\u00a0bp reads).\n    \nAnalysis of GRO-Seq and generation of output matrices\nTiming: 2\u20133\u00a0h per sample\n    \n      In this section, we process the GRO-Seq data to identify expressed or\n      differentially regulated genes.\n      Troubleshooting 2[href=https://www.wicell.org#troubleshooting]\u00a0&\n      Troubleshooting 3[href=https://www.wicell.org#troubleshooting].\n    \nCritical: The analysis was executed\n      and tested on an Ubuntu (version 20.04) virtual environment via Windows\n      Subsystem for Linux (WSL[href=https://learn.microsoft.com/en-us/windows/wsl/install], version 1).Before you start: Download the output files from the GRO-Seq and\n      install python[href=https://www.python.org/]. Using a package\n      manager like\n      anaconda[href=https://www.anaconda.com/products/distribution] with\n      useful pre-installed packages (including jupyter notebook) is highly\n      recommended. Create a new directory called \u201cCRISPRi\u201d for all the data\n      ($DIR from now on). Download the \u201cCleanup and guide picking.html\u201d,\n      \u201cOrdered csv.html\u201d, and \u201cMerge files.html\u201d files (Supplementary files) and\n      place them in the CRISPRi folder.\n    \n        In order to identify low quality regions in the generated reads, install\n        and run the tool FastQC.\n      \nNote: In general, reads classified with a\n      green flag are high-quality reads and will be used for the GRO-seq\n      analysis. In some cases, the reads classified with yellow flags require\n      further attention. In particular, rRNA contamination and other quality\n      issues should be analyzed, and if the quality of the reads is too low,\n      re-sequencing is necessary.\n    \n        To remove low-quality sequences identified in the previous step,\n        download and install the tool Trimmomatic.5[href=https://www.wicell.org#bib5]\nNote: The command line code used in this\n      step will depend on the issue identified by the FASTQC tool. Therefore, we\n      recommend to carefully read the documentation of Trimmomatic and correct\n      the reads accordingly.\n    \n        To map high-quality reads to the human reference genome with the STAR\n        aligner tool,2[href=https://www.wicell.org#bib6] download necessary files\n        to generate the reference genome:\n      \n$ wget\nhttps://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_XX/GRCh38.primary_assembly.genome.fa.gz.[href=https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_XX/GRCh38.primary_assembly.genome.fa.gz]\n$ wget\nhttps://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_XX/gencode.vXX.long_noncoding_RNAs.gtf.gz[href=https://ftp.ebi.ac.uk/pub/databases/gencode/Gencode_human/release_XX/gencode.vXX.long_noncoding_RNAs.gtf.gz].\n      \nGenerate the reference genome.\n$ STAR --runThreadN 12 \\ --runMode genomeGenerate \\ --genomeDir\n          GRCh38_star_index \\ --genomeFastaFiles\n          GRCh38.primary_assembly.genome.fa \\ --sjdbGTFfile\n          gencode.vXX.long_noncoding_RNAs.gtf \\ --sjdbOverhang 149\nNote: Here, we generate a genome with\n      lncRNA annotations only. If you would like to analyze coding genes as\n      well, repeat steps 55 and 56 with protein coding annotation files, which\n      will generate a protein coding reference genome.\n    \n        Align reads to the reference genome and generate a bam file containing\n        all mapped reads as output.\n        Troubleshooting 1[href=https://www.wicell.org#troubleshooting].$ STAR --runThreadN 12 \\ --readFilesIn ath_seed_sample.fastq \\\n          --genomeDir ath_star_index \\ --outSAMtype BAM SortedByCoordinate \\\n          --outFileNamePrefix seed_sample \\ --outSAMunmapped Within \\\n          --outWigType wiggle\n        Download and install the tool NRSA (Nascent RNA Sequencing Analysis)\n        available on:\n        https://github.com/vermeulenlab/lncRNA/[href=https://github.com/vermeulenlab/lncRNA/].\n      \nNote: The NRSA_guide.txt file includes\n      information about the download and installation of the NRSA tool. The NRSA\n      perl scripts Pause_PROseq.pl will be used for calculating the differential\n      expression of expressed genes. The NRSA contains processed reference files\n      from various organisms. In this protocol, we are using the processed\n      version of the GRC38 reference files which are the same files used for\n      mapping the GRO-seq reads. Ensure that the version of the genome used by\n      NRSA is consistent with the one to which the reads were aligned to in the\n      previous step.\n    \n        To generate output files, execute the following steps sequentially.\n      \nNote: In this example, the mapped reads\n      from two different conditions were named 'condition1.bam' and\n      'condition2.bam'. In the case of multiple BAM files corresponding\n      to different replicates, all BAM files should be added and separated by a\n      space (e.g. -in1 condition1_rep1.bam condition1_rep2.bam -in2\n      conditions2_rep1.bam condition2_rep2.bam).\n    \n$ perl ./bin/pause_PROseq.pl -o ./pause_out/ -in1\n          ./data/condition1.bam ./data/condition2.bam -in2 ./data/condition2.bam\n          -m GRCh38\n$ perl ./bin/eRNA.pl -w ./pause_out/ -in1 ./data/condition1.bam\n          ./dat/condition2.bam -in2 ./data/condition2.bam -m GRCh38\n        Obtain the output files gb_change.txt, which contain transcriptional\n        changes of genes .\n      \nNote: These output files can be imported\n      into R, Python, and Microsoft Excel for further analysis. Depending on\n      your experimental design, different cutoffs such as minimum read coverages\n      or specific fold changes (control vs treatment) can be used to define a\n      list of candidate lncRNAs.\n    \nVisualizing expression data and defining target regions\nTiming: 10\u201320\u00a0min per file\n    \nTiming: 2\u201310\u00a0min per candidate\n    \n      Here, we briefly outline the steps to visualize custom expression data onthe UCSC genome browser. We further detail how to define and target\n      genomic regions of interest.\n    \nNote: Hereafter, we only briefly describe\n      the generation of custom expression data tracks on the UCSC genome\n      browser. Detailed documentation can be found in the\n      User's Guide[href=https://genome.ucsc.edu/goldenPath/help/hgTracksHelp.html#CustomTracks]. Alternatively, the protocol using bigWig files can be found\n      here[href=https://genome.ucsc.edu/goldenPath/help/bigWig.html].\n    \n        Remove any \u201ctrack\u201d or \u201cbrowser\u201d lines from your .wig files output\n        obtained in the previous step.\n      \n        Generate bigWig files from your .wig files using the binary utilities\n        fetchChromSizes and wigToBigWig from the UCSC utilities.\n      \n        Host the bigWig files on a http(s) or ftp accessible location (e.g.,\n        github).\n      \n        Generate the tracks with the UCSC protocol for bigWig files linked\n        above.\n      \nNote: Here, we describe how to select\n      genomic regions of interest for efficient silencing with the CRISPRi\n      system.\n    \n        Open the output matrix obtained from the GRO-Seq data, and use the\n        ENSEMBL gene-IDs to visualize the regions on your UCSC track. A visual\n        example is depicted in Figure\u00a03[href=https://www.wicell.org#fig3].\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3032-Fig3.jpg\n              Figure\u00a03. Visualization of GRO-seq data with the UCSC genome\n              browser\n            \n              UCSC genome browser screenshot showing lncRNA transcripts\n              (green\\blue annotations) and sequencing tracks of 2 cell lines\n              (track 1 and 2). Read coverage on the sense (red) and antisense\n              (blue) strands are shown as separate tracks. The presence of\n              H3K27Ac (transcriptionally active chromatin) is also displayed.\n              Important features to select the region of interest are\n              highlighted; transcription start site peaks (green box),\n              transcript annotations (black box), and repeat sequences (orange\n              box).\n            \n        Zoom in on the TSS region and define a 400 nucleotides (nts) stretch by\n        selecting 50 nts upstream and 350 nts downstream of the transcription\n        initiation signal. An example is shown in Figure\u00a04[href=https://www.wicell.org#fig4].\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3032-Fig4.jpg\n              Figure\u00a04. Defining and retrieving target sequences\n            \n              (A) Zoomed in view showing the selection of a genomic regionencompassing 50 nucleotides before and 350 nucleotides after the\n              transcription start site. The \u201dView\u201d button is highlighted in\n              green.\n            \n              (B) The \u201dView\u201d dropdown menu, with the correct DNA option\n              selected.\n            \nNote: In Figure\u00a03[href=https://www.wicell.org#fig3], the\n      displayed transcript is encoded on the sense strand (red). The TSS can be\n      defined by locating the beginning of the read coverage for your gene of\n      interest. While annotations can help to pinpoint a region, significant\n      variations can be observed between previously annotated and observed TSS\n      in your specific cell system. When annotations and GRO-seq reads are not\n      perfectly aligned, we recommend to use your sequencing reads to determine\n      the TSS. In addition, most TSSs will also display some transcription\n      (initiation peak) on the opposite strand (here: blue), which is helpful to\n      pinpoint the right region.\n    \n        Following the \u201cZoom in\u201d on the region of interest, the DNA sequence is\n        retrieved by clicking on \u201cView\u201d and \u201cDNA\u201d (highlighted).\n        \n            On the new page, check the boxes \u201cMask repeats\u201d and \u201cto N\u201d, prior to\n            retrieving your DNA sequence by clicking \u201cget DNA\u201d.\n          \n        Copy the sequence and paste it in a new Notepad file, name it according\n        to the ENSEMBL Gene-ID as \u201cENSG00000XXXXXX.txt\u201d and save it in the\n        CRISPRi folder. It is imperative to save all the sequences separately to\n        properly run the guide design package.\n      \nNote: Following these steps, you will have\n      a CRISPRi folder filled with .txt files of all candidate TSS sequences\n      (\u221250 nts to\u00a0+350 nts\u00a0= 400 nt total), named by their ENSEMBL Gene-IDs.\n    \nCRISPRi screen\nDesign gRNAs\nTiming: 2\u20133\u00a0h to prepare the\n      packages/files and 5\u201315\u00a0min for the design of gRNAs (per candidate)\n    \n      Here we describe the design of guide RNAs (gRNAs) to epigenetically block\n      the transcription of selected genomic regions using CRISPRi.\n      Troubleshooting 2[href=https://www.wicell.org#troubleshooting]\u00a0&\n      Troubleshooting 3[href=https://www.wicell.org#troubleshooting].Download a human genome file by opening up a new terminal window and\n        running:\n      \ncd $DIR\nsudo wget\nhttp://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz[href=http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz].http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz[href=http://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz]\nNote: We recommend using hg38 (from\n      ENSEMBL) as reference genome and\n      bowtie[href=https://bowtie-bio.sourceforge.net/manual.shtml] to\n      create an index. To design gRNAs and assess their specificity to one\n      genomic region, we use CCTop4[href=https://www.wicell.org#bib8],11[href=https://www.wicell.org#bib7] and BLAT, respectively.\n    \n        Unpack the archive to get the file \u201chg38.fa\u201d. Then, we install all the\n        packages via the terminal, starting with bowtie:\n      \nsudo gunzip hg38.fa.gz\nsudo apt-get install -y bowtie\nThen, to install CCTop, run:\npip install CCTop.\n        The installation of BLAT is slightly complex and uses a C compiler to\n        make and make install the packages. If you don\u2019t have an\n        existing C compiler on your computer, run:\n      \nsudo apt install build-essential.\nDownload and install a library that BLAT uses:\nwget\nhttp://downloads.sourceforge.net/project/libpng/libpng16/older-releases/1.6.2/libpng-1.6.2.tar.gz[href=http://downloads.sourceforge.net/project/libpng/libpng16/older-releases/1.6.2/libpng-1.6.2.tar.gz]\ntar -xvzf libpng-1.6.2.tar.gz\ncd libpng-1.6.2\n./configure\nmake\nsudo make install\n        Go back to $DIR, download the BLAT source code archive, unpack it, and\n        move all the library files to the /lib/ folder within the BLAT folder:\n      \ncd ..\nwget\nhttp://users.soe.ucsc.edu/\u223ckent/src/blatSrc.zip[href=http://users.soe.ucsc.edu/%7Ekent/src/blatSrc.zip]\nunzip blatSrc.zip\ncp ./libpng-1.6.2/png.h ./blatSrc/lib/\ncp ./libpng-1.6.2/pngconf.h ./blatSrc/lib/\ncp ./libpng-1.6.2/pnglibconf.h ./blatSrc/lib/\n        Finally, you have to configure some variables to allow BLAT to run from\n        the terminal:\n      \necho $MACHTYPE\nMACHTYPE=x86_64\nexport MACHTYPE\nsudo mkdir -p \u223c/bin/$MACHTYPE\nmake\necho 'export MACHTYPE=x86_64' >> \u223c/.bashrc\necho 'export PATH=$PATH:\u223c/bin/$MACHTYPE' >>\n          \u223c/.bashrc\nsource \u223c/.bashrc\nRun BLAT from the terminal by typing blat in the terminal.\n        Open the \u201c.profile\u201d file in your /home/username/ directory in\n        notepad and add the following line at the end to ensure you can always\n        use blat from the terminal, and use bash loops:\n      \nexport PATH=$PATH:\u223c/bin/$MACHTYPE\n        Create a bowtie index file from the human genome for CCTop. This should\n        output multiple .ebwt files. Execute the following code in $DIR:\n      \nbowtie-build -r -f hg38.fa humanRun CCTop to design CRISPRi gRNAs on one of our candidate sequences\n        (\u201c$FILE.txt\u201d):\n      \ncctop --input $FILE.txt --targetSize 19 --index human\nYou can also run all the files automatically by running:\nfor SAMPLE in \u2217.txt; do cctop --input ${SAMPLE} --targetSize 19\n          --index human; done\n        The output should consist of 3 files per input candidate: a .bed file\n        with gRNA locations along your input sequence, a .fasta file with all\n        designed gRNA sequences, and a .xls file with extra information,\n        including the CRISPRater score for each gRNA.\n      \n        Create new directories for the different output files and move all files\n        to these directories. In addition, the hg38.fa file is required for BLAT\n        and should therefore be moved to the fasta folder as well:\n      \nsudo mkdir fasta\nsudo mkdir xls\nsudo mv \u2217.fasta ./fasta\nsudo mv \u2217.xls ./xls\nsudo mv hg38.fa ./fasta\nNote: We highly recommend to create a\n      11.ooc file before running BLAT, as it will significantly speed up your\n      query. If you want to do so, run:\n    \ncd fasta\nblat hg38.fa /dev/null/ /dev/null/ -makeOoc=hg38.fa.11.ooc\n          -repMatch=1024\n        Blat all files by making a new output directory and by running:\n        \nsudo mkdir Blat\nfor i in \u2217\".fasta\"; do echo \"{$i}\";\n              name=${i%%.fasta}; blat hg38.fa \"${i}\"\n              -ooc=hg38.fa.11.ooc -minIdentity=90 -minScore=15 -minMatch=1\n              -oneOff=1 -out=blast8 ./Blat/\"${name}\".psl; done\n            This should generate a .psl file, which includes all gRNAs targeting\n            a specific candidate region. Every file will be named according to\n            their specific ENSEMBL gene-ID.\n            \nNote: We recommend to covert the\n              .xls output file into a .csv file, which facilitates subsequent\n              steps in python. To this end, we use the libreoffice-calc package\n              from the terminal.\n            \nInstall the libreoffice-calc package if necessary.\nsudo apt install libreoffice-common\nsudo apt install libreoffice-calc\n        Convert the .xls files to .csv from the \u201cxls\u201d folder, and move them back\n        to the main directory.Note: libreoffice can only handle 250\n      files at a time, so if you have more candidates, move them to separate\n      folders and execute the code in that folder.\n    \nContinue from step 83:\ncd ..\ncd xls\nlibreoffice --headless --convert-to csv \u2217.xls\nsudo mv \u2217.csv ..\n(If running from separate folders for >250 candidates, use sudo mv\n          \u2217.csv ../..)\nNote: To rearrange the .csv file, we\n      recommend to create a folder called \u201cClean\u201d, and open the \u201cOrdered\n      csv.html\u201d file. Execute this code in jupyter notebook, and make sure to\n      substitute all the correct file paths (\u201c./\u201d is the current directory of\n      the .ipynb file).\n    \nStart jupyter notebook by running the following from $DIR:\njupyter notebook\nNote: If you are running Ubuntu via WSL,\n      add the command --no-browser.\n    \n        Now you should have an organized .csv file in the \u201cClean\u201d folder for\n        every candidate, as well as a BLAT .psl file in the \u201cfasta/Blat/\u201d\n        folder.\n      \n        Create a new folder called \u201cFinal\u201d in the CRISPRi main folder and run\n        the code from \u201cCleanup and guide picking.html\u201d in jupyter notebook.\n      \nNote: Make sure to substitute all the\n      correct file paths and to adapt the \u201cpreseq\u201d and \u201dpostseq\u201d variables\n      according to the homology arm sequences present in your selected gRNA\n      vector.\n    \n      The end result is the \u201cMerged.csv\u201d file, containing all selected gRNA\n      sequences, both with and without homology arms (\u201cOrderSeq\u201d).\n    \nStable genomic integration of KRAB-dCas9\nTiming: 5\u00a0days\n      Here we describe the transduction of cell lines to stably integrate an\n      inducible CRISPRi system. In this protocol, we use an inducible KRAB-dCas9\n      system or the TRE-KRAB-dCas9-IRES-BFP plasmid (Addgene #85449). This\n      construct allows for dox-dependent expression of KRAB-dCas9 and BFP, which\n      enables a more controlled setting as compared to constitutively active\n      promoters. The dox-dependent induction of KRAB-dCas9 and BFP requires the\n      co-expression of the reverse tetracycline transcriptional activator(rtTA). Stable integration and constitutive expression of rtTA is achieved\n      using pHR-EF1Alpha-Tet-on 3G (Plasmid #118592). Cell populations that\n      properly induce KRAB-dCas9/BFP (cells co-transduced with KRAB-dCas9/BFP\n      and rtTA) are enriched by FACS sorting. For an overview of all preparatory\n      steps, see Figure\u00a06[href=https://www.wicell.org#fig6]A.\n    \nCritical: This protocol assumes a\n      basic understanding of the production of lentiviral particles (and\n      associated biosafety procedures) and 2D cell culturing. A general protocol\n      to generate lentiviral particles can be obtained\n      here[href=https://www.addgene.org/protocols/lentivirus-production/].\n    \nDay 1\nTiming: 1 h\n        Split cells of interest and seed approximately 4\u20136 x105 cells\n        (\u223c70%\u201380% confluent) in 1\u00a0mL of medium per well of a 6-well plate.\n      \nNote: The amount of cells depends on the\n      growth rate and size of your cells.\n    \n        Thaw the pre-generated viruses (filtered 0.45\u00a0\u03bcm) OR harvest fresh\n        lentiviruses containing KRAB-dCas9/BFP and rtTA.\n        \n            Add 1\u00a0mL of virus to the cell suspension and polybrene at a final\n            concentration of 4\u00a0\u03bcg/mL.\n          \nCentrifuge the plate for 30\u00a0min at 500\u00a0\u00d7\u00a0g (32\u00b0C).\n            Place the plate in a cell culture incubator for 14\u201318\u00a0h at 37\u00b0C.\n          \nNote: In our experience, using 1mL of\n      lentiviral supernatant (not concentrated) is sufficient to obtain a\n      transduction efficiency that ranges between 1% and 10%. However,\n      concentrating lentiviral supernatants may be necessary to efficiently\n      co-transduce certain cell types.\n    \nDay 2\nTiming: 30\u00a0min\nRefresh the medium.\nDay 3\u20135\nTiming: 30\u00a0min\n        Transfer cells into a bigger vessel when the confluency reaches >90%.\n      \n        Maintain and expand cells at an appropriate confluence in at least 2\n        separate cell culture vessels.\n      \nEnriching cells harboring the inducible CRISPRi system\nTiming: Several weeks\n    \n      This section describes the selection of transduced cells by fluorescence\n      activated cell sorting (FACS). Cells co-transduced with\n      TRE-KRAB-dCas9-IRES-BFP plasmid (Addgene #85449) and EF1Alpha-Tet-on 3G\n      (Plasmid #118592) are dox-treated and BFP+ cells are sorted. To graduallyenrich cells that properly induce KRAB-dCas9/BFP, multiple FACS sorting\n      cycles are required. In each cycle, cells are dox-treated, FACS sorted\n      (BFP positive cells sorted) and cells expanded (without dox). Enrichment\n      cycles are performed until the vast majority of cells (>95%) can induce\n      the BFP expression in a dox-dependent manner. In addition, leaky cells can\n      be removed by sorting out BFP positive cells in absence of dox.\n    \nNote: Our experimental setup uses a\n      construct that allows the selection of cells by a fluorescent marker.\n      Other constructs may contain distinct selection markers and may require a\n      different selection approach.\n    \nDay 6\nTiming: 5\u00a0min\n        Add dox to the culture medium (final conc. 1\u00a0\u03bcg/mL).\n        \n            Incubate the cultures at least 24h to induce the BFP expression.\n          \n            Keep one cell culture vessel without dox to use as control sample\n            for the FACS.\n          \nNote: In this protocol, we use 1\u00a0\u03bcg/mL of\n      dox in the culture medium to induce the CRISPRi system. However, different\n      cell lines may show different sensitivity to doxycycline, which may\n      require titration experiments to establish ideal experimental conditions.\n    \nDay 7\nTiming: 2\u20133 h\n        Trypsinize and collect cells into a 15\u00a0mL tube.\n        \nSpin down for 5\u00a0min at 500\u00a0\u00d7\u00a0g.\n            Remove the supernatant and resuspend the cell pellet in 1\u20132\u00a0mL of\n            medium (depending on pellet size).\n          \n            Prepare a 15\u00a0mL collection tube for sorting with 2\u00a0mL of medium.\n          \nNote: In this protocol, we performed cell\n      sorting using the SH800 Cell Sorter (Sony). Settings may differ if other\n      cell lines and other FACS machines are used. An untreated control sample\n      (no dox) is essential to properly gate BFP+ cells.\n    \n        Determine the gating. An exemplary gating strategy is depicted in\n        Figures\u00a05[href=https://www.wicell.org#fig5]B\u20135F.\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3032-Fig5.jpg\n              Figure\u00a05. Generation of dox-inducible KRAB-dCas9-BFP cells\n            \n              (A\u2013F) (A) Schematic illustrating steps to generate CRISPRi cellline using FACS sorting. Gating strategy is shown in (B)\u00a0and (C).\n              Enrichment of BFP+ cells (dark blue) following 1 (D), 3 (E)\u00a0and 8\n              (F)\u00a0sorting cycles. Uninduced cells (light blue) were used as\n              negative control.\n            \n            To gate living cells, use the forward (FSC) and backward scatter\n            (BSC, equivalent to side scatter SSC) area (A).\n          \n            Select singlets by using an FSC-height and width density plot.\n          \nBFP+ cells are sorted using the 405\u00a0nm laser.\n        Sort the BFP+ cell population into the prepared collection tube.\n        \n            Transfer sorted cells back into an appropriate cell culture vessel\n            depending on the number of sorted cells.\n          \nDay 8\u201314\nTiming: 2\u20135 h\nCulture and expand the sorted cells accordingly.\nNote: Depending on the transduction\n      efficiency and proliferation rate of your cell line, dox-induction and\n      sorting steps may be performed with an alternative or better suited\n      time-schedule. Alternatively, single cell (BFP+) clone could be sorted and\n      expanded. However, molecular features of single cell clones may not\n      properly represent the complexity or heterogeneity of the initial\n      population.\n    \nDay 15-day x\nTiming: Several weeks\n    \n        Repeat steps 95 to 99 until you obtain a homogeneous population (>95%\n        BFP\u00a0+ cells; Figures\u00a05[href=https://www.wicell.org#fig5]D\u20135F).\n      \nNote: We recommend testing the\n      functionality of your CRISPRi cell line, by targeting an essential gene.\n      This allows to validate the transcriptional repression of a target gene\n      and assess potential cellular phenotypic changes.\n    \nSetting up and performing a CRISPRi dropout screen\n      In this section we discuss how to perform a CRISPRi-based dropout screen\n      with adherent cells. An overview of the CRISPRi-dropout screen is shown in\n      Figure\u00a06[href=https://www.wicell.org#fig6].\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3032-Fig6.jpg\n          Figure\u00a06. CRISPRi dropout screen layout\n        \n          The cartoon shows key steps required to perform a CRISPRi dropout\n          screen. Following the transduction of a gRNA library in a CRISPRi cellline (KRAB-dcas9 BFP cells), cells are treated or not with dox and\n          collected at various time points (d0, T1 and T2). Sequencing is\n          performed to evaluate the relative abundance of each gRNA across\n          collected samples.\n        \n      The gRNA library is first cloned into a suited backbone vector, which\n      allows for the generation of lentiviral particles. Then, cells containing\n      the inducible KRAB-dCas9-BFP are transduced with lentiviral particles\n      containing the gRNAs. This should be performed at a low MOI to minimize\n      the presence of multiple integrations per cell. This section describes the\n      titration to obtain a low MOI.\n    \nCritical: This section assumes a basic\n      understanding of cloning. We followed the protocol by Wang et\u00a0al.12[href=https://www.wicell.org#bib9]\n      to clone our gRNA library into a vector backbone and do not further\n      describe these steps here nor are these considered in the estimated\n      timing. We assume a general understanding of lentiviral particles\n      production (and associated safety procedures) and 2D cell culturing.\n      Troubleshooting 4[href=https://www.wicell.org#troubleshooting].\n    \nNote: We used the pDECKO-mCherry backbone\n      (Addgene #78534) to clone our gRNA library, a vector that contains the\n      mCherry fluorescent protein as well as a puromycin resistance gene for\n      selection. We use the live cell labeling with mCherry to evaluate the\n      transduction efficiency by flow cytometry and to adjust the conditions\n      (e.g., cell density and amount of virus) to obtain a low MOI. Then, we\n      select transduced cells by adding puromycin. Selection of cancer cell\n      lines is usually completed within 2\u00a0weeks. We highly recommend using both\n      the antibiotic-based selection as well as a fluorescent marker to easily\n      control and complete these steps. Prior to transduce CRISPRi cell lines,\n      the pDECKO-gRNA library DNA preparation can be PCR amplified (see section\n      \u201cPreparing sequencing libraries\u201d) and sequenced to validate the presence\n      of each gRNA (optional).\n    \nDay 1\nTiming: 1 hHarvest cell cultures and seed 5\u00a0\u00d7\u00a0106 cells per T75 flask.\n        \nPrepare at least 4 flasks.\nKeep one without virus as control.\nThaw pre-generated virus.\nNote: Number of cells may vary depending\n      on the features (e.g., size and proliferation) of your cells of interest.\n      In general, 70%\u201380% seeding confluency will yield robust and reproducible\n      results. We usually generate a big volume of viral particles containing\n      the gRNA library to avoid multiple titrations and minimize possible batch\n      effects. We filter, aliquot and store the virus suspension at \u221280\u00b0C.\n      Lentiviral particles can be stored several months without significant\n      impacts on the transduction efficiency.\n    \n        Add different amounts of virus per flask, e.g., 100, 400 and 800\u00a0\u03bcL of\n        virus.\n      \nAdd polybrene at a concentration of 4\u00a0\u03bcg/mL to each flask.\nPlace the flasks for 14\u201318\u00a0h at 37\u00b0C in a cell culture incubator.\nDay 2\nTiming: 30\u00a0min\nRefresh the medium.\nDay 3\nTiming: 2\u20133 h\n        Harvest and collect cells in tubes suited for flow cytometry analysis.\n      \nNote: We acquire the samples on the SH800\n      Cell Sorter (Sony) to assess transduction efficiencies. Settings may\n      differ if other cell lines, plasmids and FACS machines are used.\n    \n        Setup the gating strategy as mentioned in step 97.\n        \n            Visualize mCherry+ cells by using the 561\u00a0nm laser. An untransduced\n            control sample is required to properly gate mCherry+ cells.\n          \n            Examples of transduction efficiencies following the incubation of\n            CRISPRi-engineered cells with an increasing number of lentiviral\n            particles harboring the gRNA library (mCherry+) is shown in\n            Figure\u00a07[href=https://www.wicell.org#fig7].\n            Troubleshooting 7[href=https://www.wicell.org#troubleshooting].\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3032-Fig7.jpg\n                  Figure\u00a07. Titration of viral particles harboring the gRNA\n                  library\n                \n                  CRISPRi-engineered cells were either untransduced (ctrl) or\n                  transduced with an increasing amount of lentiviral particles\n                  (100, 400 and 800\u00a0\u03bcl). The transduction efficiency (percentage\n                  of mCherry+ cells) is displayed for each condition by plottingthe mCherry signal (x-axis) against the BrilliantViolet signal\n                  (y-axis).\n                \nTransduction and selection of the gRNA library\n      Here we detail the steps leading to the transduction and selection of\n      cells harboring the gRNA library.\n    \n      The library is transduced with low MOI (<0.5) and cells are selected\n      with the appropriate marker (e.g., Puromycin or Blasticidin). The screen\n      is usually initiated with a gRNA coverage of 500\u00d7 (e.g., 1,000 gRNA\n      library requires >500,000 cells) for each replicate and in presence or\n      absence of dox. We recommend harvesting several time points to follow\n      dropouts over time.\n    \nCritical: To ensure a good\n      representation of each gRNA, we usually transduce the cells of interest\n      with a 200\u00d7 coverage. Screens can also be initiated with a coverage of at\n      least 200\u00d7. However, we do recommend using a coverage of 500\u00d7 or more if\n      possible.\n    \nDay 4\nTiming: 1\u20132 h\n        Seed a sufficient amount of T75 flasks with each \u223c5\u00a0\u00d7\u00a0106\n        cells (70%\u201380% confluent).\n      \nNote: The required number of flasks should\n      be determined based on the size of your library and the titration of your\n      lentiviral suspension. We usually aim to cover every gRNA 200 times with a\n      transduction efficiency varying between 1% to 10%.\n    \nThaw pre-generated virus.\n        Add an adequate amount of virus per flask to obtain a transduction\n        efficiency ranging between 1% to 10% based on previous titration (e.g.,\n        800\u00a0\u03bcL would achieve \u223c5% mCherry+ cells in\n        Figure\u00a07[href=https://www.wicell.org#fig7]).\n      \nAdd polybrene at a concentration of 4\u00a0\u03bcg/mL to each flask.\nPlace the flasks for 14\u201318\u00a0h at 37\u00b0C in a cell culture incubator.\nDay 5\nTiming: 30\u00a0min\nRefresh the medium.\nDay 6\nTiming: 1 h\n        Maintain and expand your cells at an appropriate confluency.\n        \n            Transfer cells into a bigger vessel when they reach confluency.\n          \nDay 7\nTiming: 1 hStart the selection of transduced cells by adding puromycin to the\n        culture medium.\n      \nNote: For CRC cell lines, we used\n      concentrations ranging between 1 and 5\u00a0\u03bcg/mL of puromycin, depending on\n      their sensitivity. Alternatively, determine the amount of puromycin needed\n      to kill untransduced cells by performing a dose-response experiment.\n      Selection of cancer cell lines is usually completed within 2\u00a0weeks.\n    \nDay 8\u00a0\u2013 X\nTiming: 1\u20134\u00a0weeks\n    \n        Maintain and expand the cell cultures at an appropriate confluency.\n      \n        Keep selecting cells with puromycin until the cell population is\n        color-labeled (> 95% mCherry+).\n      \nVerify the successful selection by flow cytometry.\nNote: When maintaining the cell culture,\n      it is important to preserve the complexity of the gRNA library by seeding\n      cells with at least 200\u00d7 coverage.\n    \nDropout screen initiation and time point collection\nTiming: 7\u201330\u00a0days, depending on\n      experimental setup and harvesting time points.\n    \n      Here we describe the conditions and steps required to initiate and\n      complete a CRISPRi dropout screen.\n    \nCritical: We initiate our CRISPRi\n      screens with >500\u00d7 coverage and recommend to use at least 200\u00d7 for each\n      replicate. We also advise to freeze a stock of cells containing your gRNA\n      library that contains at least a 200\u00d7 coverage per vial. However,\n      attention should be paid during the thawing step, as important cell death\n      may impact the complexity and therefore skew the representation of gRNAs.\n    \nNote: To monitor cell population dynamics,\n      we recommend harvesting 3 time points (e.g., 0, 12 and 20\u00a0days).\n    \nDay 1\nTiming: 1\u20132 h\n        Harvest all the library-transduced cells and collect them in a 50\u00a0mL\n        falcon.\n        \nCount the concentration of cells.\nNote: For better accuracy, ensure that\n      trypsinized cells are fully singularized before counting.\n    \n        For the \u201cday 0\u201d samples, collect in 3 different tubes (3 replicates) anumber of cells corresponding to at least 200\u00d7 the size of the gRNA\n        library.\n        \n            Centrifuge the cells for 5\u00a0min at 500 \u00d7 g and remove the\n            supernatant.\n          \nStore cell pellets at \u221220\u00b0C.\n        For the screen, seed an adequate number of cells and culture vessels to\n        reach a library coverage of at least 200\u00d7 for each replicate.\n        \n            Choose ideal culture vessel and cell seeding density to accommodate\n            4\u00a0days of cell expansion without reaching confluency (>95%).\n          \nNote: Depending on the size of your\n      library or properties of your cell line, it might be necessary to seed\n      cells in multiple plates per replicate. We recommend performing 3\n      replicates in parallel with and without dox to obtain a robust statistical\n      analysis of the screen.\n    \n        Add dox to a final concentration of 1\u00a0\u03bcg/mL (or use the ideal dox\n        concentration for your cell line as mentioned above) to the 3 induced\n        (+) replicates.\n        \nMaintain the other 3 replicates untreated (-).\nLabel the culture vessels appropriately.\nPlace them back in a cell culture incubator at 37\u00b0C.\nDay 3\nTiming: 10\u00a0min\n        Refresh the medium of all culture vessels.\n        \n            Add dox (1\u00a0\u03bcg/mL) to induced (+) culture vessels to maintain a\n            sufficient expression of KRAB-dCas9.\n          \nDay 5\nTiming: 2\u20133 h\nNote: To maintain viable conditions for\n      the cells, we replate cultures every 4\u00a0days over the course of the screen.\n    \n        Trypsinize cells.\n        \nCollect each replicate in separate and labeled 50\u00a0mL tubes.\nCount the cells.\n            Seed back cells into new culture vessels as performed in step 122.\n          \nAdd 1\u00a0\u03bcg/mL dox to the medium of induced (+) culture vessels.\n        Repeat step 124 every 2\u00a0days and step 125 every 4\u00a0days until the first\n        time point (T1) is reached.\n      \nDay X: First time point (T1)\nTiming: 2\u20134 h\n        To collect timepoint samples, trypsinize cells.Collect each replicate in separate and labeled 50\u00a0mL tubes.\nCount the cells.\n        For each replicate, transfer a number of cells corresponding to at least\n        200\u00d7 the size of the gRNA library to a labeled 15\u00a0mL tube.\n        \nCentrifuge cells for 5\u00a0min at 500\u00a0\u00d7\u00a0g.\nRemove the supernatant.\nFreeze the pellets at \u221280\u00b0C.\n        If another time point (TX) is harvested, seed back cells into new\n        culture vessels as performed in step 122.\n      \n        Repeat step 124 every 2\u00a0days and step 125 every 4\u00a0days until the next\n        time point (TX) is reached.\n      \nDay Y: Next time point\nPerform steps 127 to 130.\nPreparation of sequencing libraries\nTiming: 2\u00a0days\n      To prepare sequencing libraries, we PCR-amplify gRNAs (PCR1) from gDNA\n      obtained from collected samples. In a second amplification step (PCR2),\n      adapters and indices are added to perform standard Illumina multiplex\n      sequencing. PCR primers used to generate sequencing libraries are shown in\n      Table\u00a0S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3032-Mmc2.xlsx].\n    \nNote: We do not describe the steps\n      required for the library quantification. However, a detailed protocol can\n      be found online at the manufacturer\u2019s page:\n      NEBNext\u00ae Library Quant Kit for Illumina\u00ae[href=https://www.neb.com/en/products/e7630-nebnext-library-quant-kit-for-illumina#Product%20Information].\n    \n        Extract the genomic DNA from each cell pellets with Genomic DNA\n        Purification Kit or a similar product.\n      \n        PCR-amplify gRNA sequences using PCR1 primers, which are specific to the\n        vector backbone.\n      \nNote: To properly amplify the complexity\n      of a library by PCR, each gRNA should be represented by at least 200\n      genomes (one integration per genome). Considering that the human diploid\n      genome is approximately 6.5 pg, then a library of 10,000 gRNAs requires to\n      PCR-amplify \u223c13\u00a0\u03bcg of gDNA (200\u00a0\u00d7\u00a010,000\u00a0\u00d7\u00a06.5pg).\n    \n        PCR-amplify amplicons from PCR1 with PCR2 primers, in order to add\n        standard Illumina adapters.\n      \nNote: The second PCR is also used to add\n      indices to samples, thus allowing multiplex sequencing. For details, seeTable\u00a0S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3032-Mmc2.xlsx].\n    \n        Purify amplicons obtained from PCR2 from the gel using the\n        NucleoSpin Gel\u00a0& PCR Clean-up kit[href=https://www.bioke.com/webshop/mn/740609.html]\n        or similar.\n      \n        Quantify the concentration of all libraries, using the NEBNext\u00ae Library\n        Quant Kit for Illumina\u00ae.\n      \n        Dilute samples to the desired concentration (usually between 20\u201350\u00a0nM)\n        and pool all libraries (if multiplex sequencing is performed).\n      \nNote: To improve accuracy, we recommend an\n      initial dilution of libraries at a higher (2\u00d7) concentration (40\u2013100nM).\n      Quantification is then repeated on the \u201c2\u00d7\u201d diluted samples before the\n      final dilution.\n    \nSequence all samples using next-generation sequencing.\nNote: Using single-end 50 bp sequencing\n      reads (SE50) is sufficient to retrieve your gRNA sequences. We also\n      recommend adding 5% phix and to cover each gRNA with at least 200 reads.\n    \nScreen analysis\nTiming: 30\u00a0min to install and prepare,\n      and 30\u201360\u00a0min per 30 million reads (for step 140)\n    \nTiming: 30\u201360\u00a0min per 30 million reads\n      (for step 147)\n    \n      Here we describe how to analyze the sequencing data obtained from a\n      CRISPRi dropout screen. We use the\n      MAGeCK[href=https://github.com/liulab-dfci/MAGeCK] package to\n      count the reads and\n      DESeq2[href=https://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html]\n      package for subsequent analyses. Specific cutoffs and filters are also\n      applied to the data to produce a list of dropout candidates.\n      Troubleshooting 2[href=https://www.wicell.org#troubleshooting]\u00a0&\n      Troubleshooting 3[href=https://www.wicell.org#troubleshooting].\n    \nCritical: The analysis is executed and\n      tested on an Ubuntu (version 20.04) virtual environment via Windows\n      Subsystem for Linux (WSL[href=https://learn.microsoft.com/en-us/windows/wsl/install], version 1).\n    \nBefore you start: Download the .fastq files generated by the\n      dropout screen, and place them in a convenient directory (\u201c$DIR\u201d from now\n      on). If you performed paired-end sequencing, only the forward reads (_R1\n      or similar) are necessary, therefore place the reverse reads (_R2 or\n      similar) in a separate folder in $DIR. Download and install R (The Comprehensive R Archive Network[href=https://cran.r-project.org/]\n      (r-project.org[href=https://cran.r-project.org/])) on your system.\n      We highly recommend R studio[href=https://posit.co/] for writing\n      and executing R codes.Prior to obtaining the read counts for each gRNA, adapter sequences\n      flanking the gRNA insert must be trimmed. The following steps are adapted\n      from the documentation of\n      Trimmomatic[href=https://apolo-docs.readthedocs.io/en/latest/software/applications/trimmomatic/trimmomatic-0.36/index.html].\n    \n        To install Trimmomatic, execute the following commands in a terminal\n        window:\n      \ncd $DIR\nwget\nhttp://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip[href=http://www.usadellab.org/cms/uploads/supplementary/Trimmomatic/Trimmomatic-0.36.zip]\nunzip Trimmomatic-0.36.zip\nsudo mkdir ./Trimmed\nNote: In order to run Trimmomatic, Java\n      should be installed on your system. If Java is not present, run:\n    \nsudo apt install default-jre\nsudo update-alternatives --install \"/usr/bin/java\"\n          \"java\"\njava\u00a0\u2013version\n        Identify the gRNA flanking sequences by running the following code on\n        one of your .fastq output files (\u201c$FILE\u201d)\n      \ncd $DIR\nhead $FILE.fastq\nExample output:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3032-Fx5.jpg\nNote: Adapter sequences before (yellow)\n      and after (green) the gRNA are highlighted for clarity.\n    \n        Count the length of the adapter that prepends the gRNA inserts (in this\n        case, 22 nucleotides).\n      \n        Pre-process the reads by running the following command.\n        \nReplace $FILE with your filenames\n            Replace the numbers following \u201c-threads\u201d and \u201cHEADCROP:\u201d with your\n            specific number of cores and the length of the adapter determined in\n            step 142, respectively:\n          \njava -jar ./Trimmomatic-0.39/trimmomatic-0.39.jar SE -threads 16\n          $FILE_trim.fastq ./Trimmed/$FILE_Trim.fastq HEADCROP:22 CROP:19\n        Run all the files automatically by executing a bash loop in the\n        terminal.\n      \nfor SAMPLE in \u2217.fastq; do java -jar\n          ./Trimmomatic-0.39/trimmomatic-0.39.jar SE -threads 16 ${SAMPLE}.fastq\n          ./Trimmed/${SAMPLE}_Trim.fastq HEADCROP:22 CROP:19; done\nVerify successful cropping of the reads by running:\ncd ./Trimmed\nhead $FILE_Trim.fastq\nOutput:\nA00379:586:HCLLHDSX3:4:1101:14290:1000\n          1:N:0:GGTAGCATCT+TGGTCAGTGT\nCTGGCGTCCGGTGTGCAGG\n+\nFFFFFFFFFFFFFFFFFFF\n@A00379:586:HCLLHDSX3:4:1101:24035:1000\n          1:N:0:GGTAGCATCT+TGGTCAGTGT\nACAGGCCCAGCACACTCTC\n+\nFFFFFFFFFFFFFFFFFFF\n@A00379:586:HCLLHDSX3:4:1101:27941:1000\n          1:N:0:GGTAGCATCT+TGGTCAGTGT\nATCTCCATCTGCCTACCTT\nNote: The result is a cropped version,\n      containing only the guide sequence.\n    \n        Next, we use MAGeCK to count the reads. To install MAGeCK, open a new\n        terminal window and run:\n      \nsudo wget\nhttps://sourceforge.net/projects/mageck/files/0.5/mageck-0.5.9.4.tar.gz[href=https://sourceforge.net/projects/mageck/files/0.5/mageck-0.5.9.4.tar.gz]\ntar xvzf mageck-0.5.9.4.tar.gz\ncd mageck-0.5.9.4\npython setup.py install\nNote: Other packages similar to MAGeCK\n      (e.g. BAGEL13[href=https://www.wicell.org#bib10] and JACKS14[href=https://www.wicell.org#bib11]) could also be used to perform this step.Test by typing mageck in the terminal, which should output a help prompt\n        for using MAGeCK.\n      \n        Prepare the library file by opening a new excel sheet and adding three\n        columns. An example is shown in Table\u00a05[href=https://www.wicell.org#tbl5].\n        table:files/protocols_protocol_3032_6.csv\n          Labeling: \u201cseqID\u201d is an identifier for the guide; \u201cgRNAs\u201d is the guide\n          sequence; \u201ctargetgene\u201d is the gene targeted by the gRNA.\n        \nSave this excel sheet as a .csv in the /$DIR/Trimmed directory.\n        To count all the files, run the following command in the /$DIR/Trimmed\n        directory ($FILE is the library file.\n      \nmageck count\u00a0\u2013l $FILE.csv --fastq \u2217.fastq\n        This should output multiple files, including .count.txt and\n        .count_summary.txt. Troubleshooting 5[href=https://www.wicell.org#troubleshooting].\n      \nNote: We recommend to review the counting\n      process by constructing a volcano plot from your count data (see\n      Figure\u00a08[href=https://www.wicell.org#fig8]). It is important to verify that the negative\n      controls are not dropping out, while the positive controls are. To\n      generate volcano plots, we recommend using the R package EnhancedVolcano.\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3032-Fig8.jpg\nFigure\u00a08. Dropout screen analysis\n          (A\u2013C) The dropout fold changes (log2 FC) of (A)\u00a0negative controls and\n          positive controls, and (B)\u00a0candidate-targeting gRNAs (endpoint vs. day\n          0) are plotted against the p-value (log10 p-value). Plot (C)\u00a0shows the\n          overlay of plots (A)\u00a0and (B).\n        \nIdentifying dropout hits\nTiming: 1\u00a0h (for step 153)\n    \n      In this section we describe a general approach to identify potential\n      dropout hits. To this end, we compare the relative presence of each gRNA\n      in induced (+) and untreated (-) samples, using the\n      DESeq2[href=https://bioconductor.org/packages/release/bioc/vignettes/DESeq2/inst/doc/DESeq2.html]\n      package in R. Troubleshooting 6[href=https://www.wicell.org#troubleshooting].\n    \n        Install and load DESeq2 in R via Bioconductor by running the following\n        commands:\n      \nif (!require(\"BiocManager\", quietly\u00a0= TRUE))\ninstall.packages(\"BiocManager\")\nBiocManager::install(\"DESeq2\")\nlibrary(DESeq2)\n        Format the data to remove the sgRNA column from the count matrix output.\n        Execute in R:\n      \ndata <- read.delim(\u201c$DIR/Trimmed/.count.txt\u201d, sep=\u201d\\t\u201d)\ndata\u00a0= subset(data, select=-c(sgRNA))\nwrite.csv(data, \u201c$DIR/Trimmed/count_formatted.csv\u201d,\n          row.names=FALSE)Specify which samples belong to which condition in the count matrix. The\n        sample names and their original names can be found in .countsummary.txt.\n        \nName the samples that were induced (+) \u201ctreated\u201d\nName the control samples (-) \u201cuntreated\u201d. Using R:\nname<-c(\"sample1\",\"sample2\",\"sample3\",\"sample4\",\"sample5\",\"sample6\",\"sample7\",\"sample8\",\"sample9\",\"sample10\",\"sample11\",\"sample12\",\"sample13\",\"sample14\")\ncondition <- c(\"untreated\", \"untreated\",\n          \"untreated\", \"treated\", \"untreated\",\n          \"treated\", \"untreated\", \"treated\",\n          \"untreated\", \"treated\", \"untreated\",\n          \"treated\", \"untreated\", \"treated\")\ndf <- data.frame(condition, row.names=name)\n        Load all files in R.\n        \nTurn all row names into unique values\nCast columns are as the \u201cfactor\u201d type\n            Remove possible whitespace characters from all values in the count\n            matrix\n          \nFormat the count matrix into a DESeqDataSet:\ncts <-\n          as.matrix(read.csv(\"/$DIR/Trimmed/count_formatted.csv\",sep=\",\"))\nrownames(cts) <- make.names(cts[,1], unique=TRUE)\ncts\u00a0= subset(cts, select=-c(Gene))\ndf$condition <- factor(df$condition)\ncts <- cts[, rownames(df)]\ncts <- as.data.frame(apply(cts,2,function(x)gsub('\\\\s+',\n          '',x)))\ncts[,] <- sapply(cts[,], as.numeric)\ndds <- DESeqDataSetFromMatrix(countData\u00a0= cts,\ncolData\u00a0= df,\ndesign\u00a0= \u223c condition)\nRun the DESeq function to analyze treated vs. untreated.\ndds$condition <- relevel(dds$condition, ref\u00a0=\n          \"untreated\")\nddsseq <- DESeq(dds)\n        To identify gRNAs that show a significant downregulation, we recommend\n        starting the analysis with a p\u00a0<\u00a00.05 and a Log2FoldChange\n        (LFC)\u00a0<\u00a0\u22121.0 as cutoff:\n      \nres05\u00a0<- results(ddsseq, alpha=0.05)\nresdown <- res05[which(res05$log2FoldChange\u00a0<\u00a0-1.0),]\nNote: The resulting data frame shows gRNAs\n      that are significantly dropping out in the screen. We recommend narrowing\n      down the list of candidates by selecting transcripts targeted by at least\n      2 functional gRNAs.\n    \n        Make the row names for each target gene identical\n        \n            Count all the occurrences of identical genes in a new column called\n            \u2018good_sgRNAs\u2019.\n          \n            Filter the data frame to only include genes which have more than 1\n            \u2018good sgRNA\u2019\n          \nSave it as a .csv in $DIR:\nrownames(resdown) <-\n          gsub(\"\\\\..\u2217\",\"\",rownames(resdown))\nresdown$good_sgRNAs <- as.numeric(ave(rownames(resdown),\n          rownames(resdown), FUN\u00a0= length))\ncandidates <- resdown[which(resdown$good_sgRNAs\u00a0>\u00a01),]\nwrite.csv(as.data.frame(candidates),\n          file=\u2018$DIR/candidates.csv\u201d)\n        This should output a list of all genes for which at least 2 gRNAs dropout with an adjusted p-value of\u00a0<\u00a00.05 and an LFC\u00a0<\u00a0\u22121.0.", "Step-by-step method details\nStep-by-step method details\nNuclei isolation and nuclear run-on\nTiming: 1\u00a0day\nNuclei were isolated by the Percoll gradient procedure as previously described except that the concentration of Triton X-100 was reduced to 0.5% (Zhu et\u00a0al., 2018[href=https://www.wicell.org#bib13]). For tissues that have not been tested, pilot experiments are required to optimize the concentration of Triton X-100, which dissolves the plasma membrane and chloroplasts and leaves the nucleus intact. In\u00a0vitro nuclear run-on and nascent RNA purification were performed as previously reported (Zhu et\u00a0al., 2018[href=https://www.wicell.org#bib13]).\nNuclei isolation.\nPrepare nuclei isolation buffer, gradient buffer, 80% Percoll solution, 30% Percoll solution, 10\u00d7 transcription buffer, 1\u00d7 transcription buffer and nuclei storage buffer, keep on ice.\nGrind leaf tissue with liquid nitrogen to fine powder and dispense into 3\u00a0mL aliquots. Add 20\u00a0mL cold nuclei isolation buffer to each of three replicates. Mix well and put on ice for 10\u201315\u00a0min. Shake the tube gently every 3\u00a0min.\nPlace 2 layers of nylon mesh (1 layer of 100\u00a0\u03bcm mesh on the top, 1 layer of 40\u00a0\u03bcm mesh at the bottom) over the top of a 50\u00a0mL centrifuge tube and decant the sample through the mesh. Add nuclei isolation buffer to the filtrate to a final volume of 20\u00a0mL.\nPrepare a discontinuous Percoll gradient by layering 6\u00a0mL 30% Percoll over 6\u00a0mL 80% Percoll in a 50\u00a0mL tube. Be careful and do not disrupt the 80%\u201330% interface.\nGently pipette the filtered plant extract onto the top of the 30% Percoll layer. Centrifuge the gradient at 2,000\u00a0\u00d7\u00a0g for 11\u00a0min at 4\u00b0C with a swing bucket at a low accelerating and de-accelerating level (\u00b13).\nRemove the top layer of nuclei isolation buffer and use a Pasteur pipette to collect nuclei at the 30%\u201380% interface.Add cold gradient buffer to the enriched nuclei to a total volume of 20\u00a0mL, centrifuge at 2,000\u00a0\u00d7\u00a0g for 6\u00a0min at 4\u00b0C.\nGently draw off the supernatant, then resuspend the pellet with 1\u00a0mL 1\u00d7 transcription buffer (pre-cold). Transfer the nuclei to a new 1.5\u00a0mL tube, centrifuge at 3,000\u00a0\u00d7\u00a0g for 3\u00a0min at 4\u00b0C.\nRepeat step h. Then wash nuclei with 1\u00a0mL nuclei storage buffer (pre-cold), centrifuge at 5,000\u00a0\u00d7\u00a0g for 5\u00a0min at 4\u00b0C.\nPause point: Proceed to \u201cNuclear Run-On\u201d or store nuclei at \u221280\u00b0C. If properly stored, frozen nuclei can be used several weeks after collection.\nNuclear Run-On and RNA extraction.\nResuspend nuclei with 100\u00a0\u03bcL nuclei storage buffer, and add 100\u00a0\u03bcL 2\u00d7 reaction buffer, mix well (stir the pellet first with 200\u00a0\u03bcL tip, then pipette the pellet gently for 30 times). Incubate at 30\u00b0C for 5\u00a0min immediately. Br-UTP is incorporated into the nascent RNA at this step.\nAdd 750\u00a0\u03bcL TRIzol reagent to stop the reaction. Vortex for 60\u00a0s and incubate at room temperature (\u223c25\u00b0C) for 5\u201310\u00a0min. Add 200\u00a0\u03bcL chloroform and vortex for 30 s. Spin at 12,000\u00a0\u00d7\u00a0g for 15\u00a0min at 4\u00b0C. Transfer the aqueous phase (upper phase) into a new 1.5\u00a0mL tube and add 500\u00a0\u03bcL chloroform, vortex for 30 s.\nSpin at 12,000\u00a0\u00d7\u00a0g for 5\u00a0min at 4\u00b0C. Transfer the aqueous phase (\u223c560\u00a0\u03bcL) into two new 1.5\u00a0mL tubes (\u223c280\u00a0\u03bcL each). Add 28\u00a0\u03bcL (1/10 volume) 5\u00a0M NH4OAc, 1\u00a0\u03bcL glycol-blue (15\u00a0\u03bcg/\u03bcL), 840\u00a0\u03bcL (3 volumes) 100% EtOH (pre-cold) and mix well by vortexing. Incubate at \u221280\u00b0C for at least two hours or overnight (6\u20138 h).Pellet the nuclear RNA from step c by spinning at 12,000\u00a0\u00d7\u00a0g for 30\u00a0min at 4\u00b0C.\nDiscard supernatant and wash the pellet with 1\u00a0mL 75% ethanol (pre-cold). Spin at 12,000\u00a0\u00d7\u00a0g for 5\u00a0min at 4\u00b0C. Discard the supernatant.\nRepeat step e. Then spin briefly and remove the residual 75% EtOH with a 10\u00a0\u03bcL pipette. Air-dry the pellet for 5\u201310\u00a0min.\nDissolve the RNA in 14\u00a0\u03bcL DEPC-H2O and store at \u221280\u00b0C.\nCritical: All the buffers and reaction mixtures should be prepared and kept on ice; As PMSF is toxic, wear gloves while handling.\nRibosomal RNA (rRNA) removal\nTiming: 2\u20133 h\nThe rRNA removal step after nuclear RNA isolation and before the affinity purification of nascent RNA is performed according to riboPOOL kit (siTOOLs Biotech) manual with some modifications.\nPreparation of streptavidin beads.\nResuspend the Streptavidin Magpoly Beads by carefully vortexing at medium speed.\nTransfer 100\u00a0\u03bcL bead suspension per sample to a new 1.5\u00a0mL tube. To prepare multiple samples, aliquot bead suspension for 3 samples (i.e., 300\u00a0\u03bcL) to a single tube.\nPlace tube on magnetic rack, aspirate supernatant.\nAdd 100\u00a0\u03bcL depletion buffer per sample (i.e., 300\u00a0\u03bcL for 3 samples) and resuspend the beads.\nPlace on magnetic rack, aspirate supernatant.\nRepeat step d and e.\nResuspend beads in 40\u00a0\u03bcL depletion buffer per sample (i.e., 120\u00a0\u03bcL for 3 samples). Leave the beads at room temperature (\u223c25\u00b0C) until use.\nHybridization of riboPOOL to RNA.\nTo the 14\u00a0\u03bcL RNA sample from step 2g add and mix 1\u00a0\u03bcL riboPOOL and 5\u00a0\u03bcL hybridization buffer.\nIncubate at 68\u00b0C for 10\u00a0min to denature RNA.\nAllow to cool slowly from 68\u00b0C to 37\u00b0C (3\u00b0C/min) for optimal hybridization.\nrRNA depletion.Briefly centrifuge the tube containing \u223c20\u00a0\u03bcL hybridized riboPOOL and nuclear RNA (from step 4c).\nDispense 40\u00a0\u03bcL beads (from step 3) to the hybridized riboPOOL/RNA solution and mix well by pipette.\nIncubate at 37\u00b0C for 15\u00a0min, follow by a 50\u00b0C incubation for 5\u00a0min. Pipette the mixture every 5\u00a0min.\nBriefly spin down droplets and place the tube on magnetic rack. Very carefully transfer the supernatant to a new 1.5\u00a0mL tube.\nBeads clean-up: add 90\u00a0\u03bcL (1.8\u00d7) AMPure Beads to the supernatant and mix well by pipette; immediately add 270\u00a0\u03bcL isopropanol and mix well by pipette; incubate for 5\u00a0min at room temperature (\u223c25\u00b0C) and magnetize sample for 5\u00a0min or until solution appears clear; discard supernatant and wash the beads by 180\u00a0\u03bcL 80% EtOH twice; spin briefly and remove the residual 80% EtOH. Air dry the beads for 3\u20135\u00a0min. Remove the tube from the magnetic rack and resuspend the beads in 20\u00a0\u03bcL DEPC-H2O by pipetting volume up and down. Incubate the bead suspension for 3\u00a0min at room temperature (\u223c25\u00b0C). Magnetize sample for 3\u00a0min and transfer 20\u00a0\u03bcL supernatant to a new tube.\nPause point: Proceed to \u201cRNA immunoprecipitation\u201d or store RNA at \u221280\u00b0C (stable for up to a month).\nCritical: Always use freshly prepared 80% ethanol and do not incubate the beads with 80% ethanol for more than 30 s.\nRNA immunoprecipitation\nTiming: 6\u20138 h\nThe rRNA depleted nuclear RNA is subject to RNA fragmentation, DNA removal and de-phosphorylation sequentially. Then, Br-UTP containing nascent RNA is affinity purified using anti-BrdU antibodies.\nAnti-BrdU antibody conjugation to magnetic beads.\nPrepare binding buffer.\nResuspend the Dynabeads\u2122 M-280 Sheep Anti-Mouse IgG by carefully vortexing at medium speed.\nTransfer 50\u00a0\u03bcL bead suspension per sample to a new 1.5\u00a0mL tube.Place tube on magnetic rack, aspirate supernatant.\nResuspend the beads in 1\u00a0mL binding buffer and add 30\u00a0\u03bcL anti-BrdU antibody (IIB5) per sample. Rotate at 4\u00b0C for 4\u20136 h. (During this step, proceed to steps 7 and 8).\nRNA fragmentation and purification.\nThe rRNA depleted nuclear RNA from step 5e (20\u00a0\u03bcL) is subjected to base hydrolysis by adding 5\u00a0\u03bcL 1\u00a0M NaOH (freshly prepared) and incubate on ice for 40\u00a0min. Then add 25\u00a0\u03bcL 1\u00a0M Tris-HCl (pH 7.0) to neutralize the reaction.\nRNA is desalted through a p-30 RNase-free spin column, according to the manufacturer\u2019s instructions (https://www.bio-rad.com/sites/default/files/webroot/web/pdf/lsr/literature/10000069982.pdf[href=https://www.bio-rad.com/sites/default/files/webroot/web/pdf/lsr/literature/10000069982.pdf]).\nDNA removal and de-phosphorylation.\nAdjust the eluted RNA volume to 50\u00a0\u03bcL with DEPC-H2O. Then add 6.7\u00a0\u03bcL DNase buffer and 3.5\u00a0\u03bcL RNase-free DNase (2\u00a0U/mL, 2238G2), mix by vortexing. Briefly spin down droplets and incubate at 37\u00b0C for 30\u00a0min.\nRNA is purified again through a p-30 column.\nAdjust the eluted RNA volume to 70\u00a0\u03bcL. Then add 8.5\u00a0\u03bcL 10\u00d7 antarctic phosphatase buffer, 1\u00a0\u03bcL RNase Inhibitor (Promega, N2615) and 5\u00a0\u03bcL antarctic phosphatase to the fragmented RNA and incubate for 1\u00a0h at 37\u00b0C. (During this step, proceed to step 9).\nImmunoprecipitation.\nPrepare binding buffer, blocking buffer, low salt buffer, high salt buffer, and TET buffer, keep on ice.\nPut the tube containing antibody conjugated beads (from step 6e) on magnetic rack and aspirate the supernatant. Block the beads in 1\u00a0mL blocking buffer for 1\u00a0h at 4\u00b0C with rotation.\nAdd 500\u00a0\u03bcL binding buffer to the sample (from step 8c) and denature the RNA at 65\u00b0C for 5\u00a0min, then immediately put on ice.\nTransfer the denatured RNA to beads in 500\u00a0\u03bcL binding buffer and allow to bind for 1\u00a0h at 4\u00b0C with rotation.After binding, spin at 1,500\u00a0\u00d7\u00a0g for 5\u00a0min at 4\u00b0C, remove supernatant. Then beads are washed once in 1\u00a0mL low salt buffer, twice in high salt buffer and twice in TET buffer at 4\u00b0C with 5\u00a0min rotation.\nElute the BrU-incorporated RNA in 250\u00a0\u03bcL elution buffer at room temperature (\u223c25\u00b0C) with 10\u00a0min rotation.\nAdd 800\u00a0\u03bcL TRIzol reagent to stop elution. Vortex for 1\u00a0min and incubate at room temperature (\u223c25\u00b0C) for 5\u201310\u00a0min. Add 200\u00a0\u03bcL chloroform and vortex for 30 s. Spin at 12,000\u00a0\u00d7\u00a0g for 15\u00a0min at 4\u00b0C. Transfer the aqueous phase (upper phase) into a new 1.5\u00a0mL tube and add 500\u00a0\u03bcL chloroform, vortex for 30 s.\nEthanol precipitation of the eluted RNA refers to step 2c.\nLibrary construction\nTiming: 2\u00a0days\nThe enriched BrU-incorporated RNA is end-repaired by PNK treatment and then subject to small RNA library construction. Then the target cDNA library is recovered by PAGE size selection (170\u00a0bp-330\u00a0bp) and quantified by Qubit.\nPNK treatment.\nPellet and wash the RNA (from step 9h) as described in steps 2d\u2013f.\nDissolve the RNA in 45\u00a0\u03bcL DEPC-H2O. Denature the RNA at 65\u00b0C for 5\u00a0min and immediately cool on ice for 2\u00a0min.\nAdd 5.2\u00a0\u03bcL T4 PNK buffer, 1\u00a0\u03bcL RNase inhibitor and 1\u00a0\u03bcL T4 PNK to the denatured RNA. Incubate at 37\u00b0C for 1 h.\nAdd 300\u00a0\u03bcL TRIzol reagent to stop elution. Vortex for 1\u00a0min and incubate at room temperature (\u223c25\u00b0C) for 5\u201310\u00a0min. Add 100\u00a0\u03bcL chloroform and vortex for 30 s. Spin at 12,000\u00a0\u00d7\u00a0g for 15\u00a0min at 4\u00b0C. Transfer the aqueous phase (upper phase) into a new 1.5\u00a0mL tube and add 200\u00a0\u03bcL chloroform, vortex for 30 s.Ethanol precipitation of RNA refers to step 2c.\nLibrary construction.\nPellet and wash the RNA (from step 10e) as described in steps 2d\u2013f.\nDissolve the RNA in 10.5\u00a0\u03bcL DEPC-H2O and proceed to small library construction using NEXTflex\u2122 Small RNA-Seq Kit v3 according to the manufacturer\u2019s manual exactly except for the PCR amplification (https://perkinelmer-appliedgenomics.com/wp-content/uploads/marketing/NEXTFLEX/small_rna/5132-05-NEXTflex-Small-RNA-Seq-v3_19.01.pdf[href=https://perkinelmer-appliedgenomics.com/wp-content/uploads/marketing/NEXTFLEX/small_rna/5132-05-NEXTflex-Small-RNA-Seq-v3_19.01.pdf]):\ntable:files/protocols_protocol_1945_15.csv\ntable:files/protocols_protocol_1945_16.csv\nNote: Run for 5 cycles first, then take out 5\u00a0\u03bcL sample (keep the remaining 35\u00a0\u03bcL PCR sample mix on ice) and use KAPA HiFi Real-time PCR Library Amplification Kit to determinate the remaining PCR N cycle number to generate optimal library for sequencing. Add 5\u00a0\u03bcL 2\u00d7 KAPA PCR mix to the 5\u00a0\u03bcL sample, mixed well. Take out 10\u00a0\u03bcL standard control 1\u20134, two repeats each. Choose the N cycles which make sure the sample fluorescence is between standard control 1 and standard control 3 fluorescence. Lastly amplify the remaining 35\u00a0\u03bcL PCR sample mix for additional N cycles (normally no more than 10 cycles). It is not recommended to over amplify the sample.\nGel recovery.\nTo make an 8% polyacrylamide TBE gel (non-denaturing), mix the follow items in a 15\u00a0mL corning tube: 2.2\u00a0mL 5\u00d7 TBE, 2.2\u00a0mL 40% polyacrylamide stock solution, fill up to 11\u00a0mL with ddH2O, mix well; then add 66\u00a0\u03bcL 10% APS, 5.1\u00a0\u03bcL TEMED, mix well. Put the mixture into a 1.5\u00a0mm thick glass cassette (Bio-Rad protein system IV) with a 10-well comb. Wait for half an hour.\nAdd 6\u00a0\u03bcL of 6\u00d7 loading buffer (provided by NEXTflex\u2122 Small RNA-Seq Kit v3) to the PCR product (from step 11b).Resolve the PCR product on the gel, then electrophoresis at 130\u00a0V for 1\u20132\u00a0h in 1\u00d7 TBE electrophoresis solution till the Xylene Cyanol reach the middle of gel and the Bromophenol Blue go off the gel. Stain the gel with SYBR\u00ae Gold Nucleic Acid Gel Stain /TBE (1/10,000) for 5\u201315\u00a0min.\nTake picture of the gel and excise the gel slices between 170\u00a0bp and 330\u00a0bp (Figure 1[href=https://www.wicell.org#fig1]). Recover the cDNA by soaking the grinded gel (use a fire sealed 1\u00a0mL tip) in 400\u00a0\u03bcL DNA gel elution buffer (300\u00a0mM NaCl and 1\u00a0mM EDTA) overnight (6\u20138 h) at 30\u00b0C on thermo rotator (1,200\u00d7 rpm).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1945-Fig1.jpg\nFigure\u00a01. rRNA removal GRO-seq cDNA resolved on a PAGE\nLeft, a typical PAGE of rRNA removal GRO-seq cDNA library. Right, the PAGE after gel slicing. Molecular weight and adapter self-ligation product are indicated.\nPass the gel through a 0.45\u00a0\u03bcm spin column by centrifugation at 8,000\u00a0\u00d7\u00a0g for 5\u00a0min (Corning, V123041), then precipitate the cDNA by adding: 1\u00a0\u03bcL glycol-blue (15\u00a0\u03bcg/\u03bcL), 35\u00a0\u03bcL (1/10 volume) 3\u00a0M NaOAc, 1,050\u00a0\u03bcL (3 volumes) 100% EtOH. Incubate at \u221280\u00b0C for at least two hours or overnight (6\u20138 h).\nPellet, wash and resuspend the cDNA as described in steps 2d\u2013g.\nCritical: Resolving the PCR product on a TBE native PAGE can serve as a check point for library preparation. Optionally, to save time and simplify operations, experienced experimenters can use magnetic beads for size selection instead of gel recovery if there is no obvious adapter self-ligation.\nQuality check of the cDNA library.\nQuantify the library by Qubit dsDNA HS Assay Kit according to the manufacturer\u2019s manual\u00a0(https://www.thermofisher.cn/document-connect/document-connect.html?url=https://assets.thermofisher.cn/TFS-Assets%2FLSG%2Fmanuals%2FQubit_dsDNA_HS_Assay_UG.pdf[href=https://www.thermofisher.cn/document-connect/document-connect.html?url=https://assets.thermofisher.cn/TFS-Assets%2FLSG%2Fmanuals%2FQubit_dsDNA_HS_Assay_UG.pdf]) and check the size distribution with a Fragment Analyzer.", "Step-by-step method details\nStep-by-step method details\nData preprocessing\nTiming: 2\u223c5 h\nThis section describes the preprocessing process of the scRNA-seq data, of which outputs is used as the input file to establish a classifier for the pro-embolic cells.\nRaw data quality control and filtering.\nQuality control and filtering of the raw sequencing fastq data file. The analysis process of the S1 sample is as follows, the QC filtering of other samples is the same as that of S1 sample.\n#R1 reads of the S1 sample:\n$fastqc -t 10 -o outdir -d./temp -f fastq S1_R1.fastq.gz\n#R2 reads of the S1 sample\n$fastqc\u00a0\u2013t 10\u00a0\u2013o outdir\u00a0\u2013d /temp\u00a0\u2013f fastq S1_R2.fastq.gz\nNote: The outputs of fastqc include a \u201c.zip\u201d file which records detailed information and a QC report file in html format. The report file provides a modular set of analysis which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing further analysis. For the interpretation of the report, please refer to https://www.bioinformatics.babraham.ac.uk/projects/fastqc/[href=https://www.bioinformatics.babraham.ac.uk/projects/fastqc/].\nQuality control pipeline for the BD Rhapsody platform.\nNote: This pipeline is used to process raw sequencing from the BD Rhapsody platform.\nDownload BD repository file (.zip format, see the key resources table[href=https://www.wicell.org#key-resources-table]), decompress the .zip file and obtain \u201ctemplate.yml\u201d.\nDownload the reference genome file and transcriptome annotation file (see the key resources table[href=https://www.wicell.org#key-resources-table]).\nModify the \u201ctemplate.yml\u201c file to indicate the correct location of sequencing fastq file, reference genome file and transcriptome annotation file (Figure\u00a01[href=https://www.wicell.org#fig1]) before running the following Linux commands.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2660-Fig1.jpg\nFigure\u00a01. Modify the YML file before running preprocessing pipeline of BD Rhapsody platform\nThe file location corresponding to the red character must be filled correctly, especially the sequenced fastq file.$cwl-runner --parallel --ourdir /home/MSC/S1 --tmpdir-prefix /home/MSC/tmp_Dir --tmp-outdir-prefix /home/MSC/tmp_Dir --rm-tmpdir /home/script/rhapsody_wta_1.9.1.cwl /home/script/template_wta_1.9.1.yml\nNote: Through this step, you can get the quality report.\nfile: /home/MSC/S1/S1_Metrics_Summary.csv and original read count.\nfile: /home/MSC/S1/S1_RSEC_MolsPerCell.csv which is the input of the step 2.\nQuality control pipeline for the 10X genomic platform.\nNote: This pipeline is used to process raw sequence data from the 10X genomic platform.\nDownload compressed reference file: refdata-gex-GRCh38-2020-A.tar.gz (see the key resources table[href=https://www.wicell.org#key-resources-table]) to the appropriate directory (for example, /home/database/cellranger/).\nRun following commands.\n#Make dictionary for fastq files\n$cp S1_R1.fastq.gz /home/fastqs/S1/\n$cp S1_R2.fastq.gz /home/fastqs/S1/\n$tar\u00a0\u2013zxvf /home/database/cellranger/refdata-gex-GRCh38-2020-A.tar.gz\n#Run cellranger\n$Cellranger count --id S1 --transcriptome /home/database/cellranger/refdata-gex-GRCh38-2020-A --fastqs /home/fastqs/S1 --expect-cells 1000 --localcores 10 --localmem 64\nNote: Through this step, you can get the quality report\nfile: /home/fastqs/S1/outs/ web_summary.html\nOriginal read count folder: /home/fastqs/S1/outs/filtered_feature_bc_matrix, which is the input of the step 2.\nJudge whether the sample is qualified according to quality criterion.\nNote: Criterion is as follows: Estimated Number of Cells >3000, Mean Reads per cell >50k, Median genes per cell >3000, Fraction reads in cells >70%, Valid barcodes >75%, Valid UMIs >75% and Q30 Bases in RNA Read >70%.\nBuilding the cell-gene expression seurat object.\nIn this step, the outputs in step 1 are used as the input (please select the corresponding platform) to run the commands. Of course, variable names in commands can be changed according to your own habits.\nEnter the R interactive environment and load the Seurat package.\n$R\n>library(Seurat)\nFor BD Rhapsody platform.\n>count.data <- t(read.csv(\u201c/home/MSC/S1/S1_RSEC_MolsPerCell.csv\u201d, row.names=1, check.names=FALSE, comment.char=\"#\"))\n>colnames(counts.data) <- paste(\u201cS1\u201d, colnames(counts.data), sep=\"-\")\n>S1\u00a0<- CreateSeuratObject(counts=counts.data, project=\u201dS1\u201d, min.cells=10)\nFor 10X genomic platform.\n>S1_dir <- '/home/fastqs/S1/outs/filtered_feature_bc_matrix\u2019\n>S1\u00a0<- Read10X(data.dir=S1_dir)\n>S1\u00a0<- CreateSeuratObject(counts=S1, min.cells=10, min.features=200)\n>S1$Sample <- rep(\u201cS1\u201d, dim(S1)[2])\nNote: Create the seurat object of other samples using this step.\nNormalization, removal of the batch effect and clustering analysis.\nNormalization.Note: This sub-block is to remove unwanted cells from the dataset and normalize the data followed by step 2. It employs a global-scaling normalization method \u201cLogNormalize\u201d that normalizes the feature expression measurements for each cell by the total expression, multiplies this by a scale factor (10,000 by default), and log-transforms the result.\n>ADSC1\u00a0<- merge(x=S1, y=list(T1,T2,T3,T4))\n>DefaultAssay(ADSC1) <- \"RNA\"\n>ADSC1[[\"percent.mt\"]] <- PercentageFeatureSet(ADSC1, pattern=paste(c(\"\u02c6MT-\", \"\u02c6Mt-\", \"\u02c6mt-\", \"\u02c6MT.\"), collapse=\"|\"))\n>ADSC1\u00a0<- subset(ADSC1, subset=nFeature_RNA\u00a0>\u00a0250\u00a0& nCount_RNA\u00a0>\u00a0500\u00a0& percent.mt\u00a0<\u00a010)\n>ADSC1\u00a0<- NormalizeData(ADSC1, verbose=FALSE)\nRemoval of the batch effect.\nNote: This sub-block is to correct for technical differences between datasets caused by the batch effect through identify cross-dataset pairs of cells that are in a matched biological state.\n>ADSC1_MG <- FindVariableFeatures(ADSC1_MG, selection.method=\"vst\", nfeatures=2000)\n>adsc.list <- SplitObject(ADSC1_MG, split.by=\"Sample\")\n>adsc.list <- lapply(X=adsc.list, FUN=function(x) {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0x <- NormalizeData(x)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0x <- FindVariableFeatures(x, selection.method=\"vst\", nfeatures=2000)\n})\n>features <- SelectIntegrationFeatures(object.list=adsc.list)\n>adsc.anchors <- FindIntegrationAnchors(object.list=adsc.list, anchor.features=features)\n>ADSC1_IT <- IntegrateData(anchorset=adsc.anchors)\nClustering analysis.\nNote: This sub-block is to cluster the cells without technical bias.\nShift the expression distribution of each gene across cells into the standard normal distribution using the \u201cScaleData\u201d.\nPerform linear dimensional reduction using the \u201cRunPCA\u201d.\nDetermine the \u2018dimensionality\u2019 of dataset using the \u201cJackStraw\u201d and the \u201cScoreJackStraw\u201d.\nCluster the cells using the \u201cFindNeighbors\u201d and the \u201cFindClusters\u201d.\nUse \u201cUMAP\u201d to visualize and explore the datasets.\nNote: The interpretation of the outputs in this sub-block can be referred to https://satijalab.org/seurat/articles/pbmc3k_tutorial.html[href=https://satijalab.org/seurat/articles/pbmc3k_tutorial.html].\n>ADSC1_IT <- ScaleData(ADSC1_IT, verbose=FALSE,vars.to.regress=c(\"S.Score\", \"G2M.Score\",\"nCount_RNA\", \"percent.mt\"))\n>ADSC1_IT <- RunPCA(ADSC1_IT, verbose=FALSE)\n>ElbowPlot(ADSC1_IT, ndims=50)\n>ADSC1_IT <- JackStraw(ADSC1_IT, num.replicate=100)\n>ADSC1_IT <- ScoreJackStraw(ADSC1_IT, dims=1:20)\n>JackStrawPlot(ADSC1_IT, dims=1:20)\n>ADSC1_IT <- FindNeighbors(ADSC1_IT, reduction=\"pca\", dims=1:30)\n>ADSC1_IT <- FindClusters(ADSC1_IT, resolution=0.5)\n>ADSC1_IT <- RunUMAP(ADSC1_IT, reduction=\u201cpca\u201d, dims=1:30)\n>write.csv(GetAssayData(ADSC1_IT,slot=\"data\"),file=\"ADSC1_IT_exp.csv\")\n>write.csv(ADSC1_IT@meta.data, file=\"ADSC1_IT_meta.csv\")\n>write.csv(VariableFeatures(ADSC1_IT),file=\"ADSC1_IT_HVG.csv\")Note: The \u201cADSC1_ IT_ Exp.csv\u201d file is the standardized cell gene expression matrix, the \u201cADSC1_ IT_ The meta.csv\u201d file contains the cluster, cell name, and cell sample information; and the \u201cADSC1_ IT_ HVG.csv\u201d file is the first 2000 genes with the largest variance of expression in all cells. These files are used for the future analysis.\nCritical: Step 3 consumes a lot of memory, 30 k cells across 5 datasets consumes about 30 GB memory. So please ensure that your computer has enough memory during this step.\nFeature selection\nTiming: \u223c4 h\nThis section describes the detailed procedure of the feature (genes expressed in the cell) importance ranking and the optimal feature number analysis for the classifier development using machine learning. This allows us to utilize the most effective feature information while reducing noise (such as genes involved in general biological processes of cells) in the process of classifier establishment.\nSet up training and test sets.\nSet up the training set (S1 mixed data): Randomly selected 70% cells and their HGV gene expression from each subgroup of A2105C3P5 and A2105C2P5 to form the training set (S1) using perl script \u201cRandomSelectCellsAndHVG2000Exp.pl\u201d.\n#Make dictionary for training set\n$mkdir DataSet\n#Extract cell-gene expression matrixes of HVG genes from each subgroup\n$nohup perl script/RandomSelectCellsWithHVG2000Exp.pl ADSC1_IT_meta.csv ADSC1_IT_HVG.csv ADSC1_IT_exp.csv A2105C3P5 0.7 DataSet\u00a0&\n$nohup perl script/RandomSelectCellsWithHVG2000Exp.pl ADSC1_IT_meta.csv ADSC1_IT_HVG.csv ADSC1_IT_exp.csv A2105C2P5 0.7 DataSet\u00a0&\n#Use the \u201cjobs\u201d command to check whether the task submitted to the background via nohup is completed\n$jobs\n$cd DataSet\n#Merge cells from each subgroup\n$paste A2105C3P5_C0_HVG2000Exp70.txt A2105C3P5_C1_HVG2000Exp70.txt A2105C3P5_C2_HVG2000Exp70.txt A2105C3P5_C3_HVG2000Exp70.txt A2105C3P5_C4_HVG2000Exp70.txt A2105C3P5_C5_HVG2000Exp70.txt\u00a0>\u00a0A2105C3P5_70.txt\n$paste A2105C2P5_C0_HVG2000Exp70.txt A2105C2P5_C1_HVG2000Exp70.txt A2105C2P5_C2_HVG2000Exp70.txt A2105C2P5_C3_HVG2000Exp70.txt A2105C2P5_C4_HVG2000Exp70.txt A2105C2P5_C5_HVG2000Exp70.txt\u00a0>\u00a0A2105C2P5_70.txtSet up the test set 1 (T1 mixed data): The remaining 30% cells and their HGV gene expression in A2105C2P5 and A2105C3P5 are mixed as T1 and used as test set 1.\n#Merge cells from each subgroup\ncd DataSet\n$paste A2105C3P5_C0_HVG2000Exp30.txt A2105C3P5_C1_HVG2000Exp30.txt A2105C3P5_C2_HVG2000Exp30.txt A2105C3P5_C3_HVG2000Exp30.txt A2105C3P5_C4_HVG2000Exp30.txt A2105C3P5_C5_HVG2000Exp30.txt\u00a0>\u00a0A2105C3P5_30.txt\n$paste A2105C2P5_C0_HVG2000Exp30.txt A2105C2P5_C1_HVG2000Exp30.txt A2105C2P5_C2_HVG2000Exp30.txt A2105C2P5_C3_HVG2000Exp30.txt A2105C2P5_C4_HVG2000Exp30.txt A2105C2P5_C5_HVG2000Exp30.txt\u00a0>\u00a0A2105C2P5_30.txt\nSet up the test set 2 (T2 mixed data): Get cells and their HGV gene expression from each subgroup in A2105C3P3 and A2015C2P3.\n#Extract cell-gene expression matrixes of HVG genes from each subgroup\n$nohup perl script/RandomSelectCellsWithHVG2000Exp.pl ADSC1_IT_meta.csv ADSC1_IT_HVG.csv ADSC1_IT_exp.csv A2105C3P3 1 DataSet\u00a0&\n$nohup perl script/RandomSelectCellsWithHVG2000Exp.pl ADSC1_IT_meta.csv ADSC1_IT_HVG.csv ADSC1_IT_exp.csv A2105C2P3 1 DataSet\u00a0&\n#Use the \u201cjobs\u201d command to check whether the task submitted to the background via nohup is completed\n$jobs\n$cd DataSet\n#Merge cells from each subgroup\n$paste A2105C3P3_C0_HVG2000Exp100.txt A2105C3P3_C1_HVG2000Exp100.txt A2105C3P3_C2_HVG2000Exp100.txt A2105C3P3_C3_HVG2000Exp100.txt A2105C3P3_C4_HVG2000Exp100.txt A2105C3P3_C5_HVG2000Exp100.txt\u00a0>\u00a0A2105C3P3_100.txt\n$paste A2105C2P3_C0_HVG2000Exp100.txt A2105C2P3_C1_HVG2000Exp100.txt A2105C2P3_C2_HVG2000Exp100.txt A2105C2P3_C3_HVG2000Exp100.txt A2105C2P3_C4_HVG2000Exp100.txt A2105C2P3_C5_HVG2000Exp100.txt\u00a0>\u00a0A2105C2P3_100.txt\nSet up test set 3 and set 4 (T3 and T4 mixed data) using the method of setting up the test set 2.\nLabel the class of cells.\nNote: The ADSC cells amplified using different cultivation processes were infused into 6 mice by vein, respectively. Our previous work1[href=https://www.wicell.org#bib1] demonstrated that ADSC samples amplified by MF caused embolism in all mice, while IL amplified samples did not cause embolism. Therefore, we assumed that MF-expanded cells were pro-embolic cells, while IL-expanded cells are non-embolic cells, and labeled pro- and non-embolic cells in training and test sets using the perl script \u201cadd_typeToinput.pl\u201d (Table\u00a01[href=https://www.wicell.org#tbl1]).\nLabel the cells of the S1 mixed data.\n$cd DataSet\n#matrix transpose\n$awk '{i=1;while(i <= NF){col[i]=col[i] $i \" \";i=i+1}} END {i=1;while(i<=NF){print col[i];i=i+1}}' A2105C3P5_70.txt | sed 's/[ ]\u2217$//g'\u00a0>\u00a0A2105C3P5_70_T.txt\n$awk '{i=1;while(i <= NF){col[i]=col[i] $i \" \";i=i+1}} END {i=1;while(i<=NF){print col[i];i=i+1}}' A2105C2P5_70.txt | sed 's/[ ]\u2217$//g'\u00a0>\u00a0A2105C2P5_70_T.txt#add label for each cell\n$perl script/add_typeToinput.pl A2105C3P5_70_T.txt nonembolic\u00a0>\u00a0A2105C3P5_70_T_L.txt\n$perl script/add_typeToinput.pl A2105C2P5_70_T.txt embolic\u00a0>\u00a0A2105C2P5_70_T_L.txt\n#Combine embolic and non-embolic cell data\n$cat A2105C3P5_70_T_L.txt A2105C2P5_70_T_L.txt\u00a0>\u00a0S1_exp.txt\n# Use text editor such as \u201cVim\u201d to manually remove redundant headers from the \u201cS1_exp.txt\u201d file\nLabel the cells of the T1 mixed data.\n$cd DataSet\n#matrix transpose\n$awk '{i=1;while(i <= NF){col[i]=col[i] $i \" \";i=i+1}} END {i=1;while(i<=NF){print col[i];i=i+1}}' A2105C3P5_30.txt | sed 's/[ ]\u2217$//g'\u00a0>\u00a0A2105C3P5_30_T.txt\n$awk '{i=1;while(i <= NF){col[i]=col[i] $i \" \";i=i+1}} END {i=1;while(i<=NF){print col[i];i=i+1}}' A2105C2P5_30.txt | sed 's/[ ]\u2217$//g'\u00a0>\u00a0A2105C2P5_30_T.txt\n#add label for each cell according to the Table\u00a01[href=https://www.wicell.org#tbl1]\n$perl script/add_typeToinput.pl A2105C3P5_30_T.txt nonembolic\u00a0>\u00a0A2105C3P5_30_T_L.txt\n$perl script/add_typeToinput.pl A2105C2P5_70_T.txt embolic\u00a0>\u00a0A2105C2P5_30_T_L.txt\n#Combine embolic and non-embolic cell data\n$cat A2105C3P5_30_T_L.txt A2105C2P5_30_T_L.txt\u00a0>\u00a0T1_exp.txt\n# Use text editor such as \u201cVim\u201d to manually remove redundant headers from the \u201cT1_exp.txt\u201d file\nChange the corresponding input file and label the T2, T3 and T4 cells with the same command line as T1, and then get the file T2_exp.txt, T3_exp.txt and T4_exp.txt.\nRank the importance of features and determine the optimal feature number.\nNote: We recommend using the Recursive feature elimination (RFE) to rank the feature importance and calculate cross validation accuracy when using different number of features.\nImport python packages.\n>mkdir RFECV\n>import pandas as pd\n>import numpy as np\n>from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n>from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, log_loss\n>from sklearn import svm\n>from sklearn.feature_selection import RFE,RFECV\n>import matplotlib.pyplot as plt\nBuild Linear SVM Classifiers.4[href=https://www.wicell.org#bib4]\n>input=DataSet/S1_exp.txt\n>rfecv_dir=RFECV\n>c_use=0.2 # Regularization parameter C, need set different value.\n>method=\u2019linear\u2019\n>function=\u2019ovo\u2019\n>raw_data=pd.read_csv(input,index_col=0, header='infer', sep='\\t', low_memory=False)\n>raw_data=raw_data[raw_data.columns[np.sum(raw_data)!=0]]\n>x=raw_data.drop('type', axis=1)\n>y=raw_data['type']\n>raw_embolic=raw_data[raw_data['type']=='embolic']\n>x_embolic=raw_embolic.drop('type', axis=1)\n>y_embolic=pd.DataFrame(raw_embolic['type'])\n>raw_nonembolic=raw_data[raw_data['type']=='nonembolic']\n>x_nonembolic=raw_nonembolic.drop('type',axis=1)\n>x_embolic_train, x_embolic_test, y_embolic_train, y_embolic_test=train_test_split(x_embolic, y_embolic, random_state=1, train_size=0.7, test_size=0.3)\n>x_nonembolic_train, x_nonembolic_test, y_non-embolic_train, y_nonembolic_test\u00a0= train_test_split(x_nonembolic, y_nonembolic, random_state=1, train_size=0.7, test_size=0.3)\n>x_test=pd.concat([x_embolic_test,x_nonembolic_test], axis=0)\n>y_test=pd.concat([y_embolic_test,y_nonembolic_test], axis=0)['type']\n>x_train=pd.concat([x_embolic_train,x_nonembolic_train], axis=0)\n>y_train=pd.concat([y_embolic_train,y_nonembolic_train], axis=0)['type']>clf=svm.SVC(C=c_use, kernel=method, gamma='auto', decision_function_shape=function, probability=True, class_weight='balanced', cache_size=2000).\nNote: Use svm SVC function to establish linear SVM as the estimator of RFECV. Set the regularization parameter C to 0.2, 0.6, 0.8, 1.0, 1.2, 1.6, 2.0, 2.2, 2.6 or 3.0 to balance the model complexity and the loss function, and then calculate the corresponding cross validation accuracy when unimportant features are eliminated in turn through the 10-fold cross validation.\nRun the RFE method.\n>rfecv=RFECV (estimator=clf, step=1, cv=10, scoring='accuracy')\n>rfecv.fit(x, y)\nPlot relationship between numbers of selected feature and the cross validation accuracy (Figure\u00a02[href=https://www.wicell.org#fig2]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2660-Fig2.jpg\nFigure\u00a02. Selected optimal number of most important features\n(A) the change curve of cross validation accuracy with the continuous addition of important features in a single model, (B)\u00a0zoom in at the inflection point of the change curve (A)\u00a0of the feature selection model.\n>plt.figure()\n>plt.xlabel(\"Number of features selected\")\n>plt.ylabel(\"Cross validation Accuracy\")\n>plt.plot(range(1, len(rfecv.grid_scores_)\u00a0+ 1), rfecv.grid_scores_)\n>plt.show()\nOutput the result to a file.\n>ref_txt=\u2019RFECV_\u2019+str(c_use)+\u2019.txt\u2019\n>pd.DataFrame(rfecv.grid_scores_, columns=['socre']).to_csv(ref_txt, index=False, header=False)\nNote: By setting different C values, a total of 10 feature selection models are obtained.\nCount the cross validation accuracy of different C-value models under different feature numbers to determine the optimal number of important features.\nNote: Our previous work1[href=https://www.wicell.org#bib1] demonstrated that in models with different C values, the cross validation accuracy using few important features is more than 95%. When using the first 13 important features, the cross validation accuracy is close to 100%. So we use 13 important features to train classifiers (Figure\u00a02[href=https://www.wicell.org#fig2]B). Therefore, we recommend choosing only the top few important features to train classifiers.\nTraining classifier\nTiming: \u223c10 h\nThis section describes how to build and train a classifier for pro-embolic and non-embolic cells.\nDevelop machine learning strategies.\nUse a linear SVM framework to train classifiers based on feature selection.Set the hyperparameter C to values of 0.0001, 0.0005, 0.001, 0.002, 0.004, 0.008, 0.02, 0.05, 0.2, 0.6, 1.2, 1.8, 2.4 or 3.0 to optimize the performance of classifiers on both the training and test sets.\nNote: C is essentially a regularization parameter that determines how much the SVM classifier should avoid misclassifying each training cell. For large C values, the classifier tends to correctly classify all training set cells, including abnormal cells. However, this can cause the classifier to pay too much attention to the features of training set cells, leading to reduced performance on test sets, which is called over-fitting. The opposite is true for smaller C values. Therefore, selecting an appropriate C value is a vital step in the best practice of using SVMs to develop classifiers that perform well on both the training set and test sets.\nOptimize the training parameters using 10-fold cross-validation.\nImport cell instances of the training set.\n>raw_data=pd.read_csv(S1_exp.txt, index_col=0, header='infer')\n>raw_data=raw_data[raw_data.columns[np.sum(raw_data)!=0]]\n>x_train=raw_data.drop('type', axis=1)\n>y_train=raw_data['type']\nEstablish SVM machine learning framework.4[href=https://www.wicell.org#bib4]\n>clf=svm.SVC(C=c_use, kernel='linear', gamma='auto', decision_function_shape=function, probability=True, class_weight='balanced',cache_size=2000)\n#c_use refers to different C values\nObtain the 13 most important feature genes and their expression levels.\n>rfe\u00a0= RFE(estimator=clf, n_features_to_select=1, step=1)\n>rfe.fit(x_train, y_train)\n>ranking=sorted(zip(rfe.ranking_, x_train.columns))\n>gene_list=[]\n>for i in ranking:\n\u00a0\u00a0\u00a0\u00a0if i[0]<=13:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0gene_list.append(i[1])\n>print(ranking)\nTrain classifiers and output their performance.\n>svc=clf.fit(x_train_new, y_train.ravel())\n>scores=cross_val_score(clf, x_train_new, y_train, cv=10, error_score='raise', scoring='accuracy')\n>print(\"cross validation Accuracy: %0.4f (+/- %0.4f)\" % (scores.mean(), scores.std() \u2217 1.96))\n>print(scores)\n>y_pred=clf.predict(x_train_new)\n>y_proba=clf.predict_proba(x_train_new)\n>print(list(y_train))\n>print(list(y_pred))\n>for i in y_proba:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0print(i)\nChange the super parameter C value and repeat steps 10, 11 and 12.\nNote: A total of 14 candidate classifiers were obtained.\nTest and determine the optimal classifier\nTiming: \u223c1 h\nThis section describes how to evaluate the performance of classifiers on test set and pick the optimal classifier based on test performance.Import test set 1 cell instances.\n>raw_data_test=pd.read_csv(T1_exp.txt, index_col=0, header='infer', sep='\\t')\n>raw_data_test=raw_data_test[raw_data_test.columns[np.sum(raw_data_test)!=0]]\n>x_test=raw_data_test.drop('type', axis=1)\n>y_test=raw_data_test['type']\nTest the performance of the candidate classifier 1 in test set 1.\n>x_test_new=x_test[gene_list]\n>y_pred=clf.predict(x_test_new)\n>y_proba=clf.predict_proba(x_test_new)\n>print(list(y_test))\n>print(list(y_pred))\n>for i in y_proba:\n\u00a0\u00a0\u00a0\u00a0print(i)\n>accuracy=accuracy_score(y_test, y_pred)\n>print(\"Classification Accuracy: %0.4f\" %accuracy)\n>classifi=classification_report(y_test, y_pred)\n>print(classifi)\n>logloss=log_loss(y_test, y_proba)\n>print('log_loss:'+str(logloss))\nOutput classifier 1 parameters.\n>print (\"Weighted coefficients of selected gene features:\")\n>print (svc.coef_)\n>print (\"Bias value of decision function b:\")\n>print (svc.intercept_)\n>print (\"Index of supported_vectors sample:\")\n>print (svc.support_)\n>print (\"All supported_vectors:\")\n>print (svc.support_vectors_)\n>print (\"Number of class-supported_vectors:\u201d)\n>print (svc.n_support_)\nRepeat steps 15 and 16 to test the performance of all 14 candidate classifiers in test set 1.\nRepeat steps 14, 15, 16 and 17 to test the performance of all 14 candidate classifiers in test set 2, 3 and 4.\nDetermine the optimal classifier.\nIf the prediction accuracy of the classifier in all test sets does not increase, then the classifier is optimal. Based on this, the classifier with C\u00a0= 0.2 is determined as the optimal classifier. Its prediction accuracy in the training set and the four test sets is 100%, 100%, 100%, 100%, 97% and 95% respectively (Table\u00a02[href=https://www.wicell.org#tbl2]).\ntable:files/protocols_protocol_2660_2.csv\nDetails of the performance of the optimal classifier in the four test sets (Table\u00a03[href=https://www.wicell.org#tbl3])\ntable:files/protocols_protocol_2660_3.csv\nDevelopment of mathematical model for embolic risk of ADSC cell\nTiming: \u223c1 h\nThis section describes how to develop mathematical model for embolic risk of ADSC cells based on the optimal classifier. We use the 13 key features (genes) obtained from the optimal classifier and their weight coefficients, with the expression level of these genes in cells, to establish the following mathematical model for calculating the embolic risk score of a single cell.After getting the gene expression profile of a cell, extract the expression amount of its 13 key genes, and then calculate the embolic risk of the cell according to the Equation\u00a0(1)[href=https://www.wicell.org#fd1]:\n(Equation\u00a01)\nR\nS\n=\n1\n+\ne\n\u2212\n\u2211\ni\n=\n1\nn\nW\ni\n\u2217\nG\ni\nNote: Wi is the weighted coefficient of ith gene determined by the optimal classifier and showed by the output of the \u201cprint (svc.coef_)\u201d command in the step 16, Gi is the expression of the ith key gene in this cell, and n is the number of key genes. The value of RS ranges from 0 to \u221e with small RS indicating a non-embolic cell and a larger risk score indicating a potential embolic cell.\nSelect an appropriate risk threshold according to the cell production process and determine whether the cell is a pro-embolic cell.\nUse an ROC curve analysis to determine the RS threshold of pro-embolic and non-embolic cells in test samples.\nNote: From the ROC curve, the thresholds of the four test sets were defined to be 2.131, 2.131, 2.048 and 3.368, respectively. The specificity and sensitivity of using the thresholds to distinguish cell embolism is more than 0.96 in each test dataset.\nYou can use following commands to determine RS thresholds of four test sets.\nNote: The input files are \u201cRSandLabel_test1.txt\u201d, \u201cRSandLabel_test2.txt\u201d, \u201cRSandLabel_test3.txt\u201d and \u201cRSandLabel_test4.txt\u201d. Each line of the input file is the cell name, the \u201cRiskScore\u201d column is the RS value of each cell, and the \u201cRealLabel\u201d column is the actual category label of each cell.\nlibrary(pROC)\ndata1\u00a0<- read.csv(\"RSandLabel_test1.txt\", header=T, sep='\\t')\ndata2\u00a0<- read.csv(\"RSandLabel_test2.txt\", header=T, sep='\\t')\ndata3\u00a0<- read.csv(\"RSandLabel_test3.txt\", header=T, sep='\\t')\ndata4\u00a0<- read.csv(\"RSandLabel_test4.txt\", header=T, sep='\\t')\nroc1\u00a0<- roc(data1$RealLabel, data1$RiskScore, levels=c(\"nonembolic\", \"embolic\"))\nroc2\u00a0<- roc(data2$RealLabel, data2$RiskScore, levels=c(\"nonembolic\", \"embolic\"))\nroc3\u00a0<- roc(data3$RealLabel, data3$RiskScore, levels=c(\"nonembolic\", \"embolic\"))roc4\u00a0<- roc(data4$RealLabel, data4$RiskScore, levels=c(\"nonembolic\", \"embolic\"))\nplot(roc1, print.auc=TRUE, col=\"purple\", print.auc.x=0.45, print.auc.y=0.4, print.thres=TRUE, cex.axis=1.5, cex.lab=2)\nplot.roc(roc2, add=T, col=\"black\", print.auc=TRUE, print.auc.x=0.45, print.auc.y=0.35)\nplot.roc(roc3, add=T, col=\"blue\", print.auc=TRUE, print.auc.x=0.45, print.auc.y=0.30, print.thres=TRUE)\nplot.roc(roc4, add=T, col=\"red\", print.auc=TRUE, print.auc.x=0.45, print.auc.y=0.25, print.thres=TRUE)\nlegend(\"bottomright\", legend=c(\"Test 1\", \"Test 2\", \"Test 3\", \"Test 4\"), col=c(\"purple\", \"black\", \"blue\", \"red\"), lwd=2, bty=\"n\", cex=1.5)\nCalculate the proportion of embolic cells in the sample, and predict the embolic possibility of reinfused individuals according to the established regression relationship between the embolic cell proportion and the embolic risk after reinfusion.\nUsing seven ADSC samples cultured with different culture techniques, infuse these ADSC samples into the mice (more than 6 mice per ADSC sample).1[href=https://www.wicell.org#bib1]\nCalculate the proportion of mice with pulmonary embolism, which is used as the embolism possibility after in\u00a0vivo ADSCs infusion.\nFor each ADSC sample, conduct the scRNA-seq experiment and identify the pro-embolic cells through step 20 and 21, and calculate the proportion of pro-embolic cells in each sample.\nEstablish the linear regression relationship between the pro-embolic cell proportion and the embolization possibility after the ADSC reinfusion.\nNote: NCG mice were purchased from GemPharmatech and all animal protocols were approved by the Institutional Animal Care and Use Committee, Experimental Animal Center. All mice were housed in standard SPF facility with a temperature between 18 \u00b0C and 23 \u00b0C, a humidity of 40%\u201360%, and a 12\u00a0h light-dark cycle. Eight-to-ten-week-old male and female NCG mice were used in this study. The number of mice used in each experiment was indicated, respectively. Mice were randomly assigned into groups. For the injection, 1\u00a0\u00d7\u00a0106 hADSCs were resuspended in saline and infused into each NCG mouse via tail vein slowly (about 10 s) using a 29-gauge needle. For complete details of the experimental models, please refer to our previous research.1[href=https://www.wicell.org#bib1]Note: The establishment details of this step have been described in the \u201cMathematical model to predict embolic risk\u201d in the \u201cResults\u201d section, and the established relationship curve of this protocol was shown in Figure\u00a06H of our published article.1[href=https://www.wicell.org#bib1]\nNote: For complete details on the use and execution of this protocol, please refer to our previous research.1[href=https://www.wicell.org#bib1]", "Step-by-Step Method Details\nStep-by-Step Method Details\nCell Preparation\nTiming: 10\u201320\u00a0min\nCells are trypsinized, counted, and lysed with ATAC-seq lysis buffer.\nNote: This protocol is a modification of a previous protocol by Buenrostro et\u00a0al. (2015).\nNote: Optimizing the cell number to transposase enzyme ratio is crucial to obtain DNA libraries of sufficient complexity. For example, over-transposition of chromatin results when too few cells are used and produces a library composed of mainly small DNA fragments (<500\u00a0bp). Under-transposition results when too many cells are used and generates mainly large difficult to sequence DNA fragments (>1,000\u00a0bp).\nHarvest PDCLs using trypsin and count cells\nTypically, 100,000 cells worked well for most PDCLs tested. To optimize this for different cell types a range of cell numbers should be tested first to establish optimal cell: transposase ratio.\nPipette cells using a 200\u00a0\u03bcL pipette tip to produce homogenous single cell suspension.\nCentrifuge 100,000 cells for 5\u00a0min at 600\u00a0\u00d7 g, 4\u00b0C.\nRemove and discard supernatant.\nAdd 50\u00a0\u03bcL ice-cold PBS to wash pellet, do not disturb pellet, and centrifuge for 5\u00a0min at 600\u00a0\u00d7 g, 4\u00b0C.\nRemove and discard supernatant.\nAdd 50\u00a0\u03bcL ice-cold ATAC-seq lysis buffer to pellet. Very gently dislodge the pellet. Centrifuge immediately for 10\u00a0min at 600\u00a0\u00d7 g, 4\u00b0C.\nCritical: It is crucial to treat the cell pellet gently during the lysis step to prevent over-lysis and preserve the native chromatin configuration to be analyzed.\nCritical: To proceed as quickly as possible to the transposition reaction, make the transposition master mix (described in step 9) while the pellet is spinning for 10\u00a0min at 600\u00a0\u00d7 g, 4\u00b0C (step 6)\nDiscard the supernatant and immediately continue to the transposition reaction.\nTransposition Reaction and Purification\nTiming: \u223c45\u00a0minCells are incubated with the Tn5 transposase enzyme to insert oligonucleotide tags within open regions of DNA.\nMake sure the cell pellet is placed on ice.\nTo make the transposition reaction mix, combine the following: 25\u00a0\u03bcL TD (2\u00d7 reaction buffer from Nextera kit) 4.7\u00a0\u03bcL TDE1 (Nextera Tn5 Transposase from Nextera kit) 22.3\u00a0\u03bcL nuclease-free H2O\nResuspend nuclei pellet in the transposition reaction mix.\nIncubate the transposition reaction at 37\u00b0C for 30\u00a0min at 400\u00a0rpm on an Eppendorf mixer.\nImmediately following transposition, purify the DNA using the Qiagen MiniElute PCR purification kit.\nElute transposed DNA in 10\u00a0\u03bcL DNase-free water.\nPause Point: Purified DNA can be stored at \u221220\u00b0C for up to 72 h.\nPCR Amplification\nTiming: 30\u201360\u00a0min\nTransposed DNA fragments are amplified by PCR using custom primers which contain the designated index (barcode) for each library.\nNote: Each library must contain a unique index to enable sample identification once the libraries have been pooled for sequencing.\nTo amplify transposed DNA fragments, combine the following in a 0.2\u00a0mL PCR tube: 10\u00a0\u03bcL transposed DNA 10\u00a0\u03bcL nuclease-free H2O 2.5\u00a0\u03bcL 25\u00a0\u03bcM PCR Primer 1 2.5\u00a0\u03bcL 25\u00a0\u03bcM Barcoded PCR primer 2 25\u00a0\u03bcL NEBNext High-Fidelity 2\u00d7 PCR master mix\nPerform initial amplification as follows:\ntable:files/protocols_protocol_124_2.csv\nCritical: To allow extension of both ends of the primer after transposition the first 5\u00a0min extension at 72\u00b0C is required.\nNote: The final number of PCR cycles is determined using qPCR to allow library amplification to be stopped prior to saturation. This helps to reduce GC and size biases.To run a qPCR side reaction, combine the following qPCR compatible consumables: 5\u00a0\u03bcL of previously PCR-amplified DNA 4.2\u00a0\u03bcL H2O 0.4\u00a0\u03bcL 25\u00a0\u03bcM primer 1 0.4\u00a0\u03bcL 25\u00a0\u03bcM primer 2 5\u00a0\u03bcL 2\u00d7 SYBR Select master Mix\nCritical: Use white qPCR plates to run the qPCR reaction. White plates help increase sensitivity and reduce variability in the qPCR assay.\nUsing a qPCR instrument, cycle as follows:\ntable:files/protocols_protocol_124_3.csv\nTo calculate the additional number of cycles required, plot linear Rn versus cycle and determine the additional number that corresponds to one-third of the maximin florescent intensity (Figure\u00a01[href=https://www.wicell.org#fig1]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/124-Fig1.jpg\nFigure\u00a01. Calculating the Number of Additional PCR Cycles Required for Library Amplification\nTo calculate the additional number of cycles required, plot linear Rn versus cycle and determine the additional number that corresponds to one-third of the maximum fluorescent intensity. For example, as shown for sample A replicate 1, the \u201cend point\u201d RFU value after 20 cycles is 3,900 which is divided by 3 to give \u201cone third\u201d RFU value of 1,300. A line is then drawn at 1,300 on the y-axis until it meets the amplification curve of that sample. This point is then joined to the x-axis to establish the number of additional cycles required for the library amplification. Take forward replicate libraries which require the minimum number of cycles for library amplification.\nRun the remaining 45\u00a0\u03bcL PCR reaction to the cycle number determined by qPCR.\ntable:files/protocols_protocol_124_4.csv\nPurify the amplified library using Qiagen Minielute PCR purification kit.\nElute in 20\u00a0\u03bcL EB buffer.\nDouble Size Selection\nTiming: 30\u201360\u00a0minThe first objective is to remove large DNA fragments which are >1,000\u00a0bp. Large fragments are difficult to accurately quantify and result in reduced clustering efficiencies when sequencing. The second objective is to remove smaller DNA fragments which are <100\u00a0bp. Smaller fragments are preferentially sequenced and consequently could become over-represented in the sequencing result. This step will also remove excess adapter sequenced.\nNote: This protocol is adapted from the SPRI Based Size Selection manual\nCritical: Vortex magnetic beads and bring beads and DNA to room temperature before starting size selection assay.\ntable:files/protocols_protocol_124_5.csv\nNote: Steps 22\u201327 are for Large Size Selection (>1,000\u00a0bp)\nVortex beads and bring beads and DNA to RT.\nMake all samples up to 50\u00a0\u03bcL using resuspension buffer (DNase-free water).\nTo select for fragments <1,000\u00a0bp use 0.55\u00d7 SPRIselect ratio.\nFor example: 50\u00a0\u03bcL sample \u00d7 0.55\u00d7 ratio\u00a0= 27.5\u00a0\u03bcL of SPRIselect to add to library.\nAdd beads to sample and pipette 10 times and incubate for 5\u00a0min at RT.\nCritical: Insufficient mixing of samples and SPRIselect will lead to inconsistent size selection results.\nNote: The yields of ATAC-seq libraries are typically lower as compared to other library protocols, e.g., RNA-seq or WGS. To ensure that all of the desired DNA fragments are bound to the beads the incubation time can be extended. However, extending the time of DNA and bead incubation can increase binding of unwanted size fragments.\nAdd samples to an appropriate magnetic rack or plate such as DynaMag-2 Magnet. Wait for the beads to attached to the magnet and for the sample to go clear. Bead attachment times can vary depending on the initial sample size, the volume of beads added, and the strength of the magnet.Take the supernatant and keep - the large unwanted fragments are attached to the beads.\nCritical: Be careful not to pipette any of the beads when removing the supernatant as this will contaminate the library prep with large DNA fragments.\nNote: Steps 28\u201332 are for Small Size Selection (>100\u00a0bp)\nNote: Increasing the ratio of SPRIselect volume to sample volume will decrease the efficiency of binding larger fragments.\nTo select for DNA fragments that are 100\u00a0bp and above use a final volume of 1.8\u00d7 SPRIselect based on the initial reaction volume. The second SPRIselect volume is determined after accounting for the first volume of 0.55\u00d7 SPRIselect added.\nFor example: 50\u00a0\u03bcL sample \u00d7 (1.8\u00d7 ratio \u2212 0.55\u00d7 ratio)\u00a0= 62.5\u00a0\u03bcL SPRIselect.\nAdd SPRIselect to sample, mix well, and incubate for 5\u00a0min at RT.\nAdd sample to magnetic rack.\nMake fresh 80% ethanol.\nOnce beads have attached to the magnet and the sample has gone clear, remove, and discard the supernatant. The library is now attached to the beads.\nCritical: Be careful not to remove beads during this step because the desired library is now associated with the beads. Significant bead loss will result in reduced yield.\nWith the reaction vessel still on the magnet, add 500\u00a0\u03bcL of 80% ethanol and incubate at RT for 1\u00a0min. Remove and discard the ethanol supernatant.\nCritical: Take care not to remove any beads during this step.\nCarefully remove residual ethanol with a pipette and dry for 5\u00a0min at RT.\nCritical: Allow all ethanol to evaporate but DO NOT let the beads dry out.\nThe bead supernatant will begin to crack if allowed to dry out completely \u2013 avoid this.\nRemove sample from magnetic rack and resuspend beads in 23\u00a0\u03bcL nuclease-free H2O. Mix well.Note: Elution volume should be large enough so that the liquid level is high enough for the beads to settle to the magnet.\nLeave samples to stand off the magnetic rack for 1\u00a0min.\nAdd samples back to the magnetic rack and allow beads to attach to the magnet.\nOnce clear, take 20\u00a0\u03bcL of eluted DNA and store in a DNA-low binding Eppendorf at 4\u00b0C.\nEstimate of Library Yield Using the Qubit dsDNA HS Assay Kits\nTiming: 15\u00a0min\nTo accurately quantify the ATAC-seq library we recommend using Agilent Bioanalyzer high sensitivity DNA chips. These chips are extremely sensitive and work best when 1\u00a0ng/\u03bcL library is loaded. To establish the ATAC-seq library concentration for loading onto the bioanalyzer chip, first use the Qubit dsDNA HS Assay Kits to obtain a rough estimate of library yield.\nThe Qubit dsDNA HS (High sensitivity) Assay Kits include concentrated assay reagent, dilution\u00a0buffer and prediluted DNA standards. The assay is accurate for sample concentrations from 10 pg/\u03bcL to 100\u00a0ng/\u03bcL. This assay is prepared at room temperature (RT) and is stable for\u00a03 h.\nNote: See life technologies cat# Q32851, Q32854 for Qubit dsDNA HS Assay Kits user manual.\nSet up the required number of 0.5\u00a0mL tubes for standards and samples. The Qubit dsDNA HS assay requires 2 standards.\nCritical: Use only thin-wall, clear 0.5\u00a0mL PCR tubes. Acceptable tubes include Qubit assay tubes (cat #Q32856) or Axygen PCR-05-C tubes (cat # 10011-830).\nLabel the tube lid.\nCritical: Do not label the side of the tube as this could interfere with the sample read.\nPrepare Qubit working solution by diluting the dsDNA HS reagent 1:200 in dsDNA HS buffer.Note: The final volume in each tube must be 200\u00a0\u03bcL. Each standard tube contains 190\u00a0\u03bcL of Qubit working solution, and each sample tube requires somewhere between 180\u00a0\u03bcL to 199\u00a0\u03bcL (depending on the concentration or your prepared library).\nFor the standards, add 10\u00a0\u03bcL standard to 190\u00a0\u03bcL Qubit working solution.\nFor samples, add 2\u00a0\u03bcL sample to 198\u00a0\u03bcL Qubit working solution.\nNote: Typically, 2\u00a0\u03bcL of the library prep provides a sufficient volume to give an accurate Qubit reading which fell in between the range of detection, i.e., between the two standards. However, test samples can be anywhere from 1\u201320\u00a0\u03bcL. Add corresponding volume of Qubit working solution to each tube: anywhere from 180\u2013199\u00a0\u03bcL.\nMix all samples by vortexing then spin briefly.\nIncubate at RT for 2\u00a0min\nRead samples on Qubit 3.0 Fluorometer by pressing the DNA option, then select dsDNA High Sensitivity as the assay type.\nPress Read Standards to proceed\nInsert tube containing Standard #1. Read standard and once complete, read standard #2.\nPress Run samples.\nOn the assay screen select sample volume and units: Sample volume from 1\u201320\u00a0\u03bcL Select the units for the output sample concentration, i.e., ng/\u03bcL\nRead all samples.\nLibrary Quantification and Molarity Calculation Using the Bioanalyzer\nTiming: 2 h\nThe Bioanalyzer high sensitivity DNA kits can quantify libraries from just a few amplification cycles thereby reducing the amplification bias of shorter DNA fragments and improving the quality of sequencing data. The kit also offers improved sensitivity for checking the size and quantity of DNA samples of limited abundance.\nNote: See protocol brochure for detailed step-by-step procedure: Manual Part Number: G2938-09321 Rev.B\nCritical: Exactly 1\u03bcL of sample must be loaded. A mximum of 11 samples can be run per chip.The chip can detect DNA fragments in the range of 50\u20137,000\u00a0bp. This sizing range is optimal for analyzing the full spectrum of DNA fragments generated in ATAC-seq libraries.\nQuantitative range is 5\u2013500 pg/\u03bcL\nMake sure gel matrix reagents are at RT for at least 30\u00a0min before adding to chip.\nTypically, 500 pg/\u03bcL loaded onto the chip gives an accurate quantification.\nRun HS DNA chip on Bioanalyzer and quantify library molarity using the bioanalyzer software: 2100 Expert Software (Figures 2[href=https://www.wicell.org#fig2] and 3[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/124-Fig2.jpg\nFigure\u00a02. Calculating the ATA-Seq Library Molarity Using the Agilent Bioanalyzer\n(A) Run 0.5\u00a0ng/\u03bcL of ATAC-seq library on the Bioanalyzer.\n(B) Select \u201cManual Integration\u201d to enable removal of integrated peaks.\n(C) Delete integrated peaks.\n(D) Now \u201cAdd Peaks\u201d to incorporate all distinct peaks in the library. Add the molarity values for the selected peaks to establish the library molarity.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/124-Fig3.jpg\nFigure\u00a03. Example of How to Pool the ATAC-Seq Library for Sequencing\nNote: Download for free from: https://www.agilent.com/en/product/automated-electrophoresis/bioanalyzer-systems/bioanalyzer-software/2100-expert-software-228259[href=https://www.agilent.com/en/product/automated-electrophoresis/bioanalyzer-systems/bioanalyzer-software/2100-expert-software-228259]\nPause Point: Libraries can be stored individually at \u221220\u00b0C.\nCritical: To prevent index hopping, libraries should be pooled just before sequencing. For further details please refer to https://emea.illumina.com/content/dam/illumina-marketing/documents/products/whitepapers/index-hopping-white-paper-770-2017-004.pdf[href=https://emea.illumina.com/content/dam/illumina-marketing/documents/products/whitepapers/index-hopping-white-paper-770-2017-004.pdf].", "Step-by-step method details\nStep-by-step method details\nModel training\nTiming: 120\u00a0min\nThese steps show details of training the Deep-Fundus quality detection model.\nChange directory to the \u2018Deep-fundus\u2019 folder.\nTrain model.\nRun \u2018train.py\u2019. If you have a GPU available for model training, assign \u2018os.environ[\u2018\u2018CUDA_-VISIBLE_DEVICES\u2019\u2019]\u2019 as the serial number of the GPU.\nRun the \u2018train.py\u2019 and set the arguments simultaneously. For example:\n>python train.py --aspect acceptable\nNote: There are 4 arguments: aspect, image_dir, train_file_path, val_file_path. The \u2018aspect\u2019 argument indicates the quality aspect that you want to train. If you intended to use your own data, you should specify other three arguments, meanwhile, the content format of the file must be the same as our files in the demo.\nCritical: The hyperparameters, such as the data augmentation settings, \u2018epochs\u2019, \u2018IMG_SIZE, etc., have been fine-tuned and established as the default values, reflecting the optimal choices. However, you have the flexibility to customize these values according to your preferences.\nThe following codes help you load the csv file.\n>df_train\u00a0= pd.read_csv(args.train_file_path).dropna(subset=[y_col])[:100]\n>df_train[y_col]\u00a0= df_train[y_col].astype('str')[:100]\n>df_val\u00a0= pd.read_csv(args.val_file_path).dropna(subset=[y_col])[:100]\n>df_val[y_col]\u00a0= df_val[y_col].astype('str')[:100]\nThe following codes can augment data when training, you can reset the arguments to customize your data augmentation way.\n>idg\u00a0= ImageDataGenerator(\n>rescale=1./255,\n>rotation_range=90,\n>horizontal_flip=True,\n>vertical_flip=True,\n>height_shift_range=0.02,\n>width_shift_range=0.02,\n>#brightness_range=(0.5, 1.5),\n>#channel_shift_range=0.5,\n)\n>tdg\u00a0= ImageDataGenerator(rescale=1./255)\nThe following codes create model and train the model. You can use other model or different training method by modifying the codes.\n> base\u00a0= InceptionResNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights=\"imagenet\", pooling=\"avg\")\n>x\u00a0= base.output\n>x\u00a0= Dense(1024, activation=\"relu\")(x)\n>x\u00a0= Dropout(0.5)(x)\n>x\u00a0= Dense(1, activation=\"sigmoid\")(x)\n>model\u00a0= Model(inputs=base.input, outputs=x)\n>model.compile(\"adam\", loss=focal_loss(alpha=0.5), metrics=[\"accuracy\"])\n>model.fit_generator(\n>idg.flow_from_dataframe(df_train,\n>args.image_dir,\n>y_col=y_col,\n>class_mode=\"binary\",\n\u00a0\u00a0seed=42,\n\u00a0\u00a0target_size=(IMG_SIZE, IMG_SIZE),\n\u00a0\u00a0batch_size=10,\n),\nsteps_per_epoch=890,\nepochs=500,\nvalidation_data=tdg.flow_from_dataframe(df_val,\n\u00a0\u00a0args.image_dir,\n\u00a0\u00a0y_col=y_col,\n\u00a0\u00a0class_mode=\"binary\",\n\u00a0\u00a0seed=42,\n\u00a0\u00a0target_size=(IMG_SIZE, IMG_SIZE),\n\u00a0\u00a0batch_size=10,\ntarget_size=(IMG_SIZE, IMG_SIZE),\n\u00a0\u00a0batch_size=10,\n),\nModel inference\nTiming: 20\u00a0min (Figure 2[href=https://www.wicell.org#fig2])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2998-Fig2.jpg\nFigure\u00a02. Prediction modes of DeepFundusThese steps demonstrate how to make predictions using a trained model.\nChange directory to the \u2018Deep-fundus\u2019 folder.\nMake prediction.\nRun \u2018inference.py\u2019. If you have a GPU available for model training, assign \u2018os.environ[\u2018\u2018CUDA_-VISIBLE_DEVICES\u2019\u2019]\u2019 as the serial number of the GPU.\nRun the \u2018inference.py\u2019 and set the arguments simultaneously. For example:\n>python inference.py --mode 1\nNote: There are 3 arguments: mode, image_dir, image_size. The \u2018mode\u2019 argument indicates the mode of making prediction, where 1 indicates quality classification and 2 indicates real-time guidance. The program generates result files in \u2018results\u2019 directory which will be used in evaluation step.\nThe following code can generate results of different aspect quality (mode 1)\nif args.mode\u00a0== 1:\n\u00a0\u00a0res_dict\u00a0= dict([(k, [])\u00a0for k in aspect_to_model.keys()])\n\u00a0\u00a0for aspect in list(aspect_to_model.keys()):\n\u00a0\u00a0\u00a0\u00a0clear_session()\n\u00a0\u00a0\u00a0\u00a0model\u00a0= load_model(os.path.join('models/', aspect_to_model[aspect]), compile=False)\n\u00a0\u00a0\u00a0\u00a0all_fn\u00a0= sorted(os.listdir(args.image_dir))\n\u00a0\u00a0\u00a0\u00a0for fn in all_fn:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0try:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0image\u00a0= Image.open(os.path.join(args.image_dir, fn)).convert('RGB')\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0image\u00a0= image.resize((512, 512))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0image\u00a0= np.array(image) / 255.\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0image\u00a0= image.reshape(1, 512, 512, -1)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0y_pred_prob\u00a0= float(np.squeeze(model.predict(image)))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0res_dict[aspect].append(y_pred_prob)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0except:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0print('error when opening '.format(fn))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0continue\n\u00a0\u00a0\u00a0\u00a0df_proba\u00a0= pd.DataFrame(res_dict, index=all_fn)\n\u00a0\u00a0\u00a0\u00a0df_pred\u00a0= df_proba.applymap(lambda x: 1 if x\u00a0>\u00a00.5 else 0)\n\u00a0\u00a0\u00a0\u00a0df_pred['illuminate_od']\u00a0= df_proba['illuminate_od'].map(lambda x: 1 if x\u00a0>\u00a00.2 else 0)\n\u00a0\u00a0\u00a0\u00a0df_pred['illuminate_macula']\u00a0= df_proba['illuminate_macula'].map(lambda x: 1 if x\u00a0>\u00a00.39 else 0)\n\u00a0\u00a0\u00a0\u00a0df_pred\u00a0= df_pred.drop(['cataract'], axis=1)\n\u00a0\u00a0\u00a0\u00a0df_proba\u00a0= df_proba.drop(['cataract'], axis=1)\n\u00a0\u00a0\u00a0\u00a0df_proba.to_excel('results/quality_proba.xlsx')\nThe following code can provide advice based on the input images (mode 2)\nelse:\n\u00a0\u00a0all_fn\u00a0= sorted(os.listdir(args.image_dir))\n\u00a0\u00a0predictions\u00a0= []\n\u00a0\u00a0for fn in all_fn:\n\u00a0\u00a0\u00a0\u00a0result\u00a0= ''\n\u00a0\u00a0\u00a0\u00a0for aspect in ['overall', 'position', 'illuminate', 'clarity', 'cataract']:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0clear_session()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0model\u00a0= load_model(os.path.join('/data/yellowcard/llx/models/', aspect_to_model[aspect]), compile=False)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0try:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0image\u00a0= Image.open(os.path.join(args.image_dir, fn)).convert('RGB')\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0image\u00a0= image.resize((512, 512))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0image\u00a0= np.array(image) / 255.\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0image\u00a0= image.reshape(1, 512, 512, -1)\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0y_pred_prob\u00a0= float(np.squeeze(model.predict(image)))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0except:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0print('error when opening {}'.format(fn))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0continue\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if aspect\u00a0== 'overall' and y_pred_prob <= 0.5:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0result\u00a0= 'finish'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0breakif aspect\u00a0== 'position' and y_pred_prob\u00a0>\u00a00.5:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0result\u00a0= 'recapture'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0break\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if aspect\u00a0== 'illuminate' and y_pred_prob\u00a0>\u00a00.5:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0result\u00a0= 'recapture'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0break\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if aspect\u00a0== 'clarity' and y_pred_prob <= 0.5:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0result\u00a0= 'finish'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0break\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if aspect\u00a0== 'cataract':\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0if y_pred_prob\u00a0>\u00a00.5:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0result\u00a0= 'referral'\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0else:\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0result\u00a0= 'recapture'\n\u00a0\u00a0predictions.append(result)\ndf\u00a0= pd.DataFrame({'prediction': predictions}, index=all_fn)\ndf.to_excel('results/advice_pred.xlsx')\nModel evaluation\nTiming: 20\u00a0min (Figure 3[href=https://www.wicell.org#fig3])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2998-Fig3.jpg\nFigure\u00a03. Model evaluation for quality classification and real-time guidance\nFor quality classification mode of DeepFundus, the common classification metrics, as well as ROC (Receiver Operating Characteristic) will be generated (A). For real-time guidance mode, the common classification metrics and confusion matrix will be generated (B).\nThese steps describe how to get the corresponding metrics.\nChange directory to the \u2018Deep-fundus\u2019 folder.\nGet metrics.\nRun \u2018evaluation.py\u2019. If you use your own files, you need to change the excel path in the code.\nRun the \u2018evaluation.py\u2019 and set the arguments simultaneously. For example:\n>python evaluation.py --mode 1\nNote: The definition of \u2018mode\u2019 argument is the same as above. For mode 1, the common classification metrics, as well as ROC will be generated in \u2018results\u2019 directory, and for mode 2, the common classification metrics and confusion matrix will be generated.\nResults display through heatmaps\nTiming: 60\u00a0min (Figure 4[href=https://www.wicell.org#fig4])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2998-Fig4.jpg\nFigure\u00a04. Demonstration of original input images and corresponding heatmaps\nWe presented fundus images of 3 different types of quality defects, along with the respective category prediction probability scores. Furthermore, the algorithm identifies and displays the high-risk areas through Grad-CAM heatmaps.The next steps elucidate the procedure of generating the areas where the prediction of DeepFundus was based through Grad-CAM. The Grad-CAM algorithm facilitates the production of a class-specific activation heatmap, where the significance of classification towards a particular class is denoted by each activation value. More important features are indicated by redder regions.\nDepict the heatmap through Grad-CAM.\nRun \u2018grad_cam.py\u2019. If you use your own files and have a GPU available for model training, assign \u2018os.environ[\u2018\u2018CUDA_-VISIBLE_DEVICES\u2019\u2019]\u2019 as the serial number of the GPU.\nRun the \u2018grad.py\u2019 and set the arguments simultaneously. For example:\n>python grad.py --aspect acceptable\nNote: The definition of the \u2018mode\u2019 argument is the same as above. If you use your own model, maybe you should change the convolution layer name specified in \u2018heatmap, heatmap\u00a0= grad_cam(model, image, pred, 'conv_7b')\u2019", "Step-by-step method details\nStep-by-step method details\nCell preparation\nTiming: 4 h\nMouse (Mus musculus) embryos at day 11 of development (E11; 42\u201345 somite pairs), from wild type (WT, PDGRF\u03b2+/+) and knockout (KO, PDGRF\u03b2\u2212/\u2212) strains obtained by breeding heterozygous PDGRF\u03b2+/\u2212 adults in a C57BL/6J background were used.5[href=https://www.wicell.org#bib5]\nEmbryo dissection.\nPrepare a 2.5% w/v collagenase type 1 stock solution by adding 1\u00a0g of collagenase type 1\u201340\u00a0mL of sterile PBS and filter using a large syringe with a 0.45\u00a0\u03bcm filter.\nNote: Aliquots of the collagenase type I stock solution can be stored in Eppendorf tubes at \u221220\u00b0C at long-term and thawed at room temperature (20\u00b0C\u201322\u00b0C) or for few minutes in a water bath at 37\u00b0C on the day of the experiment.\nCull pregnant dams and harvest the embryos on ice, in cold filtered PBS buffer solution enriched with 10% fetal calf serum (FCS), and 1% penicillin/streptomycin (PS), henceforth referred to as PBS/FCS/PS.\nNote: PBS/FCS/PS working solution should be stored at 4\u00b0C and used within 1\u00a0week.\nDissect the embryos right after in a petri-dish with PBS/FCS/PS under a stereoscope using a pair of scissors and tweezers as previously described.14[href=https://www.wicell.org#bib14]\nNote: The time to dissect can vary between scientists and thus all embryos should be kept on ice prior dissection.\nEnsure that the embryos used are at the desired stage of development by counting the somite pairs.\nTransfer a piece of tissue from each embryo (usually the yolk sac or the head) in an empty labeled 1.5\u00a0mL Eppendorf tube and proceed with genotyping immediately (protocol described in steps 3 and 4 below).Note: These tubes can be frozen at \u221220\u00b0C if not used on the same day. If cells are used for scRNA-seq, genotyping should be done immediately, in parallel with tissue dissection and thus, this may require 2 scientists. If AGM cells are used to derive MSCs, genotyping can be done after cells are seeded the same day or after.\nEmbryonic cell dissociation.\nDissect single AGMs and transfer them in 1.5\u00a0mL Eppendorf tubes (1 AGM/tube), containing 190\u00a0\u03bcL of PBS/FCS/PS.\nAdd 10\u00a0\u03bcL of collagenase type I from the stock solution (1:20, final 0.125% w/v) to each Eppendorf and incubate in a water bath at 37\u00b0C for 45\u00a0min.\nCritical: Do not leave the tissue for longer than 45\u00a0min since it may be detrimental to cell viability.\nAt room temperature (20\u00b0C\u201322\u00b0C), mechanically disrupt the remaining tissue by gently pipetting up and down using a P200 pipette. Repeat until the tissue is not visible anymore.\nWash the cells by adding 1\u00a0mL of cold PBS/FCS/PS in each tube.\nCentrifuge for 10\u00a0min, at 2,000\u00a0rpm and 4\u00b0C (pre-cooled).\nDiscard the supernatant and resuspend the cell pellet in either PBS/FCS/PS for library preparation or MSC media for culture.\nDNA extraction.\nAdd 100\u00a0\u03bcL of Extraction Solution and 25\u00a0\u03bcL of Tissue Preparation Solution to each sample.\nIncubate and shake all samples in a pre-heated shaker at 55\u00b0C for 10\u00a0min.\nProceed to DNA denaturation by transferring tubes, without lid, to a pre-heated heating block at 95\u00b0C for 3\u00a0min.\nRemove tubes from the heating block and add 100\u00a0\u03bcL of Neutralization Solution B, then vortex for a few seconds.Store genomic DNA on ice for a few minutes while preparing the Polymerase Chain Reaction (PCR) mix. Alternatively, genomic DNA can be stored for the next day at 4\u00b0C or for up to a month at \u221220\u00b0C.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2349-Fig1.jpg\nFigure\u00a01. PDGFR\u03b2-KO genotyping example\nThe upper band represents the ko allele (320\u00a0bp), while the lower band represents the wt allele (114\u00a0bp). Samples 1, 2, 3, 4, 6, 7 and 10 are PDGFR\u03b2\u00a0+/- (heterozygous), 5 and 11 are PDGFR\u03b2\u00a0+/+ (WT) and 9 is PDGFR\u03b2 \u2212/\u2212 (KO).\nPCR and electrophoresis.\nPrepare a PCR master mix. For one sample, add 13.9\u00a0\u03bcL of autoclaved dH2O, then 2\u00a0\u03bcL of Coral Load Buffer 10\u00d7, 1\u00a0\u03bcL of 25\u00a0mM MgCl2, 1\u00a0\u03bcL of 10\u00a0mM dNTPs, 0.1\u00a0\u03bcL of each of the 3 following primers: Forward ACA ATT CCG TGC CGA GTG ACA G, WT Reverse AAA AGT ACC AGT GAA ACC TCG CTG and KO Reverse ATC AGC CTC GAC TGT GCC TTC TAG, and 0.3\u00a0\u03bcL of HotStarTaqPlus DNA polymerase (5\u00a0U/\u03bcL).\nNote: Reagents such as the Coral Load Buffer, MgCl2 solution and DNA polymerase are included in the Hot Start Taq Plus kit. All other reagents should be ordered separately.\nDispense 18\u00a0\u03bcL of the mix to a 0.2\u00a0mL PCR tube and add 2\u00a0\u03bcL of the genomic DNA sample for a total volume of 20\u00a0\u03bcL by a short spin down.Add the PCR reaction tubes to a thermocycler and run the following program (it takes approx. 120\u00a0min): 1 cycle at 95\u00b0C for 5\u00a0min for initial DNA denaturation, 35 cycles of denaturation at 95\u00b0C for 1\u00a0min, primer annealing at 58\u00b0C for 1\u00a0min, extension at 72\u00b0C for 1\u00a0min, followed by 1 cycle for 10\u00a0min at 72\u00b0C for the final extension and hold at 10\u00b0C until stop instruction.\nPrepare 1.5% agarose gel in 1\u00d7 TAE Buffer with SYBR\u00aeSafe DNA gel stain at 1:10 dilution.\nLet gel polymerize in the chemical hood at room temperature (20\u00b0C\u201322\u00b0C) for at least 20\u00a0min.\nSet up the polymerized gel in an electrophoresis tank, previously filled with fresh 1\u00d7 TAE, and load a DNA molecular weight marker (20\u00a0\u03bcL of EasyLadder I).\nLoad the samples and run the gel at 115\u00a0V and 400 mA, for 90\u00a0min.\nAnalyze gel with a UV transilluminator and read the bands for WT at 114\u00a0bp and the KO at 320\u00a0bp (Figure\u00a01[href=https://www.wicell.org#fig1]).\nNote: The details described above are optimized for this PDGRF\u03b2 mouse strain and may differ from other murine strains.\nSingle-cell RNA-sequencing data analysis\nTiming: days to weeks\nThis major step describes an end-to-end analysis of scRNA-seq data, to understand the changes occurring in mesenchymal stem/stromal cells (MSCs) in the PDGFR\u03b2-KO AGM at a single-cell resolution. The analysis steps broadly correspond with the OSCA Bioconductor workflow, based on the scran10[href=https://www.wicell.org#bib10] and scater8[href=https://www.wicell.org#bib8] Bioconductor packages15[href=https://www.wicell.org#bib15]; similar steps may be carried out via the Seurat Bioconductor package.16[href=https://www.wicell.org#bib16] Code examples generally refer to a single sample (which we refer to as \u2018sce_wt\u2019); the analysis steps are almost identical for both WT and KO samples.\nLibrary preparation for single-cell analysis.Prepare dissociated AGMs in a single-cell suspension as mentioned above to be processed for sample loading and library preparation as described in the protocol \u201cChromium Next GEM Single Cell 3\u2032 Reagent Kits User Guide (v3.1 Chemistry[href=https://www.10xgenomics.com/support/single-cell-gene-expression/documentation/steps/library-prep/chromium-single-cell-3-reagent-kits-user-guide-v-3-1-chemistry])\u201d by 10x Genomics.\nLoad 7\u00a0\u00d7\u00a0103 cells in the Chromium Next Gem Chip G, viable cells are counted in a 1:1 trypan blue ratio (cell range set for 8\u201311\u00a0\u03bcm) to calculate the volume of cell suspension required.\nNote: A table is provided in the 10x Genomics protocol to cross cell stock concentration (cell/\u03bcL) and the desired targeted cell recovery (for example 7\u00a0\u00d7\u00a0103 cells).\nSingle-cell libraries are to be obtained according to the manufacturer\u2019s protocol which consist of barcoding, amplifying cDNA, gel-emulsion droplets (GEM) generation and cDNA amplification and quantification, detailed here: \u201cChromium Next GEM Single Cell 3\u2032 Reagent Kits User Guide (v3.1 Chemistry[href=https://www.10xgenomics.com/support/single-cell-gene-expression/documentation/steps/library-prep/chromium-single-cell-3-reagent-kits-user-guide-v-3-1-chemistry])\u201d.\nQuantify RNA concentration and the quality of the libraries.\nSend your libraries to sequencing.\nGenerating single-cell count matrices.\nTransfer your sequencing data from the sequencing facility to the computer where the data will be processed.\nNote: The exact steps for data transfer will vary, depending on your sequencing provider and your local compute infrastructure.\nAt the command line, use 10x Genomics Cell Ranger to generate single cell feature counts for each sample separately, running \u201ccellranger count\u201d with the reference dataset previously downloaded.\nNote: Here, we used 10x Genomics reference mm10/GRCm38\u20133.0.0. The \u2018id\u2019 parameter can be specified to label the sample (eg.: as WT or KO).\n$ cellranger count --id=SAMPLE_ID\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--transcriptome=/path/to/reference/refdata-cellranger-mm10-3.0.0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--fastqs=/path/to/sample/fastqs\nImport and filter single-cell data.\nDownload the \u2018raw_feature_bc_matrix\u2019 directory output from cellranger count for each sample: this will contain three files: barcodes.tsv.gz, features.tsv.gz and matrix.mtx.gz.\nIn R, use read10xCounts() from the DropletUtils Bioconductor package7[href=https://www.wicell.org#bib7] to import the Cell Ranger output into R in the SingleCellExperiment format.#import cell ranger output and assign to \u2018sce_wt\u2019 object\n> sce_wt <- read10xCounts(\"/path/to/raw_feature_bc_matrix\", col.names=T)\nGenerate barcode rank plots (Figure\u00a02[href=https://www.wicell.org#fig2]) to monitor the distribution of barcode counts, then run emptyDrops() to identify empty droplets. Remove cells predicted to contain only ambient RNA, using the default false discovery rate (FDR) of 0.1%.\n#generate barcode rank plot for \u2018sce_wt\u2019 object\n> bcrank_wt <- barcodeRanks(counts(sce_wt))\n> uniq_wt <- !duplicated(bcrank_wt$rank)\n> plot(bcrank_wt$rank[uniq_wt],bcrank_wt$total[uniq_wt],\n\u00a0\u00a0\u00a0\u00a0log=\"xy\",xlab=\"Rank\",ylab=\"Total UMI count\",\n\u00a0\u00a0\u00a0\u00a0cex.lab=1.2)\n> abline(h=metadata(bcrank_wt)$inflection,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0col=\"darkgreen\",lty=2)\n> abline(h=metadata(bcrank_wt)$knee,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0col=\"dodgerblue\",lty=2)\n> legend(\"bottomleft\",legend=c(\"Inflection\",\"Knee\"),\ncol=c(\"darkgreen\",\"dodgerblue\"),lty=2,cex=1.2)\n#remove cells with only ambient RNA via emptydrops\n> e.out_wt <- emptyDrops(counts(sce_wt))\n> sce_wt <- sce_wt[,which(e.out_wt$FDR<=0.001)]\nNote: As of v3, Cell Ranger implements a version of the EmptyDrops algorithm that provides similarly filtered barcode matrices, in the \u2018filtered_feature_bc_matrix\u2019 directory.\nAnnotate and perform quality control on single-cell data.\nIdentify mitochondrial genes by annotating each sample using Bioconductor\u2019s AnnotationHub service with an appropriate reference.\nNote: Here, we used Ensembl mm38.93 for annotation, to match that used in the creation of the 10x Genomics reference.\n#import ensembl annotation\n> ensdb_mm38.93\u00a0<- AnnotationHub()[[\"AH64461\"]]\n> chr_loc <- mapIds(ensdb_mm38.93, keys=rownames(sce_wt),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0keytype=\"GENEID\", column=\"SEQNAME\")\n> is.mito <- which(chr_loc==\"MT\")\nCalculate the library size, number of expressed features and percentage of mitochondrial reads for each cell using perCellQCMetrics() from the scuttle Bioconductor package: (Figure\u00a03[href=https://www.wicell.org#fig3]).\nUsing the median absolute deviation (MAD) definition of outliers and perCellQCFilters(), remove cells with any quality metric more extreme than 3 MADs from the median.\nFinally, remove any cells with detectable PDGFR\u03b2 expression from the KO sample.\n#compute QC metrics for \u2018sce_wt\u2019\n> df <- perCellQCMetrics(sce_wt, subsets=list(Mito=is.mito))\n> qc <- perCellQCFilters(df,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sub.fields=c(\"subsets_Mito_percent\"))\n#filter out low quality cells\n> sce_wt <- sce_wt[,!qc$discard]\n#remove cells with Pdgfrb expression from KO sample only:\n> sce_ko <- sce_ko[,(counts(sce_ko)[rowData(sce_ko)$Symbol==\"Pdgfrb\",]==0)]\nNormalization and feature selection.Prepare for normalizing the expression data for each sample by running scran\u2019s quickCluster algorithm. This is intended to quickly estimate clusters of cells with distinct expression profiles.\nUse the resultant clusters as input to computeSumFactors(), which implements a deconvolution method10[href=https://www.wicell.org#bib10] for scaling normalization of sparse count data.\nNote: We specify \u2018min.mean\u00a0= 0.1\u2019 in the call to computeSumFactors(), to define the minimum (library size-adjusted) average count of genes to be used for normalization. Setting this parameter avoids using very low-abundance genes for the sum factor computation: if too many genes have consistently low counts across all cells, the computed size factors may be close to zero.\nRun the normalization using the computed size factors with logNormCounts().\nModel the variance of the log-expression profile for each gene using scran\u2019s modelGeneVarByPoisson() function. This function decomposes log-expression into technical and biological components based on a mean-variance trend corresponding to Poisson noise and utilizes the size factors computed earlier.\nFinally, define highly variable genes (HVGs) for each sample using getTopHVGs().\n#quickly estimate clusters of cells and compute sum factors\n> wt_clusters <- quickCluster(sce_wt, use.ranks=F,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0BSPARAM=IrlbaParam())\n> sce_wt <- computeSumFactors(sce_wt, min.mean=0.1,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0cluster=wt_clusters)\n#run the normalization\n> sce_wt <- logNormCounts(sce_wt)\n#modelling gene variation:\n> dec_pois_wt <- modelGeneVarByPoisson(sce_wt)\n#define highly variable genes:\n> hvg_pois_wt <- getTopHVGs(dec_pois_wt)\nNote: modelGeneVar() performs a similar function to modelGeneVarByPoisson(), but tends to understate biological variation in heterogeneous datasets, such as whole unfractionated AGM, as we analyze here.\nDimensionality reduction and visualization.\nUse denoisePCA() to perform dimensionality reduction via principal components analysis (PCA) to eliminate random technical noise in the data. This function utilizes both the model of gene variance and the HVGs defined previously.Note: In S\u00e1 da Bandeira et\u00a0al., we explicitly set the number of principal components (PCs) to retain using the min.rank and max.rank parameters as shown in this code example. These parameter settings were chosen based on an exploratory PCA, followed by analysis of the resulting scree plots (example in Figure\u00a04[href=https://www.wicell.org#fig4]). The scree plot illustrates the proportion of variance explained by each PC: the number of PCs to retain is often chosen by identifying an \u2018elbow\u2019 on the plot, beyond which retaining additional PCs explains little additional variance.\n#run PCA\n> sce_wt <- denoisePCA(sce_wt,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0technical=dec_pois_wt,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0subset.row=hvg_pois_wt,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0min.rank=10, max.rank=10)\nCritical: The same number of PCs must be retained in all samples to subsequently integrate the samples downstream (step 13), since Batchelor\u2019s correctExperiments() function combines the dimensional reduction matrices for each sample and these are required to be the same size. Subsequently, we are required to choose a number of PCs which explain sufficient variation in all samples.\nEach sample may be visualized as t-SNE or UMAP plots via scater\u2019s runTSNE() and runUMAP() functions; both utilize the PCA reduction just computed.\n#run TSNE, UMAP\n> sce_wt <- runTSNE(sce_wt, use_dimred=\"PCA\")\n> sce_wt <- runUMAP(sce_wt, use_dimred=\"PCA\")\n#plot (example for TSNE)\n> plotReducedDim(sce_wt, \"TSNE\")\nNote: It is often useful to compare t-SNE reductions of different perplexities17[href=https://www.wicell.org#bib17] side-by-side. Rather than using runTSNE(), it may be advantageous to run the Rtsne() function underlying runTSNE() directly, specifying the desired perplexity and storing the resulting matrix in the reducedDims slot of the SingleCellExperiment object with an appropriate name (eg \u201cTSNE30\u201d rather than \u201cTSNE\u201d).\n#manually run, store TSNE reductions of different perplexity #here, for example, perplexity=30\n> t30\u00a0<- Rtsne(reducedDim(sce_wt,\"PCA\"), pca=FALSE,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0perplexity=30)\n> reducedDims(sce_wt)$TSNE30\u00a0<- t30$Y\nClustering.\nConstruct a shared nearest-neighbor (SNN) graph for each sample, using scran\u2019s buildSNNGraph() function.Compute an initial clustering for each sample (Figure\u00a05[href=https://www.wicell.org#fig5]), by using the SNN graph as input to the Walktrap community finding algorithm, using the cluster_walktrap() function from the igraph R package.\n#construct SNN graph\n> g_wt <- buildSNNGraph(sce_wt, use.dimred\u00a0= \"PCA\")\n#compute initial clustering\n> clust_wt <- igraph::cluster_walktrap(g_wt)$membership\n> sce_wt$initial_clusters <- factor(clust_wt)\nNote: In S\u00e1 da Bandeira, et\u00a0al. we use the Walktrap algorithm, but the igraph package allows for the use of other community-finding algorithms, such as Louvain.1[href=https://www.wicell.org#bib1] It may be worthwhile experimenting with different algorithms to explore the robustness of any cell clusters.\nApply doublet detection to each sample, for example via scDblFinder\u2019s computeDoubletDensity() function, which simulates random artificial doublets from real cells and tries to identify cells whose neighborhood has a high local density of artificial doublets.\n#doublet detection\n> dbl_den_wt <- computeDoubletDensity(sce_wt,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0subset.row=hvg_pois_wt))\n> sce_wt$doubletScore <- log1p(dbl_den_wt)\nNote: It is important to make sure that doublet detection is carried out on individual samples, prior to any merging or integrating processes; by definition, doublet cells can only arise from a single library. Clusters principally driven by a population of cells with high predicted doublet scores may be discarded; otherwise, we recommend keeping all cells, retaining their doublet score metadata for examination downstream if required.\nCell type annotation. In S\u00e1 da Bandeira et\u00a0al., we combined marker data computed on the clusters defined above (using scran\u2019s scoreMarkers() function) with literature-based markers for cell type annotation.1[href=https://www.wicell.org#bib1] Cell type annotations were assigned either based on the clustering computed above, or by the expression of known markers, as in the following code example:\n#annotation based on cluster number\n#1: annotating cells from a single cluster\n> sce_wt$cell_type[sce_wt$initial_clusters==0] <- \"Cell_type_A\"\n#2: annotating cells from multiple clusters\n> sce_wt$cell_type[sce_wt$initial_clusters %in% c(1,2)] <- \"Cell_type_B\"\n#annotation based on positive expression of \"Gene\"> sce_wt$cell_type[logcounts(sce_wt)[rowData(sce_wt)$Symbol==\"Gene\",]>0] <- \"Cell_type_C\"\nFollowing annotation, cells can be grouped and visualized by cell type (Figure\u00a06[href=https://www.wicell.org#fig6]), rather than by the automated clusters defined previously (Figure\u00a05[href=https://www.wicell.org#fig5]).\nFirst define non-perivascular cell types:\nPtprc (CD45)+: Macrophage/Macrophage Progenitor (MP) clusters are identified by strong expression of Adgre1 (F4/80) and Ptprc (CD45).\nPecam1 (CD31)+: EC/HEC/IAHC further separated by additional markers; intra-aortic hematopoietic clusters (IAHCs) are Kit+, and endothelial cells/hemogenic endothelial cells (EC/HECs) are Kit- Ptprc (CD45)- Runx1- and Runx1+ respectively.\nA cluster of cells enriched in Pf4 (also expressed by some MPs), but not expressing CD45, are annotated as Other Blood Cells (OBCs), likely platelets.\nErythroid cells and their progenitors (Ery/EryP) are marked by expression of Gypa.\nCell clusters deriving from the sympathetic nervous system (SNS) and skeletal muscle progenitors (SkMP) are enriched in Ngfr and Myod1, respectively.\nDefine perivascular cell types.\nPerivascular cell types were defined as Cspg4+ and/or Pdgfrb+ if they had positive log-expression of these genes. These are defined as pericytes/vascular smooth muscle cells (PC/vSMCs), also named double positive cells (DP, NG2+PDGFR\u03b2+), and are surrounded by PDGFR\u03b2-single (PDGFR\u03b2-S, NG2-PDGFR\u03b2+) and double negative cells (DN, NG2-PDGFR\u03b2-).1[href=https://www.wicell.org#bib1]\nNG2-single cells (NG2-S, NG2+PDGFR\u03b2-) were found around the notochord.1[href=https://www.wicell.org#bib1]\nNote: Cell types present (especially rare cell types) may be found in different proportions, based on the specific samples sequenced. Cell types may also be combined or split based on the research question. During the annotation process, it is helpful to visualize the expression of literature-based markers, at both the cluster level and at a sample level using violin and t-SNE plots.\nMerging samples.Merge the WT and KO samples using correctExperiments() from the batchelor Bioconductor package9[href=https://www.wicell.org#bib9]; this function applies a batch correction while combining the assay data and column metadata for downstream analysis. correctExperiments() retains batch information in the batch slot and we update this to a more useful label.\n#merging WT and KO samples\n> merged <- correctExperiments(sce_wt_filt, sce_ko_filt,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0PARAM=FastMnnParam())\n> merged$batch <- factor(merged$batch)\n> levels(merged$batch) <- c(\"WT\",\"KO\")\nApply dimensional reduction and clustering to the merged dataset, as described above (steps 10 and 11).\nt-SNE visualization of the merged dataset in conjunction with the cell type annotations defined above (step 12) should confirm a clustering of cells by cell type, rather than genotype.\nDifferential expression analysis.\nUse scran\u2019s pairwiseWilcox() function to perform differential expression analysis between groups of cells in the merged SingleCellExperiment object. The function requires a vector of group assignments for the \u2018group\u2019 parameter, which is most easily specified as a combination of batch and cell type. The desired comparison can be made by specifying a vector of the relevant groups for the \u2018restrict\u2019 parameter.\n#generate batch\u00a0+ cell type labels\n> merged$batch_celltype <- paste(merged$batch,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0merged$cell_type, sep=\"_\")\n#differential expression analysis\n> pww <- pairwiseWilcox(logcounts(merged),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0groups=merged$batch_celltype,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0restrict=c(\"WT_Cell_type_A\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"KO_Cell_type_A\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0gene.names=rowData(merged)$Symbol,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0direction=\"up\")\nThe output from pairwiseWilcox() is a list of two elements: \u2018statistics\u2019 and \u2018pairs\u2019: \u2018statistics\u2019 (itself a list of DataFrames) contains the differential expression statistics, including the AUCs (the effect size), p-values and false discovery rate (FDR) values for each gene.\n#example pairwiseWilcox() output\n> head(pww$statistics[[1]])\nDataFrame with 6 rows and 3 columns\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0AUC\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 p.value\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 FDR\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<numeric>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<numeric>\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0<numeric>\nGene1 \u00a0\u00a00.606633 \u00a0\u00a04.685919e-86 \u00a0\u00a08.225655e-85\nGene2\u00a0\u00a0 0.603982 \u00a0\u00a06.134690e-82 \u00a0\u00a01.039850e-80\nGene3\u00a0\u00a0 0.600158 \u00a0\u00a03.521710e-76 \u00a0\u00a05.637097e-75\nGene4\u00a0\u00a0 0.599188 \u00a0\u00a09.388226e-75 \u00a0\u00a01.485138e-73\nGene5\u00a0\u00a0 0.594001 \u00a0\u00a02.326035e-67 \u00a0\u00a03.460966e-66\nGene6\u00a0\u00a0 0.591236 \u00a0\u00a01.402869e-63 \u00a0\u00a02.013091e-62Note: Other statistical tests may be used (eg t-tests via the pairwiseTTests() function). Here, we used Wilcoxon rank sum tests since they are considered more robust to outliers and insensitive to non-normality in comparison to t-tests. We note, however, that the disadvantages of Wilcoxon rank sum tests compared to t-tests include their longer running time, and the reported effect sizes of Wilcoxon rank sum tests are less interpretable.10[href=https://www.wicell.org#bib10]\nFunctional analysis. Associate significantly differentially expressed genes (DEGs) with Gene Ontology (GO) terms:\nDefine DEGs as genes with FDR<0.05 and use their gene symbols as input to the PANTHER web resource [http://pantherdb.org/[href=http://pantherdb.org/]].18[href=https://www.wicell.org#bib18]\nSelect \u2018Mus musculus\u2019 as the organism and \u2018Statistical overrepresentation test\u2019 as the analysis, with \u2018GO biological process complete\u2019 as the annotation set.\nNote: By default, PANTHER returns only significant (FDR<0.05) biological processes.\nManually curate the significantly overrepresented GO terms for terms relating to osteogenesis.\nSearch GO terms of interest in the AmiGO web resource [http://amigo.geneontology.org/amigo[href=http://amigo.geneontology.org/amigo]], which lists genes associated with a given GO term.\nDownload the associated genes for any GO terms of interest and cross-reference these with\u00a0the\u00a0DEGs to find genes which contributed most strongly to these GO terms (either\u00a0by\u00a0significance, or by the AUC (effect size) computed by the Wilcoxon rank sum test) (Figure\u00a07[href=https://www.wicell.org#fig7]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2349-Fig2.jpg\nFigure\u00a02. Barcode rank plots for WT and KO samplesBarcode rank plots show the distribution of barcode counts and which barcodes were inferred to be associated with cells. The inflection and knee points correspond to transitions in the distribution of barcode counts, reflecting the difference between good quality droplets with many associated barcodes, and poor quality droplets, or those containing only ambient RNA. The plots should show a clear separation of these two groups of droplets, with a population of good quality droplets with high UMI counts above the knee point, followed by a sharp decrease in the number of UMIs associated with droplets.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2349-Fig3.jpg\nFigure\u00a03. Cell-based quality control metrics\nHistograms illustrating three cell-based quality control metrics. Dashed lines indicate computed thresholds 3 MADs away from the median: any cell with any quality metric more extreme than 3 MADs away from the median is removed from downstream analysis.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2349-Fig4.jpg\nFigure\u00a04. PCA scree plot\nPCA scree plot illustrating the proportion of variance explained by each principal component. The red line indicates an \u2018elbow\u2019 point beyond which retaining additional principal components for downstream analysis provides little additional information.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2349-Fig5.jpg\nFigure\u00a05. Initial WT cell clustering\nAn initial cell clustering for the WT sample is computed by the Walktrap algorithm and used as a basis for cell type annotation.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2349-Fig6.jpg\nFigure\u00a06. Final WT cell type annotations\nFollowing cell type annotation, the number of cell clusters is reduced and mapped to biologically relevant cell types.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2349-Fig7.jpg\nFigure\u00a07. Scatterplot of effect size (reported as area under the ROC curve, AUC) vs significance, for genes associated with the GO term \u2018Regulation of osteoblast differentiation\u2019 (GO:0045667)\nThis GO term is significantly overrepresented in genes significantly downregulated in the PDGFR\u03b2-KO niche.1[href=https://www.wicell.org#bib1] Significantly downregulated genes are plotted in red and selected genes are labeled.\nMesenchymal stem/stromal cell (MSC) derivation and cultureTiming: days to weeks\nWe developed a mesenchymal cell culture to investigate whether the osteogenic developmental potential of PDGFR\u03b2\u2212/\u2212 AGM-derived MSCs is impaired as suggested by our scRNA-seq data analysis in\u00a0vivo. We first tested whether mesenchymal stem/stromal cell lines can be derived from single WT AGMs then whether MSCs can be derived in the absence of PDGFR\u03b2. The specific steps are as follows:\nPrepare MSC medium.\nHeat-inactivate the FCS for 30\u00a0min in a 56\u00b0C water bath.\nMix all medium components (See table in \u201creagents and materials\u201d section) in a steritop filter, connected to a stericup by a vacuum pump by adding the largest volumes first.\nFilter and label the solution.\nNote: This media has been previously described4[href=https://www.wicell.org#bib4] and should be stored at 4\u00b0C for 1\u00a0month.\nSeeding of AGM cells.\nPre-coat a 6-well plate with 2\u00a0mL of sterile 0.1% cold gelatin for at least 1\u00a0h at room temperature (20\u00b0C\u201322\u00b0C) or 10\u00a0min at 4\u00b0C.\nGently aspirate out the remaining gelatin and proceed without washing.\nResuspend cell pellets obtained from a single AGM in 3\u00a0mL of MSC medium. A number of 21.2\u00a0+/- 6.2\u00a0\u00d7\u00a0104 cells are expected to be obtained from single E11 AGMs.14[href=https://www.wicell.org#bib14]\nSeed each single suspension in one well in a pre-coated 6-well plate (=passage 0).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2349-Fig8.jpg\nFigure\u00a08. Phase-contrast images of PDGFR\u03b2+/+ (WT, left) and PDGFR\u03b2\u2212/\u2212 (KO, right) single AGM-derived MSCs taken with Evos Digital Inverted Microscope\nScale: 100\u00a0\u03bcm.\nNote: After 24 h, the cells should be completely adherent to the bottom of the well.\nMaintenance and expansion of AGM-derived MSCs.\nIn the first week, refresh the stromal medium only once, approx. 2\u20133\u00a0days after cells were first seeded.When the wells are >90% confluent (approx. 4\u20136\u00a0days), passage the cells from one well of a 6-well plate to one gelatin pre-coated T25 flask (=passage 1) using pre-warmed Trypsin\u00a0+ EDTA (0.25%) to detach the adherent cells.\nFrom here on, refresh the medium bi-weekly.\nWhen the T25 flasks are >90% confluent (approx. 1\u00a0week), passage the cells from one T25 flask to one gelatin pre-coated T75 flasks (=passage 2) using 0.25% Trypsin\u00a0+ EDTA to detach the adherent cells for approx. 12\u201315\u00a0min.\nWhen the T75 flasks are >90% confluent (approx. 1\u00a0week), expand the culture at a 1:3 ratio (from one T75 to three T75 flasks and so on) for the following weeks and passages.\nNote: When we initiate the culture with freshly harvested cells (passage 0), some MSC primary lines require 2\u20133 extra days to expand. This is a relatively short time and does not influence their growth or their differential potential at later stages. We were able to expand these cells and freeze/thaw them regularly for about 12 passages, independently of their genotype. However, their osteogenic potential was only tested between passage 3 and passage 6 while their hematopoiesis support was tested up to passage 12.1[href=https://www.wicell.org#bib1]\nCritical: Cells need to be kept in the incubator at 37\u00b0C and 5% CO2 at all times.\nAfter expansion, a fraction can be used for experiments and the remaining cells can be frozen and stored in liquid N2 or passaged.Note: To freeze MSCs, use around 0.5\u20131\u00a0\u00d7\u00a0106 cells per cryovial with 90% cold FCS and 10% dimethyl sulphoxide (DMSO). To thaw cells, first prepare a 50\u00a0mL falcon tube containing 30\u00a0mL of PBS/FCS/PS. Keep the cryovial in the water bath at 37\u00b0C for a few seconds until the frozen block starts to detach, then rapidly transfer the frozen block to the 50\u00a0mL falcon tube in the tissue culture hood. Centrifuge cells at 4\u00b0C and 2,000\u00a0rpm for 5\u00a0min. Discard the supernatant, resuspend the pelleted cells with MSC medium and transfer to a gelatin pre-coated T75 flask, then incubate them at 37\u00b0C. After 24 h, renew the medium to discard the floating cells. Proceed as normal.\nBoth WT and KO stromal cells should show a fibroblast-like morphology and resemble MSCs (Figure\u00a08[href=https://www.wicell.org#fig8]).\nFunctional validation: Osteogenesis assay\nTiming: 3\u20134\u00a0weeks\nMesenchymal stem/stromal cells are multipotent and therefore they are able to differentiate into bone, adipose tissue, and cartilage.19[href=https://www.wicell.org#bib19] Since we found a significant downregulation in osteogenic gene expression (step 14), and mesenchymal and osteogenic differentiation biological processes were significantly affected (steps 15 and 16), we performed an osteogenic assay using our expanded WT and KO-derived single AGM stromal cell cultures (steps 17\u201319)1[href=https://www.wicell.org#bib1] to validate the functional changes. To reveal calcium deposits, we used an alizarin red solution.\nPrepare solutions.\nIn a 100\u00a0mL flask prepare the osteogenic medium.\nMix thoroughly and filter.\nNote: This media should be stored at 4\u00b0C for no longer than 1\u00a0month.\nIn a 100\u00a0mL flask prepare the control osteogenic medium.\nMix thoroughly and filter.\nNote: This media should be stored at 4\u00b0C for no longer than 1\u00a0month.In a medium Schott flask, prepare the Alizarin red solution. The pH of this solution needs to be adjusted to 4.1\u20134.3. Do this by adding HCl or NH4OH as needed.\nNote: This media should be stored at room temperature (20\u00b0C\u201322\u00b0C), for 1\u00a0month while protected from light covered with aluminium foil.\nCell differentiation assay.\nSeed 4\u00a0\u00d7\u00a0104 MSCs per well in a 24-well plate with MSC medium.\nAfter 24\u00a0h of culture, remove the medium gently, and replace with 500\u00a0\u03bcL of either osteogenic or control media.\nNote: The plates are uncoated to avoid autofluorescence upon staining. Cells need to be at least 80%\u201390% confluent otherwise more time is needed to have a good cell coverage. The osteogenic medium will trigger the cells to differentiate towards the osteogenic lineage only if cells have this potential, while the control medium will not.\nIncubate the plate at 37\u00b0C and 5% CO2 for 21\u00a0days.\nRefresh the media three times every week.\nAlizarin red staining to allow detection of calcium deposition.\nOn day 21, discard the osteogenic and control media.\nWash the wells gently with Milli-Q water to remove any media left.\nFix cells by incubating the wells with 4% PFA for 10\u00a0min at room temperature (20\u00b0C\u201322\u00b0C).\nRemove the PFA and wash twice with Milli-Q water.\nAdd 500\u00a0\u03bcL of alizarin red solution to each well, including both differentiated and control wells, for 15\u00a0min at room temperature (20\u00b0C\u201322\u00b0C) and in the dark.\nNote: It is important to be careful with this alizarin red solution since it easily leaves a stain mark on clothes, surfaces etc.\nCarefully remove alizarin red solution and wash twice with Milli-Q water.\nPerform a third wash with Milli-Q water but do not discard.\nImage the full wells with a brightfield microscope.Note: Stained wells can be carefully washed several times if the water is still red. Stained plates can be imaged right after the staining or kept at 4\u00b0C for several days prior imaging. In this protocol, a Zeiss Axio Observer was used and images were analyzed with Fiji/ImageJ software (v. 1.52p).\nBrightfield image analysis.\nIn the ZEN 2.3 Pro software, upon image acquisition, select the tiling and stitching options to generate high-resolution pictures of your full wells.\nExport stitched tiled images in the .czi format.\nIn Fiji/ImageJ software, import .czi images to adjust the brightness using the same settings for all wells.\nExport the final image in .tiff format.\nPerform a qualitative observation of which MSC lines are osteogenic, if you detect alizarin red staining or low/no osteogenic, if you detect low to no staining.\nNote: A quantitative measure can be done in these samples by reading the plate on a spectrophotometer at a wavelength of 450\u00a0nm.", "Step-by-step method details\nStep-by-step method details\nSingle-cell read alignment and dropEst library quantification\nTiming: 5 h, for 120 million reads on a compute node with 12 cores and 48 GB of memory. 1.5\u00a0h for example subset dataset (GSM4804820) with the same specifications. Processing time decreases linearly with the number of cores available.\nThis section encompasses the library demultiplexing, read alignment, droplet count matrix estimation, and preliminary quality assessment with the DropEst library, the STAR aligner, and the scRNABatchQC R package, respectively (Dobin et\u00a0al., 2013[href=https://www.wicell.org#bib5]; Liu et\u00a0al., 2019[href=https://www.wicell.org#bib15]; Petukhov et\u00a0al., 2018[href=https://www.wicell.org#bib18]). First, dropTag takes paired-end, raw .fastq files and tags them in the context of unique molecular identifiers (UMIs) and cellular barcodes for the demultiplexing process. This is dependent on the scRNA-seq platform\u2019s barcode whitelist; in this case we use the inDrop V1 and V2 barcodes. Before running the actual alignment process, a genome index must first be generated with respect to the reference and annotation files. STAR is a fast, scalable RNA-seq aligner which has splice awareness and takes the multiple tagged fastq.gz files generated by dropTag and aligns them using a reference genome index. The sorted .bam file generated by STAR alignment is used as an input to dropEst, which generates a barcode by gene count matrix, or droplet matrix, from the STAR aligned transcripts. Finally, scRNABatchQC is used to provide summary statistics and a quality assessment of the generated droplet matrix. This droplet matrix is further filtered in the heuristic droplet filtering[href=https://www.wicell.org#sec3.2] and automated droplet filtering with dropkick[href=https://www.wicell.org#sec3.3] section variants.\nNote: This protocol serves as a reference for an order-of-operations and their parameters in our open-source pipeline; organized and executable scripts, with proper file and directory references, are available at: https://github.com/KenLauLab/STAR_Protocol/[href=https://github.com/KenLauLab/STAR_Protocol/].Critical: The dropEst repository should be made locally available to explore its configurations and files by cloning from https://github.com/hms-dbmi/dropEst[href=https://github.com/hms-dbmi/dropEst]. This repository is also fully available within the provided Singularity container, whose configs can be displayed with the following command:\nsingularity exec -e star_dropest_star_protocols_pipeline.sif ls /usr/share/dropEst/configs\nConfig files of interest can then be copied from the container to the local directory with the following, where <example.xml> is the file of interest:\nsingularity exec -e star_dropest_star_protocols_pipeline.sif cp /usr/share/dropEst/configs/<example.xml> .\nRun DropTag with the following:\nsingularity exec -e star_dropest_star_protocols_pipeline.sif droptag -c /usr/share/dropEst/configs/indrop_v1_2.xml reads_R1.fastq reads_R2.fastq\nThe following parameter is required:\nc, config filename: The file path to the .xml file containing estimation parameters within the Singularity container, which includes platform-specific information. Further parameters contained within this .xml file are described by Petukhov, et\u00a0al. (Petukhov et\u00a0al., 2018[href=https://www.wicell.org#bib18]): https://github.com/hms-dbmi/dropEst/blob/master/configs/config_desc.xml[href=https://github.com/hms-dbmi/dropEst/blob/master/configs/config_desc.xml]\n<reads_R1>, <reads_R2>: These are positional arguments which should be replaced with the paths to the fastq files representing the R1 and R2 reads respectively; set to \u201creads_R1.fastq\u201d and \u201creads_R2.fastq\u201d in this example. R1 corresponds to the barcode read and R2 corresponds to the gene read.\nPause point: The output of step 1 is saved as multiple tagged .fastq files, further detailed in the expected outcomes[href=https://www.wicell.org#expected-outcomes] section.\nCreate a directory for operations to be performed and generate the index file:\nmkdir STAR_index && singularity exec -e star_dropest_star_protocols_pipeline.sif \\ STAR --runThreadN 16 --runMode genomeGenerate \\\n--genomeDir STAR_index --genomeFastaFiles \\ primary_assembly.fa --sjdbGTFfile \\ annotation.gtf --sjdbOverhang 99\nTo create a genome index, the user must provide the reference genome (.fasta file) and the corresponding annotation file (.gtf) and run STAR with following parameters:\nrunMode: Mode to run, example set to \u201cgenomeGenerate\u201drunThreadN: The number of threads to generate the index file with, this step speeds up with higher values and is limited by the CPU used. Set to \u201c16\u201d in the example.\ngenomeDir: Path to directory where files will be stored, set in example to \u201cSTAR_index\u201d\ngenomeFastaFiles: Path to genome .fasta file, set in example to \u201cprimary_assembly.fa\u201d\nsjdbGTFfile: Path to annotation .gtf file, set in example to \u201cannotation.gtf\u201d\nsjdbOVerhang: The number of bases to concatenate from donor and acceptor sides of splice junctions. Set in example as \u201c99\u201d.\nCritical: Step 2 must be re-run for different reference genome and annotation file versions, as the generated genomic index will be unique to each version.\nPause point: The output of step 2 is saved as a genomic index file, further detailed in the expected outcomes[href=https://www.wicell.org#expected-outcomes] section.\nRun the single-cell alignment process with STAR, using our Singularity container:\nsingularity exec -e star_dropest_star_protocols_pipeline.sif STAR \\\n--genomeDir STAR_index \\\n--readFilesIn reads.tagged.1.fastq.gz \\\n--outSAMmultNmax 1 --runThreadN 16 --readNameSeparator space \\\n--outSAMunmapped Within --outSAMtype BAM SortedByCoordinate \\\n--outFileNamePrefix reads \\\n--readFilesCommand gunzip -c\nThe following parameters are required:\ngenomeDir: The path to the directory containing the STAR index file. Set in example as \u201cSTAR_index\u201d\nreadFilesIn: The path to the tagged .fastq file(s), where multiple tagged .fastq files can be input, set as \u201creads.tagged.1.fastq.gz\u201d in example.\noutSAMmultNmax: Maximum number of multiple alignments for a read that will be output to the .sam/.bam files, example set to \u201c1\u201d\nrunThreadN: Number of threads, example set to \u201c12\u201d, increase value to speed up performance.\nreadNameSeparator: Characters separating the part of the read names that will be trimmed in output, example set to \u201cspace\u201d\noutSAMunmapped: Output unmapped reads within the main ,sam file, example set to \u201cWithin\u201d\noutSAMtype: Output formatting of .bam file, example set to \u201cBAM SortedByCoordinate\u201doutFileNamePrefix: Output file name prefix, set here as \u201creads\u201d\nreadFilesCommand: Command to decompress fastq.gz files, example set to \u201cgunzip \u2013c\u201d\nCritical: Ensure that there is sufficient memory overhead for this step, with a minimum of 32 GB allotted, as spikes in memory usage may prematurely end the alignment process.\nPause point: The output of step 3 is saved as a .bam, with several attributes, further detailed in the expected outcomes[href=https://www.wicell.org#expected-outcomes] section.\nRun DropEst, configured here for an inDrop library, with the following:\nsingularity exec -e star_dropest_star_protocols_pipeline.sif dropest -m -V -b -F -o sample_name -g annotation.gtf -L eiEIBA -c /usr/share/dropEst/configs/indrop_v1_2.xml readsAligned.sortedByCoord.out.bam\nThe following arguments are used in this step:\nm, merge-barcodes: Merge linked cell tags\nV, verbose: Output verbose logging messages\nb, bam-output: Print tagged bam files\nF, filtered-bam: Print tagged bam file after the merge and filtration\no, output-file filename: The output file name, example set to \u201csample_name\u201d\ng, genes filename: Gene annotation file (.bed or .gtf), example set to \u201cannotation.gtf\u201d\nL: This is parameter has several options which denote the inclusion of count UMIs with reads that correspond to specific parts of the genome. Set to \u201ceiEIBA\u201d in the example.\ne: UMIs with exonic reads only\ni: UMIs with intronic reads only\nE: UMIs, which have both exonic and not annotated reads\nI: UMIs, which have both intronic and not annotated reads\nB: UMIs, which have both exonic and intronic reads\nA: UMIs, which have exonic, intronic and not annotated reads\nc, config filename: XML file with estimation parameters, example set to \u201c./configs/indrop/v_1_2.xml\u201d, further details can be found at: https://github.com/hms-dbmi/dropEst/blob/master/configs/config_desc.xml[href=https://github.com/hms-dbmi/dropEst/blob/master/configs/config_desc.xml]\n<readsAligned.sortedByCoord.out.bam>: Positional argument for the input bam file. Set in example as \u201creadsAligned.sortedByCoord.out.bam\u201d.\nCritical: Like step 3, this is a memory-intensive step. Ensure that there is sufficient memory overhead for this step, with a minimum of 32 GB allotted.Pause point: The output of step 4 is saved as a .bam and a .rds file, both with several attributes, further detailed in the expected outcomes[href=https://www.wicell.org#expected-outcomes] section.\nFrom the output of DropEst, generate sparse count matrices with the code as follows:\nsingularity exec -e star_dropest_star_protocols_pipeline.sif R --vanilla --slave -f /R_scripts/RDS_to_sparesematrix.r --args sample_name.rds\nThe following arguments are used in this step:\n--args: Path to the output .rds file, set in example as \u201csample_name.rds\u201d.\nNote: The source of the invoked R script can be viewed from within singularity container using vi. This script simply loads the .rds file, its contained data, and writes matrix files that are interoperable between different processing pipelines:\nsingularity exec -e star_dropest_star_protocols_pipeline.sif vi /R_scripts/RDS_to_sparesmatrix.r\nPause point: The output of step 5 is saved as three files representing the droplet matrix, feature labels, and barcode labels, which are further detailed in the expected outcomes[href=https://www.wicell.org#expected-outcomes] section.\nFinally, generate a quality assessment report:\nsingularity exec -e star_dropest_star_protocols_pipeline.sif R --vanilla --slave -f /R_scripts/scRNABatchQC.r --args hsapiens sample_name.rds_cm.csv\nThe following arguments are used in this step:\n--args: Consists of two parts, positionally, the species and target .csv file. In this example, set as \u201chsapiens\u201d and \u201csample_name.rds_cm.csv\u201d.\nPause point: The output of step 6 is saved as an .html file, further detailed in the expected outcomes[href=https://www.wicell.org#expected-outcomes] section.\nVariant 1. Heuristic droplet filtering\nTiming: 15 to 30\u00a0min, depending on the size of the droplet matrix and cores available for certain parallelized functions.This section and its variant describe the barcode filtering of the droplet matrix, and can be used modularly if the user has a pre-computed matrix, either from the single-cell read alignment and DropEst library quantification[href=https://www.wicell.org#sec3.1] section of this protocol or an external source, so long as the rows represent cell barcodes and columns represent genes. The output for this section will be a cell matrix, differing from a droplet matrix in that it only contains gene read counts from only high-quality, intact single cells. Primarily, this section will be performed interactively with Jupyter Notebooks running within a Conda environment, making extensive use of the AnnData Python class and scanpy library. First, a data-driven cutoff, by means of finding the inflection point in a cumulative sum curve of ranked barcode counts, is generated and used to minimize information-spars barcodes. Second, a distribution of uniquely detected genes per droplet is automatically thresholded through Otsu\u2019s method, separating the remaining information-rich and information-sparse droplets and generating a binary metadata label. Third, tissue-specific gene expression signatures are visualized after DR to pinpoint cell populations of interest for downstream analysis. Fourth, unsupervised clustering is performed to discretize the single-cell transcriptional landscape. Finally, by heuristically integrating these metrics and expression signatures, populations of intact single-cells and their respective high-quality transcriptomes can be selected and saved to an independent file.\nCritical: This section is performed entirely within a Jupyter Notebook available through Github at https://github.com/KenLauLab/STAR_Protocol/[href=https://github.com/KenLauLab/STAR_Protocol/]. To use this notebook, follow the instructions described in the python environment preparation[href=https://www.wicell.org#sec1.3] section of before you begin[href=https://www.wicell.org#before-you-begin]. For further information on how to navigate Jupyter Notebooks, see its documentation page: https://jupyterlab.readthedocs.io/[href=https://jupyterlab.readthedocs.io/].\nPrepare DropEst outputs from the single-cell read alignment and DropEst library quantification[href=https://www.wicell.org#sec3.1] section for analysis in an interactive Jupyter Notebook:\nimport scanpy as sc\nimport numpy as np\nimport QCPipeadata = QCPipe.qc.read_dropest(\u201c<dir>\u201d)\nadata.write_h5ad(\u201c<filename.h5ad>\u201d,compression=\u2019gzip\u2019)\nThis step uses the arguments:\n<dir>: The directory where the DropEst results are stored, specifically from step 5, set in example as \u201cdir\u201d\n<filename.h5ad>: Filename for the output .h5ad file, set in example as \u201cfilename.h5ad\u201d\nPause point: The output of step 7 is saved as a compressed .h5ad file, further detailed in the expected outcomes[href=https://www.wicell.org#expected-outcomes] section.\nNote: For the heuristic droplet filtering[href=https://www.wicell.org#sec3.2], automated droplet filtering with dropkick[href=https://www.wicell.org#sec3.3], and post-processing and dimension reduction structure preservation analysis[href=https://www.wicell.org#sec3.4] sections of this protocol, adata is a common variable, which represents the AnnData object that scanpy methods operate on. adata is typically the parameter used for each function\u2019s first positional argument.\nRestart the notebook kernel and reload the data, from file, as an AnnData object:\nimport scanpy as sc\nimport QCPipe\nimport numpy as np\nadata = sc.read_h5ad(\u201c<filename.h5ad>\u201d)\nadata.raw = adata #set the raw attribute\nadata.var[\u2018Mitochondrial\u2019] = adata.var.index.str.startswith(<mitochondrial nomenclature>) #set the mitochondrial variable attribute\nsc.pp.calculate_qc_metrics(adata, qc_vars=[\u2018Mitochondrial\u2019], use_raw=True, inplace=True)\nThe parameters in this code are as follows:\n<filename.h5ad>: The filename of the .h5ad file generated in step 7\n<mitochondrial nomenclature>: The mitochondrial nomenclature of the dataset, given the gene symbol. This will vary depending on the gene nomenclature and species. For example, human mitochondrial gene symbols are designated with \u201cMT-\u201d, whereas mouse symbols are preceded by \u201cmt-\u201d.\nNote: The following steps assume that the notebook kernel activated in step 8 is not subsequently deactivated or restarted; thus, library import statements are not detailed further.\nPerform the first-pass inflection point-based filtering:\nInflection_estimate = QCPipe.qc.find_inflection_point(adata)\nsc.pp.filter_cells(adata, min_counts=adata[Inflection_estimate].obs[\u2018total_counts\u2019])\nAlternatively, the user can set a manual cutoff using an estimated number of encapsulated cells:\nsc.pp.filter_cells(adata, min_counts=adata[<estimated number of cells encapsulated>].obs[\u2018total_counts\u2019])\nThe parameter in this code is as follows:<estimated number of cells encapsulated>: This number represents the estimated number of cells encapsulated during the library generation process and is based on the flow time and rate of the process.\nCritical: If the droplet matrix to be used in this step was generated through an external pipeline, ensure that it is ordered, starting with barcodes associated with the most to the least detected reads. Step 9 will fail if the data are not ordered as such. This ordering, however, is automatically performed in the DropEst output preparation in step 7.\nAutomatically identify cells with relatively high transcriptional diversity:\nadata = QCPipe.qc.relative_diversity(adata)\nNormalize, log-like transform, and scale the data in preparation for dimensionality reduction:\n#Droplet matrix is normalized to the median number of counts per barcode\nsc.pp.normalize_total(adata)\n#Droplet matrix is log-like transformed with np.arcsinh, without adding a pseudocount\nadata.X = np.arcsinh(adata.X).copy()\n#Droplet matrix centered and scaled through a Z-score transformation\nsc.pp.scale(adata)\nOptional: Perform feature selection with highly_variable_genes or nvr after installing the required packages. These methods can be run in a Jupyter Notebook:\nsc.pp.highly_variable_genes(adata)\nAlternatively, install nvr through the command line:\npip install nvr\nRun NVR:\nimport nvr\nadata = nvr.nvr_feature_select(adata)\nNote: Only one of these feature selection methods should be used at a time. Also, ensure that the data\u2019s stage of normalization and transformation complies with the requirements of these feature selection methods.\nPerform the initial dimensionality reduction with PCA:\nsc.pp.pca(adata,highly_variable_genes=False)\nThe parameter for running the PCA is as follows:\nhighly_variable_genes: This parameter is used to indicate whether to use feature selected variables, set in this example as \u201cFalse\u201d.\nGenerate a K-nearest neighbors graph (KNN) from the PCA-based distance matrix. This is run with a K of approximately the square root of the total number of barcodes, balancing the influence of local and global distances:\nk_neighbors = np.sqrt(adata.n_obs).astype(int)sc.pp.neighbors(adata,n_neighbors = neighbors)\nPerform Leiden community detection:\nsc.pp.leiden(adata,resolution=1)\nThe parameter for running this Leiden clustering is as follows:\nresolution: The clustering resolution, where a higher number leads to more, smaller clusters, and a lower number leads to fewer, larger clusters. The example is set to \u201c1\u201d.\nProject the data into 2 dimensions with UMAP:\nsc.tl.umap(adata,min_dist=0.25)\nThe parameter for running this UMAP is as follows:\nmin_dist: The minimum distance allowed for each cell or data point in the 2-dimensional projection. The example is set as \u201c0.25\u201d. Lower min_dist values cause the data points to be more compact in 2D space, and vice versa for higher values.\nVisualize factors useful in the heuristic determination of high-quality cell barcodes:\nsc.pl.umap(adata,color=[\u2018gene\u2019,\u2019leiden_labels\u2019,\u2019pct_counts_Mitochondrial\u2019,\u2019pct_counts_in_top_200_genes\u2019,\u2019relative_transcript_diversity_threshold\u2019],use_raw=False)\nThe parameter for running this UMAP is as follows:\ncolor: The values stored in the AnnData object which are to be visualized in on a 2-dimensional projection, in this example we visualize some \u201cgene\u201d, \u201cleiden_labels\u201d, \u201cpct_counts_Mitochondrial\u201d, \u201cpct_counts_in_top_200_genes\u201d, and \u201crelative_transcript_diversity_threshold\u201d. These factors are used in the heuristic selection of clusters.\nuse_raw: Whether to visualize normalized and scaled values or the raw count values within the AnnData object droplet matrix. Set to \u201cFalse\u201d in this example.\nCritical: By priority, clusters of droplet barcodes should be selected based on these criteria in step 18:\nMarker gene expression and specificity: These genes will vary between the biological system of interest as well as the heterogeneity of cell input. Colorectal tumors, for example, will have a mixture of epithelial and immune cells, and markers would be used accordingly.\nThe number of uniquely expressed genes: This is a strong predictor of encapsulated cells, and empty droplets are unlikely to contain a biologically relevant diversity of gene transcripts. This should be maximized unless there is a particular cell type that is known to express very few unique transcripts.Mitochondrial gene count percentage: This is important because encapsulated cells undergoing lysis will contain a high percentage of mitochondrial reads, effectively adding noise to a droplet due to the removal of more informative genes from a limited read count pool.\nAmbient gene expression: This is akin to the mitochondrial gene count percentage, as the encapsulation substrate may contain the remnants of lysed cells, often consisting of mitochondrial genes, but may vary per cell type. This should be minimized.\nTotal counts: As the number of transcripts detected represents the amount of raw transcriptional information contained within a droplet. This should also be maximized unless there is a particular cell type that is known to express very few unique transcripts.\nSelect and visualize the cells based off the heuristic criteria using discretized Leiden clusters:\nadata.obs[\u2018Cell_Selection\u2019] = np.isin(adata.obs[\u2018leiden_labels\u2019],[<cluster selection>]).astype(bool)\nsc.pl.umap(adata,color=[\u2018leiden_labels','Cell_Selection'],legend_loc='on data',legend_fontoutline=True,legend_fontsize=10)\nThe parameters in this case are:\n<cluster selection>: The set of Leiden clusters to be selected and passed as a list of characters such as [\u20181\u2019,\u20192\u2019, \u2026 \u2018n\u2019].\nlegend_loc: This parameter indicates where the cluster legends will be displayed, set as \u201con data\u201d in this example.\nlegend_fontoutline: This parameter is used to render an outline on the cluster legends for readability, set as \u201cTrue\u201d in this example.\nlegend_fontsize: This parameter designates the size of the font, set to 10 in this example.\nEnsure that the selected cells comply with the heuristic criteria by reviewing the outputs of steps 17 and 18; then save this selection to a .h5ad file.\ndata_out = QCPipe.qc.subset_cleanup(adata,selection='Cell_Selection')\ndata_out.write_h5ad(\u201c<Filtered_Data.h5ad>\u201d,compression=\u2019gzip\u2019)\nThe parameters in this case are:\nselection: The observation attribute used to subset the data, as defined earlier, this example is set as \u201cCell_Selection\u201d\n<Filtered_Data.h5ad>: The filename to save the compressed .h5ad as, set in this example as \u201c<Filtered_Data.h5ad>\u201d.Pause point: The output of step 19, a filtered cell matrix, is saved as an .h5ad file, further detailed in the expected outcomes[href=https://www.wicell.org#expected-outcomes] section.\nVariant 2. Automated droplet filtering with dropkick\nTiming: 5 to 10\u00a0min, depending on the size of the droplet matrix.\nThis variant serves the same function as the heuristic droplet filtering[href=https://www.wicell.org#sec3.2] section. For automated droplet filtering in Python, dropkick is a machine learning tool that builds a probabilistic model of single-cell barcode transcriptome quality and returns a score for all barcodes in the input scRNA-seq droplet matrix (see step 7 for generating .h5ad from DropEst files) (Heiser et\u00a0al., 2020[href=https://www.wicell.org#bib11]). dropkick can be run from the command line or interactively in a Jupyter Notebook. A command line interface exists for its two primary modules designed for QC reporting and filtering, whose usages are outlined as follows.\nInstall dropkick through pip, or from source code at https://github.com/KenLauLab/dropkick[href=https://github.com/KenLauLab/dropkick]:\npip install dropkick\nRun the dropkick qc function to generate a quality overview report, which is saved to the current working directory as a .png image file:\ndropkick qc <path/to/counts[.h5ad|.csv]>\nThe required parameter for this function is:\n<path/to/counts[.h5ad|.csv]>: The file path to the droplet matrix file of interest, which can be either .h5ad or .csv file.\nNote: If the input counts are in .csv format, ensure that the file is in cells by genes configuration with labels for gene identities as column headers. The output from the step 7 can be used here directly.\nRun the dropkick filtering algorithm with the run function:\ndropkick run <path/to/counts[.h5ad|.csv]> -j 5\nThe required parameters for this function are:\n<path/to/counts[.h5ad|.csv]>: The file path to the droplet matrix file of interest, which can be either .h5ad or .csv file.j: The number of jobs used to parallelize the training and cross-validation of the dropkick\u00a0model. We recommend adjusting the `-j` flag according to the number of available CPUs. If using a machine with more than five cores, `-j 5` is optimal for the five-fold cross validation performed by dropkick, and model training is usually completed in less than two minutes.\nNote: All available user parameters can be found by running `dropkick run -h`. Default parameters are typically fast and robust for most datasets across encapsulation platforms, tissues, and levels of ambient background, see the troubleshooting[href=https://www.wicell.org#troubleshooting] section for further points of optimization.\nPause point: The output of step 22 is a .h5ad file, saved to disk, containing the input droplet matrix with additional metadata consisting of cell quality scores and binary labels. This is further detailed in the expected outcomes[href=https://www.wicell.org#expected-outcomes] section.\nIn a Jupyter Notebook, as described in step 19, load the dropkick-generated .h5ad file with the appropriate libraries and generate the filtered cell matrix:\ndata_out = QCPipe.qc.subset_cleanup(adata,selection='dropkick_label')\ndata_out.write_h5ad(\u201c<Dropkick_Filtered_Data.h5ad>\u201d,compression=\u2019gzip\u2019)\nThe parameters in this case are:\nselection: The observation attribute to use to subset the data, as defined earlier, this example is set as \u201cdropkick_label\u201d.\n<Filtered_Data.h5ad>: The filename to save the compressed .h5ad as, set in this example as \u201c<Dropkick_Filtered_Data.h5ad>\u201d.\nNote: It is good practice to ensure that the selected cells also comply with the heuristic cell selection criteria as discussed in step 17. The entirety of the heuristic droplet filtering[href=https://www.wicell.org#sec3.2] section can also be performed with a dropkick-labeled droplet matrix (the output from step 22), further augmenting cluster selection heuristics with learned metadata.\nPause point: The output of step 23, a filtered cell matrix, is saved as an .h5ad file, further detailed in the expected outcomes[href=https://www.wicell.org#expected-outcomes] section.\nPost-processing and dimension reduction structure preservation analysisTiming: 15\u201330\u00a0min depending on the complexity and heterogeneity of the data at hand.\nThe final phase of this pipeline is centered around generating a representative two-dimensional projection of a filtered cell matrix to accurately visualize the global and local populational heterogeneity within dataset. Using scRNA-seq data to address hypotheses necessitates robust visualizations to counteract stochasticity inherent to several popular non-linear dimensionality algorithms. This stochasticity is often unaccounted for during downstream and may interfere with the representation of cellular relationships along the transcriptomic landscape, warping the perceived distances between cell types in 2D space. This section walks through the quantitative evaluation of two popular embedding techniques, t-SNE (van der Maaten and Hinton, 2008[href=https://www.wicell.org#bib30]) and UMAP (McInnes et al., 2018[href=https://www.wicell.org#bib29]), to determine the more reliable visualization strategy for a particular dataset. First, each latent space, or non-linearly projected, representation of the cell matrix is generated. Second, after identifying putative cell types in the data, discrepancies between these latent and native, or linearly transformed, spaces are calculated on global and local scales. Finally, rearrangements in subpopulation adjacencies are calculated on a graphical basis, allowing for users to choose the latent representation which minimizes discrepancies in latent-native space distances as well as in subpopulation adjacencies; both factors may greatly influence the biological interpretation of the data.\nRefer to the normalization, transformation, scaling, and DR guidelines in steps 11\u201315, as this section uses the same processes. Ensure that the cell count matrix has been processed, up to the Leiden clustering calculation, before proceeding.\nUsing a calculated 50-component PCA, calculate a t-SNE representation of the cell count matrix.\nk_neighbors = np.sqrt(adata.n_obs).astype(int)\nsc.tl.tsne(adata, use_rep=\u201dX_pca\u201d, perplexity=k_neighbors)\nThe parameters in this case are:\nuse_rep: The representation of the single-cell data to use to initialize t-SNE nonlinear\u00a0embedding, set as \u201cX_pca\u201d, or the 50-dimensional PCA representation in this example.perplexity: This is the effective nearest neighbors that are utilized in the t-SNE embedding, set to the square-root (rounded-down) of the total number of cells being examined.\nNext, generate a coarse-grained similarity graph using communities detected through the Leiden algorithm:\nsc.tl.paga(adata)\nProject the data into 2 dimensions using UMAP, but unlike in the heuristic droplet filtering[href=https://www.wicell.org#sec3.2] and automated droplet filtering with dropkick[href=https://www.wicell.org#sec3.3] section variants, initialize this projection with the PAGA similarity values:\nsc.tl.umap(adata, init_pos=\u201dpaga\u201d)\nThe parameters in this case are:\ninit_pos: The representation of the data that is used for the initialization of the UMAP visualization, the example is set as \u201cpaga\u201d, as calculated in step 26.\nRun the structure_preservation_sc function to calculate global latent-native space discrepancies:\ncorr, EMD, knn = QCPipe.fcc.structure_preservation_sc(adata=adata, latent=\"X_tsne\", native=\"X_pca\", k=neighbors)\nThe parameters in this case are:\nlatent: The target latent representation of the data to be evaluated, in this example we start with \u201cX_tsne\u201d, this parameter can be replaced with \u201cX_umap\u201d to evaluate UMAP representations (Figures 6[href=https://www.wicell.org#fig6]C and 6D)\nnative: The native space representation of the data to compare the latent representation with, set as \u201cX_pca\u201d in the example due to the linear nature of its decomposition.\nk: The k number of nearest neighbors for use in structure preservation analysis, set to the square-root (rounded-down) of the total number of cells being examined calculated in step 25.\nPerform differential gene expression (DE) testing to derive transcriptional signatures from the detected subpopulations of cells, whose local latent-native distance discrepancies should be quantified:\nsc.tl.rank_genes_groups(adata, groupby=\u201dleiden\u201d)\nThe parameters in this case are:\ngroupby: The dataset labels between which to perform DE testing, in this case we use the \u201cleiden\u201d clusters.Critical: Ensure that all detected Leiden clusters can be reasonably identified through their gene expression signatures as described in literature. Unless a particular subpopulation is expected to be novel, cluster-to-cluster comparisons will not be biologically meaningful unless properly annotated. Note that the annotation of gene expression signatures is out of the scope of this protocol and will vary for each tissue of interest.\nSubset single-cell cluster(s) of interest to perform latent-native discrepancy evaluation on a cluster-by-cluster basis (defined through the detection of known marker genes and the signature detection of step 29):\nQCPipe.fcc.subset_uns_by_ID(adata, uns_keys=[\"X_pca_distances\",\"X_tsne_distances\",\"X_umap_distances\"], obs_col=\"leiden\", IDs=[<cluster id_c>])\nThe parameters in this case are:\nuns_keys: The distances of interest to be subset, which are stored in the unstructured (.uns) attribute of the AnnData object. Set in the example as \u201cX_pca_distances\u201d, \u201cX_tsne_distances\u201d, and \u201cX_umap_distances\u201d\nobs_col: This parameter indicates which observation attribute to use to subset the data, as defined earlier, this example is set as \u201cleiden\u201d\nIDs: The observational IDs in which subsets of cells, Leiden cluster IDs in this example, are selected.\nPerform the latent-native discrepancy calculations and visualize them using the distance_stats, SP_plot, joint_plot_distance_correlation, and plot_cumulative_distributions functions:\npca_dist_c, tsne_dist_c, corr_stats_c, EMD_c =QCPipe.fcc.distance_stats(pre=adata.uns[<\"X_pca_distances_c\">], post= adata.uns[\"X_tsne_distances_c\"])\nQCPipe.fcc.SP_plot(pre_norm=pca_dist_c, post_norm=tsne_dist_c, labels=[\"PCA (50)\",\"t-SNE\"], figsize=(4,4)).joint_plot_distance_correlation()\nQCPipe.fcc.SP_plot(pre_norm=pca_dist_c, post_norm=tsne_dist_c, labels=[\"PCA (50)\",\"t-SNE\"], figsize=(3,3)).plot_cumulative_distributions()\nThe parameters in this case are:\npre: The calculated distances before generating the latent space representation of the data, which is are the \u201cX_pca\u201d distances in this example.\npost: The calculated distances after generating the latent space representation of the data, which is are the \u201cX_tsne\u201d distances in this example. For comparisons between these latent space representations, users can replace \u201ctsne\u201d with \u201cumap\u201d to test the latter embedding (Figures 7[href=https://www.wicell.org#fig7]A, 7B, 7F, and 7G).pre_norm: A flattened vector of normalized, unique cell-cell distances before transformation, as output by \u201cdistance_stats\u201d. This is calculated for the PCA representation in the example.\npost_norm: A flattened vector of normalized, unique cell-cell distances after transformation, as output by \u201cdistance_stats\u201d. This is calculated for the t-SNE representation in the example.\nlabels: The labels for the pre- and post- transformation data, set as \u201cPCA (50)\u201d and \u201ct-SNE\u201d in this example.\nfigsize: The size of the figure, in terms of width and height. Set as \u201c(4,4)\u201d and \u201c(3,3)\u201d respectively in this example.\nNote: Step 32 should be repeated as necessary with each latent space representation of interest. Here we recommend also running it with the UMAP representation calculated in step 27.\nCompare these distances between subpopulations of cells, being clusters c1, c2, and c3 in this example as defined in step 30:\ncorr_tSNE, EMD_tSNE =QCPipe.fcc.cluster_arrangement_sc(\n\u00a0\u00a0adata= adata,\n\u00a0\u00a0pre= adata.obsm[\"X_pca\"],\n\u00a0\u00a0post= adata.obsm[\"X_tsne\"],\n\u00a0\u00a0obs_col=\"leiden\", IDs=[\"c1\",\"c2\",\"c3\"],\n\u00a0\u00a0ax_labels=[\"PCA (50)\",\"t-SNE\"],\n\u00a0\u00a0figsize=(4,4),\n)\nThe parameters in this case are:\npre: The coordinates of each single-cell before transformation, \u201cX_pca\u201d, or the 50-dimensional PCA are used in this case.\npost: The coordinates of each single-cell after transformation, \u201cX_tsne\u201d, or the 50-dimensional PCA are used in this case. For comparisons between these latent space representations, users can replace \u201ctsne\u201d with \u201cumap\u201d to test the latter embedding (Figures 7[href=https://www.wicell.org#fig7]C, 7D, 7H, and 7I).\nobs_col: This parameter indicates which observation attribute to highlight by color, this example is set as \u201cleiden\u201d\nIDs: The observational IDs in which subsets of cells, Leiden cluster IDs in this example, are selected.\nax_labels: The labels for the pre- and post- transformation data, set as \u201cPCA (50)\u201d and \u201ct-SNE\u201d in this example, to be plotted as axis labels.figsize: The size of the figure, in terms of width and height. Set as \u201c(4,4)\u201d and \u201c(3,3)\u201d respectively in this example.\nNote: Step 3.9 should also be repeated as necessary with each latent space representation of interest. Here we recommend also running it with the UMAP representation calculated in step 27. Further, additional comparisons between other Leiden clusters should be performed to evaluate all potential subpopulations of interest, and these clusters should incorporate signatures highlighted in step 29.\nGenerate a minimum-spanning tree (MST) to investigate global subpopulation arrangements and structure:\nQCPipe.fcc.find_centroids(adata, use_rep=\"X_pca\", obs_col=\"leiden\")\nQCPipe.fcc.find_centroids(adata, use_rep=\"X_tsne\", obs_col=\"leiden\")\nQCPipe.fcc.find_centroids(adata, use_rep=\"X_umap\", obs_col=\"leiden\")\nThe parameters in this case are:\nuse_rep: The representation of the single-cell data to find centroids within, set as \u201cX_pca\u201d, \u201cX_tsne\u201d, and \u201cX_umap\u201d in this example.\nobs_col: This parameter indicates which observation attribute to find centroids within, as defined earlier, this example is set as \u201cleiden\u201d\nNote: Step 33 should also be repeated as necessary with each latent space representation of interest, like in step 32.\nDetermine the edge differences from native (PCA) to latent (t-SNE and UMAP) spaces by counting edge inconsistencies in a minimum spanning tree:\ntsne_set = set(adata.uns[\"X_tsne_centroid_MST\"].edges).difference(set(adata.uns[\"X_pca_centroid_MST\"].edges))\numap_set = set(adata.uns[\"X_umap_centroid_MST\"].edges).difference(set(adata.uns[\"X_pca_centroid_MST\"].edges))\nThe parameters in this case are:\nLatent MST: The MST calculated based on the latent representation of the data, being \u201cX_tsne\u201d and \u201cX_umap\u201d in these two calculations.\nNative MST: The MST calculated based on the native representation of the data, being \u201cX_pca\u201d in these two calculations.\nPlot these calculated edge differences:\nQCPipe.fcc.DR_plot(dim_name=\"t-SNE\").plot_centroids(adata=a, obs_col=\"leiden\", use_rep=\"X_tsne\", highlight_edges=tsne_set)\nQCPipe.fcc.DR_plot(dim_name=\"UMAP\u201d).plot_centroids(adata=a, obs_col=\"leiden\", use_rep=\"X_umap\", highlight_edges=umap_set)\nThe parameters in this case are:\ndim_name: The name of the latent representation to be plotted, being \u201cX_tsne\u201d and \u201cX_umap\u201d.\nobs_col: This parameter indicates which observation attribute to highlight by color, this example is set as \u201cleiden\u201duse_rep: The representation of the single-cell data to plot, set as \u201cX_tsne\u201d and \u201cX_umap\u201d in this example.\nhighlight_edges: Which differing edges to highlight, representing a rearrangement of coarse cluster neighbors. \u201ctsne_set\u201d and \u201cumap_set\u201d in this example, calculated earlier.", "Step-by-step method details\nStep-by-step method details\nThis section details the steps required for depositing SIM data to BioStudies and SXT data to EMPIAR. An overview of the whole process is provided in Figure\u00a03[href=https://www.wicell.org#fig3].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/374-Fig3.jpg\nFigure\u00a03. Overview of the workflow for uploading CLXM data to the appropriate EMBL-EBI archives\nCritical: At present SIM data should be deposited in BioStudies first to generate a BioStudies accession code, which is needed for depositing SXT data in EMPIAR.\nDeposition of light microscopy data (BioStudies)\nTiming: 30\u00a0min to 1\u00a0week depending on number of files and overall data size\nThe deposition of light microscopy data to the BioStudies archive comprises three main steps: (a) registering an account with BioStudies, (b) uploading the data, and (c) uploading the metadata (as per Table 1[href=https://www.wicell.org#tbl1]). These steps are described in detail below.\nRegister at the BioStudies database\nOpen the BioStudies submission page: https://www.ebi.ac.uk/biostudies/submissions[href=https://www.ebi.ac.uk/biostudies/submissions].\nClick on \u201cRegister.\u201d\nComplete the form providing the following personal information: name, email address, ORCID (optional), and a password.\nNote: The data will be stored, processed, and shared in a GDPR-compliant fashion according to EMBL-EBI and BioStudies GDPR privacy notices.\nActivate your account by opening the link sent to the email address you provided.\nLogin to the BioStudies submission system using the same link as in step 1a. In the home portal you can find a history of previous data submissions, as well as those still in progress under the \u201cDrafts\u201d tab.\nTo begin the data-upload process click on the \u201cFiles\u201d tab in the upper right corner (Figure\u00a0S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]A). Depending on your chosen method for upload follow the instructions below:\nAspera:\nClick the \u201cFTP/Aspera\u201d button on the upper right corner (Figure\u00a0S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]A).In the pop-up window, note the code for the \u201csecret\u201d directory as shown in Figure\u00a0S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]B (this will be the destination directory to which the data are to be uploaded).\nIn the command prompt, type the path to the ascp file in the directory where you installed the Aspera client (refer to step 2b in the \u201cFile upload software setup and installation\u201d section above), followed by the connection settings, the path of the source directory to be uploaded and, the path of the destination \u201csecret\u201d directory. This information is provided in the FTP/Aspera pop-up window (Figure\u00a0S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]B).\nFor example, to begin the upload, first enter the path to the Aspera executable (this varies by operating system):\nWindows:\nC:\\Users\\<your_user_name>\\AppData\\Local\\Programs\\Aspera\\Aspera Connect\\bin\\ascp.exe\nMac:\n\u223c/Applications/Aspera\\ Connect.app/Contents/Resources/ascp\nLinux:\n\u223c/.aspera/connect/bin/ascp\nNote: Default paths are used in the above examples. The path to the Aspera executable (ascp) will depend on where you installed the Aspera client\nFollowed by the arguments:\n-P33001 -i ..\\etc\\asperaweb_id_dsa.openssh -d <directory to upload> bsaspera_w@hx-fasp-1.ebi.ac.uk:.<[href=https://www.wicell.orgmailto:bsaspera_w@hx-fasp-1.ebi.ac.uk:.%3c];secret directory>\nNote: Refer to Troubleshooting 1[href=https://www.wicell.org#troubleshooting] if Aspera transfer is blocked by your local firewall.\nTransfer via FTP: you can use any freely available FTP client such as FileZilla (https://filezilla-project.org/[href=https://filezilla-project.org/]) or use the command line to execute FTP commands as follows:\nClick the \u201cFTP/Aspera\u201d button on the upper right corner (Figure\u00a0S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]A).\nIn the pop-up window note the code for the \u201csecret\u201d directory as shown in Figure\u00a0S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]B (this will be the destination directory where the data are to be uploaded to).\nOpen a command prompt and type ftp then at the ftp> prompt type open followed by the link to your chosen FTP server.\nWhen prompted type your username and password.Define the source and destination directories as follows: type cd followed by the path of the source directory, then type put followed by the destination directory which will be your secret directory noted from step (ii).\nNote: Further help on how to transfer files using FTP or Aspera can be found at https://www.ebi.ac.uk/ega/about/ftp-aspera[href=https://www.ebi.ac.uk/ega/about/ftp-aspera].\nTransfer via HTTP: click \u201cFile upload\u201d or \u201cFolder upload,\u201d select from your local system the files or folder to be uploaded and confirm. The upload progress for each of the files and directories selected will be shown in the \u201cProgress\u201d column of the \u201cFiles\u201d table (Figure\u00a0S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]A).\nPause Point: 1\u00a0min \u2013 1\u00a0week depending on number of files and overall data size.\nUpload the relevant metadata describing the dataset (based on Table 1[href=https://www.wicell.org#tbl1]). This can be done in one of two ways:\nWeb form (under development):\nFill all mandatory fields.\nClick the \u201cSubmit\u201d button which appears after all relevant fields are completed.\nExcel template:\nDownload template from ftp://ftp.biostudies.ebi.ac.uk/biostudies/pub/BioImaging/templates/ (file \u201cSIM_template.xlsx\u201d).\nComplete the template.\nSend the completed file to biostudies@ebi.ac.uk[href=https://www.wicell.orgmailto:biostudies@ebi.ac.uk] for processing.\nNote: You will now receive your BioStudies accession code which can be cited in your publication. This code will also be required to complete the deposition of the corresponding SXT data to EMPIAR.\nDeposition of X-ray microscopy data (EMPIAR)\nTiming: 30\u00a0min to 1\u00a0week depending on number of files and overall data size\nRegister an account with the EMPIAR database\nOpen the EMPIAR deposition webpage: https://empiar.org/deposition/login[href=https://empiar.org/deposition/login].\nClick on \u201cRegister as a new user.\u201d\nCompete the web form providing general information, such as your name, contact details, and a password.\nNote: The data will be stored, processed, and shared in a GDPR-compliant fashion, according to\u00a0EMBL-EBI (https://www.ebi.ac.uk/data-protection/privacy-notice/embl-ebi-public-website[href=https://www.ebi.ac.uk/data-protection/privacy-notice/embl-ebi-public-website]) and EMPIAR Deposition System Data Protection (https://empiar.org/deposition/gdpr[href=https://empiar.org/deposition/gdpr]) schemes.Activate the account by opening the web link sent to the email address that you provided.\nOptional: you can use your Google, Facebook, or ORCID account to sign in.\nLogin to the EMPIAR deposition system using the same link as in step 5a to reach the deposition home portal; then click on \u201cBegin/Continue an EMPIAR deposition\u201d (Figure\u00a0S3[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]A).\nClick on \u201cCreate a new deposition\u201d (Figure\u00a0S3[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]B) to reach the \u201cDeposition overview\u201d page.\nComplete the web form providing some of the initial metadata associated with your dataset as follows:\nDeposition image: click \u201cchoose a file\u201d to upload a figure (not subject to copyright, e.g., by a journal) that illustrates a facet of your data or study (see Figure\u00a0S4[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]A).\nNote: This image will be used on the EMPIAR website to illustrate your entry.\nCitations: complete the details of the publication where the data are described (e.g., DOI, journal name, authors etc.).\nNote: Publication details can be automatically filled if you provide a DOI or PubMed ID.\nNote: More than one publication can be added by pressing the \u201cAdd Citation\u201d button. The first publication provided will be the primary one.\nNote: If the data are not published yet, then some of the fields can be omitted or filled by pressing the associated \u201cN/A\u201d button. Publication details can be updated by curators at a later stage but remains the depositor\u2019s responsibility to communicate publications associated with this data as and when they are released.\nEntry details: enter a title for the dataset, followed by \u201cEntry type\u201d and \u201cRelease instructions\u201d as follows:Entry type: select the type or modality used to acquire the dataset from the available options. For the scope of this protocol, select the \u201cCLXM\u201d option to upload the SXT data here (while the corresponding SIM data should be uploaded in the BioStudies archive as explained above).\nNote: You will be required to provide the BioStudies accession code (refer to footnote 1) where the corresponding SIM data are stored (see step 8d).\nRelease instructions: choose an option (listed in Table 3[href=https://www.wicell.org#tbl3]) to indicate when the entry should be released (made available to the public) once it has been submitted and processed successfully.\ntable:files/protocols_protocol_374_3.csv\nCross-references: enter the accession codes of related datasets. For CLXM data, the BioStudies code for the corresponding light microscopy data is required. Click \u201cN/A\u201d for other unrelated codes such as EMDB (see Figure\u00a0S4[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]B).\nEntry authors, Corresponding author and Principal Investigator(s): provide the relevant information of the research team that conducted the study.\nNote: Entry authors may differ from the publication(s) authors specified previously.\nNote: All authors must be listed under \u201cEntry authors\u201d including those listed as corresponding author and principal investigator(s).\nPress the \u201cSave & Validate\u201d button on the \u201cDeposition overview\u201d page. If all fields have been successfully completed, you will be transferred to the \u201cUpload data\u201d page.\nUpload the data using one of the three available methods:\nTransfer via Globus:\nIn the \u201cUpload data\u201d page, click \u201cGlobus\u201d under \u201cOptions for upload\u201d and note your username and password for the EMBL-EBI endpoint (Figure\u00a0S5[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]A).\nEnsure you have set up your Globus account and personal endpoint (refer to step 2a in \u201cFile upload software setup and installation\u201d).\nLogin at the Globus webpage www.globus.org[href=http://www.globus.org] to reach the \u201cFile Manager\u201d interface.In the \u201cCollection\u201d field, search for and select \u201cEMBL-EBI Private endpoint,\u201d then enter your authentication username and password noted from step (i). Enter your EMPIAR directory by selecting the link provided back in the \u201cUpload data\u201d page under the option \u201cGlobus\u201d (Figure\u00a0S5[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]B).\nIn the Globus \u201cFile Manager,\u201d click \u201cTransfer or Sync\u2026\u201d then search for and select your personal endpoint in the second \u201cCollection\u201d field (Figure\u00a0S5[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]C).\nUnder your personal endpoint, navigate to the directory to be uploaded and click \u201cStart\u201d to begin the transfer to the EMPIAR directory. You can follow the status of your transfer under the \u201cActivity\u201d tab (Figure\u00a0S5[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]C).\nTransfer via Aspera (command-line client):\nIn the command prompt, type the path to the ascp file in the directory where you installed the Aspera client (refer to step 2b in the \u201cFile upload software setup and installation\u201d section above), followed by the connection settings, the path of the source directory to be uploaded, and the path of the destination EMPIAR directory. This information is provided by selecting \u201cAspera Command Line\u201d under \u201cOptions to upload.\u201d\nFor example, to begin the upload, first enter the path to the Apera executable (this varies by operating system):\nWindows:\nC:\\Users\\<your_user_name>\\AppData\\Local\\Programs\\Aspera\\Aspera Connect\\bin\\ascp.exe\nMac:\n\u223c/Applications/Aspera\\ Connect.app/Contents/Resources/ascp\nLinux:\n\u223c/.aspera/connect/bin/ascp\nNote: Default paths are used in the above examples. The path to the Aspera executable (ascp) will depend on where you installed the Aspera client\nFollowed by the arguments:\nQT -P 33001 -l 200M -L- -k3\u00a0<directory to upload> emp_dep@hx-fasp-1.ebi.ac.uk:upload/<;your_unique_directory_name>/data[href=https://www.wicell.orgmailto:emp_dep@hx-fasp-1.ebi.ac.uk:upload/%3c;your_unique_directory_name%3e/data]\nWhen prompted enter the provided password for your EMPIAR upload.\nTransfer via Aspera Connect (web interface):\nClick on \u201cAspera web interface\u201d under \u201cOptions to upload.\u201d\nSelect either the \u201cUpload files\u201d or \u201cUpload directories\u201d option.Select the files or directories to upload from the pop-up dialog (a maximum limit of files/directories selected will be set by your web-browser and operating system).\nTransfer via FTP: you can use any freely available FTP client such as FileZilla (https://filezilla-project.org/[href=https://filezilla-project.org/]) or use the command line to execute FTP commands following similar steps as in step 3b of the \u201cDeposition of Light Microscopy Data\u201d section. You will need to contact EMPIAR team (empdep-help@ebi.ac.uk[href=https://www.wicell.orgmailto:empdep-help@ebi.ac.uk]) to obtain your destination directory.\nNote: Refer to Troubleshooting 1[href=https://www.wicell.org#troubleshooting] if Aspera transfer is blocked by your local firewall.\nNote: Refer to Troubleshooting 2[href=https://www.wicell.org#troubleshooting] if problems are encountered using the above upload methods.\nPause Point: 1\u00a0min \u2013 1\u00a0week depending on number of files and overall data size\nIn the \u201cUpload data\u201d page, click the \u201cAssociate image sets with the data\u201d link from the left-hand-side menu (Figure\u00a0S5[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/protocol-id-Mmc1.extension]A).\nComplete the web form of the requested metadata to describe at least one image set. Additional metadata (in accordance with Table 1[href=https://www.wicell.org#tbl1]) should be entered in the \u201cDetails\u201d field of the web form.\nNote: This is for bioimage data only. Any textual and other supplementary data should not be created as a separate image set, but rather described in the free text \u201cDetails\u201d field. Some of the fields can be automatically filled from the header of an image file by clicking the \u201cPopulate Form\u201d button.\nOptional: check the data for consistency by:\nDownloading a script that compares checksums of files on your machine and those stored in EMPIAR using the MD5 message-digest algorithm.\nDownloading a listing of files present on the EMPIAR side.\nPress the \u201cSave\u00a0+ Validation\u201d button on the \u201cAssociate image sets with the data\u201d page.Optional: describe EMDB-SFF (https://emdb-empiar.org/emschema/segmentation.html/[href=https://emdb-empiar.org/emschema/segmentation.html/]) segmentation files. Segmentation is the decomposition of 3D volumes into regions that can be associated with defined (biological) objects. You can also provide the original segmentation files from software such as Amira HyperSurface (.surf), Segger (.seg), EMDB Map masks (.map), IMOD (.mod), etc.\nNote: Click the \u201cAdd segmentations\u201d button at the bottom of the \u201cAssociate image sets with the data\u201d page. A page \u201cAssociate segmentations with the data\u201d will be opened where you can select one of your image sets and associate it with a segmentation file. This can be done for multiple files / image sets. If you change your mind you can click \u201cRemove all segmentations.\u201d You will not be able to submit unless you either remove all segmentations or save them with validation.\nClick the \u201cSubmit entry\u201d button.\nNote: You will be redirected to the main deposition page where you can see your accession code, which can be cited in your publication.\nNote: Once the entry has been checked by the EMPIAR curation team, you will receive a request to approve the entry.", "Step-by-step method details\nStep-by-step method details\nIn the following sections, we describe step-by-step how to perform an imaging transcriptomics analysis of a neuroimaging map from start to finish. This includes identifying genes whose expression correlate spatially with the neuroimaging map and performing gene set enrichment analysis to inform the biological interpretation of the results. Such analyses require detailed information on gene expression across multiple regions of the post-mortem human brain, which right now can only be accessed through the Allen Human Brain Atlas (AHBA).\nThe toolbox allows implementing two types of analyses to quantify the association between neuroimaging and gene expression data: i) a simple mass-univariate Spearman correlation analysis; ii) a multi-variate PLS regression analysis. The method to be used is defined as an input by the user. Both methods have been used in previous works applying imaging transcriptomics (Fulcher et\u00a0al., 2021[href=https://www.wicell.org#bib7]; Morgan et\u00a0al., 2019[href=https://www.wicell.org#bib17]) and have their own strengths and weaknesses that should be considered on a case-by-case basis. Ultimately, the toolbox provides a list of genes ranked by how well they associate with the distribution of a neuroimaging marker input by the user and identifies which genes are significantly associated with the marker using state-of-the-art methods that account for bias induced by the spatial autocorrelation of the data.In order to perform imaging transcriptomics analyses, both neuroimaging and gene expression should be mapped into the same space. Currently, the analyses implemented in the toolbox are based on the Desikan-Killiany (DK) parcellation (Desikan et\u00a0al., 2006[href=https://www.wicell.org#bib5]). For the neuroimaging data, the toolbox implements a simple averaging of the signal across all voxels of each parcel in the atlas. For the gene expression data, the process of mapping the AHBA data to DK parcels was implemented a priori using the abagen toolbox (https://www.github.com/netneurolab/abagen[href=https://www.github.com/netneurolab/abagen]). Briefly, genetic probes were reannotated and only probes that could reliable be matched to genes were kept and filtered based on their value relative to the background noise by using a threshold of 50%, yielding a total of 15,633 probes (Arnatkevi\u010di\u016bt\u0117 et\u00a0al., 2019[href=https://www.wicell.org#bib3]). Next, tissue samples were assigned to brain regions using their corrected MNI coordinates (https://github.com/chrisfilo/alleninf[href=https://github.com/chrisfilo/alleninf]), samples were matched to regions constraining this to hemisphere and cortical/subcortical subdivisions. Samples were assigned to brain regions in the atlas if their coordinates in MNI space were within 2\u00a0mm of a given parcel. To reduce the potential for misassignment, sample-to-region matching was constrained by hemisphere and gross structural divisions (i.e., cortex, subcortex/brainstem, and cerebellum). All tissue samples not assigned to a brain region in the provided atlas were discarded (Markello et\u00a0al., 2021[href=https://www.wicell.org#bib13]). Samples were then averaged across donors and normalized, resulting in a final single matrix with rows corresponding to brain regions and columns corresponding to the 15,633 genes.Irrespectively of the statistical method selected by the user (PLS or Spearman correlation), the inferential statistics is calculated using gold-standard methods that are robust to the intrinsic autocorrelation of the imaging data. All significance testing is based on permutation testing, where 1,000 null spatial maps are derived using a combination of spin rotations of the cortical regions and resampling of the subcortical regions. The spin rotations are implemented using the Vasa method as in previous studies (Alexander-Bloch et\u00a0al., 2013a[href=https://www.wicell.org#bib1], 2013b[href=https://www.wicell.org#bib2]; Markello and Misic, 2021[href=https://www.wicell.org#bib14]; V\u00e1\u0161a et\u00a0al., 2018[href=https://www.wicell.org#bib19]). The same nulls are then used in the ensemble gene set enrichment analyses to control for false positives related to the spatial autocorrelation of the data, as recently recommended (Fulcher et\u00a0al., 2021[href=https://www.wicell.org#bib7]).\nTo illustrate the various steps of the analysis with the toolbox, we will use as an example a publicly available positron emission tomography (PET) average template of the serotonin receptor 5-HT2A ([11C]Cimbi-36) from (Beliveau et\u00a0al., 2017[href=https://www.wicell.org#bib4]).\nNote: The scan downloadable from the online repository (https://xtra.nru.dk/FS5ht-atlas/[href=https://xtra.nru.dk/FS5ht-atlas/]) must be reshaped since it has a data matrix of 182\u00a0\u00d7\u00a0218\u00a0\u00d7\u00a0182\u00a0\u00d7\u00a01 (for more see troubleshooting 2[href=https://www.wicell.org#sec5.3]). In addition, to avoid problems with the file system the scan should be renamed by replacing the dots ( . ) in the name with underscores ( _ ). For the scope of the following example the scan has been renamed to 5-HT2A_mean_bmax.nii.gz.\nSelect the path of your input file.\nNote: For the input, either common neuroimaging scan formats (NIfTI - .nii, .nii.gz) or text files (i.e., .tsv, .csv, .txt) can be used. The path should be provided as an absolute path (e.g., \u201c/home/username/data_folder/myfile\u201d instead of \u201c./data_folder/myfile\u201d).Critical: If the input is a neuroimaging scan, this must be already in standard MNI152 space and have a voxel size of 1\u00a0mm isotropic (imaging matrix size of 182\u00a0\u00d7\u00a0218\u00a0\u00d7\u00a0182). On the other hand, if the input is a text file, this must have only one column, with no header and a regional value in each row, following the order of the DK atlas (the full list of regions is available in the supplement file Table\u00a0S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1578-Mmc1.pdf]).\nPartial least square regression analysis\nTiming: \u00a0<\u00a05\u00a0min\nThis step is the first of two alternative ways to run the analysis, and it employs PLS regression to identify latent components that maximize the correlation between neuroimaging and gene expression data.\nRun the script imagingtranscriptomics using the pls option:\n> imagingtranscriptomics --input <input_path> [--output <output-path>][--regions <all|cort+sub|cort>] [--no-gsea] [--genest] pls <pls_options>\nThe arguments to be provided are:\n--input <input_path>: the path to the input file (the path from step 1).\n--output <output_path> (optional): the path where the results should be saved.\nNote: If this is not provided the results will be saved in the path of the input file.\n--regions <all|cort+sub|cort>(optional): Allows the user to select which regions to use in the analysis. This is particularly useful with certain types of data (e.g., EEG) where subcortical regions might not be available. The available options are all (or equivalently cort+sub), which specifies that all regions should be used, or cort where only cortical regions are used.\n--no-gsea (optional): this flag allows running the script without performing GSEA.\nNote: If this is not provided, the script will also run the GSEA step (described below).\n--geneset (optional): Name of the gene set to be used in the GSEA analysis.Note: If the --no-gsea flag is provided, this option will be ignored. If you also want to perform GSEA (i.e., excluding the --no-gsea flag), a gene set should be selected - for more information on the available gene sets refer to the GSEA step.\npls <pls_options>: uses PLS regression to analyze the data. After the pls keyword, only one of the following inputs is required:\n--ncomp <n>: number of components to use in the PLS regression (this must be an integer between 1 and 15).\n--var <n>: percentage of the variance to explain. With this option, the optimal number of components will be automatically calculated by the script (this must be a float between 0 and 1).\nFor instance, with the example data, we can run the command:\n> imagingtranscriptomics --input 5-HT2A_mean_bmax.nii.gz --regions all --no-gsea pls --ncomp 1\nWhich will run the analysis with one PLS component on the example scan, without running GSEA, and save the results the same directory as the input scan (the results will be in a folder named Imt_5-HT2A_mean_bmax_pls).\nMass-univariate correlation analysis\nTiming: \u00a0>\u00a030\u00a0min\nAs an alternative to PLS regression, the toolbox also offers the option to run the analysis using mass-univariate Spearman correlations. This option will simply calculate Spearman correlations between the neuroimaging vector and the expression of each gene.\nNote: if you want to analyze the data with PLS regression, you can skip this step.\nRun the script imagingtranscriptomics using the correlation option:\n> imagingtranscritomics --input <input_path> [--output <output-path>][--regions <all|cort+sub|cort>] [--no-gsea] [--genest] corr [--cpu <n_cpu>]\nThe first optional input is described in the PLS analysis section (points 2a-2e); the additional optional input for the script in this case is:\n--cpu <n_cpu>: number of cpu to be used for the calculation of the correlations (the default number is 4).Note: This step takes a considerably longer time compared to the PLS analysis, since the number of correlations to estimate is much greater.\nWith the example data we can run the command:\n> imagingtranscriptomics --input 5-HT2A_mean_bmax.nii.gz --regions all --no-gsea corr\nThis command will run the analysis on all brain regions, without running GSEA, with mass univariate correlation and save the results in the same directory as the input scan (the results will be saved in a folder named Imt_5-HT2A_mean_bmax_corr).\nNote: Irrespective of the method selected (PLS regression or mass-univariate correlation), the toolbox produces lists of genes ranked according to the strength of the spatial alignment between the neuroimaging phenotypes (e.g., regional distribution of a PET tracer, statistical map reflecting effects of a drug or case-control differences for a certain neuroimaging metric) and their expression. Please, note that when the user does not have a priori hypotheses about specific genes or pathways, interpreting the output in biological terms can be challenging. For instance, one might be interested in understanding if the top genes positively associated with a certain neuroimaging phenotype belong to specific biological pathways or brain cell-types. Answers to this type of questions can be provided by gene set enrichment analyses, which we will describe in the next section.\nEnsemble gene set enrichment analysis (GSEA)\nTiming: \u00a0>\u00a01 hGSEA uses a statistical hypothesis-testing framework to assess which categories of genes (i.e., set of genes sharing a certain biological function, such as neuronal genes or astrocytic genes) are most strongly related to a given phenotype, leveraging annotations of genes to categories from open ontologies, like the Gene Ontology (GO). Performing GSEA in the context of imaging transcriptomics is associated with methodological challenges that the application of the same algorithms in other circumstances do not necessarily raise, mainly, within-category gene\u2013gene co-expression and spatial autocorrelation are now known to drive false-positive bias, which requires particular attention in the way it is dealt with (for further information on this topic, please see Fulcher et\u00a0al., 2021[href=https://www.wicell.org#bib7]). In this toolbox, we implement the recently introduced ensemble GSEA framework, which overcomes false-positive gene-category enrichment in the analysis of spatially resolved transcriptomic brain atlas data, using a pre-ranked approach. These analyses are implemented through the imt_gsea script, which requires the .pkl file generated as a result of the previous step.\nNote: This step can be run as part of a single command as explained above; this is equivalent to omitting the --no-gsea flag and specifying the input --geneset in the previous script.\nDefine which gene set you want to use for the analysis; as an example, we will use the \u201cLake\u201d brain cell-type gene set included with the toolbox (this set includes genes expressed in 30 brain cell-types as identified in a previous single-cell transcriptomic study (Lake et al., 2018[href=https://www.wicell.org#bib21])). Other available gene sets are provided and can be searched by running the command:\n> imt_gsea --geneset availNote: The toolbox offers the users the possibility to select their own gene set file; this should nevertheless be in a compatible format, i.e., gmt (see here the instructions on how to create your own gene set file https://www.gsea-msigdb.org/gsea/doc/GSEAUserGuideFrame.html[href=https://www.gsea-msigdb.org/gsea/doc/GSEAUserGuideFrame.html]). If the users decide to use their own genes, the file must be provided as an absolute path as the argument of the --geneset flag.\nRun the ensemble GSEA using the command:\n> imt_gsea --input /path_to_yourfile/file.pkl --geneset lake\nThe imt_gsea script accepts the following arguments:\n--input: path to the .pkl file generated by the previous step.\n--output (optional): path where the results will be saved; if none is provided, the parent directory of the input file will be used instead.\n--geneset: name of the gene set to be used in the analysis.\nNote: Depending on the gene set and analysis used, the GSEA will take longer to run, i.e., running the ensemble GSEA on an analysis with 2 PLS components will take twice the amount of time as running GSEA on an analysis with 1 PLS component.\nWith the results from either step 2 we can run the GSEA analysis by running the command (similar for the results from step 3):\n> imt_gsea --input Imt_5-HT2A_mean_bmax_pls/pls_analysis.pkl --geneset lake\nTo run the GSEA analysis using the lake gene set.\nCheck the results in the folder where the .pkl file was stored if no output path was specified.The interpretation of the ensemble GSEA output does not differ much from the standard GSEA analysis. The primary result of the gene set enrichment analysis is the enrichment score (ES), which reflects the degree to which a gene set is overrepresented at the top (positive score) or bottom (negative score) of a ranked list of genes. Significant enrichment is identified by p-values, corrected for multiple comparisons, less than 0.05 (i.e., pFDR<0.05). In ensemble GSEA, this means that the enrichment observed is higher than one would expect for a null neuroimaging phenotype with the same embedded spatial autocorrelation.", "Step-by-step method details\nStep-by-step method details\nStereotaxic surgery\nTiming: 1\u20131.5\u00a0h for stereotaxic surgery\nMice are stereotaxically injected with an AAV construct to express the optogenetic actuator ChR2 in the target region and implanted with a low-profile, small-bore MRI-compatible optic cannula.\nPrepare animal for stereotaxic surgery.\nInduce anesthesia for stereotaxic surgery in a dedicated chamber using 4% isoflurane in a 1:4 O2 to air mixture.\nWeigh the animal and prepare anesthesia mixture according to the weight.\nInject anesthesia mixture intramuscularly into the quadriceps femoris muscle.\nGenerously shave hair from the neck up to the front of the eyes.\nApply epilation cream to the shaved head and let sit for 1\u00a0min. Remove with a lint-free cloth.\nSecure the animal in the stereotaxic frame, placed on top of a heating pad maintained at \u223c36\u00b0C.\nCover eyes with an artificial tear ointment to avoid drying of eyes.\nInject lidocaine (20\u201350\u00a0\u03bcL) under the scalp to prevent pain.\nCut a large oval section of the scalp to expose the skull using sterile forceps and scissors.\nRemove all tissue covering the skull using sterile cotton buds. Avoid damage to the muscles.\nClamp the surrounding skin with spring scissors to evenly expose anterior, posterior, and parietal areas of the skull.\nApply a thin layer of dental etching gel to the exposed skull while avoiding contact with soft tissue. Let sit for 30\u00a0s before removing it with sterile cotton buds.\nCarefully scrape the bone surface of the animal\u2019s skull with a dental scraper in a horizontal, vertical, and diagonal way to create ridges for the dental cement to adhere to (Figure\u00a01[href=https://www.wicell.org#fig1]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2076-Fig1.jpg\nFigure\u00a01. Skull surface preparation\nUsing a dental scaler, horizontal, vertical, and diagonal ridges are scraped on the skull surface. The resulting bone debris is removed using saline and lint-free sponge material.Wash off bone debris with saline and dry the skull using lint-free, medical-grade sponge material. Do not desiccate.\nLocate bregma and lambda positions and correct for tilt and scaling accordingly.\nNavigate to the desired coordinates (AP -0.1; ML -2.6; DV 3.7, relative to bregma).\nCarefully start drilling a small hole using a dental drill (30\u2032000\u00a0rpm), without severing the cortical surface.\nRemove any bone debris from the skull using a rubber air puffer.\nCritical: Clean any bloody tissue surrounding the drill hole using saline and lint-free, medical-grade sponge material. Any trapped and clotted blood will result in MR signal loss due to the paramagnetic properties of deoxyhemoglobin. After cleaning, keep any exposed brain tissue submerged in sterile saline.\nPerform viral injection.\nLet the injection needle sit in place for \u223c5\u201310\u00a0min (depending on injected volume) to avoid any fluid drawback during needle retraction.\nCritical: Stop any bleeding that may occur during needle retraction with medical-grade sponge material before optical fiber implantation. This will ensure no additional MR artifacts due to clotted blood surrounding the optical fiber. Keep exposed tissue submerged in sterile saline.\nPlace optical fiber in stereotactic holder and lower it toward the cortical surface.\nSlowly (\u223c1200\u00a0\u03bcm/min) drive the optical fiber into the brain tissue, making sure no bleeding occurs.\nOnce the optical fiber is in place, dry the skull using a rubber air puffer. Do not desiccate.\nApply a thin layer of bonding agent to the exposed skull and light-cure for 20\u00a0s with a dental curing light.\nCritical: Steer clear of the optical implant and drill hole as the reagent has an acidic pH that will damage both the optical fiber and brain tissue, resulting in substantial MR artifacts.Apply the dental composite Permaplast LH flow and light-cure it for 20 s. Start at the plastic edges of the optical fiber and build up in layers. This will embed the optical implant and contribute to its longevity.\nOnce the optical implant is secure, remove the stereotaxic holder.\nApply the dental composite in layers to the remaining exposed skull and light cure each layer for 20 s.\nCritical: Avoid introducing air bubbles while building the dental composite layers as this will create MR artifacts. This can be easily avoided by handling the dental cement syringe in a gentle manner (i.e., no shaking) and removing any bubbles forming at the tip of the syringe using a tissue. Keep the total dental composite layer to the height of the optical implant.\nApply non-absorbable silk stitches to the anterior and posterior skin flaps as needed.\nSubcutaneously inject the anesthesia antidote and 100\u00a0\u03bcL of Saline to rehydrate mice. Put them in a heating chamber for recovery.\nOpto-fMRI recording\nTiming: 10\u201315\u00a0min for animal preparation, 1\u00a0h for opto-fMRI recordings\nCombined optogenetic stimulation and fMRI recordings in mice capture cell-type specific contributions to changes in brain-wide activity.\nSlide a 4\u00a0cm intubation tube with a demarcation at 2.5\u00a0cm onto a 25 gauge needle tip and prepare a 4\u00a0cm cannulation tube connected to a 30 gauge needle tip. Connect the cannulation tube to a separate, longer tubing attached to a 1\u00a0mL insulin syringe filled with sterile saline (Figures\u00a02[href=https://www.wicell.org#fig2]A and 2B).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2076-Fig2.jpg\nFigure 2. Endotracheal intubation and tail vein cannulation procedure for opto-fMRI recordings in the anesthetized state\n(A) Cannulation and intubation tubing 4 cm. Insert a 30G needle tip into the cannulation tube.(B) Connect the cannulation tube to an insulin syringe filled with sterile saline via a longer tube connected to a 30G needle and pre-fill tubings and needles with sterile saline. Slide intubation tube onto a 25G needle tip for guide purposes.\n(C) Place the anesthetized mouse onto a platform facing belly up and secure front incisors with an elastic. Tape the tail to the platform surface for additional stability and position an isoflurane tubing along the buccal tissue inside the mouth to maintain anesthesia.\n(D) Place a light source above the animal\u2019s neck to visualize the trachea. Once located, intubate the mouse using the guided intubation tube.\n(E) Once the mouse is successfully intubated, a faint rhythmic condensation pattern should be visible inside the intubation tube. Cut the tube at the demarcation line creating a bevel.\n(F) Position the intubated mouse on a heating pad, reducing isoflurane anesthesia to 2%. Locate the lateral tail vein and cannulate the animal. Once successfully cannulated, blood should easily be drawn from and re-injected into the tail vein using the insulin syringe connected to the cannulation tubing.\nAnesthetize the mouse in a separate chamber with 4% isoflurane (1:4 O2 to air mixture) for 4\u00a0min.\nPosition the animal on an elevated platform facing belly up once the timer has expired.\nSecure upper incisors in an elastic hoop to overextend the neck.\nTape the tail to the platform surface using adhesive tape (Figure\u00a02[href=https://www.wicell.org#fig2]C).\nMaintain anesthesia at 4% isoflurane flow through redirected tubing placed inside the animal\u2019s mouth (Figure\u00a02[href=https://www.wicell.org#fig2]C).\nPut a small heating pad (\u223c36\u00b0C) on top of the tail to maintain a stable core temperature.\nPosition a light source above the animal\u2019s neck to readily locate the trachea (Figure\u00a02[href=https://www.wicell.org#fig2]D).Perform endotracheal intubation with the dedicated, 4\u00a0cm long tube (Figure\u00a02[href=https://www.wicell.org#fig2]D). For an instructive videographic documentation of the intubation process please see (Das et\u00a0al., 2013[href=https://www.wicell.org#bib4]).\nCut the tube at the demarcation, creating a bevel (Figure\u00a02[href=https://www.wicell.org#fig2]E).\nTransfer the animal to a heating pad and reduce the isoflurane flow to 2%.\nCannulate the lateral tail vein (Figure\u00a02[href=https://www.wicell.org#fig2]F).\nSecure tail vein tubing with a thin adhesive tape without touching the cannulation needle.\nPlace the mouse on a continuously heated MR cradle.\nConnect the endotracheal tube to the MR compatible ventilation system supplying a rhythmic (80 breaths/min) delivery of a 20% O2 to 80% air mixture with an isoflurane flow at 2%.\nSet the ventilation system to a respiration cycle of 25% inhalation, 75% exhalation, and an inspiration volume of 1.8\u00a0mL/min.\nCritical: Ensure the breathing rate of the mouse is synchronized to the ventilation rhythm before proceeding.\nConnect the tail vein tubing to the MR-compatible continuous infusion system.\nInject an initial bolus containing a mixture of pancuronium bromide (0.25\u00a0mg/kg) and medetomidine (0.05\u00a0mg/kg) for muscle relaxation and sedation, respectively.\nReduce isoflurane flow to 1.5% and set a timer to 5\u00a0min.\nNote: Following the bolus injection, the breath-related motion should become significantly shallower within 20\u201330 sec.\nCritical: It is important to measure the systemic physiological parameters under anesthesia in a benchtop experiment that mimics the experimental conditions of the fMRI recordings. This to prevent any negative impact on the Blood Oxygenation Level Dependent (BOLD) fMRI signal that could result from anesthesia-related hyperoxia or hypercapnia. Our approach ensures a steady heart rate (measured in beats per minute, or bpm), pulse distension (measured in \u03bcm), and oxygen saturation (measured in %) for at least one hour of anesthesia, as demonstrated in (Zerbi et\u00a0al., 2019b[href=https://www.wicell.org#bib15]).Secure the animal\u2019s head within MR-compatible ear bars attached to the cradle (Figure\u00a03[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2076-Fig3.jpg\nFigure 3. Opto-fMRI cradle positioning and setup\nThe intubation tubus of the mouse is connected to the mechanical ventilation machine through a small mouthpiece. Once breathing is confirmed to be synchronized to the ventilation rhythm, secure the mouse\u2019s head within the earbars and connect the fiber patch cable to the optical fiber using a phosphor bronze mating sleeve. Apply an artificial tear gel to both eyes to prevent drying.\nPlace an MR-compatible rectal probe to monitor a constant animal temperature of 36\u00a0\u00b1 0.5\u00b0C throughout scanning.\nConnect the fiber patch cable to the optical fiber implant using a phosphor bronze mating sleeve.\nNote: Choosing such an opaque, MR-compatible mating sleeve greatly reduces light spill at the fiber contact site compared to generic ceramic sleeves. This step should not re-position the animal\u2019s head within the ear bars.\nApply an artificial tear gel to the animal\u2019s eyes to prevent drying.\nOptional: Place an additional fiber patch cable beneath the animal\u2019s head for continuous illumination at the experimental stimulation wavelength throughout scanning. This measure will control for potential visual artifacts during optogenetic stimulation.\nReduce the isoflurane flow to 0.5% once the timer has expired and start a continuous infusion of medetomidine (0.1\u00a0mg/kg/h) and pancuronium bromide (0.4\u00a0mg/kg/h).\nInsert the cradle into the MR bore equipped with the cryogenic probe, placing the animal\u2019s head at the center of the magnet.\nOptional: As an additional light control measure, place an MR-compatible LED light source at the back of the MR cradle and switch it on. If installed, turn on the continuous laser emission of the control fiber path cable.\nRun initial MR preparation scans suitable for your experimental design.Note: This step may include measures to tune and match the frequency of the coil, adjustment for radiofrequency power, and standard adjustments to compensate for MR field inhomogeneities (e.g., MAPSHIM in a Bruker console).\nOptional: Run a structural scan (T1-weighted or T2-weighted images) prior to functional data acquisition to assess correct optical fiber placement above the targeted regions and - if applicable - exclude animals that do not show correct targeting.\nCritical: Before functional data acquisition, ensure the laser system as well as the trigger program/device of choice is correctly set up to deliver precisely timed optogenetic stimuli.\nAlternatives: This protocol uses the COSplay trigger device to precisely define, time, and deliver laser stimulation to the target brain region during fMRI recordings. Any other trigger device (e.g., Arduino) or software that can be coupled with the fMRI sequence can be used as\u00a0well.\nStart the desired sequence on the 7 Tesla Bruker PharmaScan scanner for functional data acquisition timed with the optogenetic trigger system.\nNote: For BOLD fMRI recordings, we suggest using a gradient echo (GE) - echo planar imaging sequence (EPI) with a repetition time TR\u00a0= 1000\u00a0ms, a slice thickness ST\u00a0= 0.45\u00a0mm, and an in-plane spatial resolution RES\u00a0= 0.22\u00d70.2\u00a0mm2.\nSwitch off the continuous laser illumination and stop the delivery of isoflurane as well as the continuous infusion of the anesthesia mixture after the functional recordings.\nNote: Routinely check on the animal connected to the ventilation machine until the muscle relaxant pancuronium bromide is metabolized. When this occurs, the animal will start counter-breathing, attempt small movements, and try to detach itself from the intubation line.Critical: Remove the intubation tube from the mouse trachea only when you are confident that the animal can breathe autonomously. Inject up to 1\u00a0mL of saline solution subcutaneously to rehydrate the mouse and put it in a heated chamber to fully recover from the anesthesia. Regularly check the breathing of the animal.", "Step-by-step method details\nStep-by-step method details\nClinical sample collection\nTiming: 1 h\nThis step describes the procedure of collecting clinical samples for single-cell sequencing.\nInstitutional approval and informed consent need to be in place before the usage of any clinical samples. In this protocol, the collected specimen can be obtained by surgical resection or core needle biopsy of the neurofibroma.\nBiospecimen collection from patient neurofibroma.\nResected tumor may be obtained from a debulking surgery.\nThe surgically removed tumor comes from the operation room in a sterile container.\nOpen the sterile container a laminar flow hood within the procurement suite.\nApply pathology ink to the biospecimen to better visualize tumor margins.\nMeasure the three-dimensional size of the inked specimen with a ruler and record the measurements.\nCut open the biospecimen lengthwise in the middle.\nRemove one 0.5\u00a0\u00d7\u00a00.5\u00a0\u00d7\u00a00.5\u00a0cm3 cube of tissue from one side and immediately place in DMEM in a 15\u00a0mL conical tube (Figure\u00a01[href=https://www.wicell.org#fig1])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2683-Fig1.jpg\nFigure\u00a01. Illustration of tissue processing to achieve single-cell suspension from fresh nerve sheath tumor\n(A) Example NF1-associated nerve sheath tumor obtained from the operation room.\n(B) Picture of an inked and transected tumor with a 0.5 x 0.5 x 0.5\u00a0cm3 cube of tissue acquired from the tumor.\n(C) Picture of minced tumor with the presence of Tissue Dissociation Media in a petri dish.\n(D) Picture of a c-tube that can be used on a gentleMACSTM dissociator. This tube can be directly placed in the incubator for shaking and enzymatic dissociation.\nIf the specimen is a core needle biopsy obtained for diagnostic purposes.\nObtain the ultrasound-guided biopsy from the area-of-interest by proceduralists in the Intervention Radiology department.\nOnce a core needle biopsy is obtained, immediately place the biospecimen in DMEM in a 15\u00a0mL conical tube.Optional: Multi-regional sampling may be necessary if there is grossly visible heterogeneity (regional coloring or texture of the resected tumor) or magnetic resonance imaging (MRI) indicated that significant heterogeneity exists within the same tumor. Process the samples from different regions of the same tumor separately in the following steps.\nCritical: It is important to keep appropriate labeling and annotation for each specimen throughout the procedure. Pictures and annotations should be properly documented.\nGenerating single-cell suspensions of nerve sheath tumor\nTiming: 2 h\nThis step describes how to use enzymatic dissociation to generate a single-cell suspension of nerve sheath tumors.\nDissection of the biospecimen and curation of resected tumor\nSpray the outside of the conical tube with 70% ethanol and open it under a laminar flow hood. The tissue will be processed in the hood.\nTransfer the biospecimen to a 60\u00a0mm cell culture plate. Using sterile surgical scalpel and tweezers, dissect the tumor into pieces of 2\u00a0\u00d7\u00a02\u00a0\u00d7\u00a02\u00a0mm3 (Figure\u00a02[href=https://www.wicell.org#fig2])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2683-Fig2.jpg\nFigure\u00a02. Schematic of the workflow of NF1-associated nerve sheath tumor processing from the surgical suite to the laboratory\nFix one piece of the biospecimen in 10% buffered formalin solution at 25\u00b0C for a maximum of 24\u00a0h before transferring to 80% alcohol.\nNote: Process this piece for the paraffin embedded tissue block. This piece can be used for downstream validation experiments.\nPlace two to three pieces individually in Eppendorf tubes and snap freeze in liquid nitrogen.\nNote: Store these pieces at \u221280\u00b0C. They may be used for bulk sequencing or other experiments.\nMince the remaining tumor into 0.5\u00a0\u00d7\u00a00.5\u00a0\u00d7\u00a00.5\u00a0mm3 pieces in the cell culture plate.\nCritical: When processing a core needle biopsy, no dissection or curation is needed. Directly transfer the biopsy biospecimen into a c-tube for enzymatic dissociation.Enzymatic dissociation\nFor each sample to be dissociated, prepare 10\u00a0mL of complete Tumor Dissociation Media.\nUse a 10\u00a0mL serological pipette to add the Tumor Dissociation Media to the minced tumor and transfer them to a c-tube (Miltenyi).\nPlace the c-tube on the gentleMACS\u2122 dissociator and run \u201chTumor_2.1\u201d twice.\nShake the c-tube at 37\u00b0C for up to 40\u00a0min at 200\u00a0rpm.\nCritical: To process a core need biopsy sample, reduce the incubation time for enzymatic dissociation in step 4d to 20\u00a0min.\nPlace the c-tube on the gentleMACS\u2122 dissocator and run \u201chTumor_3.1\u201d twice.\nPlace a 40\u00a0\u03bcm cell strainer on a 50\u00a0mL conical tube under the laminar flow hood.\nUse a 10\u00a0mL serological pipette to agitate the dissociated sample in the tube under the hood to agitate the tumor into single-cell suspension.\nTransfer the suspension in the c-tube (dissociated sample may contain small pieces of tumor chunk) onto the cell strainer.\nUse the plunger from a 10\u00a0mL syringe to push the tumor chunks on the cell strainer.\nRinse the cell strainer with 10\u00a0mL DMEM twice. The filter-through will contain \u223c 30\u00a0mL of dissociated tumor cells (Figure\u00a03[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2683-Fig3.jpg\nFigure\u00a03. Illustration of using cell strainer to ensure the generation of single-cell suspension\n(A) Picture of the setup of a 40\u00a0\u03bcm cell strainer on an opened 50\u00a0mL conical tube.\n(B) After using a syringe plunger to mechanically force the remaining tissue chunks through the cell strainer, there should be minimal residual of tissue on the cell strainer.\nAdd 60\u00a0\u03bcL of sterile 0.5\u00a0M EDTA solution into the filter-through single-cell suspension and mix by inverting the tube.\nCentrifuge the cells at 300 g for 5\u00a0min to collect the cell pellet.Critical: Always add EDTA to the single-cell suspension to a final concentration of 1\u00a0mM before spinning the cells to prevent aggregation.\nUsing a 10\u00a0mL serological pipette, carefully remove all but 0.5\u00a0mL of the media. Resuspend the cell pellet.\nCount the cells using Acridine Orange/Propidium Iodide (AO/PI) fluorescent dye by mixing 20\u00a0\u03bcL cell suspension with 20\u00a0\u03bcL dye (Nexcelom) and loading 20\u00a0\u03bcL of the mixture to the disposable slide (Nexcelom).\nInsert the slide into the chamber of a fluorescent cell counter.\nNote: Cellometer Auto 2000 was used in this study but other automatic cell counter or manual counting using a hemocytometer can also be used.\nNote: Live cells fluoresce green fluorescent and dead cells fluoresce orange fluorescent.\nNote: The automatic cell counter counts the number of green fluorescent cells as live cells and the number of orange fluorescent cells as dead cells.\nCalculate the cell concentration in the single-cell suspension using a dilution factor of 2.\nWash the cells with 20\u00a0mL of DMEM and repeat steps l-p.\nDetermine the cell concentration and viability again using AO/PI dye and a fluorescent cell counter (Figure\u00a04[href=https://www.wicell.org#fig4]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2683-Fig4.jpg\nFigure\u00a04. Example of cell count and viability determined by AO/PI staining on a fluorescent cell counter\n(A) Bright field image. Note that the \u201cstring\u201d-shaped cells are neoplastic Schwann cells isolated from a NF1-associated nerve sheath tumor.\n(B) Fluorescent image showing live nucleated cells stained by Acridine Orange (green) and dead nucleated cells stained with Propidium Iodide (orange). Scale bar\u00a0= 200\u00a0\u03bcm.\nAdjust the cell concentration to be in the range of 700\u20131200 live cells per microliter in DMEM before proceeding to the single-cell sequencing.\nProcess the cells immediately for single-cell capture.Critical: The success of the downstream single-cell RNA sequencing depends on good preparation of single-cell suspension with the viability above 70% and devoid of cell debris and dead cells.\nOptional: If a gentleMACS\u2122 dissociator is not available in the lab, use a 10\u00a0mL serological pipette to mix and agitate the minced tumor in Tumor Dissociation Media.\nOptional: If significant amount of red blood cells exists in the sample, which will appear pink in the cell pellet, add 10\u00a0mL ACK lysing buffer to the resuspended cells after step 4m and incubate the sample at room temperature for 5\u00a0minutes, followed by PBS wash twice.\nOptional: If the cell viability is less than 60%, an optional dead cell removal step maybe added after step 4p. Commercially available dead cell removal kit (Miltenyi Biotec) has been used and proved to be successful in enrichment of live cells. Be cautious that up to 50% of total cell loss should be expected using the dead cell removal kit.\nOptional: It is possible to increase the content of Schwann cells in the dissociated single-cell suspension without affecting the downstream single-cell sequencing. We have previously identified that CD56 (encoded by NCAM1) is a unique cell surface marker expressed by mouse14[href=https://www.wicell.org#bib5] and human1[href=https://www.wicell.org#bib1] Schwann cells. Using a CD56 antibody-based selection kit (STEMCELL), it is possible to enrich for Schwann cells isolated from the tumor for downstream single-cell sequencing.\nSingle-cell RNA sequencing using the 10x platform\nTiming: 1\u20132\u00a0weeks\nThis step describes the procedure for 3\u2032 gene expression profiling of single cells including single-cell capture using 10x chromium platform, library preparation and sequencing. The recommended user guide \u201cCG000315_ChromiumNextGEMSingleCell3-_GeneExpression_v3.1_DualIndex__RevE.pdf\u201d is followed.\nSingle-cell capture.Count the cells again using fluorescent automated cell counter to determine viability and concentration before loading the cells on to a NextGEM chip for single-cell capture.\nIf necessary, adjust the cell concentration to be in the range of 700\u20131200 live cells/\u03bcL by diluting or concentrating the cells from previous step.\nCount the cells one more time to check the cell concentration and viability just before loading.\nLoad the cells with the target recovery of 6000 cells per capture lane.\nNote: For most tumors processed in this project, two capture lanes per tumor sample are used, targeting 12,000 cells per tumor sample.\nCritical: Make sure not to introduce air bubbles while loading the cell suspension and other reagents into the wells.\nCritical: Before loading the cells for capture, make sure to check the viability and concentration of the samples one more time.\nLibrary preparation.\nAfter the GEM generation, check the consistency and volume of the GEMs.\nNote: If the quality of the GEMs is not as expected, for example (1) low volume of GEMs is observed, (2) GEMs appear less cloudy, or (3) transparent liquid is observed instead of the cloudy GEMs, it indicates either a wetting failure or partial emulsification has occurred.\nCritical: It is necessary to repeat the capture if the quality of the GEMs is not as expected. Before loading the cells for recapture, make sure to completely resuspend and count the cells again.\nCritical: After GEM formation, pay close attention to the volume, opacity, and consistency of the GEM for a clog or possible wetting failures.\nProceed to GEM-RT incubations immediately after GEM generation.\nSafe pause point: store the tubes at 4\u00b0C for up to 72\u00a0h or at \u221220\u00b0C for up to a week.Process the GEMs through GEM recovery, cleanup with Dynabeads, and prepare the cDNA.\nSafe pause point: store the cDNA at 4\u00b0C for up to 72\u00a0h or at \u221220\u00b0C for up to 4\u00a0weeks.\nCheck the quality and concentration of cDNA using one of the following quantification methods: Bioanalyzer, TapeStation, or Labchip.\nNote: The quality and concentration of the cDNA provide some insights about the progress of the assay and helps for troubleshooting later if needed.\nContinue with final library preparation from cDNA that includes fragmentation, end repair and A-tailing, adaptor ligation, size selection using SPRIselect beads and sample index PCR.\nSafe pause point: store the final library at 4\u00b0C for up to 72 h.\nAfter the completion of sample index PCR, perform one additional round of size selection using SPRIselect beads.\nSafe pause point: store the tubes at 4\u00b0C for up to 72\u00a0h or at \u221220\u00b0C for long-term storage.\nCheck the quality and concentration of the final library with a Bioanalyzer, TapeStation, or Labchip.\nCritical: Pay close attention to the size distribution and concentration of the final library and the size of the adaptor dimer peak.\nSequencing.\nThe Final 3\u2032 gene expression dual indexed libraries are standard Illumina paired end constructs.\nNote: Read 1 is 28 base pare (bp) long which encodes 16\u00a0bp 10x cell barcode and 12\u00a0bp unique molecule index (UMI). Read 2 is 90\u00a0bp long which contains the cDNA fragment that is later aligned to the reference genome to indicate the expressed RNA. The sample index sequences, i5 and i7, are 10\u00a0bp long each. They allow the pooling of several 10x cDNA libraries for sequencing.\nCalculate the molarity of the libraries using the concentration and the size of the final library.Prepare 2\u00a0nM library for each cDNA library, pool the 2\u00a0nM libraries from multiple 10x cDNA libraries and re-check the molarity.\nDilute the pooled libraries to 1\u00a0nM as the final loading concentration.\nNote: The final loading concentration is dependent on the type of Illumina sequencer used, and it needs to be optimized for each sequencer.\nSequence the pooled libraries using an Illumina sequencer.\nWhen sequencing multiplexed 10x libraries, perform an initial shallow sequencing run to normalize the libraries so that each library will be sequenced at a similar molarity.\nCritical: The flow cell to use for running the libraries depend on the number of multiplexed libraries and the number of cells targeted. For example, for five multiplexed libraries and each with a target of approximately 6000 cells, the sequencing depth will be: 50,000 reads\u00a0\u00d7\u00a06000 cells\u00a0\u00d7\u00a05 libraries\u00a0= 1.5 billion reads. NovaSeq S2 flow cell will yield approximately 1.3\u20131.6 billion single-end reads.\nPause point: According to the manufacturer\u2019s protocol, safe pause points are available after the fresh single-cell suspension has been captured at steps 6b, 6c, 6e, and 6f.\nCritical: The time to complete this step varies based on the number of samples. The more samples captured and to be sequenced, the longer it will take to complete this step. One to two weeks are sufficient to process a typical 2 lanes of single-cell capture from one tumor sample.\nIntegrative data analysis, marker genes identification, and cell type annotation\nTiming: 24 h\nThis step describes how to perform basic integrative analysis, marker gene identification and cell type annotations of single-cell RNAseq data generated from NF1 nerve sheath tumors.\nUse the standard 10x Genomics cellranger (version 6.0.0) pipeline to extract fastq files and to perform data processing.Demultiplex the raw base call files (BCL) generated by the Illumina sequencer into fastq files using cellranger mkfastq.\nRun cellranger count pipeline for alignment to the reference genome (refdata-gex-GRCh38-2020-A), filtering, barcode and UMI counting.\nApply SoupX7[href=https://www.wicell.org#bib6] to the count matrix of each capture lane to correct the ambient RNA contamination in the data.\nLoad the count matrix generated from cellranger.\nManually specify the contamination fraction to 10%, 20%, 30%, and 40%.\nGenerate corrected count matrix to be used in downstream analysis.\nVisually examine the expression of known markers of each cell type generated from using each contamination fraction. For example, B cell marker CD79B should be expressed in B cells but not the other cell types.\nOnce a desired contamination fraction is determined, use this contamination fraction to generate the corrected count matrix to be used in the next step.\nApply Scrublet8[href=https://www.wicell.org#bib7] to the SoupX-corrected read count matrix for the prediction of doublets in the data.\nBased on the number of cells captured per capture lane, a predicted doublet rate is calculated as the following:\nP\nr\ne\nd\ni\nc\nt\ne\nd\nd\no\nu\nb\nl\ne\nt\nr\na\nt\ne\n(\n%\n)\n=\nn\nu\nm\nb\ne\nr\no\nf\nc\ne\nl\nl\ns\nc\na\np\nt\nu\nr\ne\nd\n10000\n\u00d7\n0.8\nUse the calculated predicted doublet rate on a per capture lane basis in function scr.Scrublet in Scrublet. This step will define the expected_doublet_rate.\nRemove marked doublets before advancing to the next step in this data analysis pipeline.\nUse Seurat15[href=https://www.wicell.org#bib3] for the integrative analysis of data from multiple captures to correct for potential batch effect.\nRemove cells with low quality (nFeature_RNA\u00a0<\u00a0500, nFeature_RNA\u00a0>\u00a06000, or percent of mitochondrial gene fraction\u00a0>\u00a020%).Normalize corrected gene counts using the \u201cLogNormalize\u201d method with the scale.factor set to 10000.\nCalculate the top 3000 most variable genes using the method \u201cvst\u201d.\nAssign the cell cycle identity of each cell using the function CellCycleScoring.\nIdentify anchors using the FindIntegrationAnchors function with the setting \u201cdims\u00a0= 1:40\u201d.\nApply anchor-based reciprocal principal component analysis (rPCA)16[href=https://www.wicell.org#bib4] to integrate data from different capture lanes.\nScale the data with the unwanted source of variations regressed out (\"percent.mt\", \"nCount_RNA\", \"nFeature_RNA\", \"S.Score\", \"G2M.Score\").\nUtilize the standard dimensional reduction for the clustering analysis using the RunPCA (npcs\u00a0= 40), RunUMAP (reduction\u00a0= \u201cpca\u201d, dims\u00a0= 1:40), FindNeighbors (reduction\u00a0= \u201cpca\u201d, dims\u00a0= 1:40), and FindClusters (resolution\u00a0= 0.8) functions in Seurat.\nIdentify cluster biomarkers by finding the differentially expressed genes in each cluster with comparison to all other clusters, using the FindAllMarkers function in Seurat with the setting \u201conly.pos\u00a0= TRUE, min.pct\u00a0= 0.25, logfc.threshold\u00a0= 0.25\u201d to derive the most highly expressed marker genes of each cluster.\nManually annotated cell types based on the identified marker genes in each cluster.\nOptional: Several other R-based or Python-based tools are available for integrative analysis of single-cell RNA sequencing data. Users may choose any suitable bioinformatic tools for these steps.\nOptional: Instead of manually specifying a contamination fraction, the function autoEstCont can be used in SoupX to automatically identify a desired contamination fraction (rho).\nBuilding a normal Schwann cell developmental trajectory\nTiming: 2\u00a0weeks\nThis step describes how to build a normal Schwann cell developmental transcriptional trajectory using publicly available single-cell RNAseq data. All source code used in this step can be found on: https://github.com/vishakagopalan/nf_hackathon[href=https://github.com/vishakagopalan/nf_hackathon].\nData download and processing.\nIdentify publicly available single-cell RNAseq data obtained from mouse normal developing Schwann cells. Download the read count matrices of the following data from GEO database.Obtain data of neural crest stem cells at embryonic day 9.5 (E9.5) from GSE129114.\nObtain data of Schwann cell precursors at E12.5 and E13.5 from GSE150150.\nObtain data of mature myelinating and non-myelinating Schwann cells from adult mice from GSE142541.\nOptional: Emerging single-cell RNAseq datasets of neural crest stem cells, Schwann cell precursors, as well as other developing and differentiated Schwann cell subtypes are available to be integrated. These datasets can also be used to build the normal Schwann cell developmental trajectory.\nUse published cell-type annotations for GSE129114 (Table\u00a0S9 in source publication) and GSE142451.\nFor GSE150150, perform batch correction and clustering using the default parameters in Seurat, and identify annotated cell types based on marker genes employed in the source publication.\nMerge all datasets together in Seurat.\nUse the library size (sequencing depth in each cell), cell cycle status, and batch as co-variates to estimate the p-value of differential expression for each gene.\nTrajectory analysis in Monocle3.\nDown-sampling of reads from datasets that contain a much larger library sizes is required.\nNote: Among the three datasets that we use in this study, GSE129114 and GSE150150 are downsampled to match the library size distribution of cells in GSE142541.\nUse Monocle3 to construct a trajectory from the merged data.\nMerge the three datasets after the down-sampling.\nPerform batch correction using the internal function of Monocle3.\nUse E9.5 neural tube cells as the root state of the trajectory (these cells will be assigned a pseudotime value of 0) to compute pseudotime values of the remaining cells.\nRegulon analysis in SCENIC.\nRegulons are defined as a target gene set for each transcription factor, active in each developing cell type, based on the presence of binding sites within 10 kilobases of the transcription starting site of expressed genes.Retain transcription factors whose mean regulon activity is higher than 0.03 in at least one of the cell types for future analysis.\nDetermine markers of developing Schwann cells using the FindAllMarkers function in Seurat.\nCompute cell cycle status (G2M, S, or G1 phase scores) using AddModuleScore.\nNote: The cell cycle status of each cell is considered as co-variates for determining the p-values of the log-fold changes of each differentially expressed gene.\nComparative analysis between neoplastic Schwann cell and malignant peripheral nerve sheath tumor cell\nTiming: 6 h\nThis step describes how to transcriptionally project the neoplastic Schwann cells onto the normal Schwann cell developmental trajectory. All source code used in this step can be found on: https://github.com/vishakagopalan/nf_hackathon[href=https://github.com/vishakagopalan/nf_hackathon].\nCompute at most 300 signature genes of each cell type.\nNote: Signature genes of the developing Schwann cells are markers of each cell type from step 16 that have an adjusted p-value of less than 0.1.\nComparison between Schwann cells and MPNST malignant cells.\nDerive marker genes of neoplastic Schwann cells and MPNST malignant cells from the integrative analysis of patient nerve sheath tumors in step 12. Specifically, using the function FindMarkers with the setting (only.pos\u00a0= TRUE, min.pct\u00a0= 0.25, logfc.threshold\u00a0= 0.25), derive the most highly expressed marker genes in both malignant and Schwann cells.\nAs part of the function of SCENIC package, score the activity of each of the signature gene sets (signature genes derived from step 15) in each cell using the AUCell function.\nCompare the AUCell activities of each developmental Schwann cell types between malignant MPNST cells and Schwann cells from the patient MPNST single-cell dataset using two-sided Wilcoxon test.", "Step-by-step method details\nStep-by-step method details\nContinuum score assignment (RNA-sequencing)\nTiming: \u223c5\u201315\u00a0min (Timing can vary\n      depending on the number of samples in your dataset)\n    \n      Here, we project our existing NMF model onto a previously unseen group\n      3/group 4 medulloblastoma RNA-sequencing dataset to derive a continuum\n      score for each sample. Several R packages, custom functions and data\n      objects are required. Data objects and functions can be found in the\n      associated GitHub repository (https://github.com/hackingjpr/Group3-4App[href=https://github.com/hackingjpr/Group3-4App]). We also provide sample usage data along with expected outputs in the\n      GitHub. All in-silico analyses detailed in this\n      section are performed in R and require the following lines of code to be\n      run to set up the package environment and load the necessary data objects\n      prior to continuum score assignment:\n    \nWithin terminal.\nClone GitHub repository.\n> git clone\nhttps://github.com/hackingjpr/Group3-4App.git[href=https://github.com/hackingjpr/Group3-4App.git]\n/your/directory/\n# This will download the git repository into a directory of your\n            choosing\nWithin RStudio.\nInstall/load required R packages and their dependencies.\n> install.packages(\"NMF\", dependencies\u00a0= TRUE)\n> install.packages(\"MASS\", dependencies\u00a0= TRUE)\n> BiocManager::install(\"biomaRt\")\n# For specific package versions, see\nkey resources table[href=https://www.wicell.org#key-resources-table]\nsection. When confronted with\nyes/no questions, answer yes to install dependency packages.\n> library(NMF)\n> library(MASS)\n# This loads the packages required into your working\n            environment.\nLoad required data objects.\nCritical: You MUST update\n      \u2018/your/directory/\u2019 to the location which you cloned the GitHub repository\n      in step 1.\n    \n> nmf.res <- readRDS(file\u00a0= \"/your/directory/Group3-\n4App/StarProtocols_Guide/data/nmf.res.rds\")\n# This loads in the precalculated NMF model.\nLoad the required custom functions.\n> source(file\u00a0= \"/your/directory/Group3-\n4App/StarProtocols_Guide/R/Project_NMF.R\")\n#Wrapper function used to project NMF model onto unseen\n            group3/group4 sample data. A function breakdown is provided below\n            (see\nFigure\u00a01[href=https://www.wicell.org#fig1])\n        Load sample data as a matrix object.\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2914-Fig1.jpg\n              Figure\u00a01. Step-by-step breakdown of project.NMF() function\n            \n> tpms.mat <- read.delim(\"/your/directory/Group3-\n4App/StarProtocols_Guide/data/tpms.mat.txt\")\nCritical: If you wish to use your ownRNA-sequencing sample data, you must ensure that it follows the same\n      format as tpms.mat. This object is a matrix, where columns correspond to\n      samples and rows correspond to genes, with expression counts presented in\n      the transcripts per million (TPM) format or equivalent. All genes (rows)\n      must use HUGO gene nomenclature i.e., gene symbols. If your input dataset\n      is not annotated correctly, please see problem 1[href=https://www.wicell.org#sec5.1] in\n      the troubleshooting[href=https://www.wicell.org#troubleshooting] section. Note that a\n      column-rank normalization procedure is employed, this coupled with the NMF\n      projection and other normalization procedures renders the results somewhat\n      resistant to noise and compatible with representations of expression other\n      than TPM. We have for example used Rlog, or variance stabilized transforms\n      from DESeq or even other platforms such as Affymetrix microarray or\n      nanostring data with success.\n    \nNote: When projecting onto platforms other\n      than bulk RNA-seq, appropriate filtering strategies to remove invariant\n      genes/probes may be necessary.\n    \nProject NMF model onto sequencing data\n> tpms.H <- project.NMF(input.array\u00a0=\n          as.matrix(tpms.mat),\nnmf.result\u00a0= nmf.res)\n#Apply project.NMF function to input dataset. Function breakdown is\n            provided below (see\nFigure\u00a01[href=https://www.wicell.org#fig1])\n        Extract Group 3 and Group 4 metagenes from data and transpose matrix.\n      \n> g3g4.tpms <- t(tpms.H[c(3,1),])\n#Rows 3 and 1 in tpms.H correspond to the metagenes for Groups 4\n            and 3 respectively\nApply logistic transformation to metagenes.\n> logistic.g3g4.tpms <- apply(g3g4.tpms,2,function(x){(1 / (1\u00a0+\n          exp(-x)))})\n# apply a logistic transformation\n> logistic.g3g4.tpms.score\n          <-apply(logistic.g3g4.tpms,1,function(x){x[2]/(x[1]+x[2])})\n# calculate a ratio between logistically transformed Group3 and\n            Group4 metagene\nScale values between 0 and 1.\n> scaling.function <- function(x){(x-min(x)) /\n          (max(x)-min(x))}\n# create a function to scale values between 0 and 1\n> logistic.g3g4.tpms.continuum.score <-\n          scaling.function(logistic.g3g4.tpms.score)\n# apply the function to the unscaled g3g4 scores\nCritical: If you are using a small\n      dataset or one that does not represent the full spectrum of Group 3/Group4 medulloblastomas you may want to omit this step and present unscaled\n      G3/G4 ratios (see Limitations below).\n    \nAlternatives: You may wish to\n      append to the precalculated G3/G4 ratios from Williamson et\u00a0al. and then\n      scale together with your new samples in which case the following\n      alternative command should be used:\n    \n> scaling.function1\u00a0<- function(x){(x - 0.3953062) /\n          (0.5964371\n- 0.3953062)}\n# create a function to scale values between 0 and 1 using\n            Williamson et\u00a0al. data)\n> logistic.g3g4.tpms.continuum.score <-\nscaling.function1(logistic.g3g4.tpms.score)\n# Apply scaling\nNote: Theoretically, this could lead to\n      some sample returning values under 0 or over 1. The user would need to\n      take a considered view on such samples. They could simply be producing\n      values close to 1 or 0 but otherwise consistent with samples at the\n      extreme limits of the G3/G4 continuum, in which case manually assigning\n      them the maximum 1 or minimum 0 value may be a valid approach. Should they\n      massively exceed previous limits they may simply be outliers or technical\n      artefacts that would be best noted but excluded from further analysis.\n    \nPresent output as data.frame for export.\n> logistic.g3g4.tpms.continuum.score <-\nas.data.frame(logistic.g3g4.tpms.continuum.score)\n> colnames(logistic.g3g4.tpms.continuum.score) <-\n          'Continuum Score'\n# Renaming for easier interpretation\n> write.csv(logistic.g3g4.tpms.continuum.score, file\u00a0=\n'/your/directory/my_continuum_scores.csv ', row.names\u00a0=\n          TRUE)\n#Export as .csv table\nContinuum score assignment (DNA methylation microarray)\nTiming: \u223c5\u201315\u00a0min (Timing can vary\n      depending on the number of samples in your dataset)\n    \n      Here, we project our existing NMF model onto a previously unseen group\n      3/group 4 medulloblastoma DNA methylation dataset to derive a continuum\n      score for each sample. Several R packages, custom functions and data\n      objects are required, each of which can be found in the associated GitHub\n      repository (https://github.com/hackingjpr/Group3-4App[href=https://github.com/hackingjpr/Group3-4App]). We also provide example usage data along with expected outputs in the\n      GitHub. All in-silico analyses detailed in thissection are performed in R, and require the following code to be run to\n      set up the package environment and load the necessary data objects prior\n      to continuum score assignment:\n    \nInstall/Load required packages and their dependencies\n> install.packages('mlbench', dependencies\u00a0= TRUE)\n> install.packages('caret', dependencies\u00a0= TRUE)\n> install.packages('randomForest', dependencies\u00a0=\n          TRUE)\n# For specific package versions, see\nkey resources table[href=https://www.wicell.org#key-resources-table]\nsection.\n> library(mlbench)\n> library(caret)\n> library(randomForest)\n# This loads each package into your working environment\nCritical: You MUST update\n      \u2018/your/directory/\u2019 to the location which you cloned the GitHub repository\n      in step 1 of Continuum score assignment (RNA-Sequencing).\n    \nLoad in the prediction object\n> load(file\u00a0= \"/your/directory/Group3-\n4App/StarProtocols_Guide/data/g3.g4.cont.rfe.Rdata\")\n# This loads in the precalculated random forest model\nLoad in example methylation dataset.\n> mvals.mat <- read.delim(\"/your/directory/Group3-\n4App/StarProtocols_Guide/data/mvals.mat.txt\")\nCritical: The random forest model in\n      this protocol requires that test data be provided as a matrix of M-values\n      (logit-transformed beta values), where columns correspond to sample ID and\n      rows correspond to probes. If your data is a matrix of beta values (object\n      below named as \u201cyour.betas\u201d), you can easily convert these to M-values\n      using the following:\n    \n> mvals.mat <- log2(your.betas/(1-your.betas))\n#logit-transformation\nSubset M-Value matrix to probes used as predictors in model\n> mvals.mat <-\n          as.matrix(mvals.mat[predictors(g3.g4.cont.rfe),])\n#Removes probes that are not used for prediction\nAlternatives: If you are working\n      with data obtained using other methylation platforms such as bisulfite\n      sequencing (BS-Seq), it is still possible to derive a continuum score. A\n      typical BS-Seq methylation data output will contain Chromosome, Start and\n      End columns which can be used to \u2018lift\u2019 over CpGs that map to those\n      covered by the Illumina Methylation Manifest. This will create a matrix of\n      BS-Seq derived beta values that are annotated with associated Illumina\n      probe IDs that can be used to subset to the required predictors in therandom forest model. Whilst BS-Seq methylation values are known to be\n      correlated to those obtained from the methylation array platform and in\n      theory should produce similar predicted continuum scores, it should be\n      noted that the random forest model in this protocol was not explicitly\n      trained using sample data from this platform. An example script that can\n      be used to achieve this is provided in a separate GitHub related to\n      processing whole genome bisulfite sequencing data at\n      https://github.com/tezla93/WGBS_MB/blob/main/InterPlatform/Liftover.R[href=https://github.com/tezla93/WGBS_MB/blob/main/InterPlatform/Liftover.R]\n        Apply test set to model and get predicted continuum scores using\n        predict()\n      \n> pred.cont.rand.for <-\n          as.data.frame(predict(g3.g4.cont.rfe,\nt(mvals.mat)))\n> write.csv(pred.cont.rand.for, file\u00a0=\n'/your/directory/my_continuum_scores_Methylation.csv',\n          row.names\u00a0= TRUE)\n#Export as .csv\nInteractive shiny app\nTiming: \u223c5\u201310\u00a0min (Timing can vary\n      depending on number of samples in your dataset)\n    \n      An interactive Shiny app has been created in which user\u2019s datasets can be\n      uploaded, continuum scores obtained, and downloaded, in either CSV or PDF\n      format. The relevant scripts and files can be downloaded from the GitHub\n      repository created to accompany this publication, this is located at:\n      https://github.com/hackingjpr/Group3-4App.git[href=https://github.com/hackingjpr/Group3-4App.git], an in-depth tutorial can be found in the GitHub Readme and on the app\n      itself, there is also a tutorial video (Methods video S1[href=https://www.wicell.org#mmc1]). If you wish to run the app on your own system, you may clone the\n      GitHub repository by directly downloading from the repository (see\n      \u2018Continuum Score Assignment (RNA-Seq)\u2019 section above.)\n    \n        Your browser does not support HTML5 video.\n      \n          Methods video S1. A video tutorial covering steps 1\u20134 of the Shiny App\n        \n      You may run the app on your machine using RStudio. Upon opening the\n      \u201capp.R\u2033 file you will either be able to run the app or will need to run\n      the whole block of code so that RStudio can recognize that it encodes aShiny app. To run a block of code select it all with CTRL+ A and then run\n      it by pressing CTRL+ENTER. Further installation information is included in\n      the Group3-4App (https://github.com/hackingjpr/Group3-4App.git[href=https://github.com/hackingjpr/Group3-4App.git]) repository\u2019s ReadMe file (README.md).\n    \n        Select Expression or Methylation and Upload Data: Select the appropriate\n        button on the left-hand side of app and then press \u201cBrowse...\u201d\n      \nNote: For Methylation idat files, we\n      recommend uploading no more than 10 files at a time; this will speed up\n      the score generation and keep the PC\u2019s memory use low. There is no limit\n      for expression samples, although with too many the resulting plots can\n      begin to look crowded.\n    \nCritical: If using your own\n      RNA-sequencing sample data, ensure that it follows the same format as\n      tpms.mat. tpms.mat is a matrix, where columns correspond to samples and\n      rows to genes. Expression counts need to be presented in the transcripts\n      per million (TPM) format or equivalent. All genes (rows) must use HUGO\n      gene nomenclature, i.e., gene symbols. If your input dataset differs to\n      this, please see problem 1[href=https://www.wicell.org#sec5.1] in the\n      troubleshooting[href=https://www.wicell.org#troubleshooting] section.\n    \nGenerate Group 3\u20134 Scores by clicking on the \u201cGenerate Group 3/4\n        Score\u201d button.\n      \nNote: The app will take you to the results\n      tab once the scores are calculated. This tab will contain a table\n      displaying your samples and their group 3/4 score and three different\n      plots. These plots include placing your samples along the group 3/4\n      continuum and two survival curves. The first being a general survival\n      curve based on the patient\u2019s score and the second showing the effect that\n      age can have on patient\u2019s survival.\n    \nExport Data: Navigate to the Download tab, which is above your\n        results. You will export the results in either CSV or PDF format. CSV\n        will be the raw table and PDF will contain the table as well as anyplots. Figure\u00a02[href=https://www.wicell.org#fig2] shows an example of the plots\n        generated in the PDF file.\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2914-Fig2.jpg\n              Figure\u00a02. An example of the plots generated and exported to PDF\n            \n              (A) Cumulative frequency plot showing where your samples lie upon\n              the continuum.\n            \n(B) Survival curve with no risk factors accounted for.\n(C) Survival curve taking into account patient\u2019s age.\n(D) Table\u00a0of raw group 3\u20134 scores.\nReset: If more samples are to be tested then press the reset\n        button, this takes you back to the start and allows you to upload more\n        samples.", "Step-by-step method details\nStep-by-step method details\nSECM lysis\nTiming: 2\u20133 h\nIn this step, cfDNAs of SECM are released after degradation of proteins and other components.\nMeasure the volume of each SECM sample with a micropipette and add nuclease-free water to reach a total volume of 16.6\u00a0\u03bcL. If the original volume of the sample is more than 16.6\u00a0\u03bcL, use only 16.6\u00a0\u03bcL.\nNote: As the volumes of the collected SECM samples are generally more than 5\u00a0\u03bcL with variation, we have adapted the lysis step of the single-cell PBAT protocol for SECM by increasing the total volume of the lysis reaction from 5\u00a0\u03bcL to 20\u00a0\u03bcL.\nCritical: The volume of the SECM sample should be accurately measured.\nAdd the lysis buffer (see materials and equipment[href=https://www.wicell.org#materials-and-equipment]) to each SECM sample from step 1 to make a final lysis reaction volume of 20\u00a0\u03bcL.\nMix well by vortexing and spinning down for a few seconds.\nImmediately place the tube in a heating block, incubating for 1\u00a0h at 50\u00b0C and then 30\u00a0min at 75\u00b0C.\nCentrifuge at 9,000\u00a0g for 1\u00a0min and immediately place the tube on ice.\nPause point: Lysed SECM samples can be stored at \u221280\u00b0C for up to six months.\nBisulfite conversion and purification\nTiming: 5\u20137 h\nIn this step, unmethylated Cs are changed to Us while methylated Cs (most in the CpG context) are preserved, and thus the information on DNA methylation is provided at single base resolution.\nDissolve the CT Conversion Reagent by shaking it in a Thermomixer at 37\u00b0C until completely clear.\nCritical: The CT Conversion Reagent should be stored away from light and fully dissolved when used. The temperature can be increased to 42\u00b0C to ensure no precipitation.Add 130\u00a0\u03bcL prepared CT conversion reagent to each 20\u00a0\u03bcL SECM lysate. Mix by vortexing. Incubate at 98\u00b0C for 8\u00a0min, 64\u00b0C for 3.5\u00a0h for bisulfite conversion of the SECM cfDNA, and then hold at 4\u00b0C.\nPause point: The bisulfite-converted SECM cfDNA can be stored at \u221220\u00b0C for up to 3\u00a0days or 4\u00b0C for 1\u00a0day.\nTroubleshooting[href=https://www.wicell.org#troubleshooting].\nPurify the bisulfite-converted SECM cfDNA using the Zymo EZ-96 DNA Methylation-Direct\u2122 MagPrep kit.\nAdd 10\u00a0\u03bcL of MagBinding Beads and 600\u00a0\u03bcL of M-Binding Buffer to 1.5\u00a0mL DNA LoBind Tubes.\nAdd the 150\u00a0\u03bcL bisulfite-converted SECM cfDNA from step 5 to the MagBinding Beads and M-Binding Buffer, vortexing fully for 30 s.\nIncubate the mixture at 20\u00b0C\u201325\u00b0C for 5\u00a0min to bind the DNA to the beads.\nPlace the tubes on a magnetic stand until the solution clears, and then remove and discard the supernatant by pipetting.\nRemove the tubes from the magnetic stand and add 400\u00a0\u03bcL M-Wash Buffer. Mix the beads by vortexing.\nPlace the tube on the magnetic stand until the solution clears, and then remove the supernatant by pipetting and discard the supernatant.\nRemove the tubes from the magnetic stand and add 200\u00a0\u03bcL of M-Desulfonation Buffer, and mix thoroughly by vortexing for 30 s.\nLet tubes stand at 20\u00b0C\u201325\u00b0C for 15\u00a0min. Then place the tubes on the magnetic stand until the solution clears, and then discard the supernatant by pipetting.\nRemove the tubes from the magnetic stand and add 400\u00a0\u03bcL of M-Wash Buffer to the beads.\nPlace the tubes on the magnetic stand until the solution clears, and then discard the supernatant by pipetting. Repeat this step once.\nTransfer the tubes to the heating block and dry the beads for 20\u00a0min at 55\u00b0C.Resuspend the beads with 21.5\u00a0\u03bcL of M-Elution Buffer, and heat the elution at 55\u00b0C for 2\u00a0min to fully elute the DNA.\nPlace the tubes on the magnetic stand until the solution clears, and then remove and transfer 19.5\u00a0\u03bcL supernatant to a clean PCR tube.\nNote: As the MagBinding Beads precipitate very easily, keep the solution well mixed throughout the process. One can use the preamplification priming mixture for dissolving the DNA, as described by Clark et\u00a0al.18[href=https://www.wicell.org#bib18]\nPause point: Purified bisulfite-converted DNA can be stored at \u221220\u00b0C for no more than two weeks.\nPreamplification by random priming\nTiming: 4 h\nIn this step, four rounds of DNA stands (the first strands) complementary to the bisulfite-converted DNA are synthesized using Klenow DNA polymerase and random primers with the adapter I being tagged.\nPrepare the preamplification priming mixture as follows:\ntable:files/protocols_protocol_2606_3.csv\nMix well by gently pipetting up and down. Incubate the mixture on a heating block at 65\u00b0C for 3\u00a0min. And then immediately transfer the PCR tubes on ice.\nNote: After adding 1\u00a0\u03bcL of Klenow DNA polymerase in step 9, the total volume will reach 25\u00a0\u03bcL.\nCritical: The tubes need to be sufficiently cooled to avoid reduction of the activity of the Klenow DNA polymerase.\nAdd 1\u00a0\u03bcL of Klenow DNA polymerase (3\u2032-5\u2032 exo-, 50\u00a0U/\u03bcL, from TIANGEN or New England Biolabs) to the mixture. Mix well by gently pipetting up and down 15 times. Centrifuge at 300\u00a0g for 10\u00a0s at 20\u00b0C\u201325\u00b0C and then place the tubes on a pre-cooled (4\u00b0C) thermocycler.\nIncubate the tubes in the thermocycler for one round of preamplification as follows:\ntable:files/protocols_protocol_2606_4.csv\nHeat the mixture to 95\u00b0C for 1\u00a0min in a thermocycler and then immediately cool it at 4\u00b0C.Critical: The tubes need to be sufficiently cooled to avoid reduction of the activity of the Klenow DNA polymerase.\nAdd the following freshly prepared solution for the next round of preamplification:\ntable:files/protocols_protocol_2606_5.csv\nNote: The dNTPs mixture of 2.5\u00a0nM is diluted from the dNTPs mixture of 10\u00a0mM, and the P5-N9-oligo1 of 25 p\u039c is diluted from the P5-N9-oligo1 of 10\u00a0\u03bc\u039c, both with 1\u00d7 Blue Buffer.\nMix well by gently pipetting up and down. Incubate the tubes in the thermocycler as follows:\ntable:files/protocols_protocol_2606_6.csv\nRepeat steps 11\u201313 for additional two times, making four rounds of preamplification in total.\nAfter the PCR cycles, centrifuge the tubes at 500\u00a0g for 1\u00a0min. Next, purify the First-strand random priming product using 0.8\u00d7 AMPure XP beads.\nAdd 26\u00a0\u03bcL of AMPure XP Beads to the PCR Tube.\nGently shake the PCR tube until a homogeneous solution is seen.\nIncubate the PCR tubes for 10\u00a0min at 20\u00b0C\u201325\u00b0C for maximum recovery.\nPlace PCR tubes on the magnetic stand at 20\u00b0C\u201325\u00b0C for 5\u00a0min until the solution becomes clear.\nRemove and discard the supernatant carefully.\nAdd 200\u00a0mL of 70% ethanol to each tube and pipet up and down 10 times gently, then remove the supernatant.\nRepeat step f.\nRemove the tube from the magnetic stand and place it at 20\u00b0C\u201325\u00b0C until the ethanol is evaporated.\nNote: In this step, XP beads become light brown, which means they are in a dry state.\nAdd 21.5\u00a0\u03bcL nuclease-free water, and mix by vortexing for 30 s.\nIncubate at 20\u00b0C\u201325\u00b0C for 2\u00a0min and then place the PCR tube again in the magnetic stand until the solution becomes clear.\nTransfer 19.5\u00a0\u03bcL of the supernatant to the next step.\nNote: XP beads need to be placed at 20\u00b0C\u201325\u00b0C for 30\u00a0min before use.\nTagging adapter IITiming: 3 h\nIn this step, the second DNA strand complementary to the first DNA strand is synthesized using Klenow DNA polymerase and random primers with the adapter II being tagged.\nPrepare the adapter II tagging mixture on ice as follows:\ntable:files/protocols_protocol_2606_7.csv\nNote: After adding 1\u00a0\u03bcL of Klenow DNA polymerase in step 18, the total volume will reach 25\u00a0\u03bcL.\nHeat the mixture at 95\u00b0C for 3\u00a0min in a PCR thermocycler, then immediately place the PCR tubes on ice.\nAdd 1\u00a0\u03bcL of Klenow DNA polymerase (3\u2032\u20135\u2032 exo-, 50\u00a0U/\u03bcL) to the mixture. Mix well by gently pipetting and then centrifuged at 300\u00a0g for 10 s.\nIncubate the tubes in the thermocycler for tagging adapter II as follows:\ntable:files/protocols_protocol_2606_8.csv\nUse 0.8\u00d7 AMPure XP beads to purify the second-strand synthesis product referring to step 15, adding 13.5\u00a0\u03bcL nuclease-free water for elution and transferring 11.5\u00a0\u03bcL for the next step.\nPCR amplification\nTiming: 2 h\nIn this step, the final library product is PCR amplified using primers against adapter I and II.\nPrepare the PCR amplification mixture on ice as follows:\ntable:files/protocols_protocol_2606_9.csv\nMix thoroughly by vortexing 30 s, centrifuge the tubes at 300\u00a0g for 10 s, and perform PCR amplification using the following program:\ntable:files/protocols_protocol_2606_10.csv\nCentrifuge the tubes at 300\u00a0g for 1\u00a0min. Purify the PCR products twice using 0.8\u00d7 AMPure XP beads referring to step 15, add 27\u00a0\u03bcL nuclease-free water for elution and transfer 25\u00a0\u03bcL eluate to a new tube each time.\nPause point: The purified PCR products can be stored at \u221220\u00b0C for one month.\nLibrary quality control\nTiming: 2 h\nIn this step, the library is examined for concentration and size distribution as quality control.Quantify the concentration of the PCR products from step 23 using a Qubit dsDNA HS assay kit.\nTroubleshooting[href=https://www.wicell.org#troubleshooting].\nAssess the sizes of the PCR products from step 23 using a fragment analyzer equipment.\nDilute the PCR products to a concentration of 1\u20132\u00a0ng/\u03bcL.\nAdd 2\u00a0\u03bcL of diluted PCR products to 22\u00a0\u03bcL of Diluent Marker, and then mix vigorously and centrifuge at 500\u00a0g for 1\u00a0min.\nRun the diluted PCR products on the fragment analyzer.\nNote: Run the samples immediately after preparation. If the samples have not been used right away, cover and keep them at 4\u00b0C for up to 3\u00a0days, and warm them to 20\u00b0C\u201325\u00b0C and centrifuge at 300\u00a0g for 30\u00a0s before running.\nTroubleshooting[href=https://www.wicell.org#troubleshooting].\nSequencing\nTiming: 1\u20138\u00a0days\nIn this step, the library is processed for high-throughput sequencing.\nIn this step, pooled libraries with compatible indexes are sequenced on Illumina platforms.\nCarry out 150\u00a0bp paired-end sequencing on NovaSeq 6000 (Illumina) or other platforms according to the manufacturer\u2019s protocol.\nData processing\nTiming: 6\u20138 h\nIn this step, the data are processed for mapping to the reference genome and the information on DNA methylation levels at individual cytosine site is obtained.\nInstalling system requirements of the scBS-map toolkit.\nNote: ScBS-map toolkit can be downloaded from https://github.com/wupengomics/scBS-map[href=https://github.com/wupengomics/scBS-map]. Click in the green \u2018Code\u2019 tab for downloading the zip folder, which contains a workflow figure, perl scripts including qcreads.pl, align-end2end.pl, align-local.pl, qcbam.pl, mergebam.pl, and a ReadMe file. Install system requirements including SAMtools, bowtie2 and BS-Seeker2 according to the ReadMe file.\nLaunching scBS-map toolkit to process raw data.\nRemove sequencing adapters, amplification primers, and low-quality bases from the raw bisulfite sequencing reads using qcreads.pl in the scBS-map toolkit. On the PowerShell Prompt Console, type command as follows:\n> perl qcreads.pl -f Sample.R1.fastq.gz -l 10 -o Sample_R1_val_1.fq.gz> perl qcreads.pl -f Sample.R2.fastq.gz -l 10 -o Sample_R2_val_2.fq.gz\nDiscard R2 reads that have more than 3 unmethylated CHs (H\u00a0= A, T or C), and the corresponding R1 reads are also discarded. Type command as follows:\n> perl ch3deleate.pl Sample_R2_val_2.fq.gz Sample_ R2_clean.fq.gz\n> zcat Sample_ R2_clean.fq.gz | grep '\u02c6@' | awk '{print $1}' >> R2_head.txt\n> perl extract_R1.pl R2_head.txt Sample_R1_val_1.fq.gz Sample_ R1_clean.fq.gz\nNote: The scripts ch3deleate.pl and extract_R1.pl can be found at https://github.com/jasminexiao/niPGT[href=https://github.com/jasminexiao/niPGT].\nThe clean reads are first mapped to the human reference genome (hg19) in the end-to-end alignment mode using align-end2end.pl. Type command as follows:\n> perl align-end2end.pl Sample_ R1_clean.fq.gz -g hg19.genome.fa -p 5 -u Sample _R1.unaligned.fq -o Sample_R1.end2end.bam\n> perl align-end2end.pl Sample_ R2_clean.fq.gz -g hg19.genome.fa -p 5 -u Sample _R2.unaligned.fq -o Sample_R2.end2end.bam\nCarry out local alignment to unaligned reads using align-local.pl. Type command as follows:\n> perl align-local.pl -f Sample_R1.unaligned.fq -g hg19.genome.fa -p 5 -o Sample_R1.local.bam\n> perl align-local.pl -f Sample_R2.unaligned.fq -g hg19.genome.fa -p 5 -o Sample_R2.local.bam\nRemove low-confidence local-aligned reads using qcbam.pl. Type command as follows:\n> perl qcbam.pl -f Sample_R1.local.bam -n 10 -p 4 -o Sample_R1.local.hc.bam\n> perl qcbam.pl -f Sample_R2.local.bam -n 10 -p 4 -o Sample_R2.local.hc.bam\nMerge all alignments from end-to-end and local mapping mode using mergebam.pl. Type command as follows:\n> perl mergebam.pl -e Sample_R2.end2end.bam -l Sample_R2.local.hc.bam -p 4 -o Sample_R2.merge.bam\n> samtools merge -f Sample.bam Sample_R1.merge.bam Sample_R2.merge.bam\n> samtools sort Sample.bam Sample.sort\n> samtools index Sample.sort.bam\nRemove PCR duplicates using the Picard tools (https://broadinstitute.github.io/picard/[href=https://broadinstitute.github.io/picard/]). Type command as follows:\n> java -Xmx4g -jar picard.jar MarkDuplicates I=Sample.sort.bam O=Sample.rmdup.bam M=Sample.rmdup.txt CREATE_INDEX=true ASSUME_SORTED=true VALIDATION_STRINGENCY=SILENT REMOVE_DUPLICATES=true 2> $output/$sample/${sample}.picard.log\nsamtools index $output/$sample/${sample}.rmdup.bam\n> bamToBed -i Sample.rmdup.bam\u00a0>\u00a0Sample.bedExtract cytosine methylation from the BAM file using singleC_metLevel.hg19.pl in https://github.com/jasminexiao/niPGT[href=https://github.com/jasminexiao/niPGT]. The DNA methylation level is calculated as the ratio of the number of reads with methylated C to that of total reads (methylated and unmethylated). Type command as follows:\n> samtools view\u00a0\u2013h Sample.rmdup.bam | samtools view -uSb /dev/stdin | samtools mpileup -O -f hg19.genome.fa /dev/stdin\u00a0>\u00a0Sample.pileup\n> perl singleC_metLevel.hg19.pl Sample .pileup\u00a0>\u00a0Sample.single5mC_tmp\n> grep -v \" chrM \" Sample.single5mC_tmp\u00a0>\u00a0Sample.single5mC\nInferring copy number variations (CNVs)\nTiming: 30\u00a0min\nIn this step, chromosome copy number variations are calculated.\nInstalling system requirements of Ginkgo toolkit.\nNote: Ginkgo is a single-cell CNV analysis tool that can be downloaded from https://github.com/robertaboukhalil/ginkgo[href=https://github.com/robertaboukhalil/ginkgo]. Click in the green \u2018Code\u2019 tab for downloading the zip folder, which contains several directories and a ReadMe file. And then install the requirements according to the ReadMe file.\nInferring CNVs.\nEnter the uploads directory in the ginkgo-master folder and create your own analysis directory. Type command as follows:\n> cd uploads\n> mkdir analyse\nIn the analysis catalog, prepare files consisting of config, list, sample.bed or sample.be.gz (e.g., PBAT_test_S1.bed.gz) and reference sample (e.g., XX_NG_ICM_E_4M.median.bed). Type command as follows:\n> cd analyse\n> rz config\n> rz XX_NG_ICM_E_4M.median.bed\n> rz list\n> rz PBAT_test_S1.bed.gz\n> sh ../../scripts/analyze.sh analyse\nNote: If there is no reference sample, the parameter can be modified as segMeth=0 in the config file.\nRun analyze.sh to call CNV. Type command as follows:\n> sh ../../scripts/analyze.sh analyse\nNote: There is an example named PBAT_test_S1.bed.gz in https://github.com/jasminexiao/niPGT[href=https://github.com/jasminexiao/niPGT].\nDeducing cellular components\nTiming: 15\u00a0min\nIn this step, three cellular components of SECM cfDNA are inferred via DNA methylation.Note: The cfDNA from the SECM of the ICSI-generated blastula has three main cellular components: the blastula, the cumulus cell, and the polar body. We have identified 769 cumulus-specific differentially methylated regions (C-DMRs) and 548 oocyte/polar body-specific DMRs (O-DMRs).1[href=https://www.wicell.org#bib1] The average methylation levels of the C-DMRs of the blastula, the cumulus cell and the oocyte/polar body are 4%, 92% and 3%, respectively. The average methylation levels of the O-DMRs of the blastula, the cumulus cell, and the oocyte/polar body are 22%, 19% and 82%, respectively. The mathematical relationship between the methylation levels of each DMRs (DMRi, referring to C-DMRs or O-DMRs) in the SECM cfDNA and the proportional contribution of each component (component k, referring to the blastula, the cumulus cell, and the polar body) can be expressed by the formula MMi\u00a0= \u03a3MCik\u00a0\u00d7\u00a0Pk\u00a0\u00d7\u00a0aik, in which MMi represents the methylation levels of DMRi in the SECM cfDNA, MCik represents the average methylation levels of DMRi in component k, Pk represents the proportional contribution of component k to the SECM cfDNA, and aik represents PCR amplification efficiency of DMRi in component k (Table\u00a01[href=https://www.wicell.org#tbl1]). Therefore, there are three unknown Pk (P for the blastula, the cumulus cell and the polar body) in three formulas (the formulas for C-DMRs and O-DMRs, as well as the formula that the sum of three Pk is equal to 100%). By solving the three-variable linear equation, we can get the values for three Pk. The parameters of MCik and aik are listed in Table\u00a01[href=https://www.wicell.org#tbl1].\ntable:files/protocols_protocol_2606_11.csv\nNote: \u201cMC\u201d: the average methylation level of DMR i in component k. \u201ca\u201d: correction factor for the potential bias of PCR amplification toward the unmethylated allele.19[href=https://www.wicell.org#bib19]\nCalculate the methylation level of C-DMRs for the SECM cfDNA.> cat Sample.single5mC | grep -w 'CpG' | awk 'BEGIN{OFS=\"\u2216t\"}{if($5>='3'){print $1,$2-1,$2,$5,$6}}' | bedtools sort -i\u00a0>\u00a0Sample.CpG_sorted.bed\n> bedtools intersect -a C_DMRs.bed -b Sample.CpG_sorted.bed -wb -wa | awk '{OFS\u00a0= \"\u2216t\"; print $1, $2, $3,$7,$8}' | bedtools groupby -g 1,2,3 -c 4,5 -o sum,sum | awk '{OFS\u00a0= \"\u2216t\";print $1,$2,$3,$4,$5,$6=$5/$4}'\u00a0>\u00a0Sample_Cumulus.single5mC\nCalculate the methylation levels of O-DMRs for the SECM cfDNA.\n> bedtools intersect -a O_DMRs.bed -b Sample.CpG_sorted.bed -wb -wa | awk '{OFS\u00a0= \"\u2216t\"; print $1, $2, $3,$7,$8}' | bedtools groupby -g 1,2,3 -c 4,5 -o sum,sum | awk '{OFS\u00a0= \"\u2216t\";print $1,$2,$3,$4,$5,$6=$5/$4}' >Sample_polarbody.single5mC\nDeducing the proportions of the blastula, the cumulus cell and the polar body within the SECM cfDNA.\n> Rscript calculate_components_ratio.R\nNote: All differentially methylated regions and scripts for calculating component ratios can be found in https://github.com/jasminexiao/niPGT[href=https://github.com/jasminexiao/niPGT].", "Step-by-step method details\nStep-by-step method details\nDefine the druggable target gene set\nTiming: \u223c3 days\nAny user-defined target gene set can be used for this protocol towards more general applications. As a specific example, the definition of target gene set against Coronaviridae viruses is shown in the following steps.\nCollect public datasets to define Coronaviridae-specific HDGs.\nCollect the references performing high throughput genetic perturbation screening for Coronaviridae virus resistance in human cells. In these studies, gene-trap, RNAi and CRISPR techniques are employed to perturb a gene\u2019s function. For example, use the search key word \u201cSARS-CoV-2 AND screen\u201d to collect SARS-CoV-2 virus-related screening references from PubMed (https://pubmed.ncbi.nlm.nih.gov/[href=https://pubmed.ncbi.nlm.nih.gov/]). References for other Coronaviridae viruses such as MERS-CoV and SARS-CoV can be collected similarly. Pinpoint the datasets reporting the viral resistance HDGs associated with these references.\nCollect scattered HDGs for Coronaviridae viruses from individual literatures in which specific genes are shown to be critical or essential for complete viral life cycle (non-screen study).\nFilter the collected data to pinpoint HDGs. If a host gene or its encoding protein is shown only to physically interact with viral proteins or regulated by viral genes but without functional implication on viral life cycle upon gene\u2019s loss-of-function, the gene is not classified as a HDG. A gene is defined as a HDG only when it meets any of the following criteria:\nIts loss-of-function impedes or reduces viral infection or activity by experimental evidence in non-screen studies.\nIt has been clearly classified into HDG group in screen studies.When HDG group is not specified in screen studies, arbitrarily take the top \u223c5% of all the interrogated genes in the positive selection list as HDGs with a custom log-fold change cutoff in CRISPR knockout or RNAi screens. For example, in a typical result output generated by MAGeCK (Li et\u00a0al., 2015[href=https://www.wicell.org#bib3]; Li et\u00a0al., 2014[href=https://www.wicell.org#bib4]) analytic pipeline for CRISPR screens, genes can be ranked according to their negative or positive selection trend by jointly considering the log-fold change and statistical significance of their corresponding guide RNAs. HDGs can be arbitrarily defined as the top \u223c5% of all the genes with a log-fold change of 1.0 (loose cutoff) or 2.0 (stringent cutoff).\nDefine high confidence HDG gene set for Coronaviridae family viruses.\nAs there are several independent studies and datasets for HDG identification against Coronaviridae family viruses, we only take a subset of HDGs that occurs more than once among different datasets as high confidence HDGs for further analysis. A total of 165 high confidence HDGs are defined for Coronaviridae viruses (Figure\u00a01[href=https://www.wicell.org#fig1]A). After that, prepare a HDG file in the structure of \u201cgene symbol\u00a0+ amino acid sequence\u201d (Figure\u00a01[href=https://www.wicell.org#fig1]B, Supplemental File S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/784-Mmc1.zip]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/784-Fig1.jpg\nFigure\u00a01. Prepare DeepCPI input file\n(A) Gene symbol list of target gene set exemplified by 165 high confidence HDGs for Coronaviridae viruses.\n(B) The structure, layout and information of the text files for drugs and targets.\n(C) The structure, layout and information of the merged text file generated as DeepCPI input.\nNote: In addition to PubMed, public integrated database such as \u201cCRISP-view\u201d (http://crispview.weililab.org/[href=http://crispview.weililab.org/]) can also be used to search high throughput genetic screen studies or datasets (Cui et\u00a0al., 2021[href=https://www.wicell.org#bib1]). In addition, virus-specific HDGs for 10 families and 29 species of RNA viruses can be downloaded from (Li et\u00a0al., 2021[href=https://www.wicell.org#bib5]).Define the cohort of candidate drugs or chemicals for repurposing\nTiming: \u223c1 day\nCollect FDA approved drug information.\nDrug information is extracted from Database: DrugBank (version 5.1.7, released 2020-07-02; https://www.drugbank.ca[href=https://www.drugbank.ca]) (Wishart et\u00a0al., 2018[href=https://www.wicell.org#bib14]). Open DrugBank website -> Download -> Structures -> Structure External Links -> Approved -> Download. (#Download FDA approved drug data with InChI (the IUPAC International Chemical Identifier) information from the DrugBank website)\nExtract the DrugBank ID and InChI, and save them as separate files in the structure of \u201cDrugBank ID\u00a0+ InChI\u201d (Supplemental File S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/784-Mmc1.zip]).\nA total of 2457 FDA approved drugs are collected with InChI information. Note that the InChI value is required for DeepCPI.\nCollect natural compound information.\nNatural compound information is downloaded from Database: Traditional Chinese Medicine Systems Pharmacology (TCMSP) (version 2.3, released 2014-05-31; https://tcmspw.com/tcmsp.php[href=https://tcmspw.com/tcmsp.php]) which is a unique systems pharmacology platform of Chinese herbal medicines (Ru et\u00a0al., 2014[href=https://www.wicell.org#bib10]).\nFilter the pool of 1455 natural compounds for better druggability by requiring each candidate passing the criteria of oral bioavailability (OB) \u2265 30.0%, drug-likeness (DL) \u2265 0.18 and blood-brain barrier (BBB) \u2265 -0.30. Finally, 1062 selected natural compounds with InChI information are kept for the downstream DTI analysis.\nExtract the compound ID and InChI, and save them as separate files in the structure of \u201ccompound ID\u00a0+ InChI\u201d (Supplemental File S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/784-Mmc1.zip]).\nNote: The above drug cohort information used in this protocol can be found in Table S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/784-Mmc2.zip].\nPrepare DeepCPI input file\nTiming: \u223c2\u00a0h (variable)\nDeepCPI requires two layers of information for DTI prediction: \u201cthe InChl information of drugs\u201d and \u201cthe amino acid sequence of target gene-encoding proteins\u201d.\nPrepare a txt file (e.g., \u201cDrugbank_Approved.txt\u201d or \u201cTCM_selected.txt\u201d) containing the InChl information for each drug (Figure\u00a01[href=https://www.wicell.org#fig1]B, Supplemental File S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/784-Mmc1.zip]).Prepare a txt file (e.g., \u201cCoronaviridae_HDGs.txt\u201d) containing the amino acid sequence for each target protein (Figure\u00a01[href=https://www.wicell.org#fig1]B, Supplemental File S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/784-Mmc1.zip]). The amino acid sequences are extracted from UniProt database (https://www.uniprot.org/[href=https://www.uniprot.org/]).\nSave the two files (\u201cCoronaviridae_HDGs.txt\u201d and \u201cDrugbank_Approved.txt\u201d) under the same directory.\nOpen Terminal.\nChange directory to where the files (\u201cCoronaviridae_HDGs.txt\u201d and \u201cDrugbank_Approved.txt\u201d) are located by typing \u201ccd /your/working/path\u201d.\nRun python script \u201cDrugTargtPairGenerator.py\u201d by typing \u201cpython DrugTargtPairGenerator.py --f1 Coronaviridae_HDGs.txt --f2 Drugbank_Approved.txt\u201d to generate a merged txt file (e.g., \u201cDrug_Target_Pair.txt\u201d) with each possible drug-target pair (Figure\u00a01[href=https://www.wicell.org#fig1]C).\nDTI prediction by DeepCPI\nTiming: \u223c2h\nRun the DeepCPI pipeline and calculate the DeepCPI score for drug-target pair.\nPaste the merged input file (e.g., \u201cDrug_Target_Pair.txt\u201d) into the DeepCPI folder and rename it as \u201cexample.tsv\u201d. (#DeepCPI uses \u201cexample.tsv\u201d as default input file)\nOpen Terminal.\nActivate conda environment by typing \u201csource activate DeepCPI\u201d.\nChange directory to the home directory of the DeepCPI folder named \u201cDeepCPI-master\u201d by typing \u201ccd [The path of DeepCPI]\u201d. (e.g., \u201ccd /\u2026/DeepCPI-master\u201c)\nRun the DeepCPI pipeline under the DeepCPI folder by typing the command \u201cpython DeepCPI.py\u201d.\nA file named \u201cPrediction_results.tsv\u201d is generated at the end of the run. Each drug-target pair is assigned a DeepCPI score (range 0\u20131) representing their interaction potential. The higher score indicates higher interaction potential.\nChange directory to where the files (\u201cPrediction_results.tsv\u201d, \u201cCoronaviridae_HDGs.txt\u201d, and \u201cDrugbank_Approved.txt\u201d stored under the same directory) are located by typing \u201ccd /your/working/path\u201d.\nRun python script \u201cMatricesGenerator.py\u201d by typing \u201cpython MatricesGenerator.py --f1 Prediction_results.tsv --f2 Coronaviridae_HDGs.txt --f3 Drugbank_Approved.txt\u201d to create a score matrices    T  C P I     named \u201cPrediction_results.matrix.txt\u201d with DeepCPI score for each drug-target pair (   x  C P I    ), where l refers to the length of drug list and k refers to the length of target list:T  C P I   \u2208  R  l \u00d7 k   ,  x  C P I   \u2208  T  C P I    \nRun python script \u201cFilterOutNonSignificant.py\u201d by typing \u201cpython FilterOutNonSignificant.py -f Prediction_results.matrix.txt -c 0.892\u201d to filter out the non-significant DTI scores and only keep the confident scores. The output file is \u201cPrediction_results.matrix.filtered.txt\u201d. The optimal standardized DeepCPI score threshold (0.892, sensitivity: 37.2%, specificity: 86.8%) is determined by receiver operating characteristics (ROC) analysis with benchmark datasets (Li et\u00a0al., 2021[href=https://www.wicell.org#bib5]). This pre-defined threshold may change when different benchmark datasets are used to evaluate DeepCPI performance. Once defined, such threshold is applicable to any DTI analysis using DeepCPI for different target gene sets and drug sets.\n   T  C P I _ s i g   =  {      x ,  i f  x \u2265 0.892        0 ,  i f  x < 0.892       x \u2208  T  C P I    \nOptional: When more DTI prediction algorithms are applied to alleviate the bias of each algorithm and improve the prediction precision, each method generates a prediction score for the same drug-target pair. However, the score distribution pattern is usually different between different methods. To make these DTI scores comparable, a z-score based normalization is recommended as exemplified in the following steps to standardize DeepCPI score. DTI scores derived from other prediction algorithms can be normalized in the similar manner.\nOpen and run R script \u201cZscoreNormalization.Rmd\u201d to generate z-score matrices    Z  C P I     named \u201cz_Prediction_results.txt\u201d, where, \u03bc is mean value of the original scores and \u03c3 is standard deviation of the original scores:\n   z  C P I   =     x  C P I   \u2212   \u03bc  C P I     \u03c3  C P I    ,   x  C P I   \u2208  T  C P I    \nOpen Terminal.Change directory to where the files (\u201cz_Prediction_results.txt\u201d, \u201cCoronaviridae_HDGs.txt\u201d, and \u201cDrugbank_Approved.txt\u201d stored under the same directory) are located by typing \u201ccd /your/working/path\u201d.\nRun python script \u201cMatricesGenerator.py\u201d by typing \u201cpython MatricesGenerator.py --f1 z_Prediction_results.txt --f2 Coronaviridae_HDGs.txt --f3 Drugbank_Approved.txt\u201d. This command will create a z-score matrices    Z  C P I     named \u201cz_Prediction_results.matrix.txt\u201d with standardized DeepCPI score for each drug-target pair (   z  C P I    ), where l refers to the length of drug list and k refers to the length of target list:\n   Z  C P I   \u2208  R  l \u00d7 k   ,   z  C P I   \u2208  Z  C P I    \nRun python script \u201cFilterOutNonSignificant.py\u201d by typing \u201cpython FilterOutNonSignificant.py -f z_Prediction_results.matrix.txt -c 0.641\u201d. This command will filter out the non-significant DTI scores and only keep the confident scores. The output file is \u201cz_Prediction_results.matrix.filtered.txt\u201d. The optimal standardized DeepCPI score threshold (0.641, sensitivity: 73%, specificity: 51.9%) is determined by receiver operating characteristics (ROC) analysis with benchmark datasets (Li et\u00a0al., 2021[href=https://www.wicell.org#bib5]). This pre-defined threshold may change when different benchmark datasets are used. Once defined, such threshold for standardized DeepCPI score is applicable for different target gene sets and drug sets.\n   Z  C P I _ s i g   =  {     z ,  i f  z \u2265 0.641        0 ,  i f  z < 0.641       z \u2208  Z  C P I    \nPrioritize repurposed drug candidates\nTiming: \u223c10\u00a0min\nRepurposed drug candidates are ranked primarily according to their targeting range (the number of target) and targeting strength (the interaction potential of target).\nPrioritize the drug candidates using P_score that only considers the HDG target-associated DTIs. P_score is calculated for each drug candidate by the following formula, where    x  C P I _ s i g     represents filtered DeepCPI score for each drug-target pair and k refers to the length of target list.P _ s c o r  e  C P I   =    \u2211   i = 1  k   x i  C P I _ s i g   / k ,   x  C P I _ s i g   \u2208  T  C P I _ s i g    \nOpen the file \u201cPrediction_results.matrix.filtered.txt\u201d using Excel sheet. Drugs are listed in rows and targets are listed in columns.\nFor each drug, calculate P_score using the above formula (AVERAGE function). The higher of P_score, the better the corresponding drug is prioritized.\nThe drug candidates can be ranked according to their P_score.\nOptional: If using normalized z-score, calculate P_score for each drug candidate corresponding to each DTI prediction method by the following formula exemplified by DeepCPI, where    z  C P I _ s i g     represents filtered DeepCPI score for each drug-target pair and k refers to the length of target list. Drug candidates can be ranked by integrative consideration of multiple P_score derived from each DTI prediction methods.\n  P _ s c o r  e  C P I   =     \u2211   i = 1  k   z i  C P  I  s i g      k  ,   z  C P I _ s i g   \u2208  Z  C P I _ s i g    \nMolecular docking analysis of top ranked drugs\nTiming: \u223c4 h\nTo further examine the potential binding interface and free energy between top ranked drugs and their predicted target proteins, molecular docking analysis can be performed. Using Baricitinib (one of the top ranked repurposed drugs against Coronaviridae viruses) and its predicted target DYRK1A as an example, molecular docking analysis is performed as in the following steps. The docking parameters may vary depending on the interrogated drug/target pair.\nPrepare the ligand.Download the chemical structure file for Baricitinib (PubChem CID: 44205240) from PubChem website (https://pubchem.ncbi.nlm.nih.gov/[href=https://pubchem.ncbi.nlm.nih.gov/]) in SDF format (named as \u201cBaricitinib.SDF\u201d).\nOpen a PyMOL software browser and input the ligand file \u201cBaricitinib.SDF\u201d.\nExport and save as \u201cligand.PDB\u201d formatted file.\nOpen the AutoDock software and input the \u201cligand.PDB\u201d file (Figure\u00a02[href=https://www.wicell.org#fig2]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/784-Fig2.jpg\nFigure\u00a02. Pre-processing procedures of molecular docking analysis\n(A) Illustration of \u201cInput the ligand\u201d step by AutoDock software.\n(B) Illustration of \u201cChoose the torsions of the ligand\u201d step in AutoDock.\n(C) Illustration of \u201cOutput ligand.pdbqt file\u201d step in AutoDock.\n(D) Illustration of \u201cRemove waters of protein\u201d step in AutoDock.\n(E) Illustration of \u201cAdd polar hydrogens of protein\u201d step in AutoDock.\n(F) Illustration of \u201cDelete pre-embedded ligand\u201d step in AutoDock.\n(G) Example of \u201cDelete the other chains and solvents of the protein\u201d step (D chain of DYRK1A in 6SIE.pdb) in AutoDock.\nClick \u201cLigand->Torsion Tree\u201d and select \u201cChoose Torsions\u201d module (Figure\u00a02[href=https://www.wicell.org#fig2]B). The red chemical bond means un-rotatable, the green chemical bond means rotatable.\nOutput and save as \u201cligand.pdbqt\u201d formatted file (Figure\u00a02[href=https://www.wicell.org#fig2]C).\nPrepare the protein receptor.\nThe protein structure of DYRK1A (PDB: 6EIS) is downloaded from RCSB PDB website (http://www1.rcsb.org[href=http://www1.rcsb.org]) in PDB format.\nOpen a PyMOL software browser to input the file \u201c6SIE.pdb\u201d.\nRemove waters (Figure\u00a02[href=https://www.wicell.org#fig2]D) and add polar hydrogens (Figure\u00a02[href=https://www.wicell.org#fig2]E).\nChoose the primary ligand of DYRK1A at the 321st amino acid position of A chain, and remove the pre-embedded ligand (Figure\u00a02[href=https://www.wicell.org#fig2]F).\nDelete the other chains (B, C, and D chains of DYRK1A in 6SIE.pdb) and solvents of the protein (Figure\u00a02[href=https://www.wicell.org#fig2]G).\nSave as \u201cprotein.pdb\u201d formatted file.\nOpen the AutoDock software and input the \u201cprotein.pdb\u201d file.\nSet the atoms using \u201cAssign AD4 type\u201d module (Figure\u00a03[href=https://www.wicell.org#fig3]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/784-Fig3.jpg\nFigure\u00a03. Continued procedures of molecular docking analysis(A) Illustration of setting the atoms using \u201cAssign AD4 type\u201d module in AutoDock software.\n(B) Illustration of computing the Gasteiger charges for protein molecules in AutoDock.\n(C) Illustration of exporting and saving as \u201cprotein.pdbqt\u201d formatted file in AutoDock.\n(D) Example of setting the center of grid box size to cover the active pocket in AutoDock.\n(E) Illustration of outputting the Lamarckian GA result.\n(F) Illustration of showing the interactions between ligand and protein.\n(G) Illustration of analyzing different conformations of the ligand.\n(H) Example of docking result showing the interaction between Baricitinib and DYRK1A.\nCompute the Gasteiger charges for protein molecules (Figure\u00a03[href=https://www.wicell.org#fig3]B).\nExport and save as \u201cprotein.pdbqt\u201d formatted file (Figure\u00a03[href=https://www.wicell.org#fig3]C).\nSet the grid box.\nOpen the \u201cGrid\u201d module and input the \u201cprotein.pdbqt\u201d file.\nSet map types and input the \u201cligand.pdbqt\u201d file.\nOpen \u201cGrid Box\u201d module to set the position of grid box.\nSet the center of grid box size: X center: -0.424, Y center: -16.948, Z center: -8.144. Then, set the number of points in X (60), Y (60) and Z (60) dimension of grid box to cover the active pocket (Figure\u00a03[href=https://www.wicell.org#fig3]D).\nSave as \u201cdock.gpf\u201d formatted file.\nAnalyze the grid docking.\nChoose the \u201cDocking\u201d module, and input the protein and ligand files (\u201cprotein.pdbqt\u201d and \u201cligand.pdbqt\u201d).\nClick \u201cDocking->Search Parameters\u201d and choose \u201cGenetic Algorithm\u201d module.\nClick \u201cDocking->Docking Parameters\u201d and use the default settings.\nOutput the Lamarckian GA result and save as \u201cdock.dpf\u201d formatted file (Figure\u00a03[href=https://www.wicell.org#fig3]E).\nRun the \u201cAutoGrid\u201d and \u201cAutoDock\u201d module with \u201cdock.gpf\u201d and \u201cdock.dpf\u201d file, respectively. A \u201cdock.dlg\u201d file is then generated.\nOpen the \u201cdock.dlg\u201d file and protein file (\u201cprotein.pdbqt\u201d).\nShow the interactions between ligand and protein (Figure\u00a03[href=https://www.wicell.org#fig3]F).Analyze the conformations of ligand and click this button (imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/784-Fx3.jpg) (Figure\u00a03[href=https://www.wicell.org#fig3]G). The DashBoard shows the binding energy under different ligand conformations with the lowest binding energy of -8.07\u00a0kcal/mol for potential interaction between Baricitinib and DYRK1A A chain.\nOutput the complex interactions, and save as \u201cresult.pdbqt\u201d formatted file.\nVisualize the results of docking.\nOpen the PyMOL browser and input the \u201cresult.pdbqt\u201d file.\nSet the shape and color of the protein or the ligand.\nDisplay the background as \u201cwhite\u201d.\nOutput and save the picture of docking result as \u201cdocking.png\u201d file (Figure\u00a03[href=https://www.wicell.org#fig3]H).\nNote: Other molecular docking software can also be utilized. The binding interface and free energy may differ when using different molecular docking platforms.\nNote: If there is no structure of interrogated target protein available in PDB website, protein structure prediction by homology modeling may be performed. If there is only apo-structure available where the target protein is not in complex with drugs or small molecules, binding pocket prediction or blind docking can be performed with molecular docking software.\nOptional: If a deeper computational investigation on the binding-function relationship is needed, molecular dynamics (MD) simulation can be performed as elaborated in other literatures (Maximova et\u00a0al., 2016[href=https://www.wicell.org#bib6]; Mei et\u00a0al., 2021[href=https://www.wicell.org#bib7]; Yang et\u00a0al., 2020[href=https://www.wicell.org#bib15]).", "Step-by-step method details\nStep-by-step method details\nPreprocessing of fluorescence time-series image stacks collected from GECI reporter cells\nTiming: \u00a0<30\u00a0min\nThe Preprocessing Step takes as input, a stack of fluorescence time-lapse images (\u2217.tiff) and an associated ROI file (\u2217.roi), and generates as output, a list of single cells (one for each ROI) with associated centroids and mean fluorescence time series\u2019. Fiji/ImageJ is first used to subtract background fluorescence from each image, extract centroids, and quantify mean fluorescence time series\u2019 for each single cell (ROI). Next, MATLAB is used to calculate differential fluorescence time series, remove cells without transients, and smooth signals before peak-finding in step 2. Upon completion of this step, the signal is prepared for investigation of spatiotemporal correlations.\nOpen the fluorescence time series (\u2217.tiff) in Fiji/ImageJ.\nCreating a Median Intensity Projection (MedIP) image.\nSubtract it from each frame of the fluorescence time series to remove background fluorescence.\nNote: Specifically, Image\u00a0>\u00a0Stacks\u00a0>\u00a0Z Project\u00a0>\u00a0Projection type (Median) to create the MedIP image, and Process\u00a0>\u00a0Image Calculator\u00a0>\u00a0Image 1 (Raw fluorescence)\u00a0>\u00a0Subtract\u00a0>\u00a0Image 2 (MedIP) to subtract the MedIP image (Figures\u00a02[href=https://www.wicell.org#fig2] and 3[href=https://www.wicell.org#fig3]) (See troubleshooting[href=https://www.wicell.org#troubleshooting], problem 2[href=https://www.wicell.org#sec6.3]).\nLoad the pre-saved ROIs using Analyze\u00a0>\u00a0Tools\u00a0>\u00a0ROI manager> More >Open.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1934-Fig2.jpg\nFigure\u00a02. Example of a single-cell calcium transient with and without background fluorescence correction\nafu\u00a0= arbitrary fluorescence units.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1934-Fig3.jpg\nFigure\u00a03. Example of fluorescence time-series preprocessing\n(A) Remove background. Specifically, Image\u00a0>\u00a0Stacks\u00a0>\u00a0Z Project\u00a0>\u00a0Projection type (Median), Process\u00a0>\u00a0Image Calculator\u00a0>\u00a0Image 1 (Raw fluorescence)\u00a0>\u00a0Subtract\u00a0>\u00a0Image 2 (MedIP).\n(B) Load pre-saved ROIs. Specifically, Analyze\u00a0>\u00a0tools\u00a0>\u00a0ROI manager\u00a0>\u00a0open.\n(C) Select quantitative features (e.g., mean gray value, centroid). Specifically, Analyze\u00a0>\u00a0set measurement.(D) Record quantitative features for all ROIs. Specifically, Analyze\u00a0>\u00a0tools\u00a0>\u00a0ROI manager\u00a0>\u00a0More\u00a0>\u00a0multi measure.\nStandardize the image orientation by setting the origin (0,0) to the lower left-hand corner using Image\u00a0>\u00a0Adjust\u00a0>\u00a0Coordinates.\nNote: A dialog box will appear. Input 0 after Left and 0 after Bottom.\nExtract quantitative features (mean gray value and centroid) across all time points for each ROI using Analyze\u00a0>\u00a0Set Measurements\u00a0>\u00a0Mean Gray Value and Centroid followed by Analyze\u00a0>\u00a0Tools\u00a0>\u00a0ROI Manager > More\u00a0>\u00a0Multi Measure. Save the resulting file (\u2217.csv) (Figure\u00a03[href=https://www.wicell.org#fig3]).\nDownload the cell communication inference script inferring_cell_communication.m (key resources table[href=https://www.wicell.org#key-resources-table]). Open the script in MATLAB and run it by pressing F5.\nThe MATLAB script will prompt the user to select a file (choose the .csv file saved above in substep 4).\nThe script will automatically calculate \u0394F/F[t], a matrix of normalized differential fluorescence time series\u2019 for each cell.\nThe script will automatically use F[t], a matrix of raw fluorescence values defined at each time point for each cell.\nThe script will automatically use Fmed, a vector of median fluorescence values, one for each cell.\n    \u0394 F  /  F  [ t ]  =   (  F  [ t ]  \u00a0 - \u00a0  F med   )  /  F medNote: Use of Fmed as a reference assumes that cells spend at least 50% at baseline. If this assumption is satisfied, then Fmed will provide a good estimate of cell-specific baseline dynamics. If it is only valid during part of the record, one should subset the time series and perform the analysis to derive Fmed. An alternative approach to calculating relative fluorescence is to subtract an estimate of initial fluorescence, F0. However, this assumes that cells are not in the midst of a calcium transient at time zero (See troubleshooting[href=https://www.wicell.org#troubleshooting], problem 1[href=https://www.wicell.org#sec6.1]).\nThe MATLAB script will automatically open filterDesigner app, an interactive tool used to design and evaluate digital filters, which will be used to smooth the signal.\nChange the filter type from finite impulse response (FIR) to infinite impulse response IIR. Use the default values or customize the filter for your data.\nSelect Design Filter and export the file using File\u00a0>\u00a0Export (to Workspace, as coefficients) and check the Overwrite Variables box.\nClose the app. The resulting filter will be automatically applied to the fluorescence time series.\nEvaluate adequacy of filtering.\nThe script will prompt the user to select an ROI.\nThe pre- and post-filtered fluorescence signals will be displayed.\nIf satisfied with the result, select \u201cyes\u201d; to reject and redesign the filter, select \u201cno\u201d.\nFor more information on filterDesigner select Help on its dialog box (See troubleshooting[href=https://www.wicell.org#troubleshooting], Problems 1[href=https://www.wicell.org#sec6.1] and 3[href=https://www.wicell.org#sec6.5]).\nPeak finding and impulse train determination from Ca2+ GECI time-series fluorescence\nTiming: <30\u00a0min\nThe MATLAB script will guide the user through a process that identifies peaks in the fluorescence time series\u2019 and converts them into binary impulse trains for each cell (Figure\u00a04[href=https://www.wicell.org#fig4]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1934-Fig4.jpg\nFigure\u00a04. Generation of a single-cell binary impulse train from the normalized differential fluorescence time-series of a single ROINormalized differential fluorescence time series\u2019 are low pass filtered, subjected to peak-finding, and converted to single cell binary impulse trains. Figure\u00a0reprinted with permission from Taghdiri et\u00a0al. (2021)[href=https://www.wicell.org#bib7].\nInput the parameters for the MATLAB function findpeaks, namely threshold, Min peak Height, Min distance between peaks, Min prominence, Min width, and Max width. Default values are suggested.\nEvaluate the performance. The MATLAB script displays fluorescence vs time with the identified peaks superimposed for the most active cells.\nAccept the peaks by selecting \u201cyes\u201d if they match what one would label manually.\nReject by clicking \u201cno\u201d if there are extra peaks or missed peaks. Then proceed with optimization of parameters (See troubleshooting[href=https://www.wicell.org#troubleshooting], Problems 1[href=https://www.wicell.org#sec6.1] and 4[href=https://www.wicell.org#sec6.7]).\nOptimize peak-finding parameters focusing primarily on Min prominence to adjust for fluorescence amplitude and Min width and Max width to adjust for fluorescence duration.\nOnce accepted, peaks are automatically transformed into a matrix of binary impulses, B[t], which contains a \u201c1\u201d at the time point of each peak for each cell, and a \u201c0\u201d everywhere else. A histogram of the number of impulses per cell will be automatically displayed. This will be used to fit a probability distribution function and to create synthetic cell impulse trains with the same statistics below.\nCritical: Optimization of peak-finding parameters is critical because fluorescence kinetics and amplitudes different across experimental systems since they depend on many factors including cell type, microenvironmental context, and choice of genetically encoded calcium reporter.\nInference of cell communication pipeline based on \u201cexcess synchrony\u201d metric\nTiming: \u223c1 h\nStep 3 of this protocol infers putative cell communication events from the spatiotemporal synchrony of single cell calcium impulse trains. The strategy is illustrated in Figure\u00a05[href=https://www.wicell.org#fig5]. An overview of the logic and approach are presented first, followed by the detailed sub-steps.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1934-Fig5.jpgFigure\u00a05. Diagram of cell communication pipeline inference steps\n(A\u2013C) (A) \u201cReal\u201d experimental binary impulse trains are (B)\u00a0modeled based on their impulse train statistics to create (C) \u201cgenerated\u201d binary impulse trains.\n(D\u2013F) Real and generated impulse trains are independently used to calculate synchrony, S (w, \u03c4), specifically (E/F) Sreal (w, \u03c4) and Sgen (w, \u03c4).\n(G) A threshold called Sth is defined based on generated synchrony and a user-generated z score.\n(H) Excess synchrony, \u0394S/w.\n(I and J) Cell communication events are identified at time \u03c4comm\u2019s and spatial locations (Xcomm\u2019s, Ycomm\u2019s). Figure\u00a0reprinted with permission from Taghdiri et\u00a0al. (2021)[href=https://www.wicell.org#bib7].\nFor purposes of this protocol, synchrony, S (w, \u03c4), is defined as the number of impulses within a time window of length w, beginning at time \u03c4. Synchronous impulses can occur either because of true biological information transfer or simply because of chance. By first modeling the \u201cchance\u201d component, we can then limit our search for cell communication to those times during which the experimentally observed synchrony exceeds the amount of synchrony expected \u201cby chance\u201d. We call the difference, after normalization to window size, \u201cexcess synchrony\u201d, \u0394S/w.\nNote: The reason for normalizing is because synchrony increases monotonically with window size. Normalizing to w enables comparison of excess synchrony across different window sizes.To determine the synchrony expected by chance, we model the statistics of experimental impulse trains from \u201creal cells\u201d, fit it to a probability distribution, and then sample the distribution to create generated impulse trains from \u201cgenerated cells\u201d having the same statistics. This allows calculation of generated synchrony due to chance alone, Sgen (w, \u03c4). To convert generated synchrony into a single number that can be easily subtracted from real experimentally observed synchrony, Sreal (w, \u03c4), to calculate excess synchrony, we define a threshold, Sth, at a specified percentile of Sgen (w, \u03c4) using a z score. Excess synchrony, \u0394S/w, is then calculated by subtracting Sreal (w, \u03c4) and Sth and normalizing to w. Selection of Sth via the choice of z score is a critical step because high Sth values will predict few cell communication events but have fewer false positives than low Sth values, which will predict more cell communication events but include more false positives (predicted communication where synchrony is actually due to chance alone).\nTo determine the timing of putative cell communication events we derive a vector of communication times \u03c4comm by grouping nearby values of \u03c4 for which excess synchrony \u0394S/w is greater than zero. For each \u03c4comm, spatial analysis is limited to only those cells with calcium impulses between \u03c4comm and \u03c4comm\u00a0+ w. k-Means clustering is performed and the centroid of the cluster with the greatest cell density is taken as the location of putative cell communication Xcomm, Ycomm. The results of putative cell communication events and their component cells are reported in tabular form in a single \u2217.csv file. The following describes detailed sub steps with a focus on the how the user interacts with the script.The MATLAB script will automatically open the interactive distributionFitter app to facilitate fitting a probability distribution function to the experimental impulse frequencies (Figure\u00a06[href=https://www.wicell.org#fig6]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1934-Fig6.jpg\nFigure\u00a06. Modeling experimental impulse train statistics to generate synthetic cell impulse trains\nIn the parent window, select the Data tab and a daughter window will open.\nIn the Data field of the daughter window, enter \u201cImpulseHistogram\u201d, and then select Create Data Set.\nReturning to the parent window, select the NewFit\u2026 tab, which will open a new daughter window.\nIn the Data field of the new daughter window, select \u201cImpulseHistogram\u201d.\nIn the Distribution field, select \u201cnegative binomial\u201d.\nFinally, select Apply to create the probability distribution object and select Save to Workspace to save it.\nClose the app.\nThe MATLAB script will perform a Chi square goodness of fit test using the built-in function chi2gof.\nThe MATLAB script will return a test decision for the null hypothesis that the experimental impulse histogram data comes from the specified probability distribution (selected from a dropdown in 9e).\nIf the test decision rejects the null hypothesis at the 5% significance level, then the app will reopen.\n9a\u2013f must be repeated with a different distribution.\nAlternatively, the MATLAB script will proceed to automatically generate synthetic cell impulse trains by sampling from the approved distribution and placing the impulses at random time points. Therefore, the synchrony of resulting generated data will be normally distributed.\nNote: The script will run automatically with the assistance of only two input parameters - window\u00a0size, which determines temporal resolution, and a z-score, which determines the inclusiveness or exclusiveness of communication inference (default values are provided for both).The user will be prompted, \u201cDo you want to choose the window size automatically?\u201d To use the default window size or specify it manually, answer \u201cno.\u201d Alternatively, to perform a systematic comparison of window sizes in search of the one yielding the maximum excess synchrony, answer \u201cyes\u201d (See troubleshooting[href=https://www.wicell.org#troubleshooting], problem 5[href=https://www.wicell.org#sec6.9]).\nThe user will be prompted to enter a z score, z, (a default is provided), which will be combined with the normally distributed generated data of mean \u03bc and standard deviation \u03c3 to give a threshold synchrony Sth (See troubleshooting[href=https://www.wicell.org#troubleshooting], problem 5[href=https://www.wicell.org#sec6.9]).\n   S th  \u00a0 = \u03bc\u00a0 + \u00a0z\u2217 \u03c3  \nCritical: Optimization of the z score is critical. The default z score is set at 1.28, the 80th percentile. However, Sth is a tunable parameter, which allows the user to alter the stringency of this threshold. At lower percentiles (lower Sth values), the number of identified peaks will increase, resulting in more predicted events but more false positives (i.e., more synchronous impulses that are labeled communication will actually be due to chance). Conversely, at higher percentiles (higher Sth values), the number of identified peaks will decrease resulting in fewer predicted events but more risk of false negatives (i.e., missing true communication events). The z score should be optimized to balance the competing goals of maximizing the number of events detected (by lowering the z score) while minimizing the number of false positives (by increasing the z score). In practice, if one has a biologically negative control sample (i.e., few calcium transients per time) one can reduce the z score until an unacceptable number of cell communication events (of which all are considered false positives) are detected.\nThe MATLAB script will calculate Excess Synchrony \u0394S/w is calculated (Figure\u00a07[href=https://www.wicell.org#fig7]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1934-Fig7.jpgFigure\u00a07. Determination of real and generated synchrony from single-cell impulse trains\nSynchrony is defined as the sum of impulses within a window of length w beginning at time \u03c4. \u201cReal\u201d synchrony Sreal derives from real experimental single impulse trains. \u201cGen\u201d synchrony Sgen derives from generated single impulse trains. Excess synchrony is shown without normalization \u0394S. Figure\u00a0reprinted with permission from Taghdiri et\u00a0al. (2021)[href=https://www.wicell.org#bib7].\n  \u00a0   \u0394 S  / w  =   [   S real   ( w,\u00a0\u03c4 )  - \u00a0  S th   ]  / w   \nThe MATLAB script will return the timing of putative cell communication events which is vector of communication times \u03c4comm by grouping nearby values of \u03c4 for which excess synchrony \u0394S/w is greater than zero (Figure\u00a08[href=https://www.wicell.org#fig8]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1934-Fig8.jpg\nFigure\u00a08. Example of how varying Sth affects predicted cell communication events\nSth corresponding to the 50th, 80th, and 100th percentiles of Sgen are illustrated. Communication is predicted where Sreal exceeds Sth, (e.g., where excess synchrony is greater than zero). The initiation of each putative communication event where Sreal sustainably exceeds Sth is deemed a putative communication event assigned to the start time \u03c4comm. Figure\u00a0reprinted with permission from Taghdiri et\u00a0al. (2021)[href=https://www.wicell.org#bib7].\nThe script will open a window prompting the user to select a criterion for unsupervised learning algorithm that is used to solve clustering problems.\nThe script will calculate an optimal number of clusters k, evalclusters.\nThe script will use optimal k to preform perform k-Means clustering, kmeans function in MATLAB (See troubleshooting[href=https://www.wicell.org#troubleshooting], problem 6[href=https://www.wicell.org#sec6.11]).\nThe script will return \u03c4comm and perform spatial analysis on only those cells with calcium impulses between \u03c4comm and \u03c4comm\u00a0+ w. Then the script will preform k-Means clustering and report the centroid of the cluster with the greatest cell density as the location for putative cell communication Xcomm, Ycomm (Figure\u00a09[href=https://www.wicell.org#fig9]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1934-Fig9.jpgFigure\u00a09. Finding high spatiotemporal synchronous cells\nPlot of cluster density vs cluster number for a single communication event (left). Cells with impulses in the time interval between \u03c4comm and \u03c4comm\u00a0+ w are subjected to k-Means clustering (right). The centroid of the cluster with the maximum density is interpreted as the location of putative communication (right).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1934-Fig10.jpg\nFigure\u00a010. Example of the output folder contents at analysis completion\nThe folder includes an output file, FinalResults.csv, and associated plots.\nUpon completion the script will create an output file called FinalResults.csv and save all associated plots and figures. The FinalResults file contains a \u2217.csv file with the following column headings: EventIndex, StartTime, EndTime, Duration, NumOfCells, CellIndex, CentroidX, CentroidY.\nNote: For clarity, each row refers to a distinct cell and its participation in each cell communication event. Therefore, EventIndex StartTime, EndTime, Duration, Number of Cells are properties of the event, while CellIndex, CentroidX, CentroidY are defined for the individual cells within each event (Figure\u00a010[href=https://www.wicell.org#fig10]).", "Step-by-step method details\nStep-by-step method details\nStep 1: install dependencies\nTiming: 1 h\nThe complete installation of PAN2HGENE starts with the installation of the system dependencies and then continues with the installation of other programs that are part of the pipeline.\nTo start the installation run the commands in the Box 1. Below to update the system.\nBox 1\nsudo apt-get install make\nsudo apt-get install build-essential\nsudo apt-get install curl\nCritical: If you face any problem with the commands of the Box 1, we suggest update your system with the commands \u2018sudo apt-get update\u2019 and \u2018sudo apt-get upgrade\u2019. After this try to run the Box 1 again.\nThen we will install some packages and programs with the commands below.\nBox 2\nsudo cpan install YAML.pm\nsudo cpan install YAML::XS\nsudo cpan install Bio::AlignIO\nsudo cpan install Statistics::LineFit\nsudo cpan install Statistics::Distributions\nsudo cpan install Bio::Perl\nsudo cpan install Bio::SearchIO:::hmmer3\nsudo apt-get install screen\nsudo apt-get install bowtie2\nsudo apt-get install blast2\nsudo apt-get install samtools\nsudo apt-get install python3-distutils\nsudo apt-get install python\nsudo cpan install DBI\nsudo apt-get install mafft\nsudo apt-get install mcl\nsudo apt-get install phylip\nStep 2: installation of software that compose the pipeline\nTiming: 2\u00a0h 22\u00a0min 30 s\nThe following are the steps to install the external software that are part of the PAN2HGENE pipeline.\nSPAdes Installation. SPAdes is available at http://cab.spbu.ru/software/spades/[href=http://cab.spbu.ru/software/spades/], the installation process follows below.\nBox 3\nwget http://cab.spbu.ru/files/release3.15.3/SPAdes-3.15.3-Linux.tar.gz[href=http://cab.spbu.ru/files/release3.15.3/SPAdes-3.15.3-Linux.tar.gz]\ntar -xzf SPAdes-3.15.3-Linux.tar.gz\nAt the end of the installation, the user must move the SPAdes folder to the /opt directory.\nBox 4\nmv SPAdes-3.15.3-Linux/ SPAdes/\nsudo mv SPAdes/ /opt/\ncd /opt/\nsudo chmod 777 -R SPAdes/\nTo validate the installation, run the command (see Box 5), shown in Figure\u00a01[href=https://www.wicell.org#fig1].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig1.jpg\nFigure\u00a01. SPAdes test: Demonstrates the SPAdes test runBox 5\n/opt/SPAdes/bin/spades.py\nPGAP Installation. The PGAP software is available at https://sourceforge.net/projects/pgap/[href=https://sourceforge.net/projects/pgap/], after downloading it follow the steps below, it is important to note that at the time of building this protocol the most current version was PGAP-1.2.1.\nBox 6\ntar -xzf PGAP-1.2.1.tar.gz\nmv PGAP-1.2.1/ PGAP/\nsudo mv PGAP/ /opt/\ncd /opt/\nsudo chmod 777 -R PGAP/\nThe following step is the configuration of the PGAP script (/opt/PGAP/PGAP.pl), it can be done using gedit or other text editor preferred by the user.\nBox 7\nsudo gedit /opt/PGAP/PGAP.pl\nModify the path of the software leaving the lines of the file the same as the lines without comments (lines that do not start with the # character). It is necessary to adjust the execution path of the programs e.g., /usr/bin/formatdb according to the path where the software is in your operating system. In Box 8 below, the lines that do not start with the character # demonstrate the configuration performed in the Ubuntu operating system.\nBox 8\n### programs from BLAST\nmy $formatdb=\"/usr/bin/formatdb\";\nmy $blastall=\"/usr/bin/blastall\";\n### programs from mcl\nmy $mcl=\"/usr/bin/mcl\";\n### programs from mafft\nmy $mafft=\"/usr/bin/mafft\";\n### programs from PHYLIP\nmy $seqboot=\"/usr/lib/phylip/bin/seqboot\";\nmy $neighbor=\"/usr/lib/phylip/bin/neighbor\";\nmy $consense=\"/usr/lib/phylip/bin/consense\";\nmy $dnaml=\"/usr/lib/phylip/bin/dnaml\";\nmy $dnadist=\"/usr/lib/phylip/bin/dnadist\";\nmy $dnapars=\"/usr/lib/phylip/bin/dnapars\";\nLook for the 3 lines below and replace \"./\" with \"/opt/PGAP/\". After the replacement the lines should look like Box 10.\nBox 9\nsystem(\"perl ./multiparanoid.pl -species \".join(\".pep+\",@species).\".pep -unique 1\");\nsystem(\"perl ./Blast_Filter.pl All.blastp All.pep $coverage $identity $score | $mcl - --abc -I 2.0 -o All.cluster\");\nsystem(\"perl ./inparanoid.pl $blastall $thread $formatdb $score $global $local\n$species[$i].pep $species[$j].pep\");\nBox 10\nsystem(\"perl /opt/PGAP/multiparanoid.pl -species \".join(\".pep+\",@species).\".pep -unique 1\");\nsystem(\"perl /opt/PGAP/Blast_Filter.pl All.blastp All.pep $coverage $identity $score | $mcl - --abc -I 2.0 -o All.cluster\");\nsystem(\"perl /opt/PGAP/inparanoid.pl $blastall $thread $formatdb $score $global $local $species[$i].pep $species[$j].pep\");Save the file and close gedit. To validate that PGAP was installed correctly, run the command below and result should be similar to this (Figure\u00a02[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig2.jpg\nFigure\u00a02. PGAP test: Demonstrates the PGAP test run\nBox 11\n/opt/PGAP/PGAP.pl\nProkka Installation. The Prokka software is available https://github.com/tseemann/prokka[href=https://github.com/tseemann/prokka], to install the version for Ubuntu/Debian/Mint run the commands below.\nBox 12\nsudo apt-get install libdatetime-perl libxml-simple-perl libdigest-md5-perl git default-jre bioperl\nsudo cpan Bio::Perl\nsudo git clone https://github.com/tseemann/prokka.git/opt/prokka[href=https://github.com/tseemann/prokka.git/opt/prokka]\nsudo chmod 777 -R /opt/prokka\n/opt/prokka/bin/prokka --setupdb\nsudo chmod 777 -R /opt/prokka\nTo verify that Prokka was installed correctly, run the command below and the result should be similar to this (Figure\u00a03[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig3.jpg\nFigure\u00a03. Prokka test: Demonstrates the Prokka test run\nBox 13\n/opt/prokka/bin/prokka\nR Installation. The R software is installed by following the commands below.\nBox 14\nsudo apt-get update\nsudo apt-get install r-base r-base-dev\nAfter installation, run R and install the libraries.\nBox 15\nsudo R\ninstall.packages('ape')\ninstall.packages(\"plotrix\")\ninstall.packages(\"minpack.lm\")\ninstall.packages('ctv')\nlibrary('ctv')\ninstall.views('Phylogenetics')\nupdate.views('Phylogenetics')\nq()\nTbl2asn Installation. The tbl2asn tool is installed by running the following commands.\nBox 16\nwget -N\nftp://ftp.ncbi.nih.gov/toolbox/ncbi_tools/converters/by_program/tbl2asn/linux64.tbl2asn.gz\ngunzip linux64.tbl2asn.gz\nsudo chmod\u00a0+x linux64.tbl2asn\nmv linux64.tbl2asn /usr/local/bin/tbl2asn\nAnd to test if everything is correct, run the command below and the result should be similar to this (Figure\u00a04[href=https://www.wicell.org#fig4]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig4.jpg\nFigure\u00a04. Tbl2asn test: Demonstrates the Tbl2asn test run\nBox 17\ntbl2asn --help\nMySQL Installation. To install MySQL server run the following commands.\nBox 18\nsudo apt-get install mysql-server\nsudo mysql_secure_installation\nMySQL will ask you to create a password for the root user. Enter the password and answer Y when asked.\nThe component checks to see if the new password is strong enough. Choose one of the three levels of password validation:\nLow. A password containing at least 8 characters.Medium. A password containing at least 8 characters, including numeric, mixed case characters, and special characters.\nStrong. A password containing at least 8 characters, including numeric, mixed case characters, and special characters, and compares the password to a dictionary file. Enter 0, 1, or 2 depending on the password strength you want to set.\nThe script then prompts for the following security features: Remove anonymous users? Disallow root login remotely? Remove the test database and access to it? Reload privilege tables now?\nTo check if your MySQL was installed correctly run the command below and the result should be similar to this (Figure\u00a05[href=https://www.wicell.org#fig5]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig5.jpg\nFigure\u00a05. MySQL test: Demonstrates the MySQL test run\nBox 19\nsudo systemctl status mysql\nStep 3: using PAN2HGENE\nTiming: 1\u00a0h 27\u00a0min\nAfter installing all dependencies, the user must download PAN2HGENE. To start using it, follow the steps below.\nPAN2HGENE Download. The PAN2HGENE jar package is available at (https://sourceforge.net/projects/pan2hgene-software/[href=https://sourceforge.net/projects/pan2hgene-software/]). Download the pan2hgenev2.0.jar and lib_v2.tar.xz files leaving both in the same directory. See the example below. The PAN2HGENE pipeline can be executed in three different ways, each one performing a specific analysis.\nBox 20\ncd /home/pablo/panTest/\ntar -xf lib_v.2.tar.xz\nsudo chmod 777 -R /home/pablo/panTest/\njava -jar pan2hgenev2.0.jar\nPAN2HGENE Product Identification analysis. The input for this analysis is a pair of files in FASTA and FASTQ format, the FASTA file can contain a complete genome or a draft of the organism. An attempt is made to identify possible new gene products for the analyzed genomes.\nNote: In this example, product identification analysis is performed using a Bifidobacterium breve DSM20213 genome and paired reads from the Illumina HiSeq 2000. To start the Product Identification analysis, place the fasta genome and fastq reads in the same folder, follow the steps below.\nBox 21\njava -jar pan2hgenev2.0.jarIf this is your first use, enter the root user and password and press the create DB button, else enter the root user and root password in the indicated fields and then click the Connect button, then click on the Next button (Figure\u00a06[href=https://www.wicell.org#fig6]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig6.jpg\nFigure\u00a06. Database management: The window has all functions to handle database, like status, drop, create e connect database\nOn the following screen, it is necessary to enter the project name and select the type of analysis to be performed. In the following example, the name \u201cTest1\u201d and the Product Identification option were added, after that press the New button (Figure\u00a07[href=https://www.wicell.org#fig7]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig7.jpg\nFigure\u00a07. Project management: In this window, the user will create the project, so just inform the name of the project and the type of analysis to be performed\nData input is done in the following window. Press the Browse button to select the FASTA file (Remember that fasta genome and fastq reads must be in the same folder). The reads files will be displayed below, select the appropriate reads for the organism, inform the type of reads and if it is paired, inform the order and orientation (Figure\u00a08[href=https://www.wicell.org#fig8]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig8.jpg\nFigure\u00a08. Input data: In this window, the user will add the files in fasta and fastq format depending on the analysis chosen\nPress Add Read button and confirmation message will be displayed (Figure\u00a09[href=https://www.wicell.org#fig9]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig9.jpg\nFigure\u00a09. Confirmation warning\nNote that the reads are now marked as used, repeat the same process if there are more genomes. Then click Next (Figure\u00a010[href=https://www.wicell.org#fig10]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig10.jpg\nFigure\u00a010. Input data confirmation: Demo files after being added to the analysisThe screen below will be shown, here it is possible to modify the parameters of PAN2HGENE for Bowtie, Comparative Analysis, and Annotation process. In this case, we will use the default parameters. So just click the Save Data button, then click Next (Figure\u00a011[href=https://www.wicell.org#fig11]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig11.jpg\nFigure\u00a011. Parameters window: In this window, the user can edit the parameter values or can choose to use the default value\nThe screen below will be shown. To start the analysis, click on the Perform analysis button. And in the Logs field, it is possible to check the analysis steps being performed. When the analysis is finished, the message Complete Analysis will appear in the Logs field, as can be seen on the screen below. Now close PAN2HGENE and go to the folder where the data was analyzed (Figure\u00a012[href=https://www.wicell.org#fig12]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig12.jpg\nFigure\u00a012. Perform analysis and Log: In this window, the user executes the analysis and follows the execution steps\nSeveral files will be inside the folder, in addition to the fasta genome and the fastq reads used in the analysis. The result of the Product Identification analysis are the three files marked below, GenomeNameBlastResult_Products.fasta, GenomeNameBlastResult_report.pdf, and GenomeNameBlastResult_Report.txt (Figure\u00a013[href=https://www.wicell.org#fig13]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig13.jpg\nFigure\u00a013. Product identification analysis results\nPAN2HGENE Comparative analysis. In this example, three fasta genomes, Bifidobacterium breve DSM20213 (complete), Bifidobacterium breve NCTC11815 (complete) and Bifidobacterium breve PRL2020 (draft with six contigs) were used. To start Comparative Analysis, place all fasta genomes in the same folder as shown below.\nCritical: The initial steps are the same as shown in Figures\u00a06\u201312[href=https://www.wicell.org#fig6], with the exception of the input data window that changes for this specific analysis.\nWhen informing the directory of the FASTA files, they will be listed as shown in the figure below (Figure\u00a014[href=https://www.wicell.org#fig14]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig14.jpgFigure\u00a014. Input data window: In this window, the user input complete or draft genomes in FASTA format\nThe analysis results are organized in the pgfiles directory. The files that are the results of the Comparative Analysis are the files that start with the numbers 1, 2, 3, 4, 5 and the figures in PNG format (Figure\u00a015[href=https://www.wicell.org#fig15]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig15.jpg\nFigure\u00a015. Comparative analysis results\nPAN2HGENE Full pipeline. The Full Pipeline analysis performs Product Identification analysis and Comparative Analysis automatically and sequentially. Thus, the new gene products identified in the Product Identification step will be used in the Comparative Analysis step.\nNote: Now follow the steps described previously in the item \u201810. PAN2HGENE Product Identification analysis\u2019, selecting Full Pipeline analysis instead of Product Identification (Figure\u00a016[href=https://www.wicell.org#fig16]).\nMain graphical results. The graphs are produced by running the comparative analysis, so the creation is included in the processing time (Figures\u00a017[href=https://www.wicell.org#fig17] and 18[href=https://www.wicell.org#fig18]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig16.jpg\nFigure\u00a016. Full pipeline analysis results\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig17.jpg\nFigure\u00a017. Graphics results of pangenome: On the left we have the graph with the information about pangenome, together with the Heap\u2019s Law calculation and on the right, in the pie chart, the information about the amount of unique gene products and genes shared between the analyzed organisms\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig18.jpg\nFigure\u00a018. Graphics results of unique genes: On the left we have the graph with information about the amount of unique genes per organism present in the analysis and on the right an example of a phylogenetic tree graph\nStep 4: patric optional usage\nTiming: 2\u00a0min\nPatric software has been integrated into the pipeline as an alternative to automatic annotation software. Thus, the user is free to choose between Patric or Prokka, for the annotation execution.Your PAN2HGENE is now ready to use, If the user does not want to use Patric to make the annotation, it is not necessary to perform the following steps.\nNote: However, the PAN2HGENE offers the option to perform all annotation analyses through PATRIC instead of Prokka (which is the default option). If you want to use PRATIC in the annotation process, follow the steps below. If you do not already have a PATRIC account, you will have to register on (https://patricbrc.org/[href=https://patricbrc.org/]).\nInstall PATRIC Command Line Interface.\nBox 22\ncurl -O -L https://github.com/PATRIC3/PATRIC-distribution/releases/download/1.034/patric-cli-1.034.deb[href=https://github.com/PATRIC3/PATRIC-distribution/releases/download/1.034/patric-cli-1.034.deb]\nsudo dpkg -i patric-cli-1.034.deb\nsudo apt-get -f install\nIf you prefer you can also install PATRIC using the tool gdebi.\nBox 23\nsudo apt-get install gdebi-core\nsudo gdebi patric-cli-1.034.deb\nSetting, copy the file \u201cp3-login.pl\u201d, provided with the pan2hgene files, and replace it in the installation directory of Patric-cli.\nBox 24\ncp -r\u00a0<\u00a0file directory\u00a0>\u00a0usr/share/patric-cli/deployment/plbin/\nWhen performing any of the PAN2HGENE analyses, it is possible to choose the Patric annotation instead of the Prokka annotation. Before saving the parameters, click on the PATRIC button (Figures\u00a019[href=https://www.wicell.org#fig19] and 20[href=https://www.wicell.org#fig20]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig19.jpg\nFigure\u00a019. Input data parameters: The user can select Patric annotation tool\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1581-Fig20.jpg\nFigure\u00a020. Parameter patric: The user enter with your credentials, taxonomy ID and Domain organism, for instance, Escherichia coli, Taxonomy ID 562 and Domain Bacteria", "Step-by-step method details\nStep-by-step method details\nPerturb gene expression in mp53/Ras cell populations by stable retroviral transduction and drug selection\nTiming: Two to three weeks\nThis step creates cell populations with expression of desired cDNA or knock-down of desired target gene by shRNA. The derived cells are used for additional experiments described below.\nOne day prior to infection, plate mp53/Ras cells at 33\u00b0C.\nUse one plate for each perturbation to be derived, including an empty vector control each time infections are performed.\nMake an additional plate of cells for \u201cmock\u201d infection (no virus added).\nThe mock infection allows monitoring of drug selection. Populations infected with retrovirus should have surviving cells, while the \u201cmock\u201d infected cells should all die in the presence of drug.\nPlate mp53/Ras cells at 250,000 cells / 10\u00a0cm collagen-coated dish.\nPrepare polybrene (also known as hexadimethrine bromide) in PBS at desired stock concentration.\nFor 100\u00d7, dissolve 800\u00a0\u03bcg/mL. For 1000\u00d7, dissolve 8\u00a0mg/mL.\nPolybrene can be stored at 4\u00b0C for up to 6\u00a0months or can be frozen at \u201320\u00b0C for longer periods.\nOne day after plating cells (18\u201324 h), aspirate media from plates and pipet collected supernatants containing viruses on to each plate to be infected.\nRetrovirus-containing supernatants should have a 5\u00a0mL volume if generated as described in \u201cbefore you begin[href=https://www.wicell.org#before-you-begin]\u201d steps 20 and 21 above.\nUse only one type of virus per plate of cells.\nFor mock infection, replace media with 5\u00a0mL of fresh 33\u00b0C media.\nAdd 8\u00a0\u03bcg of polybrene per mL of supernatant to each dish being infected (i.e., dilute 100\u00d7 or 1000\u00d7 stock to 1\u00d7 final concentration in volume of supernatant on each dish).\nIncubate for 1.5\u20133\u00a0h at 33\u00b0C in CO2 incubator.Aspirate supernatant and replace with second aliquot of supernatant for virus containing the same perturbagen. In other words, the same dish should receive multiple rounds of infection with empty vector or virus harboring a single cDNA or shRNA insert.\nGenerally, we do 2 rounds of infection but YAMC and mp53/Ras cells can tolerate up to 6 rounds of infection before experiencing toxic effects from exposure to polybrene.\nFollowing all desired rounds of infection, replace supernatant with fresh 33\u00b0C media.\nAllow cells to recover for 48\u201372\u00a0h prior to beginning drug selection.\nFor drug selection of YAMC or mp53/Ras cells, i.e., elimination of uninfected cells in the population, add the appropriate selective agent to 33\u00b0C media at concentrations listed below and depending on the resistance marker present in the retroviral vector used.\nPuromycin: 5\u00a0\u03bcg/mL media; Selection takes 2\u20134\u00a0days.\nHygromycin: 200\u00a0\u03bcg /mL media; Selection takes 4\u20137\u00a0days.\nBleomycin: 100\u2013400\u00a0\u03bcg /mL media; Selection takes up to 2\u00a0weeks.\nNeomycin/Geneticin: 100\u2013400\u00a0\u03bcg /mL media; Selection takes up to 2\u00a0weeks.\nOnce perturbed populations are derived, they should be maintained in media with the appropriate selective agent during routine maintenance / cell splitting.\nCritical: Selected cell populations should be used for RNA isolation, tumor formation studies and other experimentation within two weeks of derivation. Long-term culture of perturbed populations can allow for drift in the population and altered cell behavior over time in culture.\nNote: For experiments described in McMurray et\u00a0al., Cell Reports, 2021, we freshly transduced and selected polyclonal populations for each biological replicate and each perturbation described in the paper.Note: For double / combined perturbation, derive polyclonal population harboring one perturbation and then repeat above steps for second perturbagen. Be sure to use perturbagens with distinct selectable markers to enable selection of pure populations of perturbed cells harboring both perturbations. When selecting with multiple selective agents, use half the dosage recommended above to avoid non-specific cell killing.\nExtract RNA and measure gene expression in perturbed mp53/Ras cell populations\nTiming: 5\u20137\u00a0days\nThis step involves re-plating perturbed cell populations for short-term growth at 39\u00b0C and then starving them of serum to remove the influence of serum-contributed growth factors from the gene expression patterns observed. The temperature switching is unique to the YAMC cells and their derivatives (mp53/Ras cells) used in our exemplar experiments. We measured gene expression of genes downstream of the perturbed gene in the absence of serum to be consistent with our prior work using YAMC and mp53/Ras cells (McMurray et\u00a0al., 2008[href=https://www.wicell.org#bib4]). For any other cell types or cell lines, it should be determined whether growth in the presence or absence of serum is more appropriate, as serum can have substantial impact on observed gene expression patterns. Following serum starvation, cells are harvested for RNA isolation and reverse transcription to generate cDNA that is used for TaqMan quantitative PCR assays.\nTrypsinize and count perturbed cell populations to be used experimentally. Include an empty vector control in each experiment performed. For mp53/Ras cells, plate 250,000 cells per 10-cm dish.\nPlate cells onto fresh collagen-coated dishes with 39\u00b0C media, which excludes interferon\u00a0gamma and any eukaryotic-selective agents from the media (i.e., should be free of puromycin or similar). The media should contain anti-microbials, here kanamycin and gentamycin.\nAllow cells to adhere and grow in 39\u00b0C CO2 incubator for 48 h.Serum-starve cells by aspirating media from each dish and replacing with fresh 39\u00b0C media without serum.\nAllow cells to grow for an additional 24\u00a0h in 39\u00b0C CO2 incubator.\nHarvest cells following trypsinization. Inhibit trypsin activity using 39\u00b0C media with serum, then immediately collect each cell population into 15\u00a0mL conical tubes (one per perturbation) and pellet cells by centrifugation at 500\u00a0g for 5\u00a0min at 4\u00b0C.\nAspirate media and trypsin from cell pellets. Re-suspend cells in 5\u00a0mL 1\u00d7 PBS (can be room temperature or ice-cold), then pellet again by centrifugation.\nAspirate PBS from cell pellets. Use immediately for RNA extraction OR snap freeze and store at \u221220\u00b0C.\nCell pellets can be stored at \u221220\u00b0C for up to one month prior to RNA extraction.\nIsolate RNA by following manufacturer\u2019s protocol for the QIAGEN RNeasy Mini kit with On-Column DNase Digestion.\nIsolated RNA can be stored at \u221220\u00b0C for two years or longer if handled in an RNase-free manner and not exposed to repeated freeze-thaw cycles.\nPrepare reverse transcription reactions:\nDenature 10\u00a0\u03bcg of each RNA sample to be used by incubation on a heat block or in a water bath at 70\u00b0C for 10\u00a0min.\nPlunge samples into ice to stop denaturation. Then add the components listed in the table below to each, keeping samples on ice.\nFor multiple reverse transcription reactions, all components except for RNA and water can be made into a master mix for the appropriate number of reactions and aliquoted into RNA\u00a0+ water for necessary final concentration.\nOnce all components have been added, incubate samples at 42\u00b0C for 60\u00a0min.\nHeat inactivate RT enzyme by shifting samples to 70\u00b0C for 10\u00a0min.\nHold samples on ice and proceed to PCR setup OR store at \u221220\u00b0C for long-term storage.\ntable:files/protocols_protocol_2051_5.csv\ntable:files/protocols_protocol_2051_6.csvHold samples on ice and proceed to PCR setup OR store at \u221220\u00b0C for long term storage (step 24).\nPrepare TaqMan qPCR assays reaction mix.\nThe reactions were run on TaqMan Low Density Arrays, 384 well cards with primer pairs and probes pre-loaded into each well.\nEach reaction contained forward and reverse primer at a final concentration of 900\u00a0nM each and a TaqMan MGB probe (6-FAM) at 250\u00a0nM final concentration.\nFor each sample:\ntable:files/protocols_protocol_2051_7.csv\nLoad mixture into each of 8 ports on the array at 100\u00a0\u03bcL per port.\nEach individual sample of cDNA sample was processed on a separate card.\nNote: As described below in step 32 and further sections on TopNet modeling, four biological replicates of each perturbation were measured.\nNote: As noted in step 13 of the before you begin[href=https://www.wicell.org#before-you-begin] section, we considered a replicate to be an independently derived population of perturbed cells. Each replicate of a given perturbation had an empty vector control population of cells that was derived in parallel with the perturbed cell populations.\nSeal arrays with a TaqMan Low-Density Array Sealer (Applied Biosystems) to prevent cross-contamination.\nRun real-time amplifications on an ABI Prism 7900HT Sequence Detection System (Applied Biosystems) with a TaqMan Low Density Array Upgrade.\ntable:files/protocols_protocol_2051_8.csv\nAfter real-time amplification is complete, obtain threshold cycle (Ct) values via Sequence Detection Software (SDS, Applied Biosystems) following manufacturer\u2019s instructions.\nSDS is graphical user interface software designed for use by wet-lab biologists. For this protocol, each sample was analyzed individually using this software package.\nExport Ct values into a .csv file for use in the following steps.\nThe Ct values from each sample were merged into a single table with probe sets as rows and perturbed samples or matched vector controls in each column.\nPrepare gene expression data for network modelingTiming: \u223c10\u00a0min\nThis step prepares the data for input to the network estimation algorithm.\nRead in gene expression measurements for each perturbation and control experiment.\nAs an example, here we read in Ct values from the crgnet experimental data R package available via github.\nif(!require(remotes)){\n\u00a0\u00a0install.packages(\"remotes\")\n}\nremotes::install_github('mccallm/crgnet')\nlibrary('crgnet')\ndata(\"crgdata\")\nNormalization and missing data imputation\nTiming: \u223c1 h\nSelect one or more control features to normalize the data. In this example, we use Becn1 as a control because it appears to track the lower quantile of the distribution of expression across the controls reasonably well (Figure\u00a01[href=https://www.wicell.org#fig1]A) and has relatively low variability across the control samples compared to the other genes (Figure\u00a01[href=https://www.wicell.org#fig1]B). This suggests that normalizing to this house-keeping gene may perform well in these data.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2051-Fig1.jpg\nFigure\u00a01. Rationale for the use of Becn1 to normalize qPCR data\n(A) shows the cycle threshold value (Ct) distribution for all measured genes in each control sample. Each box represents an independently derived cell population transduced with an empty vector.\n(B) shows the median average deviation (MAD) versus the median Ct value for each measured gene across all control samples. The red dot denotes Becn1 in both Panel A & B.\nExamine variability in the distribution of expression across control samples. While some variability might be ascribed to the different control vectors, it is unlikely that the effect would be of this magnitude and consistency. Moreover, note that while there is substantial variability in the location of the expression distribution across the empty vector control samples, the range (e.g., IQR) remains relatively constant (except for one sample with the lowest overall expression that also had noticeably higher variability).\nictl <- which(is.na(crgdata$pGene1))\ncontrolSamples <- assay(crgdata)[ ,ictl]\nboxplot(controlSamples, xaxt=\"n\", xlab=\"Control Samples\")\nbecn1\u00a0<- controlSamples[rownames(controlSamples)==\"Becn1\",]\npoints(becn1, pch=20, col=\"red\")\nlegend(\"bottom\", pch=20, col=\"red\", \"Becn1\")The house-keeping gene, Becn1, appears to track the lower quantile of the distribution of expression across the controls reasonably well (Figure\u00a01[href=https://www.wicell.org#fig1]A). This suggests that normalizing to this house-keeping gene may perform well in these data.\nFurther examine the suitability of Becn1 for normalization by looking at the median absolute deviation (MAD) vs median expression.\ncontrolMedians <- apply(controlSamples, 1, median)\ncontrolMADs <- apply(controlSamples, 1, mad)\nplot(x=controlMedians, y=controlMADs, pch=20, ylab=\"MAD\", xlab=\"Median\")\nind <- which(rownames(controlSamples)==\"Becn1\")\npoints(x=controlMedians[ind], y=controlMADs[ind], pch=20, col=\"red\")\ntext(x=controlMedians[ind], y=controlMADs[ind], \"Becn1\", pos=4)points(becn1, pch=20, col=\"red\")\nlegend(\"bottom\", pch=20, col=\"red\", \"Becn1\")\nBecn1 has relatively low variability across the control samples compared to the other genes (Figure\u00a01[href=https://www.wicell.org#fig1]B).\nPerform normalization and missing data imputation.\nExamine the relationship between the proportion of non-detects (those reactions failing to produce fluorescence values above a certain threshold) and the average observed expression value in the empty vector control samples. When the plot argument is set to TRUE, the model_prep function returns a scatterplot of the proportion of non-detects versus average expression for each gene across all control samples.\ncrgprepL1\u00a0<- model_prep(crgdata, plot=TRUE)\nNote: The proportion of non-detects general increases for larger Ct values (lower gene expression). However, if this is not the case, an alternative imputation procedure, such as mean imputation, should be used. Additionally, if there are a large number of non-detects spread randomly across all measured genes, this may indicate poor sample quality.\nAdditionally, the model_prep function converts the data to the qPCRset object format needed to impute the non-detects and returns normalization factors. Specifically, the sampleType field of the sample annotation contains a unique description of each experiment: which gene(s) were perturbed and the direction of perturbation. This is used to define replicates for use in the imputation.Treat non-detects as non-random missing data and impute using the R/Bioconductor package nondetects (McCall et\u00a0al., 2014[href=https://www.wicell.org#bib2]). This replaces missing values (non-detects) with an imputed values based on an estimated missing data mechanism as well as the expression values seen in replicate experiments.\nNote: Attempting to measure lowly expressed genes will result in a higher number of non-detects. As the number of non-detects increases, the imputation procedure becomes less reliable. Therefore, while there isn\u2019t a universal threshold for the applicability of the non-detects imputation, there is a trade-off between the ability to measure lowly expressed genes and the reliability of the imputation. Often this can be addressed by increasing the number of replicates such that the number of observed values for each gene is sufficiently large.\ncrgdataImputed <- qpcrImpute(crgprepL1$object, groupVars=\"sampleType\")\nNote: This function takes a while to run.\nNormalize the data. Here, we normalize the data to the house-keeping gene, Becn1.\nlibrary(HTqPCR)\ncrgdataNorm <- normalizeCtData(crgdataImputed, deltaCt.genes=\"Becn1\")\nexprs(crgdataNorm) <- -exprs(crgdataNorm)\nIf we were normalizing to multiple control genes, we would first calculate the mean expression of the control genes then proceed as above.\nNote: After normalization, we have lost the cycle threshold (Ct) interpretation but retained the inverse relationship between expression values (on the Ct scale) and the amount of transcript in the sample. Therefore, we consider the negative \u0394Ct value as our measure of normalized expression.\nQuantify response to perturbations\nTiming: \u223c30\u00a0min\nIn the previous sections we have dealt with non-detects (missing values) and normalized the data. We now turn out attention to assessing which genes are up- or down-regulated in response to each perturbation.\nCalculate \u0394\u0394Ct values that quantify the change in expression from controls for each perturbation.Examine whether a given perturbed sample is more similar to the control sample from the same batch then to control samples from other batches by running the check_matched_controls function. If there is a batch-effect, it is advantageous to compare each perturbed sample to the control sample from the same batch.\ncheck_matched_controls(crgdataNorm)\nIn almost all cases, the control sample from the same batch is the most similar to the perturbed sample (Figure\u00a02[href=https://www.wicell.org#fig2]). This suggests that there is a difference between batches and motivates the calculation of paired \u0394\u0394Ct values as our measure of normalized change in expression in response to each perturbation. If there were no difference between batches, the distance from a given perturbed sample to its matched control (points in Figure\u00a02[href=https://www.wicell.org#fig2]) would be randomly distributed within the set of pairwise differences. An eyeball test is often sufficient to assess this; however, a permutation-based statistical test could also be used. If a batch effect does not exist, then we can simply compare each perturbed sample to the average of all the control samples to compute unpaired \u0394\u0394Ct values.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2051-Fig2.jpg\nFigure\u00a02. Matched control samples capture potential batch effects\nFor each perturbed sample (x-axis), the distribution of Euclidean distances from that the vector of Ct values for that perturbed sample to each control sample is shown. The matched control sample for each perturbed sample is highlighted in red. In general, a perturbed sample is most similar to its corresponding control sample, suggesting that matched control and perturbed samples are similarly affected by potential batch effects.\nCalculate paired \u0394\u0394Ct values by computing the difference in expression between each perturbed sample and its corresponding control sample from the same batch by running calculate_ddCt().\nddCt <- calculate_ddCt(crgdataNorm)One final check of the non-detect imputation procedure from before is to examine the distribution of residuals stratified by the presence of imputed non-detect values. These can exist in either the perturbed sample, the control sample, or both samples.\nresids <- examine_residuals(ddCt, plot=TRUE)\nHere, we see what one would expect: a median of zero and roughly equal spread when there are no non-detects, a median slightly below zero when there is a non-detect in only the perturbed sample, and a median slightly above zero when there is a non-detect in only the control sample (Figure\u00a03[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2051-Fig3.jpg\nFigure\u00a03. The effect of non-detects on estimates of gene expression\nThe distribution of residuals stratified by the presence of imputed missing values (non-detects) is shown. Note that on average, a non-detect in the perturbed sample results in slightly negative residuals, while a non-detect in the control sample results in slightly positive residuals.\nCalculate approximate z-scores for each perturbation after removing any control genes. Here, we remove the control gene Becn1.\nddCt_no_ctl <- ddCt[-which(rownames(ddCt)==\"Becn1\"), ]\nzscores <- calculate_zscores(ddCt_no_ctl)\nCalculate the probability of up- / down-regulation in response to each perturbation by fitting a uniform / normal / uniform mixture model.\nprobabilities <- calculate_probs(zscores)\nFilter genes that were measured but are not perturbed in any experiment. While these genes are useful in modeling the missing data mechanism for non-detect imputation and estimating probabilities of up- / down-regulation, they will not be used in the subsequent network modeling and can be removed at this point. Here, we also filter several samples that do not represent the type of perturbation being modeled, a single perturbation back to normal expression levels.\nperts <- gsub(\":.+\",\"\",colnames(probabilities))\nind <- which(!rownames(probabilities) %in% perts)\nprobabilities <- probabilities[-ind, -c(1,9,12,13,20,23)]\n-c(1,9,12,13,20,23)\nFormat the probabilities for input to the network model fitting algorithm.networkInputData <- format_network_input(probabilities)\nOptional: Connectivity graph analysis. Either the z-scores or probabilities could be thresholded to produce a connectivity graph, in which nodes represent genes and directed edges denote that perturbation of the parent gene results in a change in expression of the child gene. Here, we create a connectivity graph based on the probabilities by thresholding the probabilities at an absolute value of 0.5. In other words, if the probability that perturbation of gene A results in a change in expression of gene B exceeds 0.5 then an edge from gene A to gene B is included in the connectivity graph.\nprobs <- networkInputData$ssObj\ncolnames(probs) <- rownames(probs)\nprobs[which(abs(probs)<0.5, arr.ind=TRUE)] <- 0\ndiag(probs) <- 0\nNext, we use the network package to create and plot the connectivity graph (Figure\u00a04[href=https://www.wicell.org#fig4]). We also add data on tumor inhibition and direction of response.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2051-Fig4.jpg\nFigure\u00a04. A connectivity graph showing the effect of perturbation for 20 CRGs\nArrows originating from a node denote the effect of perturbation of that node. Arrows terminating at a node represent the effect on that node, and the edge color denotes up-regulation (red) or down regulation (blue). The tumor inhibitory effect of each perturbation is encoded by the color of each node, tumor inhibitory (yellow) or not tumor inhibitory (grey).\nlibrary(network)\ncgraph <- network(t(probs), matrix.type=\"adjacency\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0ignore.eval=FALSE, names.eval=\"probs\")\ndata(\"tumor_inhibition\")\nset.vertex.attribute(cgraph, \"tumor_inhibition\", tumor_inhibition$TumorEffect)\nset.edge.attribute(cgraph, \"direction\", sign(get.edge.attribute(cgraph,\"probs\")))\nplot(cgraph, displaylabels=TRUE, mode=\"circle\", boxed.labels=TRUE,\n\u00a0\u00a0label.bg=ifelse(get.vertex.attribute(cgraph, \"tumor_inhibition\")==\"Smaller\", \"yellow\", \"grey\"),\n\u00a0\u00a0edge.col=ifelse(get.edge.attribute(cgraph, \"direction\")==1, \"red\", \"blue\"))\nlegend(\"topleft\", c(\"Tumor Inhibitory\",\n\u00a0\u00a0\"Not Tumor Inhibitory\"), title=\"Node Color\",\n\u00a0\u00a0fill=c(\"yellow\",\"grey\"))\nlegend(\"topright\", c(\"Up-regulation\", \"Down-regulation\"), title=\"Edge Color\",\n\u00a0\u00a0col=c(\"red\",\"blue\"), lty=1, lwd=5)\nPerform network modeling\nTiming: \u223c5\u00a0days\nThis step estimates the network of interactions.Network modeling. Use a ternary network model that accounts for the dynamic nature of gene regulatory networks and facilitates the evaluation of uncertainty to model a gene regulatory network. Specifically, use a parallel tempering algorithm (Swendsen and Wang, 1986[href=https://www.wicell.org#bib5]) to search the model space for networks that produce attractors that are most similar to the observed steady state data. Pseudocode for the network modeling algorithm is supplied in Methods S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2051-Mmc1.pdf], while a reproducible workflow of the analyses in this paper is included as the vignette of the crgnet package.\nNote: Unlike previous approaches, here we have incorporated uncertainty in the differential expression estimates via probabilities of up- / down-regulation. This allows the network model to give more weight to data points with higher certainty. Additionally, the ability to produce non-integer network scores eased transitions between network models and significantly decreased computational time.\nHere, we show example code to fit a network using 1,000,000,000 cycles in parallel across 20 processors with temperatures ranging from 0.001 to 1. These parameters should be chosen such that either the network reaches a score of zero (indicating a fit that perfectly explains the observed data) or until additional cycles do not produce a reduction in the network score. The code used to produce these network fits is shown below:\nlibrary(\"ternarynet\")\ndata(\"crgnet_scores\")\nresults <- parallelFit(experiment_set=crgnet_scores,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0max_parents=4,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0n_cycles=1e9,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0n_write=10,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0T_lo=0.001,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0T_hi=1,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0target_score=0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0n_proc=20,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0logfile=\"tnet-fit.log\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0seed=as.integer(112358)\n)\nNote: The computational time required to generate these network models is substantial: each independent network fit presented in McMurray et\u00a0al. (2021)[href=https://www.wicell.org#bib3] took approximately 12\u00a0h to run in parallel on 20 compute nodes. However, less than 1 GB of RAM was sufficient to fit these network models.\nCreate network summaries and visualization\nTiming: \u223c1\u00a0h (\u223c1\u00a0month with optional steps included)This step prepares the data for input to the network estimation algorithm.\nSummary statistics can be computed by calculating the proportion of networks in which a given feature or features are present. One can also examine the transition functions, attractors, and trajectories all stored in the fits object. One of the most common ways to visualize a network model is to present the topology. Here we calculate proportion of networks in which a given gene is a parent of another given gene.\ndata(networkInputData)\ndata(networkFits)\ntopo <- topology(fits)\nrownames(topo) <- rownames(networkInputData$ssObj)\ncolnames(topo) <- rownames(networkInputData$ssObj)\nThis information can be exported to Cytoscape or other network visualization software to create a graphical representation of these results.\nOptional: One question of interest is whether it is significant that one can obtain a low scoring network model. This can be examined by permuting the network input data. For each gene, permute its response to all of the experiments while retaining the number of experiments to which each gene responds. In other words, the number of parents in a connectivity graph remains constant but which other genes are parents\u2019 changes. Then fit a network model to the permuted data using the same parameters as above and repeat this process to generate multiple permuted fits. Here, we load precomputed permuted fits from the crgnet package.\ndata(networkFits)\ndata(permutedNetworkFits)\nreal_scores <- sapply(fits, function(x) x$unnormalized_score)\npermuted_scores <- sapply(pfits, function(x) x$unnormalized_score)\nhist(permuted_scores, breaks=25, main=\"\",\nxlab=\"Network Scores\")\nrug(real_scores, lwd=3)The network scores based on the real data are less than nearly all the scores based on the permuted data (Figure\u00a05[href=https://www.wicell.org#fig5]; empirical p-value \u2264 0.01). With the current network constraints, we can obtain good scores for the real data but not for the permuted data. This suggests that we are not extensively overfitting these data and that the current network constraints are reasonable.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2051-Fig5.jpg\nFigure\u00a05. Network scores produced by real data are lower than those produced by permuted data\nFor each gene, its response to all of the experiments was permuted while retaining the number of experiments to which each gene responds. In other words, the number of parents in a connectivity graph was held constant but the identities of the parent genes were changed. A network model was then fit to the permuted data using the same parameters as used for the real data, and this process was repeated 100 times to generate multiple permuted fits. A\u00a0histogram of network scores corresponding to the permuted fits is shown. Tick marks on the x-axis denote the scores produced by the real data network fits.\nOptional: To examine whether we could obtain similar fits with a simpler network model, one can vary the in-degree and compare model fits. As an example, we reran the network modeling algorithm with the max_parents reduced from 4 to 3 and also considered a more complex model by increasing the max_parents parameter to 5. We ran both models on permuted data as well as the real data.\ndata(networkFits)\ndata(networkFitsIndeg3)\ndata(networkFitsIndeg5)\nindeg4_scores <- sapply(fits, function(x) x$unnormalized_score)\nindeg3_scores <- sapply(fits3, function(x) x$unnormalized_score)\nindeg5_scores <- sapply(fits5, function(x) x$unnormalized_score)\ndata(permutedNetworkFits)\ndata(permutedNetworkFitsIndeg3)\ndata(permutedNetworkFitsIndeg5)\nindeg4_permuted_scores <- sapply(pfits, function(x) x$unnormalized_score)\nindeg3_permuted_scores <- sapply(pfits3, function(x) x$unnormalized_score)\nindeg5_permuted_scores <- sapply(pfits5, function(x) x$unnormalized_score)\nplot(x=jitter(rep(c(1:6), c(length(indeg3_scores), length(indeg3_permuted_scores), length(indeg4_scores), length(indeg4_permuted_scores), length(indeg5_scores), length(indeg5_permuted_scores)))),y=c(indeg3_scores, indeg3_permuted_scores, indeg4_scores, indeg4_permuted_scores, indeg5_scores, indeg5_permuted_scores),\n\u00a0\u00a0ylab=\"Network Score\", xlab=\"\", xaxt=\"n\")\naxis(1, line=1.5, at=c(1.5,3.5,5.5), labels\u00a0= c(\"In-degree 3\", \"In-degree 4\", \"In-degree 5\"), tick=FALSE, cex.axis=1.25)\naxis(1, at=c(1:6), labels\u00a0= rep(c(\"Real\", \"Permuted\"), 3))\nNote: Figure\u00a06[href=https://www.wicell.org#fig6] illustrates that increasing the in-degree cap from 3 to 4 results in a sizeable\u00a0reduction in the model score; however, increasing the in-degree cap to 5 produces only a modest improvement (the in-degree 4 network models already do quite well). Regardless of the in-degree cap, better scores were achieved using the real data (as expected). The separation between real and permuted scores is greatest for an in-degree cap of 4. This lends further support to the choice of a maximum in-degree of four for these data.\nOptional: Comparison between network models generated using different in-degree thresholds. Here, we demonstrate the effect of the maximum in-degree on the resulting network topology. Note that a larger in-degree threshold will produce a network with more edges. Of primary interest is whether the high confidence edges are retained for varying in-degrees.\ndata(networkFits)\ndata(networkFitsIndeg3)\ndata(networkFitsIndeg5)\ntopo_i4\u00a0<- topology(fits)\ntopo_i3\u00a0<- topology(fits3)\ntopo_i5\u00a0<- topology(fits5)\nrownames(topo_i3) <- colnames(topo_i3) <-\n\u00a0\u00a0rownames(topo_i4) <- colnames(topo_i4) <-\n\u00a0\u00a0rownames(topo_i5) <- colnames(topo_i5) <- rownames(networkInputData$ssObj)\npar(mfrow=c(1,2))\nplot(x=topo_i4, y=topo_i3, pch=20,\n\u00a0\u00a0xlab=\"In-degree 4 edge proportions\",\n\u00a0\u00a0ylab=\"In-degree 3 edge proportions\",\n\u00a0\u00a0main=paste0(\"Correlation\u00a0= \", round(cor(as.vector(topo_i3), as.vector(topo_i4)), digits\u00a0= 2)))\nabline(v=0.8)\nplot(x=topo_i4, y=topo_i5, pch=20,\n\u00a0\u00a0xlab=\"In-degree 4 edge proportions\",\n\u00a0\u00a0ylab=\"In-degree 5 edge proportions\",\n\u00a0\u00a0main=paste0(\"Correlation\u00a0= \", round(cor(as.vector(topo_i4), as.vector(topo_i5)), digits\u00a0= 2)))\nabline(v=0.8)\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2051-Fig6.jpg\nFigure\u00a06. The effect of different in-degree limits on network scores and the ability to distinguish between real and permuted dataThe network modeling algorithm was run with in-degree limits of 3, 4, and 5 on both real and permuted data. Larger in-degree limits produce generally better scores, and the real data produce better scores than the permuted data across all three in-degree limits.\nTesting tumor formation capacity of cells with two genetic perturbations based on features of the network model\nTiming: 5\u20136\u00a0weeks\nHere, we describe the process of measuring tumor formation in perturbed mp53/Ras cell populations and linking these measurements to features of the network model. This is done by perturbation of multiple target genes selected from among the network nodes in mp53/Ras cells. Subsequently, the perturbed cell populations are implanted into allogeneic, immune compromised mice and tumor growth is measured by monitoring tumor size over time. We perform these studies in CD-1 nude mice (Crl:CD-1-Foxn1nu, Charles River Laboratories, purchased at 6\u20138\u00a0weeks of age), but a number of other immune compromised mouse strains could be used (e.g., NOD/SCID animals).\nFollow steps 1 through 16 in the step-by-step method details[href=https://www.wicell.org#step-by-step-method-details] section to generate cell populations with desired perturbation of combinations of target genes selected based on network modeling results.\nTrypsinize cells following 48\u00a0h of growth at 39\u00b0C. Pellet cells, wash with 1\u00d7 PBS (as in step 17 above) BUT DO NOT FREEZE.\nRe-suspend cell pellets in 500\u00a0\u03bcL of RPMI with no additives and keep on ice. Count cells with an automated cell counter or hemacytometer.\nDilute cells to 5\u00a0\u00d7\u00a0105 per 100\u00a0\u03bcL in RPMI 1640 media with no additives.\nImplant 100\u00a0\u03bcL of re-suspended cells into each flank of each experimental animal (i.e., two implantations per animal).\nAnimals should be implanted with only one type of cell population \u2013 vector control or a single gene perturbation per animal.For experiments in McMurray et\u00a0al. (2021)[href=https://www.wicell.org#bib3], six implantations were done using cells from each perturbed cell population. In parallel, six implantations were done with cells from a freshly derived empty vector control population included in each experiment.\nNote: At implantation, mark or number animals in some manner (ear punch, tattoo or alternative method) to keep track of tumor growth per animal per week.\nAt 7, 14, 21 and 28\u00a0days post-implantation, measure tumor diameter by caliper at two distinct points of each tumor for each animal on both flanks independently.\nUse tumor diameter data to calculate tumor volume using the standard formula for volume of a sphere (volume=(4/3)\u03c0r3).\nExamine and quantify the association between predicted gene interactions and phenotypic outcomes (here tumor size).", "Step-by-step method details\nStep-by-step method details\nPerform SELEX (repeat these steps 2\u20136 times)\nTiming: 1.5\u00a0days (\u00d72\u20136)\nDuring SELEX, a library of random oligonucleotides is mixed with a DNA-binding protein of interest fused with an affinity tag. Protein-DNA complexes are purified and bound sequences are amplified by the polymerase chain reaction (PCR). This material is re-used for successive cycles of SELEX until most of the library contains high affinity binding sites. For transcription factors, 2\u20133 cycles are usually sufficient for successful HT-SELEX (Jolma et\u00a0al., 2010[href=https://www.wicell.org#bib3], 2013[href=https://www.wicell.org#bib4]). However, we performed up to 6\u00d7 SELEX cycles to characterize SALL4 ZFC4 which promiscuously binds to multiple AT-rich sequences (Pantier et\u00a0al., 2021[href=https://www.wicell.org#bib9]).\nPrepare buffers.\nOn the day of the experiment, prepare a mastermix of \u201cSELEX binding buffer\u201d (SELEX buffer supplemented with 5\u00a0\u03bcg/mL poly(dI-dC) and 0.5\u00a0mM DTT) and \u201cSELEX wash buffer\u201d (SELEX buffer supplemented with 0.5\u00a0mM DTT).\ntable:files/protocols_protocol_1750_11.csv\nKeep on ice until use.\ntable:files/protocols_protocol_1750_12.csv\nKeep on ice until use.\nEquilibrate Ni Sepharose 6 Fast Flow beads in SELEX binding buffer.\nTake out the required amount of Ni Sepharose 6 Fast Flow resin (55\u00a0\u03bcL\u00a0\u00d7\u00a0number of samples) and transfer into a in a 1.5\u00a0mL tube (e.g., for 6 samples, take out 330\u00a0\u03bcL of Ni Sepharose 6 Fast Flow resin).\nNote: The total amount of resin includes a 10% excess to account for small inaccuracies when pipetting multiple samples.\nNote: If a large volume of Ni Sepharose 6 Fast Flow resin is required, split into several 1.5\u00a0mL tubes (maximum 500\u00a0\u03bcL resin/tube) and prepare additional SELEX binding buffer accordingly.\nAdd 1\u00a0mL of SELEX binding buffer and resuspend beads thoroughly by inverting the tube multiple times.\nCentrifuge for 1\u00a0min at 400\u00a0\u00d7\u00a0g. Discard the supernatant without disturbing the beads pellet.Wash beads 2\u00d7 more times (steps 2b-c).\nResuspend beads in SELEX binding buffer in the initial volume of resin pipetted in step a (e.g., for 6 samples, resuspend in a total volume of 330\u00a0\u03bcL). Keep on ice until use.\nIncubate DNA-binding proteins with SELEX libraries.\nSet up SELEX reactions in 1.5\u00a0mL tubes:\ntable:files/protocols_protocol_1750_13.csv\nNote: For the first SELEX cycle, use 1.5\u00a0\u03bcg of \u201ccycle 0\u201d random library (see generation of cycle 0 libraries). For subsequent cycles, use 200\u00a0ng of SELEX library from the previous cycle (e.g., To perform SELEX cycle 2, use library amplified at the end of cycle 1).\nNote: It is important to include a negative control SELEX reaction, without addition of proteins, to control for any sequence bias that could be associated with repeated PCR cycling. It is also advised to perform SELEX with independent libraries, which are used as technical replicates (see generation of cycle 0 libraries).\ne.g., Sample 1: SALL4 ZFC4\u00a0+ library 1 (replicate 1). \u00a0\u00a0Sample 2: SALL4 ZFC4\u00a0+ library 2 (replicate 2). \u00a0\u00a0Sample 3: SALL4 ZFC4\u00a0+ library 3 (replicate 3). \u00a0\u00a0Sample 4: Negative control (no protein)\u00a0+ library 1 (replicate 1). \u00a0\u00a0Sample 5: Negative control (no protein)\u00a0+ library 2 (replicate 2). \u00a0\u00a0Sample 6: Negative control (no protein)\u00a0+ library 3 (replicate 3).\nIncubate on a rotating wheel for 10\u00a0min at room temperature.\nPurify protein-DNA complexes.\nTo capture protein-DNA complexes, add 50\u00a0\u03bcL of equilibrated Ni Sepharose 6 Fast Flow resin (from step 2) to each SELEX sample.\nIncubate for 20\u00a0min on a rotating wheel at room temperature.\nTo remove non-specifically bound DNA-protein complexes, add 1\u00a0mL of SELEX wash buffer and resuspend beads thoroughly by inverting the tube multiple times.Centrifuge for 1\u00a0min at 400\u00a0\u00d7\u00a0g. Discard the supernatant without disturbing the beads pellet.\nWash beads 4\u00d7 more times (steps 4c-d).\nResuspend the resin in 100\u00a0\u03bcL H2O.\nNote: Elution of DNA from the beads is not necessary, as this material can be directly used as a template for PCR amplification of SELEX libraries.\nPause point: The resin (protein-DNA complexes) can be stored at \u221220\u00b0C (long term). This material can be used at a later time for PCR amplification.\nPCR-amplify enriched DNA.\nCritical: The amount of DNA bound to the resin is unknown and usually varies between SELEX samples. Therefore, it is important to empirically determine the optimal number of PCR cycles to amplify each SELEX library (see the following steps).\nFor each SELEX sample, prepare a PCR mastermix (for 4\u00d7 PCR reactions) in a 1.5\u00a0mL tube:\ntable:files/protocols_protocol_1750_14.csv\nNote: It is not recommended to increase the amount of resin (DNA template) in the mix, as an excess can inhibit the PCR reaction.\nDivide mastermix between 4 PCR tubes (50\u00a0\u03bcL/tube).\nNote: Before transferring the mix to PCR tubes, ensure that beads are homogeneously resuspended by pipetting up and down multiple times.\nRun each of the 4\u00d7 PCR reactions with a different PCR programme (increasing numbers of PCR cycles):\ntable:files/protocols_protocol_1750_15.csv\ntable:files/protocols_protocol_1750_16.csv\ntable:files/protocols_protocol_1750_17.csv\ntable:files/protocols_protocol_1750_18.csv\nTo control the amplification of libraries, run a small amount of PCR reaction (5\u00a0\u03bcL) on a 10% polyacrylamide gel and stain with a 0.5\u00a0\u03bcg/mL ethidium bromide solution (see Figure\u00a03[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1750-Fig3.jpg\nFigure\u00a03. 10% polyacrylamide gel showing the optimization of PCR conditions to amplify enriched DNA from Ni Sepharose beads following a cycle of SELEXFor this sample, we purified the product following to 12\u00d7 PCR cycles, as it showed a robust amplification of the library (83\u00a0bp) without detectable heteroduplexes (see troubleshooting 1[href=https://www.wicell.org#sec7.1]).\nFor each SELEX sample, select the optimal PCR reaction and discard other tubes (see Figure\u00a03[href=https://www.wicell.org#fig3]).\nPurify DNA using the Qiagen MinElute PCR purification kit and following manufacturer\u2019s protocol. Elute with 20\u00a0\u03bcL of EB Buffer (included in the kit, 10\u00a0mM Tris-HCl pH8.5) or H2O.\nNote: A single PCR reaction will yield enough DNA to proceed with the protocol.\nEvaluate DNA concentration and integrity of purified SELEX libraries using a Nanodrop spectrophotometer.\nPause point: Store purified SELEX libraries at \u221220\u00b0C (long term).\nUse DNA as an input to repeat an additional cycle of SELEX (N+1).\nCritical: Remember to save an aliquot of purified SELEX library (\u224820\u00a0ng) for high-throughput sequencing (see generation of HT-SELEX libraries for Illumina sequencing).\nGenerate HT-SELEX libraries for Illumina sequencing\nTiming: 1.5\u00a0days\nAfter multiple SELEX cycles, DNA libraries contain a significant proportion of high affinity DNA binding sites for the target protein. This step describes the conversion of SELEX libraries into HT-SELEX libraries containing Illumina adapters and unique barcodes (see Figure\u00a04[href=https://www.wicell.org#fig4]). These samples are subsequently pooled and submitted to high-throughput sequencing to reveal preferred DNA motifs.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1750-Fig4.jpg\nFigure\u00a04. Strategy for generating HT-SELEX libraries for high-throughput sequencing\nFollowing PCR, each SELEX library contains Illumina sequencing adapters (P5/P7) and unique barcodes (i7 indexes).\nSelect SELEX samples to submit to high-throughput sequencing.Note: For most purposes, it is not necessary to sequence libraries for all SELEX cycles. However, it is important to sequence initial random libraries (cycle 0) in order to assess the initial frequency of all DNA motifs. While libraries at the last SELEX cycle will contain the highest proportion of DNA binding motifs, sequencing intermediate SELEX cycles will provide useful information regarding the kinetics of enrichment of preferred DNA motifs.\nCritical: For all selected SELEX cycles, include technical replicates (i.e., different libraries) as well as a negative control (i.e., without addition of DNA-binding proteins). These important controls will allow the measurement of technical bias during the SELEX protocol (e.g., base composition bias of DNA polymerase during PCR amplification).\nHere is an example of SELEX dataset for SALL4 ZFC4 (6\u00d7 SELEX cycles, 3 replicates):\n\u00a0\u00a0Sample 1: Cycle0 - Initial random SELEX library 1 (replicate 1).\n\u00a0\u00a0Sample 2: Cycle0 - Initial random SELEX library 2 (replicate 2).\n\u00a0\u00a0Sample 3: Cycle0 - Initial random SELEX library 3 (replicate 3).\n\u00a0\u00a0Sample 4: Cycle 1 - SALL4 ZFC4 (replicate 1).\n\u00a0\u00a0Sample 5: Cycle 1 - SALL4 ZFC4 (replicate 2).\n\u00a0\u00a0Sample 6: Cycle 1 - SALL4 ZFC4 (replicate 3).\n\u00a0\u00a0Sample 7: Cycle 1 - Negative control (no protein) (replicate 1).\n\u00a0\u00a0Sample 8: Cycle 1 - Negative control (no protein) (replicate 2).\n\u00a0\u00a0Sample 9: Cycle 1 - Negative control (no protein) (replicate 3).\n\u00a0\u00a0Sample 10: Cycle 3 - SALL4 ZFC4 (replicate 1).\n\u00a0\u00a0Sample 11: Cycle 3 - SALL4 ZFC4 (replicate 2).\n\u00a0\u00a0Sample 12: Cycle 3 - SALL4 ZFC4 (replicate 3).\n\u00a0\u00a0Sample 13: Cycle 3 - Negative control (no protein) (replicate 1).\n\u00a0\u00a0Sample 14: Cycle 3 - Negative control (no protein) (replicate 2).\n\u00a0\u00a0Sample 15: Cycle 3 - Negative control (no protein) (replicate 3).\n\u00a0\u00a0Sample 16: Cycle 6 - SALL4 ZFC4 (replicate 1).Sample 17: Cycle 6 - SALL4 ZFC4 (replicate 2).\n\u00a0\u00a0Sample 18: Cycle 6 - SALL4 ZFC4 (replicate 3).\n\u00a0\u00a0Sample 19: Cycle 6 - Negative control (no protein) (replicate 1).\n\u00a0\u00a0Sample 20: Cycle 6 - Negative control (no protein) (replicate 2).\n\u00a0\u00a0Sample 21: Cycle 6 - Negative control (no protein) (replicate 3).\nPCR amplify HT-SELEX libraries.\nCritical: PCR conditions were optimized to amplify HT-SELEX libraries. However, the amount of DNA template and the number of PCR cycles might need to be adjusted in order to avoid the formation of heteroduplexes (see troubleshooting 1[href=https://www.wicell.org#sec7.1]).\nFor each SELEX sample, set a PCR reaction in a PCR tube using a unique reverse primer (Seqlib RV):\ntable:files/protocols_protocol_1750_19.csv\nNote: Each \u201cSeqlib RV\u201d primer contains a distinct barcode which will allow the pooling of multiple samples for high-throughput sequencing (see materials and equipment[href=https://www.wicell.org#materials-and-equipment]).\nRun the following PCR programme:\ntable:files/protocols_protocol_1750_20.csv\nTo control the amplification of libraries, run a small amount of PCR reaction (5\u00a0\u03bcL) on a 10% polyacrylamide gel and stain with a 0.5\u00a0\u03bcg/mL ethidium bromide solution (see Figure\u00a05[href=https://www.wicell.org#fig5]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1750-Fig5.jpg\nFigure\u00a05. 10% polyacrylamide gel showing the generation of 9\u00d7 HT-SELEX libraries (144\u00a0bp) for high-throughput sequencing\nPurify HT-SELEX libraries using the Qiagen MinElute PCR purification kit and following manufacturer\u2019s protocol. Elute with 20\u00a0\u03bcL of EB Buffer (included in the kit, 10\u00a0mM Tris-HCl pH8.5) or H2O.\nNote: For each SELEX sample, a single PCR reaction will yield enough DNA to proceed with high-throughput sequencing.\nNote: Long PCR primers were used to generate HT-SELEX libraries, and these oligonucleotides are not completely eliminated following PCR purification with the Qiagen MinElute column.\nEvaluate DNA concentration and integrity of purified HT-SELEX libraries using a Nanodrop spectrophotometer.Pause point: Store purified HT-SELEX libraries at \u221220\u00b0C (long term). These samples can be pooled and submitted to high-throughput sequencing at a later time.\nPrepare a sequencing library pool and submit to high-throughput sequencing.\nUse Nanodrop quantification to pool all HT-SELEX libraries in equimolar amounts in a 1.5\u00a0mL tube.\nCritical: Make sure that all libraries in the pool contain unique indexes, so that each library can be de-multiplexed following high-throughput sequencing.\nTo ensure complete removal of leftover PCR primers contaminating libraries, perform a clean-up with KAPA Pure beads following manufacturer\u2019s protocol. Use a 3\u00d7 bead-to-sample ratio (e.g., add 150\u00a0\u03bcL of beads to 50\u00a0\u03bcL of HT-SELEX pool) to eliminate oligonucleotides below 100\u00a0bp (see Figure\u00a06[href=https://www.wicell.org#fig6]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1750-Fig6.jpg\nFigure\u00a06. Bioanalyzer profile showing the successful elimination of PCR primers from HT-SELEX library pool following a 3\u00d7 clean-up with KAPA Pure beads\nPause point: Store purified HT-SELEX library pool at \u221220\u00b0C (long term). This material can be submitted to high-throughput sequencing at a later time.\nPerform a final quality control on the library pool using the Agilent High Sensitivity DNA Kit and the 2100 Bioanalyzer instrument (following manufacturer\u2019s protocol) (see Figure\u00a06[href=https://www.wicell.org#fig6]).\nAlternatives: Run the library pool on a 10% polyacrylamide gel and stain with a 0.5\u00a0\u03bcg/mL ethidium bromide solution, as previously described.\nSubmit the HT-SELEX library pool to high-throughput sequencing using an Illumina instrument (e.g., Miseq/NextSeq/NovaSeq). Single-end sequencing is sufficient to cover the 20\u00a0bp insert containing putative DNA binding motifs (see Figure\u00a04[href=https://www.wicell.org#fig4]). A sequencing depth of 10,000\u201350,000 reads per sample should be sufficient to obtain robust quantification of DNA motifs for HT-SELEX (see troubleshooting 2[href=https://www.wicell.org#sec7.3]).", "Step-by-step method details\nStep-by-step method details\nEnrollment of study subjects\nTiming: 2\u00a0days\n      Males are dying at a disproportionately higher rate than females, and over\n      half of the HIV patients are aged 30\u201350 years. Therefore, only male\n      participants aged 30\u201350 years are recruited for the present experiment.\n      Inclusion and exclusion criteria for enrolling HIV-infected IRs and INRs:\n    \n        Inclusion criteria for HIV-infected IRs and INRs.\n        The number of CD4+ T\u00a0cells can reflect the immune status\n        after ART in HIV-infected patients. Therefore, IRs and INRs samples are\n        distinguished by CD4+ T\u00a0cell number at the same periods of\n        effective ART.\n        \nMale gender.\n            Patients who have been infected and examined as HIV positive in the\n            past.\n          \n30\u201350 years of age.\n            Patients have sustainable viral load suppression (<50 copies/mL\n            plasma) over 2 years after antiretroviral treatment (ART).\n          \n            The duration of infection and time to start ART among patients are\n            similar.\n          \nHave not blipped HIV viral load during ART.\n            No immune reconstitution inflammatory syndrome (IRIS) occurred\n            during ART.\n          \n            IRs are defined as CD4+ T\u00a0cell count >500 cells/\u03bcL.\n            INRs are defined as CD4+ T\u00a0cell count <350 cells/\u03bcL.\n          \n        Exclusion criteria for HIV infected IRs and INRs.\n        To study the effects of HIV infection on B-cell function, the diseases\n        that may affect the immune system must be excluded.\n        \n            Coinfection with pathogens such as hepatitis virus, syphilis, or\n            herpes simplex virus.\n          \n            Having tuberculosis and serious liver, kidney, heart, or brain\n            dysfunction.\n          \n            Having immune reconstitution inflammatory syndrome (IRIS) occurs\n            during ART.\n          \nHaving HIV viral load blipped during ART.\nIsolate PBMCs\nTiming: 1 h\n        Add 5\u00a0mL peripheral blood of participants into EDTA-anticoagulant tube,\n        turn over the tube gently (3 times), then put into a water bath\n        (Bluepard, Cat#BWS-10) preheated to 25\u00b0C for 20\u00a0min.\n      \n        An equal volume of modified HBSS buffer is added to dilute peripheral\n        blood.The Ficoll-Paque PREMIUM (GE health, Cat#17-5442-03) is inverted to make\n        thoroughly mix, and then the polypropylene cap is snapped off and\n        removed. A syringe is used to withdraw 3\u00a0mL of Ficoll-Paque PREMIUM,\n        which is then added to a 15\u00a0mL centrifuge tube (Nest, Cat#601002).\n      \n        Gently layer the bathed blood samples on top of Ficoll-Paque PREMIUM.\n        Blood samplse and Ficoll Histopaque should stay as two different layers.\n      \nCritical: Do not mix the layer of the\n      blood sample and Ficoll-Paque PREMIUM.\n    \n        Layer PBMCs between blood platelets and Ficoll after centrifugation\n        (Eppendorf 5810R, Cat#5811000495) at 400\u00a0\u00d7\u00a0g for 30\u00a0min at 25\u00b0C\n        (Figure\u00a01[href=https://www.wicell.org#fig1]A).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2583-Fig1.jpg\n              Figure\u00a01. Cell numbers and cell viability of PBMCs\n            \n              (A) PBMCs are layered by centrifugation of the peripheral blood\n              with Ficoll-Paque PREMUIM.\n            \n              (B) Calculate the number of isolated PBMCs in the blood counting\n              chamber.\n            \n              (C) Viable cells in the counting area of the blood cell counting\n              chamber in the field of the microscope (40\u00d7).\n            \nCritical: Set the function of \u201cspeed\n      down\u201d to no break before centrifugation.\n    \n        Discard the upper layer of plasma, and transfer the lymphocyte to a\n        clean 15\u00a0mL centrifuge tube.\n      \n        Then, 10\u00a0mL of modified HBSS buffer is added into the centrifuge tube,\n        and the cells are gently suspended.\n      \n        Centrifuge at 130\u00a0\u00d7\u00a0g for 10\u00a0min at 25\u00b0C, and repeat wash once.\n      \n        The supernatant is removed, and the PBMCs are resuspended in 1\u00a0mL of\n        RPMI 1640 complete medium.\n      \nCell viability assay\nTiming: 30\u00a0min\n        Resuspend the PBMCs in 1\u00a0mL of RPMI 1640 complete medium, avoid heavy\n        bubbles.\n      \n        Ten microliters of trypan blue (0.4%) (Solarbio, Cat# C0040) is added to\n        90\u00a0\u03bcL of cell suspension in a 1.5\u00a0mL microcentrifuge tube, and the\n        mixture is incubated at 25\u00b0C for 3\u00a0min after mixing.Ten microliters of mixed cells is added to the blood counting chamber,\n        and the cell numbers and viability are detected (Figures\u00a01[href=https://www.wicell.org#fig1]B and 1C).\n      \nCritical: PBMC viability should be\n      maintained at least 90%, and the cell concentration should be at least\n      1000 cells/\u03bcL.\n    \n        PBMCs are diluted to 0.7\u20131.2\u00a0\u00d7\u00a0106 cells/mL, and GEM\n        generation is performed for PBMCs on a 10\u00d7 Chromium Controller (10\u00d7\n        Genomics, Cat# PN110203).\n      \n      GEM generation, library construction, scRNA-seq BCR V(D)J, and\n      transcriptomic analysis\n    \nTiming: 22\u00a0h plus\n    \n      The generation of GEMs containing single cells , reverse transcription,\n      cDNA amplification, and library construction are performed by using\n      Chromium Next GEM Single Cell V(D)J Reagent Kits v1.1 (https://www.10xgenomics.com/support/single-cell-immune-profiling/documentation/steps/library-prep/chromium-single-cell-v-d-j-reagent-kits-v-1-1-chemistry[href=https://www.10xgenomics.com/support/single-cell-immune-profiling/documentation/steps/library-prep/chromium-single-cell-v-d-j-reagent-kits-v-1-1-chemistry]) according to the manufacturers\u2019 guidelines (Figure\u00a02[href=https://www.wicell.org#fig2]). 10\u00d7 Genomics Gem codeGemcode is used to pack barcoded single-cell 5\u2032\n      gel beads (10\u00d7 Genomics, PN-1000020, PN-1000009, PN-1000003), a Master Mix\n      with cells, and partitioning oil on a microfluidic chip to generate\n      single-cell GEMs. The detailed process is listed below (Figure\u00a02[href=https://www.wicell.org#fig2]).\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2583-Fig2.jpg\n          Figure\u00a02. Flow diagram of single-cell RNA sequencing\n        \n          GEM generation, cDNA amplification, library construction, and scRNA\n          sequencing were performed on the NovaSeq 6000 platform.\n        \n        Approximately 16 000 cells per sample are loaded onto the genomics\n        chromium controller chip.\n      \n        GEMs are generated by mixing the Master Mix with cells, and partitioning\n        oil on chromium next GEM Chip G (10\u00d7 Genomics, Cat# PN-2000177). The gel\n        beads are dissolved and lysed copartitioned cells to release\n        oligonucleotides, the latter are mixed with cell lysate and a Master\n        Mix, poly-adenylated mRNA is reversed transcription to 10\u00d7 full-length\n        cDNA.\n      \n        Recycle and purify the GEMs by Dynabeads MyOne\u2122 SILANE (10\u00d7 Genomics,\n        Cat# PN-2000048) cleanup mix and Buffer EB (Qiagen, Cat# 19086), and\n        then break the GEMs and recover pooled post-GEM-RT.\n      \n        Amplify 10\u00d7 barcoded full-length cDNA via polymerase chain reaction(PCR) with primers against common 5\u2032 and 3\u2032 ends. cDNA amplifications\n        are purified using the SPRIselect Reagent Kit (Beckman Coulter, Cat#\n        B23318), and then the quality test of cDNA amplifications is performed\n        on a 2100 Bioanalyzer instrument (Agilent, Cat# SD-UF0000034 Rev. D) (Figure\u00a03[href=https://www.wicell.org#fig3]A).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2583-Fig3.jpg\n              Figure\u00a03. Quality control of amplified cDNA libraries and\n              filtration of raw sequences data\n            \n              (A) Under the electropherogram, the region map (left) shows that\n              the peak shapes are distributed in the region of 700\u20132,000\u00a0bp,\n              which corresponds to the segment line on the map of agarose gel\n              electrophoresis (right).\n            \n              (B) Available cell diagrams (one sample). The abscissa represents\n              the barcode number, and the ordinate represents the UMI number.\n              The barcode in green is the valid cell, and the gray line is the\n              background.\n            \n              (C) Filtration of abnormal cells. The top panel shows the\n              information of cells in each group before filtering; the bottom\n              panel shows the information of cells in each sample after\n              filtering. The left side shows the gene number in a single cell.\n              The middle shows the UMI in a single cell. The right side shows\n              the percentage of mitochondrial genes in a single cell.\n            \n              (D) The gene number of a single cell after filtering in each\n              sample. A dot represents a single cell.\n            \n        Construct BCR V(D)J and 5\u2032 gene expression (GEX) libraries:\n        \n            Enrich the 10\u00d7 barcoded full-length V(D)J segments (Chromium Single\n            Cell V(D)J Enrichment Kit, 10\u00d7 GENOMICS, Cat# PN-1000016) and 5\u2032\n            gene segments (Chromium Single Cell 5\u2032 Library Kit, 10\u00d7 GENOMICS,\n            Cat# PN-1000002) from cDNA amplifications (step 19), respectively.\n          \n            Perform enzymatic fragmentation and size selection to generate\n            variable length segments.\n            \n                Add the sample index P7, an Illumina Read 2 primer sequence,\n                A-tailing, adapter ligation.\n              \n                sample index PCR on a thermal cycler to construct BCR V(D)J\n                libraries.Note: The BCR V(D)J libraries are cleaned\n      using the SPRIselect Reagent Kit, and the final V(D)J enriched libraries\n      and 5\u2032 gene expression libraries are sequenced on the NovaSeq 6000\n      platform (Illumina, Cat#20012850).\n    \n        The raw data of single-cell sequencing are prepared for downstream\n        analysis, and uploaded to BioProject, which is mentioned in the\n        \u201cDeposited data\u201d section of the\n        key resources table[href=https://www.wicell.org#key-resources-table].\n      \nProcess single-cell BCR V(D)J sequencing data\nTiming: 4 h\n      The processing of single-cell BCR V(D)J sequencing data is based on Cell\n      Ranger (https://support.10xgenomics.com/single-cell-vdj/software/overview/welcome[href=https://support.10xgenomics.com/single-cell-vdj/software/overview/welcome]), and the detailed procedures are listed below.6[href=https://www.wicell.org#bib6],7[href=https://www.wicell.org#bib7]\nData preparation.\n      Transfer the BCL format (raw data) to Fastq format using the Cellranger\n      Mkfastq.\n    \n        Cell filtering.\n        A small part of 10\u00d7 V(D)J microdroplets contains no cells with detected\n        barcode sequences, which is the background noise and may influence the\n        accuracy of downstream analysis. Effective barcodes are selected based\n        on the following criteria (Figure\u00a03[href=https://www.wicell.org#fig3]B).\n        \nBarcodes should contain effective, highly convincing contigs.\n            Each barcode contains at least three filtered UMIs, and each UMI\n            contains at least two read pairs.\n          \nThe cells with barcodes are removed as below:\nN\n50\nL\nm\na\nx\n<\n3\n%\n      N50: When all the UMI read lengths are summed in descending order until\n      the total length is over 50% of the genome, the length of the last UMI is\n      N50. Lmax: The maximum length of all UMI read lengths.\n    \nReads filter.\n      Align all sequencing reads to the reference of V(D)J segments. The read\n      pairs with over 15\u00a0bp nucleotides that aligned referenced segments are\n      reserved. The reserved read pairs are used for contig V(D)J genes (Figures\u00a03[href=https://www.wicell.org#fig3]C and 3D).\n    \n        Contig assembly.\n        \n            Construct a De Bruijn directed graph based on k-mers in every\n            single-cell barcode.8[href=https://www.wicell.org#bib8]\n            Expand k-mers to generate the paths (putative contigs) based on UMI\n            and the base quality of reads.Assign each UMI to a single path based on the alignment scores of\n            reads.\n          \nOutput the paths with at least one UMI (Contig).\n        Contig annotation.\n        The assembled contig is aligned to the reference sequence of the germ\n        cell line to annotate the regions of V, D, J, C, and the 5\u2032UTR using the\n        Smith Waterman algorithm. Then, the CDR3 region is identified in the\n        contigs, and the contigs are selected based on the following criteria:\n        \n            Full length: extending from the beginning of the V gene to the end\n            of the J gene.\n          \n            Initiation codon: annotated V gene contains an identifiable\n            initiation codon.\n          \nCDR3 region: contains an annotated CDR3 region.\n            No termination codon: no termination codon in the region from the V\n            gene to the J gene.\n          \n            Special structure: | Lvj\u00a0\u2013 Dvj\n            |\u00a0<\u00a025.\n            Lvj: The sum of the lengths of the V gene and the J gene.\n            Dvj: The length of the region from the beginning of the V\n            gene to the end of the J gene. IgH: -55\u00a0<\u00a0Lvj\u00a0\u2013 Dvj\n            <\u00a0+25.\n          \nContig selection.\n      Each cell barcode corresponds to two contigs, including the heavy and\n      light chains of the BCR. The contig with the maximum number of UMIs is\n      defined as the \u201cTop Contig\u201d by Cell Ranger. A contig is selected under the\n      following conditions: contained CDR3 region and at least 2\u00a0UMIs; the\n      number of UMIs is over 20% of the Top Contig.\n    \n        Assembly of the consensus sequence and classification of clonotypes.\n      \n      The selective contigs are assembled and polymorphic sites are integrated\n      to obtain the consensus sequence. A clonotype is defined as cells with the\n      same CDR3 nucleotide sequence. The efficacious barcodes are the abundance\n      of this certain clonotype.\n    \nAnalysis of BCR V(D)J sequencing data\nTiming: 2\u20134\u00a0weeks\n    \nCharacter analysis.The characteristics of CDR3, the V/J gene, and V-J pairs are analyzed\n      using Cell Ranger. In this section, the length of CDR3 amino acids, the\n      distribution and frequencies of V/J gene segments, and the frequencies of\n      V-J pairs are presented.\n    \nAnalysis of the BCR clonotypes among different samples.\n      The clonotypes with the same CDR3 nucleotides may be named differentially\n      among different samples, and these clonotypes are renamed by merging into\n      one. The statistical analysis of overlapping and nonoverlapping clonotype\n      numbers is carried out with the Sunflower diagram (Figure\u00a04[href=https://www.wicell.org#fig4]).\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2583-Fig4.jpg\nFigure\u00a04. The characteristics of BCR\n(A) The distribution of BCR IGHV gene segments in na\u00efve B cells.\n(B) The distribution of BCR IGHJ gene segments in na\u00efve B cells.\n(C) The distribution of BCR V-J pairs in na\u00efve B cells.\n          (D) The distribution of CDR3 amino acid lengths in na\u00efve B cells in\n          IRs.\n        \n          (E) The distribution of CDR3 amino acid lengths in na\u00efve B cells in\n          INRs.\n        \n      Processing of 5\u2032 gene expression of single-cell RNA-seq data\n    \nTiming: 4 h\nFilter the scRNA-seq data.\n      Low-quality barcodes and UMIs are removed to obtain clean reads (Figures\u00a03[href=https://www.wicell.org#fig3]C and 3D), and UMI numbers are counted to obtain a gene matrix and then\n      imported to Seurat 3.1.1.3[href=https://www.wicell.org#bib3] The code for filtering the\n      scRNA-seq data is shown in Code box 1.\n    \nCODE BOX 1. FILTER THE scRNA-SEQ DATA\nlibrary(Seurat)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(patchwork)\ndata_name\u00a0=\n          c(\"INR-1\",\"INR-2\",\"INR-3\",\"INR-4\",\"INR-5\",\"INR-6\",\"INR-7\",\"INR-8\",\"IR-1\",\"IR-2\",\"IR-3\",\"IR-4\",\"IR-5\",\"IR-6\",\"IR-7\",\"IR-8\",\"IR-9\",\"IR-10\")\nobject.list <- list()\nfor (i in seq(data_name)) {\nmat <- Read10X(data.dir\u00a0= data_name[i], gene.column\u00a0= 1)\nobject.list[[i]] <- CreateSeuratObject(counts\u00a0= mat, project\u00a0=\n          data_name[i], assay\u00a0= assay)}\n## merge Seurat object\nobject <- merge(x\u00a0= object.list[[1]], y\u00a0= unlist(object.list[-1]),\n          add.cell.ids\u00a0= data_name)\n## filtering\nobject <- PercentageFeatureSet(object, pattern\u00a0= \"\u02c6MT-\",\n          col.name\u00a0= \"Percent_mito\")\nobject <- subset(object, subset\u00a0= Percent_mito\u00a0<\u00a010)object <- subset(object, subset\u00a0= nCount_RNA\u00a0<\u00a08000)\nobject <- subset(object, subset\u00a0= nFeature_RNA\u00a0>\u00a0500\u00a0&\n          nFeature_RNA\u00a0<\u00a04000)\n        Normalize the gene expression data and perform batch effect correction.\n        \n            The gene expression of a single cell is normalized by performing the\n            global-scaling normalization method \u201cLogNormalize\u201d.\n            \nCritical: The UMI count data\n              for each gene are divided by the total UMI count of each cell.\n              Then, the scale factor is multiplied. Finally, log transformation\n              is used for normalization.\n            \n            Batch effects are corrected using canonical correlation analysis and\n            mutual nearest neighbor (MNN) analysis in Seurat 3.1.1 to aggregate\n            all samples.9[href=https://www.wicell.org#bib9] A total of 2000 highly variable\n            genes are selected from each sample based on a variance stabilizing\n            transformation, anchors are identified between two individual data\n            points, and correction vectors are calculated to generate an\n            integrated expression matrix, which is used for subsequent cell\n            clustering.\n          \n        CODE BOX 2. NORMALIZE THE GENE EXPRESSION AND CORRECT BATCH EFFECT\n      \n## integration\nobject <- NormalizeData(object, normalization.method\u00a0=\n          \"LogNormalize\",scale.factor\u00a0= 10000)\nobject <- FindVariableFeatures(object, selection.method\u00a0=\n          \"vst\",nfeatures\u00a0= 2000)\nobject <- ScaleData(object\u00a0= object, vars.to.regress\u00a0=\n          NULL,features\u00a0= rownames(object))\nobject.list <- SplitObject(object, split.by\u00a0=\n          \"orig.ident\")\nobject.list <- SplitObject.Image(object.list)\nobject <- FindIntegrationAnchors(object.list, dims\u00a0= 1:50,\n          normalization.method\u00a0= \"LogNormalize\", anchor.features\u00a0=\n          3000, k.filter\u00a0= 200)\n        Perform dimensional reduction and cell clustering.10[href=https://www.wicell.org#bib10]\n            The normalized data of 2000 highly variable genes (step 32) are\n            subjected to principal component (PC) analysis.\n          \n            The variability between two single cells is plotted in a\n            two-dimensional diagram with the parameters of 50 PCs, and the\n            uniform manifold approximation and projection (UMAP) and\n            t-distributed stochastic neighbor embedding (t-SNE) are employed to\n            reduce the dimensionality.\n          \n            Similar cells with the same PC parameters are clustered using\n            \u201cFindClusters\u201d in Seurat based on graph-based clustering.\n          \n            The cell clusters are visualized through t-SNE and UMAP diagrams.CODE BOX 3. DIMENSIONAL REDUCTION AND CELL CLUSTERING\n## Find clusters\nobject <- FindNeighbors(object, reduction\u00a0= \"pca\",\n          dims\u00a0= seq(50),force.recalc\u00a0= TRUE)\nobject <- FindClusters(object, resolution\u00a0= \"pca\",\n          temp.file.location\u00a0= getwd())\n## Draw PCA scatter plot\nDimPlot(object, reduction\u00a0= \"pca\")+ NoLegend()\n## Draw tsne plot\np1\u00a0<- DimPlot(object, reduction\u00a0= '\ntsne', group.by\u00a0= \"orig.ident\",\n          cols=rainbow(nlevels(object@meta.data$orig.ident)), label\u00a0= FALSE)\np2\u00a0<- DimPlot(object, reduction\u00a0= '\ntsne', group.by\u00a0= \"seurat_clusters\",\n          cols=rainbow(nlevels(object@meta.data$seurat_clusters)), label\u00a0= TRUE)\nggsave(p1+p2, file\u00a0= \" tsne.pdf\")\n## Draw UMAP plot\np1\u00a0<- DimPlot(object, reduction\u00a0= 'umap', group.by\u00a0=\n          \"orig.ident\", cols=rainbow(nlevels(object@meta.data$orig.ident)), label\u00a0= FALSE)\np2\u00a0<- DimPlot(object, reduction\u00a0= 'umap', group.by\u00a0=\n          \"seurat_clusters\", cols=rainbow(nlevels(object@meta.data$seurat_clusters)), label\u00a0= TRUE)\nggsave(p1+p2, file\u00a0= \"UMAP.pdf\")\n## Save data object\nDefaultAssay(object) <- \"RNA\"\nsave(object, file\u00a0= \"obj.Rda\")\nIdentification of B cell subpopulations\nTiming: 1\u00a0day\n        Identify B cell subpopulations.\n        \n            Based on SingleR R packages, six cell populations (NK cells, B\n            cells, T\u00a0cells, DCs, platelets and monocytes) are annotated,\n            including three B cell subtypes based on the expression of specific\n            marker genes (Figure\u00a05[href=https://www.wicell.org#fig5]).\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2583-Fig5.jpg\n                  Figure\u00a05. Identification of B cell subpopulations by scRNA\n                  sequencing\n                \n(A) PCA of valid cells for cell clustering.\n                  (B) T-SNE plot displaying clustering and annotation of\n                  sequenced and bioinformatically evaluated six cell\n                  populations.\n                \n                  (C) UAMP plot displaying clustering and annotation of\n                  sequenced and bioinformatically evaluated six cell\n                  populations.\n                \n                  (D) UAMP plot displaying the expression of specific marker\n                  genes of na\u00efve B cells, memory B cells and plasma cells.\n                \n                  (E) The dotplot diagram shows the expression of marker genes\n                  of na\u00efve B cells, memory B cells and plasma cells.\n                \n            The \u201cFindAllMarkers\u201d function in the Seurat package is employed to\n            investigate the expression of specific marker genes of B cell\n            subtypes using Wilcoxon rank sum test.11[href=https://www.wicell.org#bib11] The\n            specific marker genes of na\u00efve B cells are TCL1A,IL4R and CD69. The specific marker genes of memory B\n            cells are AIM2 and TNFRSF13B. The specific marker\n            genes of plasma B cells are JCHAIN, MZB1 and\n            XBP1 (Figures\u00a05[href=https://www.wicell.org#fig5]D and 5E).\n          \nCritical: Specific marker genes are\n      defined genes that are highly expressed in one B cell subtype.\n    \nCODE BOX 4. IDENTIFY B CELL SUBPOPULATIONS\n## Find DE Gene\nIdents(object) <- \"seurat_clusters\"\nobj.markers <- FindAllMarkers(object\u00a0= object, only.pos\u00a0=\n          TRUE,\nmin.pct\u00a0= 0.25, logfc.threshold\u00a0= 0.25,\nreturn.thresh\u00a0= 0.01, pseudocount.use\u00a0= 0)\nsave(obj.markers, file\u00a0= \"markers.Rda\")\n## Express distribution of marker genes\numapplot1\u00a0<- DimPlot(recluster_B, reduction\u00a0= \"umap\",\n          group.by\u00a0= \"orig.ident\", pt.size\u00a0= 1.5)\nFeaturePlot(cluster_B, features\u00a0= c(\"TCL1A\")\nstat<-table(cluster_B$orig.ident,cluster_B$seurat_clusters)\nstat\n## Draw Dotplot diagram\nmarker <-\n          c(\"CD19\",\"MS4A1\",\"CD79A\",\"CD79B\",\"TCL1A\",\"IL4R\",\"CD69\",\"AIM2\",\"TNFRSF13B\",\"CD27\",\"JCHAIN\",\"MZB1\",\"XBP1\")\nDotPlot(cluster_B, features\u00a0= marker)+coord_flip()+\ntheme_bw()+\ntheme(panel.grid\u00a0= element_blank(), axis.text.x=element_text(hjust\u00a0=\n          1,vjust=0.5))+\nlabs(x=NULL,y=NULL)+guides(size=guide_legend(order=3))+\nscale_color_gradientn(values\u00a0= seq(0,1,0.2),colours\u00a0=\n          c('#007799','#CCEEFF','#FFC8B4','#C63300'))\n      Analysis of the differentiation trajectory of B cell subtypes\n    \nTiming: 2\u00a0weeks\n      The differentiation trajectory is analyzed using the Monocle R package\n      with the parameters of sigma\u00a0= 0.001, lambda\u00a0= NULL, and param. gamma\u00a0=\n      10, and tol\u00a0= 0.001.12[href=https://www.wicell.org#bib12] Monocle develops BEAM to test\n      specific gene expression of cellular differentiation.5[href=https://www.wicell.org#bib5]\n      In addition, Monocle identified the genes that are differentially\n      expressed in different B cell subtypes. The main processes and the code\n      are listed below.\n    \nInformation selection.\n      Select specific genes for trajectory analysis using the normalized\n      expression profiles of B cell subtypes.\n    \nPlot cell trajectory.\n      Dimension reduction for the distribution of single cells is performed\n      using selected genes (step 35) by the DDRTree reduction approach.5[href=https://www.wicell.org#bib5]\n      After dimension reduction, cells are distributed in two-dimensional space.\n      Then, the centroids of B cell subtypes are identified using K-mean and\n      these centroids are linked by fitted lines (forming a tree). Each single\n      cell is updated to the nearest tree trunk (current trajectory). Smallredundant branches are filtered, and the two-dimensional tree is\n      transformed into a higher dimensional structure to obtain optimum cell\n      trajectories (Figure\u00a06[href=https://www.wicell.org#fig6]A).\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2583-Fig6.jpg\n          Figure\u00a06. Functional analysis of B cell subtypes\n        \n          (A) Differentiation trajectory of B cell subtypes. Cells with similar\n          differentiation are clustered together. Two cell subtypes with the\n          same differentiation track may have a differentiation relationship. A\n          dot represents a single cell.\n        \n          (B) Heatmap of the differentially expressed genes in na\u00efve B cells\n          between the IR and INR groups.\n        \n          (C) GSEA-scoring plots of the gene expression profile in na\u00efve B cells\n          from the IR and INR groups.\n        \nCritical: The origin of the cellular\n      trajectory is determined by the biological background.\n    \nCODE BOX 5. ANALYZE THE DIFFERENTIATION TRAJECTORY\nlibrary(Seurat)\nlibrary(monocle)\nlibrary(cowplot)\nlibrary(igraph)\nlibrary(dplyr)\n#Trajectory\n##obj created by Seurat\nload(\"obj.Rda\")\nobj@meta.data$Samples<-obj@meta.data$orig.ident\n        obj@meta.data$Clusters<-obj@meta.data$seurat_clusters\n      \ndata <- as(as.matrix(obj@assays$RNA@counts),\n          \"sparseMatrix\")\npd <- new(\"AnnotatedDataFrame\", data\u00a0=obj@meta.data)\nfData <- data.frame(gene_short_name\u00a0= row.names(data),row.names\u00a0=\n          row.names(data))\nfd <- new(\"AnnotatedDataFrame\", data\u00a0= fData)\nmnc_obj <- newCellDataSet(data, phenoData\u00a0= pd, featureData\u00a0=\n          fd,lowerDetectionLimit\u00a0= 0, expressionFamily\u00a0=\n          negbinomial.size())\nmnc_obj <- setOrderingFilter(mnc_obj, ordering_genes\u00a0=obj@assays[[\"RNA\"]]@var.features)\nmnc_obj <- reduceDimension(mnc_obj, method\u00a0= 'DDRTree',\n          verbose\u00a0= F, scaling\u00a0= T, max_components\u00a0= 2, norm_method\u00a0=\n          'none', maxIter\u00a0= 10, ncenter\u00a0= NULL, tol\u00a0= 0.001, sigma\u00a0= 1,\n          lambda\u00a0= NULL, param.gamma\u00a0= 10 )\nmnc_obj <- orderCells(mnc_obj)\npData(mnc_obj) <- droplevels(pData(mnc_obj))\n##draw\np1\u00a0<- plot_cell_trajectory(mnc_obj, color_by\u00a0=\n          \"Samples\",show_branch_points\u00a0= F)\np2\u00a0<- p1\u00a0+ facet_wrap(\u223cSamples,nrow\u00a0= 1)\np3\u00a0<- plot_cell_trajectory(mnc_obj, color_by\u00a0=\n          \"Clusters\",show_branch_points\u00a0= F)\np4\u00a0<- p3\u00a0+ facet_wrap(\u223cClusters,nrow\u00a0= 1)\np5\u00a0<- plot_cell_trajectory(mnc_obj, color_by\u00a0=\n          \"State\",show_branch_points\u00a0= F)\np6\u00a0<- p5\u00a0+ facet_wrap(\u223cState,nrow\u00a0= 1)\n##state diff\ndiff_state_res <- differentialGeneTest(mnc_obj,\n          fullModelFormulaStr\u00a0= \"\u223cState\", cores\u00a0= 4)\nsaveRDS(diff_state_res, file\u00a0= \"diff_state.rds\")\nsig_gene_names <-\n          rownames(subset(diff_state_res[order(diff_state_res$qval),],\n          qval\u00a0<\u00a01e-7))\np1\u00a0<- plot_genes_jitter(mnc_obj[head(sig_gene_names, 10),],\n          grouping\u00a0= \"State\", color_by\u00a0= \"State\", ncol\u00a0=\n          5)ggsave(\"Diff.state.pdf\", p1, width\u00a0= 12, height\u00a0= 5,\n          limitsize\u00a0= FALSE)\n##pseudotime diff\ndiff_Pseudotime_res <- differentialGeneTest(mnc_obj,\n          fullModelFormulaStr\u00a0= \"\u223csm.ns (Pseudotime)\", cores\u00a0=\n          cores)\nsaveRDS( diff_Pseudotime_res, file\u00a0= \"diff_Pseudotime.rds\"\n          )\nsig_gene_names <- rownames(subset(\n          diff_Pseudotime_res[order(diff_Pseudotime_res$qval),],\n          qval\u00a0<\u00a01e-7))\np1\u00a0<- plot_genes_in_pseudotime(mnc_obj[head(sig_gene_names,10),],\n          ncol\u00a0= 5, color_by\u00a0= \"Samples\")\np2\u00a0<- plot_pseudotime_heatmap(mnc_obj[sig_gene_names,],\n          num_clusters\u00a0= nlevels(pData(mnc_obj)$Clusters), cores\u00a0= 4,\n          show_rownames\u00a0= T, return_heatmap\u00a0= T)\nggsave(\"Diff.genes_in_pseudotime.pdf\", p1, width\u00a0= 12,\n          height\u00a0= 5, limitsize\u00a0= FALSE)\nggsave(\"Diff.pseudotime_heatmap.pdf\", p2, width\u00a0= 10,\n          height\u00a0= 30, limitsize\u00a0= FALSE)\n##branch diff\nBEAM_res <- BEAM(mnc_obj, branch_point\u00a0= 1, cores\u00a0= 4)\nsaveRDS(BEAM_res, file\u00a0= \"BEAM.1.rds\")\nsig_gene_names <- rownames(subset(BEAM_res[order(BEAM_res$qval),],\n          qval\u00a0<\u00a01e-7))\np1\u00a0<-\n          plot_genes_branched_pseudotime(mnc_obj[head(sig_gene_names,10),],\n          branch_point\u00a0= 1, color_by\u00a0= \"Samples\", ncol\u00a0= 5)\np2\u00a0<- plot_genes_branched_heatmap(mnc_obj[sig_gene_names,],\n          branch_point\u00a0= 1, num_clusters\u00a0= 5, cores\u00a0= 4, show_rownames\u00a0= T,\n          return_heatmap=T)\nggsave(\"Branch.1.genes_pseudotime.pdf\", p1, width\u00a0= 12,\n          height\u00a0= 5, limitsize\u00a0= FALSE)\nggsave(\"Branch.1.genes_heatmap.pdf\", p2, width\u00a0= 10,\n          height\u00a0= 30, limitsize\u00a0= FALSE)\n      Analysis of the transcriptome and the function of B cells\n    \nTiming: 2\u00a0weeks\n        Differentially expressed genes (DEGs) are analyzed among different\n        groups. DEGs are identified under the condition of log2Fold\n        change (FC) \u22651 and adjusted p\u00a0<\u00a00.05 (Figure\u00a06[href=https://www.wicell.org#fig6]B). Gene set enrichment analysis (GSEA) (GSEA 4.2.3,\n        https://www.gsea-msigdb.org/gsea/index.jsp[href=https://www.gsea-msigdb.org/gsea/index.jsp]) is employed to analyze the functions of B cells using the B cell\n        transcriptome with 1,000 permutations. The functions with Nom\n        p\u00a0<\u00a00.05, FDR\u00a0<\u00a00.25 and |NES| >1 are significantly\n        enriched (Figure\u00a06[href=https://www.wicell.org#fig6]C).", "Step-by-step method details\nStep-by-step method details\nIn this section, detailed step-by-step instructions for this protocol are provided, including computation environment setup, knowledge representation, data pre-processing, training and tuning, and evaluation and visualization, as exhibited in Figure\u00a03[href=https://www.wicell.org#fig3].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig3.jpg\nFigure\u00a03. The guideline of the knowledge-integrated deep learning framework for cellular image analysis in microbiologySteps 1\u20134. Computing environment setup. Steps 5\u20137. Human expert knowledge representation. Quantitative knowledge refers to knowledge that can be represented in a quantitative form, such as the microscopic and macroscopic correlations and geometric spectra. Qualitative knowledge refers to knowledge that cannot be represented quantitatively, such as knowledge from natural images. Steps 8\u201314. Data pre-processing. These steps ensure the suitability of the input data for model training and testing. The process begins by reading the images and their corresponding ground truths and then splitting them into training, testing, and validation data. For consistency, the images are reshaped into a unified shape as the original sizes of the image may vary. Finally, data normalization is performed to scale the value within a specific range. Depending on the size of the training data, some data augmentation techniques such as scaling, cropping, and color changes can be applied to augment the training data. Steps 15\u201316. Loss function and gradient descent algorithm selection. Tuning the possible combinations of different loss functions and gradient descent algorithms as much as possible for optimal training efficiency. Steps 17\u201318. Hyperparameter tuning. There are many hyperparameter settings such as learning rate, number of iterations, etc. They could be tested to help the model find the optimal convergence route. Steps 19\u201320. Performance evaluation. Different task-specific evaluation metrics can be used to evaluate the performance of the trained model. Steps 21\u201322. Visualization analysis. Different visualization techniques can be employed for intuitively evaluating the performance of the trained model. Results visualization reveals the effectiveness of the trained model used, while model visualization provides an understanding of the overall performance of the trained model. Steps 23\u201325. Ablation study is a set of experiments by removing or replacing different components of the DL model to monitor their impacts on the prediction of the model.Occlusion experiment involves blocking certain parts of the input image and analyzing the performance changes in model outputs and evaluation metrics. This reveals whether the trained model is using the\u00a0relevant part of the image for prediction. Generalization analysis aims to test the generalizability of the trained model. It can be done by testing the trained model on some unseen datasets.Part 1: Computing environment setup\nTiming: 1\u00a0h (depending on the network speed)\nInstalling Anaconda.\nDownload Anaconda from \u201chttps://repo.anaconda.com/archive/Anaconda3-2023.03-1-Windows-x86_64.exe[href=https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Windows-x86_64.exe]\u201d.\nInstall the Anaconda Navigator following the default settings.\nLaunch the Anaconda Navigator.\nInstall the \u201cCMD.exe Prompt\u201d from the \u201cHome\u201d page (Figure\u00a04[href=https://www.wicell.org#fig4]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig4.jpg\nFigure\u00a04. Anaconda Navigator\n(A) \u201cHome\u201d page illustration. The \u201cCMD.exe Prompt\u201d is used to execute the commands and the \u201cSpyder IDE\u201d is used to execute the codes.\n(B) \u201cEnvironments\u201d page illustration. It is used to manage the virtual environments.\nNote: As shown in Figure\u00a04[href=https://www.wicell.org#fig4]A, on the left navigation bar, the \u201cHome\u201d page button is used to install the Spyder IDE and \u201cCMD.exe Prompt\u201d. And the \u201cEnvironments\u201d page, as shown in Figure\u00a04[href=https://www.wicell.org#fig4]B is used to manage the created virtual environments. Since this is the first installation, only one virtual environment named \u201cbase (root)\u201d will be listed under the environments list.\nVirtual environment setup (Figure\u00a05[href=https://www.wicell.org#fig5]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig5.jpg\nFigure\u00a05. Virtual environment setup\n(A) Launch the \u201cCMD.exe Prompt\u201d.\n(B) Enter the command in the command prompt to create the corresponding virtual environment.\n(C) After successful installation, a new virtual environment named \u201ccellular_image_analysis\u201d will be displayed on the \u201cEnvironments\u201d page.\nLaunch the \u201cCMD.exe Prompt\u201d on the \u201cHome\u201d page (Figure\u00a05[href=https://www.wicell.org#fig5]A).\nEnter different commands based on the type of hardware being used.\nIf the CUDA-compatible GPU is being used, use the following command:\n> conda create -n cellular_image_analysis cudatoolkit=10.0.130 cudnn=7.6.5 python=3.7.16\nIf the CPU is being used, enter the command below:\n> conda create -n cellular_image_analysis python=3.7.16\nWhen prompted, type in \u201cy\u201d to confirm the installation process for the virtual environment (Figure\u00a05[href=https://www.wicell.org#fig5]B).\nAfter successful installation, a virtual environment named \u201ccellular_image_analysis\u201d will be created and can be accessed in the environments list (Figure\u00a05[href=https://www.wicell.org#fig5]C).\nNote: For more detailed information on managing virtual environments, please refer to\u00a0the\u00a0following link: \u201chttps://docs.anaconda.com/anaconda/navigator/tutorials/manage-environments/[href=https://docs.anaconda.com/anaconda/navigator/tutorials/manage-environments/]\u201d.Package installation (Figure\u00a06[href=https://www.wicell.org#fig6]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig6.jpg\nFigure\u00a06. Package installation\n(A) Install \u201cCMD.exe Prompt\u201d in the newly installed \u201ccellular_image_analysis\u201d virtual environment.\n(B) Launch the \u201cCMD.exe Prompt\u201d and enter the commands to install the packages.\nNavigate to the \u201cHome\u201d page.\nSwitch to the newly installed \u201ccellular_image_analysis\u201d environment.\nInstall \u201cCMD.exe Prompt\u201d within this new virtual environment and launch it.\nInstall pytorch.\nIf the CUDA-compatible GPU is being used, run the following command to install the GPU-compatible version of pytorch.\n> pip install https://download.pytorch.org/whl/cu100/torch-1.2.0-cp37-cp37m-win_amd64.whl https://download.pytorch.org/whl/cu100/torchvision-0.4.0-cp37-cp37m-win_amd64.whl\nIf the CPU is being used, use the following command to install the CPU-only version of pytorch.\n> pip install https://download.pytorch.org/whl/cpu/torch-1.2.0%2Bcpu-cp37-cp37m-win_amd64.whl https://download.pytorch.org/whl/cpu/torchvision-0.4.0%2Bcpu-cp37-cp37m-win_amd64.whl\nInstall the tensorflow and keras.\nIf the CUDA-compatible GPU is being used, use the following command to install the GPU-compatible versions of tensorflow and keras.\n> pip install tensorflow-gpu==1.15.0 tensorboard==1.15.0 tensorflow-estimator==1.15.1 keras==2.2.4 h5py==2.10.0 git+https://www.github.com/keras-team/keras-contrib.git\nIf the CPU is being used, use the following command to install the CPU-only versions of tensorflow and keras.\n> pip install tensorflow==1.15.0 tensorboard==1.15.0 tensorflow-estimator==1.15.1 keras==2.2.4 h5py==2.10.0 git+https://www.github.com/keras-team/keras-contrib.git\nInstall the remaining necessary Python packages using the command provided below.\n> pip install numpy==1.21.6 pandas==1.3.5 opencv-python==4.6.0.66 matplotlib==3.5.3 scikit-learn==1.0.2 scikit-image==0.17.2 tqdm==4.64.1 pycocotools==2.0.5 protobuf==3.19.0\nNote: If no error (error messages are usually indicated by red text) is displayed in the command prompt, it signifies that the installation has been successful. Otherwise, please retype the command to initiate the installation process again.\nCritical: Because the compatibility of these installation packages has been tested for this step, the \u201cpip install\u201d command is used for optimal installation efficiency. However, in the future, if readers need to install other packages in this virtual environment, it is recommended to use the \u201cconda install\u201d command whenever possible. This is because \u201cconda install\u201d will check for compatibility between the new and existing packages, which can help to avoid package conflict automatically.Alternatives: We offer an alternative one-step installation method using two \u201c.txt\u201d files named \u201cpackages_gpu.txt\u201d and \u201cpackages_cpu.txt\u201d, which can be found on our Github repository. To use this method, simply download the \u201c.txt\u201d file and place it in the C drive, then use the following command for installation:\n> cd C:\\\n> pip install -r <\npackages_gpu.txt/packages_cpu.txt>\nSpyder IDE installation.\nNavigate to the \u201cHome\u201d page.\nSwitch to the \u201ccellular_image_analysis\u201d virtual environment.\nClick \u201cinstall\u201d to start the installation of Spyder IDE.\nNote: A quick guide on Spyder IDE is available at the following source: \u201chttps://docs.spyder-ide.org/current/quickstart.html[href=https://docs.spyder-ide.org/current/quickstart.html]\u201d.\nPart 2: Knowledge representation\nTiming: More than 2\u00a0hours or days for each model (if considering data annotation and which human expert knowledge to use, the time will be extended to several days)\nThe provided templates represent human expert knowledge from multiple perspectives, as mentioned in the \u201cdescription of the methods\u201d section. In summary, DCTL uses selected morphologically similar macroscopic objects for model training. As for GFS-ExtremeNet, it incorporates the geometric spectrum into the post-processing process of the backbone network. In terms of COMI, pixel-level annotations provided by human experts and two VGGNets pre-trained on large-scale datasets are used for knowledge representation and integration. The codes have already incorporated this human expert knowledge. When using a new dataset, the human expert knowledge will be automatically integrated into the model. The following are the specific steps to follow when using a new dataset.\nPreparing new dataset in DCTL (Figure\u00a07[href=https://www.wicell.org#fig7]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig7.jpg\nFigure\u00a07. Example of naming for DCTL when using a new dataset\n(A\u2013D) Naming and placement of folders and files when using a new dataset.\nCollect images of the objects to be identified (Figure\u00a07[href=https://www.wicell.org#fig7]A).\nDivide the collected images into two folders named \u201ctrain\u201d and \u201ctest\u201d (Figure\u00a07[href=https://www.wicell.org#fig7]B) in a certain proportion (e.g., 8:2, 9:1, etc).Create a folder named with a numeric code (e.g., 0, 1, 2, etc) (Figure\u00a07[href=https://www.wicell.org#fig7]C) and place the \u201ctrain\u201d and \u201ctest\u201d folders inside it.\nPlace the folder in the \u201cY\u201d folder under the path \u201cDCTL/dataset/\u201d (Figure\u00a07[href=https://www.wicell.org#fig7]D).\nCollect extra images of objects with similar shapes to assist the model training. For example, Toxoplasma has a similar shape to a banana, so banana images can be collected from the internet as additional training samples. The steps for placing and naming the files are the same as the above steps, except that the \u201cY\u201d folder in step c should be replaced with the\u00a0\u201cX\u201d folder.\nNote: The number of images in the folders \u201cX\u201d and \u201cY\u201d does not have to be equal. When collecting images, it is recommended to use a clean background that only includes the object itself, as shown in Figure\u00a02[href=https://www.wicell.org#fig2]A.\nPreparing new dataset in GFS-ExtremeNet (Figure\u00a08[href=https://www.wicell.org#fig8]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig8.jpg\nFigure\u00a08. Example of naming for GFS-ExtremeNet when using a new dataset\n(A\u2013D) Naming and placement of folders and files when using a new dataset.\nDivide the collected images into training, validation, and testing data, usually with a ratio of 8:1:1.\nPlace the training data, validation data, and testing data in folders named \u201ctrain2017\u201d, \u201cval2017\u201d, and \u201ctest2017\u201d respectively (Figure\u00a08[href=https://www.wicell.org#fig8]A).\nName the corresponding ground truths as \u201cinstances_extreme_train2017.json\u201d, \u201cinstances_extreme_val2017.json\u201d, and \u201cimage_info_test-dev2017.json\u201d (Figure\u00a08[href=https://www.wicell.org#fig8]B).\nPlace the image and annotation files in folders named \u201cimages\u201d and \u201cannotations\u201d respectively (Figure\u00a08[href=https://www.wicell.org#fig8]C).\nCreate a folder named after the object to be recognized (Figure\u00a08[href=https://www.wicell.org#fig8]D).\nPlace the \u201cimages\u201d folder and the \u201cannotations\u201d folder inside it.\nPut this folder under the path \u201cGFS-ExtremeNet/dataset/\u201d.Note: In GFS-ExtremeNet, each image sample may contain multiple objects, but all of these objects should belong to the same category. For example, as shown in Figure\u00a02[href=https://www.wicell.org#fig2]B, the image of Toxoplasma should only contain Toxoplasma. Other parasites such as Babesia or Trypanosoma should not be included as they belong to different categories.\nPreparing new dataset in COMI (Figure\u00a09[href=https://www.wicell.org#fig9]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig9.jpg\nFigure\u00a09. Example of naming for COMI when using a new dataset\n(A\u2013D) Naming and placement of folders and files when using a new dataset.\nName the collected images. For example, the first image collected under the z\u00a0= 7 setting should be named \u201cZ7_0\u201d (Figure\u00a09[href=https://www.wicell.org#fig9]A).\nPlace the newly captured in-focus images in a folder named \u201cZ007\u201d (Figure\u00a09[href=https://www.wicell.org#fig9]B).\nPlace the newly captured out-of-focus images in the folder named based on the used z-axis settings during collection, such as \u201cZ004\u201d for z\u00a0= 4 (Figure\u00a09[href=https://www.wicell.org#fig9]B).\nStore the dataset of in-focus and out-of-focus images in the same folder, named based on the category of the objects being captured (Figure\u00a09[href=https://www.wicell.org#fig9]C).\nPlace this folder in the \u201cCOMI/dataset/BPAEC\u201d directory (Figure\u00a09[href=https://www.wicell.org#fig9]D).\nNote: It is important to pair each collected in-focus image with its corresponding out-of-focus image. Furthermore, it is recommended that the images only capture the target object without any other objects in the background, as shown in Figure\u00a02[href=https://www.wicell.org#fig2]C. These factors will help to ensure the reliability of the dataset.\nPart 3: Data pre-processing\nTiming: 5\u201320\u00a0mins for each model (depending on the hardware and datasets)\nThis part explains the procedures for reading and pre-processing images. The time required for these processes generally varies from 5 to 20\u00a0min, depending on the size of the dataset and CPU performance.Optional: To include additional data pre-processing steps, please refer to the relevant code files for DCTL at \u201clib/reader_image.py\u201d and \u201clib/utils.py\u201d, or the \u201c.py\u201d files located in the \u201cdb\u201d and \u201csample\u201d folders for GFS-ExtremeNet. For COMI, refer to the \u201cutils/read_image.py\u201d file. Details for configuring all three models can be found in their respective \u201ctrain.py\u201d and \u201ctest.py\u201d files. Additionally, GFS-ExtremeNet utilizes a separate configuration file at \u201cconfig/GFS-ExtremeNet.json\u201d.\nFolder structure check (Figure\u00a010[href=https://www.wicell.org#fig10]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig10.jpg\nFigure\u00a010. File structure consistency check\n(A and B) File structure examination. Switch to the \u201ccellular_image_analysis\u201d virtual environment and launch the Spyder IDE. Switch to the directory of the code repository to be run, and the File pane will display the file structure of the code and dataset.\n(C) File structure of different code repositories. The downloaded code repositories need to keep an identical file structure in order to get the entire pipeline running.\n(D) File structure of different datasets. Three different tasks in cellular image analysis were shown from left to right, including classification (DCTL), detection (GFS-ExtremeNet), and reconstruction (COMI). Apart from the dataset of COMI, the datasets for DCTL and GFS-ExtremeNet have been split into training data and testing data.\nSwitch to the \u201ccellular_image_analysis\u201d virtual environment (Figure\u00a010[href=https://www.wicell.org#fig10]A).\nLaunch Spyder IDE (Figure\u00a010[href=https://www.wicell.org#fig10]A).\nChoose the directory where the code repository is located (Figure\u00a010[href=https://www.wicell.org#fig10]B).\nCheck whether the folder structure in the Files pane is consistent with Figure\u00a010[href=https://www.wicell.org#fig10]C.\nCheck whether the folder structure of the downloaded datasets is consistent with Figure\u00a010[href=https://www.wicell.org#fig10]D.\nDataset specification and reading.\nSpecify the dataset used in GFS-ExtremeNet.\nOpen the \u201cconfig/GFS-ExtremeNet.json\u201d file.\nSpecify a variable named \u201cdataset\u201d (i.e., Babesia, Toxoplasma, and Trypanosoma), which is stored in a dictionary variable named \u201cdb\u201d.\n>{\n> \"sytem\":{\n> \"data_dir\": \"./dataset\",\n> },\n> \"db\":{\n> \"dataset\": \"Babesia\",\n> }\n>}\nSpecify the dataset used in COMI.Open the \u201ctrain.py\u201d file.\nSpecify a variable named \u201ccell_type\u201d that determines the image category to be used (i.e., actin, mitochondria, and nucleus).\nSpecify a variable named \u201cz_depth\u201d that determines the out-of-focus image datasets to be used (i.e., Z004, Z005, etc).\n> cell_type = 'actin'\n> z_depth = 'Z005'\nNote: Once the code starts running, the images (.png or .jpg) and their corresponding ground truths (.jpg or .json) are read sequentially from the \u201cdataset\u201d folders and converted into numpy arrays. Since DCTL utilizes the entire image datasets for training, there is no need to specify dataset parameters.\nDataset splitting.\nSpecify the ratio of training data and testing data in COMI.\nOpen the \u201ctrain.py\u201d file.\nSpecify the variable \u201cratio\u201d as 0.8 to split the dataset into training data and testing data at a proportion of 8:2.\n> ratio\u00a0= 0.8\nNote: The loaded images are typically divided into different subsets for the purpose of training and testing. In most cases, the ratio of training to testing data is set between 9:1 and 7:3.\nNote: In the case of DCTL and GFS-ExtremeNet, the datasets provided have already been split into training and testing data with a proportion of 8:2. Therefore, they do not need to specify the ratio of training data and testing data.\nData reshaping.\nSpecify the input image size used by DCTL.\nOpen the \u201ctrain.py\u201d file.\nSpecify the \u201cimage_size\u201d variable as follows:\n> tf.flags.DEFINE_integer('image_size', 256, 'image size, default: 256')\nSpecify the input image size used by GFS-ExtremeNet.\nOpen the \u201cconfig/GFS-ExtremeNet.json\u201d file.\nSpecify the variable \u201cinput_size\u201d as follows:\n>\n{\n> \"db\":{\n> \"input_size\": [511, 511],\n> }\n>}\nSpecify the input image size used by COMI.\nOpen the \u201ctrain.py\u201d file.\nSpecify the variable \u201cinput_shape\u201d as follows:\n> input_shape\u00a0= (128, 128, 3)Note: It is necessary to reshape the image into a uniform size to ensure compatibility with the network\u2019s input size. For DCTL, GFS-ExtremeNet, and COMI, the default input image sizes are (3, 256, 256), (3, 511, 511), and (3, 128, 128) respectively.\nCritical: It should be noted that changing the input image size would require adjusting the layer settings of the model accordingly. Hence, if it is not necessary, please do not modify this setting.\nData normalization.\nData normalization process used in DCTL.\nOpen the \u201clib/reader_image.py\u201d file.\nThe corresponding code segment is as follows:\n> image\u00a0= np.array(image) / 127.5 - 1.\nData normalization process used in GFS-ExtremeNet.\nOpen the \u201cutils/image.py\u201d file.\nThe corresponding code segment is as follows:\n>def normalize(image, mean, std):\n> image -= mean\n> image /= std\nData normalization process used in COMI.\nOpen the \u201clib/read_image.py\u201d file.\nThe corresponding code segment is as follows:\n>hrhq_train = np.array(hrhq_train)\n>hrhq_train = hrhq_train.astype('float32') /127.5 - 1.\n>hrhq_test = np.array(hrhq_test)\n>hrhq_test = hrhq_test.astype('float32') /127.5 - 1.\nNote: Data normalization is a technique used to scale the value of each tensor within a specific range, ensuring a similar data distribution among input tensors. The provided templates use two data normalization methods. DCTL and COMI use a simple method, as shown in Equation\u00a01[href=https://www.wicell.org#fd1], which scales the values between -1 and 1 by first dividing the image tensor x by 127.5 and then subtracting by 1. GFS-ExtremeNet uses a more formal z-score normalization, as shown in Equation\u00a02[href=https://www.wicell.org#fd2], which scales the values between 0 and 1 by dividing the image tensor x by its mean value \u03bc and then subtracting the standard deviation \u03c3.\n(Equation\u00a01)\nx\n\u2032\n=\nx\n127.5\n\u2212\n1\n(Equation\u00a02)\nx\n\u2032\n=\nx\n\u2212\n\u03bc\n\u03c3Note: Different normalization methods have a limited impact on performance, but they are important for the backbone network to learn more general knowledge.\nData augmentation.\nSpecify the data augmentation operations used in GFS-ExtremeNet.\nOpen the \u201cconfig/GFS-ExtremeNet.json\u201d file.\nSet the variables \u201crand_scale_min\u201d, \u201crand_scale_max\u201d, \u201crand_scale_step\u201d, and \u201crand_scales\u201d to 0.6, 1.4, 0.1, and null respectively.\nSet the variable \u201crand_crop\u201d to true to enable the cropping augmentation.\nSet the variable \u201crand_color\u201d to true to enable the color jittering augmentation.\n>{\n> \"db\":{\n> \"rand_scale_min\": 0.6,\n> \"rand_scale_max\": 1.4,\n> \"rand_scale_step\": 0.1,\n> \"rand_scales\": null,\n> \"rand_crop\": true,\n> \"rand_color\": true,\n> }\n>}\nNote: Data augmentation is a technique that expands training data without the need for manual collection and labeling of additional images. In GFS-ExtremeNet, due to the sparsity of objects within each image, multiple data augmentation techniques are supported.\nBatch size configuration.\nSpecify the batch size setting used in DCTL.\nOpen the \u201ctrain.py\u201d file.\nSpecify the variable \u201cbatch_size\u201d based on the hardware resources.\n> tf.flags.DEFINE_integer('batch_size', 1, 'batch size, default: 1')\nSpecify the batch size setting used in GFS-ExtremeNet.\nOpen the \u201cconfig/GFS-ExtremeNet.json\u201d file.\nSpecify the variable \u201cbatch_size\u201d based on the hardware resources.\n>{\n> \"system\":{\n> \"batch_size\": 1,\n> }\n>}\nSpecify the batch size setting used in COMI.\nOpen the \u201ctrain.py\u201d file.\nSpecify the variable \u201cbatch_size\u201d based on the hardware resources.\n> batch_size\u00a0= 1\nNote: In most scenarios, increasing the batch size can improve GPU utilization and accelerate model training. However, it is important to note that if model training is performed on a CPU, increasing the batch size may not significantly speed up the training process. Nevertheless, it is always advisable to set the batch size to the maximum possible value based on the available hardware resources.\nPart 4: Training and tuningTiming: More than 2\u00a0days or more for each model (depending on the hardware, initial setting, and datasets)\nRepeatedly fine-tuning a DL model is crucial for achieving its optimal performance and generalization. The duration of model training and tuning can vary depending on various factors (e.g., the hardware used, initial hyperparameter setting, etc). It may take several hours or even days to make delicate adjustments.\nOptional: Customization of the training pipeline can be done by editing the \u201ctrain.py\u201d file in each code repository.\nCritical: The model should be fine-tuned based on the results from validation data, as tuning on the testing data may result in model overfitting and reduced generalization performance.\nCode execution for model training and training visualization (Figure\u00a011[href=https://www.wicell.org#fig11]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig11.jpg\nFigure\u00a011. Code execution for model training and training visualization\n(A) Code execution procedure. Open the \u201ctrain.py\u201d file in the Spyder IDE and run it. If successful, the IPython Console will display the training information. After training, the trained model weights will be saved in the \u201ccheckpoint\u201d folder and the loss curve during the training process will be displayed in the Plots pane.\n(B) Visualization by loss curves under different learning rate settings. An appropriate learning rate setting can help the model find the global optimal solution while a lower or higher learning rate setting will lead to the suboptimal solution.\nOpen the \u201ctrain.py\u201d file within each code repository.\nRun the selected \u201ctrain.py\u201d file.\nDisplay the training information in the IPython Console.\nSave the trained model weights for DCTL (.index, .meta, and .data-00000-of-00001), GFS-ExtremeNet (.pkl), and COMI (.h5) in their corresponding \u201ccheckpoints\u201d folder.\nVisualize the loss curve of the training process in the Plots pane.Note: Using the trend of the loss curve to guide the tuning process can help the DL model find a more optimal convergence path. To demonstrate how to tune the model based on the loss curve, the following steps will use Figure\u00a011[href=https://www.wicell.org#fig11]B as an illustrative example.\nFine-tune the loss functions and gradient descent algorithms (optional). The default gradient descent algorithm used in the provided templates is the adaptive moment estimation (Adam),21[href=https://www.wicell.org#bib21] and the loss functions employed are consistent with those shown in Figure\u00a02[href=https://www.wicell.org#fig2].\nOptional: The choice of loss functions and gradient descent algorithms can impact whether the model can escape local optima and converge to a better location. Customization of the loss functions and gradient descent algorithms can be done by modifying the related code files. For DCTL, the changes can be made in the \u201ctrain.py\u201d and \u201cmodels/model.py\u201d files, while for GFS-ExtremeNet, it can be done in the \u201cmodels/py_utils/exkp.py\u201d and \u201cnnet/py_factor.py\u201d files. As for COMI, the customization can be implemented in the \u201ctrain.py\u201d file.\nFine-tune the learning rate configuration. Decrease the learning rate when the model converges too rapidly and increase it when the model converges too slowly.\nTune the learning rate configuration used in DCTL.\nOpen the \u201ctrain.py\u201d file.\nTune the variable \u201clearning_rate\u201d to regulate the learning rate configuration of CycleGAN.\nTune the variable \u201cfeature_learning_rate\u201d to regulate the learning rate configuration of feature extractor.\n>tf.flags.DEFINE_float('learning_rate', 2e-4, 'initial learning rate for CycleGAN, default: 0.0002') >tf.flags.DEFINE_float('feature_learning_rate', 2e-6, 'initial learning rate for feature extractor, default: 0.000002')\nTune the learning rate configuration used in GFS-ExtremeNet.\nOpen the \u201cconfig/GFS-ExtremeNet.json\u201d file.\nTune the variable \u201clearning_rate\u201d to regulate the learning rate configuration of GFS-ExtremeNet.\n>{\n> \"system\":{\n> \"learning_rate\": 0.0025,\n> }\n>}\nTune the learning rate configuration used in COMI.\nOpen the \u201ctrain.py\u201d file.Tune the variable \u201clearning_rate\u201d to regulate the learning rate configuration of COMI.\n> learning_rate\u00a0= 1e-4\nRetrain the model based on step 15.\nNote: Learning rate is a positive floating value between 0 and 1 that controls how much the model parameters should be adjusted in response to the loss values. Figure\u00a011[href=https://www.wicell.org#fig11]B illustrates some typical loss curves under different learning rate settings. If the learning rate is set too high (as shown by the red curve), the model may converge too rapidly and miss better solutions. Conversely, if the learning rate is set too small (as shown by the blue curve), the convergence may be slow and lead to a suboptimal solution.\nNote: When conducting a learning rate experiment, it is generally recommended to commence with a high learning rate and reduce it by a factor of 10 each time until the optimal range is found. Once the optimal range is identified, further fine-tuning can be performed within this range to identify the optimal setting.\nFine-tune the iteration setting by selecting the value at which the loss values stabilize and cease decreases.\nTune the iteration setting used in DCTL.\nOpen the \u201ctrain.py\u201d file.\nTune the variable \u201cmax_iter\u201d to set the maximum number of iterations.\n> tf.flags.DEFINE_integer('max_iter', 100000, 'max_iter, default: 100000')\nTune the iteration setting used in GFS-ExtremeNet.\nOpen the \u201cconfig/GFS-ExtremeNet.json\u201d file.\nTune the variable \u201cmax_iter\u201d to control the maximum number of iterations.\n>{\n> \"system\":{\n> \"max_iter\": 100000,\n> }\n>}\nTune the iteration setting used in COMI.\nOpen the \u201ctrain.py\u201d file.\nTune the variable \u201cmax_iter\u201d to regulate the maximum number of iterations.\n> max_iter\u00a0= 25000\nRetrain the model based on step 15.Note: The setting of iteration is another important factor in determining the convergence of DL models. This parameter is an integer value starting from 1 and can be set to an infinite positive value. As shown in Figure\u00a011[href=https://www.wicell.org#fig11]B if the number of iterations is set too small (e.g., 5000), the model will stop training before reaching its optimal convergence point. Conversely, if the number of iterations is set too large (e.g., 35000), the model will be updated too many times, resulting in overfitting to the training data.\nNote: An epoch is a pass-through of the entire dataset to the DL model. However, passing the entire dataset at once will generate too much updated information, which is likely to exceed the hardware capacity. Therefore, the dataset is generally partitioned into mini-batches and passed through the DL model several times, and each pass-through process of a mini-batch is an iteration. In most cases, the iteration is set based on the number of epochs to ensure that the entire dataset can be passed through the model multiple times. For consistency, the provided codes use iteration as the configurable hyperparameter, and the model is saved in the last iteration.\nPart 5: Evaluation and visualization\nTiming: 5\u201330\u00a0min for each model (depending on the hardware, models, and datasets)\nThis part explains how to use testing data to evaluate the overall performance of the trained DL model after fine-tuning. Due to differences between target tasks and DL frameworks being used, the supported functionalities between different codes may vary slightly.\nSpecify the directory of the trained model to be evaluated.\nSpecify the model weights that are to be evaluated for DCTL.\nOpen the \u201ctest.py\u201d file.\nSet the variable \u201cFLAGS.meta_dir\u201d to the path of the \u201c.meta\u201d file.Set the variable \u201cFLAGS.saved_weights_dir\u201d as the parent directory for storing the \u201c.meta\u201d file.\n>tf.flags.DEFINE_string('meta_dir', 'checkpoints/20230504-155358/our_model.ckpt-49.meta','directory of the saved .meta file of the saved model weights that you wish to testify (e.g.\u00a0checkpoints/20221119-2211/bestmodel/our_model.ckpt-10.meta), default: None') >tf.flags.DEFINE_string('saved_weights_dir', 'checkpoints/20230504-155358', 'direcotry of the saved model weights folder that you wish to testify (e.g. checkpoints/20221119-2211/bestmodel), default: None')\nSpecify the model weights to be evaluated for GFS-ExtremeNet.\nOpen the \u201ctest.py\u201d file.\nSet the variable \u201cargs.weights_dir\u201d to the directory of the model weights.\nSet the variable \u201cargs.split\u201d to the proportion of the dataset used for evaluation (i.e., training, testing, validation).\n>args.weights_dir = \"checkpoints/20221127-165855/GFS-ExtremeNet_best.pkl\"\n>args.split = \"validation\"\nSpecify the model weights that are to be evaluated in COMI.\nOpen the \u201ctest.py\u201d file.\nSet the variable \u201cweights_dir\u201d to the path of the \u201c.h5\u201d file.\n> weights_dir\u00a0= 'checkpoints/20221129-014343/deblursrgan4_actin_Z005_weights/iteration_240'\nResult of the evaluation metrics (Figure\u00a012[href=https://www.wicell.org#fig12]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig12.jpg\nFigure\u00a012. Code execution for testing and evaluation\nOpen the \u201ctest.py\u201d file within each code repository.\nRun the selected \u201ctest.py\u201d file.\nDisplay the results of the evaluation metrics in the IPython Console.\nNote: Details about the evaluation metrics can be found in the literature of DCTL,1[href=https://www.wicell.org#bib1] GFS-ExtremeNet,2[href=https://www.wicell.org#bib2] and COMI.3[href=https://www.wicell.org#bib3]\nResult visualization (Figures\u00a012[href=https://www.wicell.org#fig12] and 13[href=https://www.wicell.org#fig13]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig13.jpg\nFigure\u00a013. Visualization methods for evaluating the deep learning models\n(A) Storage path for visualization results. (i)-(iii) display the storage paths for the visualization results of DCTL, GFS-ExtremeNet, and COMI, located in the \u201cresults\u201d folder respectively.(B) Visualization by receiver operating characteristic curve and t-distributed stochastic neighbor embedding. (i) The area under the curve reflects the classification performance of the trained model. The larger the covered area the higher the classification performance of the trained model. (ii) The distribution of different color points reflects whether the model learns the decisive features for classification. If the model does not learn the decisive features, different color points will be mixed without a clear boundary. While if the model does learn the decisive features, a relatively clear boundary can be observed between different color points.\n(C) Visualization by extreme-point heatmap. (i)-(iv) Extreme-point heatmaps in four different orientations. (v) Center-point heatmap is obtained by calculating the center point of the extreme points in four orientations.\nOpen the \u201ctest.py\u201d file within each code repository.\nRun the selected \u201ctest.py\u201d file.\nDisplay the visualization results on the Plots pane.\nSave the visualization results for each model in their corresponding \u201cresults\u201d folder (Figure\u00a013[href=https://www.wicell.org#fig13]A).Note: Result visualization for DCTL includes the receiver operating characteristic (ROC) curve and the t-distributed stochastic neighbor embedding (t-SNE) visualization. The ROC curve area reflects the classification performance of the model. As displayed on the left of Figure\u00a013[href=https://www.wicell.org#fig13]B, the diagonal blue line represents random classification. If the ROC curve is below the diagonal blue line, like the red curve, it means that the trained model is worse than random classification. Otherwise, if the ROC curve is above the diagonal blue line, it means the training is effective and the trained model is better than randomly classifying the samples. The right of Figure\u00a013[href=https://www.wicell.org#fig13]B is the t-SNE visualization, where each color denotes a parasite category, and the points with the same color form a cluster. If different clusters are well separated, and the inner points with the same color are tightly clustered, then the model is deemed to have learned the decisive features to identify different parasites.\nNote: Result visualization of GFS-ExtremeNet includes bounding boxes visualization and extreme-point heatmaps visualization. The bounding box visualization is similar to Figure\u00a02[href=https://www.wicell.org#fig2]B and the extreme-point heatmaps are shown in Figure\u00a013[href=https://www.wicell.org#fig13]C. These visualizations demonstrate the detection capability of the trained DL model.\nNote: Result visualization of COMI is the restored in-focus images. The restored in-focus images can be compared with the corresponding input images and ground truth images to assess whether the trained DL model has learned useful features for image reconstruction.\nCritical: The following steps 22 to 25 require proficiency in DL and programming skills. Readers without a background in these areas can choose to skip them.\nClass activation maps (CAM) (optional).\nLoad the trained model weights.\nSelect a convolutional layer to generate feature maps.\nExtract a weight vector from the feature maps.Transform the feature maps into a single activation map which indicates the region of focus.\nNote: For professionals who want to explore different visualization techniques, there are several options available, such as CAM and their variants.22[href=https://www.wicell.org#bib22] Figure\u00a014[href=https://www.wicell.org#fig14] illustrates the use of CAM for model visualization. If the visualization outcomes focus on the target to be identified, it indicates that the trained model is using the correct part of the images for prediction. Otherwise, some adjustments and retraining of the model may be necessary.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig14.jpg\nFigure\u00a014. Visualizations by class activation map\nA well-trained DL model should focus on the parasite itself whereas a poorly trained model will focus more on some unnecessary regions like the external cell or only part of the parasite.\nAblation study (optional).\nAblation study for DCTL.\nRandomize the order of the macroscopic and microscopic object pair.\nRetrain the model\nMonitor the changes in the model classification performance.\nNote: A decrease in the performance after disruption indicates that the use of morphologically similar objects is effective during model training. Other components, such as feature extractors outlined in Figure\u00a01[href=https://www.wicell.org#fig1]B, can also be tested to verify their validity.\nAblation study for GFS-ExtremeNet.\nChange the cluster radius Rs into a negative value to disable the use of geometric spectrum.\n>{\n> \"db\":{\n> \"cluster_radius\": 500,\n> }\n>}\nObserve the changes in the model detection performance.\nAblation study for COMI.\nRemove the use of pre-trained VGGNet.\nRetrain the model.\nMonitor the changes in model restoration performance.\nNote: Ablation study is a set of experiments used to test whether the used components in the DL models are necessary or not.Critical: It is worth noting that an ablation study should be conducted only after the model tuning process is completed. Otherwise, the results might not reflect the validity of the components used. Moreover, further adjustments on hyperparameters may be needed to fully exploit the potential of each component.\nOcclusion experiment (optional).\nOcclusion experiment for DCTL.\nBlock a portion of parasites and their external parasitized cells in the image.\nMonitor the classification performance changes.\nNote: The occlusion experiment of DCTL reveals whether the use of macroscopic images is helping the model to learn the essential morphological features for parasite classification.\nOcclusion experiment for GFS-ExtremeNet.\nBlock a small part of the target to be detected in the image.\nMonitor the changes in the bounding box and confidence score.\nNote: The occlusion experiment of GFS-ExtremeNet indicates which parts of the image the trained model is using for detection.\nOcclusion experiment for COMI.\nBlock parts of the out-of-focus image.\nMonitor the changes in the restored in-focus image.\nNote: The occlusion experiment of COMI allows us to see whether the trained model uses global information for deblurring.\nNote: Occlusion experiment is used to determine whether the trained model is utilizing the appropriate parts of the image for prediction.23[href=https://www.wicell.org#bib23],24[href=https://www.wicell.org#bib24] This is done by blocking certain parts of the input image and analyzing the changes in model outputs and evaluation metrics, as demonstrated in Figure\u00a015[href=https://www.wicell.org#fig15]. In practical applications, Photoshop can be used to block a portion of an image with black color.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig15.jpg\nFigure\u00a015. Visualizations by occlusion experiment\nAreas of interest can be tested by blocking different parts of the images. Then, changes in the model outputs and evaluation metrics can be measured to highlight the areas that the DL models consider important in prediction.\nGeneralization analysis (optional).Prepare a new dataset (steps 5 to 7).\nEvaluate the model performance on the new dataset.\nNote: The generalization performance of a model refers to how well the trained model performs on datasets that it has not been trained on. It can also be assessed by comparing its predicted results with those of human experts. By identifying the samples on which the model performs poorly, it is possible to use these challenging examples to fine-tune the model and enhance its capabilities.", "Step-by-step method details\nStep-by-step method details\nIDR and multivalent domain prediction\nTiming: 30\u00a0min\nThe phase separation potential proteins are usually concluded as the intrinsically disordered regions and or multivalent domains in the molecule, and many bioinformatic software have been developed for these predictions. Here, the IUPred2A was used for IDR analysis for RPB1 (RNA Pol II largest subunit) and CTCF.\nGet amino acid sequences of the target protein from the UniProt (https://www.uniprot.org/[href=https://www.uniprot.org/]).\nSearch the target protein and select the desired species (here are Human CTCF: P49711 and Human RPB1: P24928) to show the detailed information of the target protein.\nClick the \u201csequence\u201d to show the protein amino acid sequence, copy the sequence to clipboard.\nOpen the webpage IUPred2A (https://iupred2a.elte.hu/[href=https://iupred2a.elte.hu/]), paste the amino acid sequence into the search box, set the parameter as default, and \u201csubmit\u201d directly. After a while, the result will be shown in a new window (Figure\u00a01[href=https://www.wicell.org#fig1]).\nCritical: The region with continuous IUPred2 (red line) score (>0.5) is usually defined as an intrinsically disordered region (IDR), e.g., CTD of RPB1 (aa 1593-1970), and CTD of CTCF (aa 573-727) shown IDR possibilities. The DBD (aa 266-577) of CTCF showed multivalent zinc fingers feature. The length of the IDR and multivalent region is important. If it is too short, the possibility of phase separation is low.\nClone and recombinant express the IDR and multivalent domain\nTiming: 1\u00a0week\nIt is difficult to express IDR and multivalent domain proteins in E.coli because they are easily aggregated. Thus, suitable tag fusing is necessary to help the target peptide fold correctly and soluble expression. So, the different plasmid and expressed host cells should be tested.\nClone the IDR and multivalent domain for recombinant expression. Troubleshooting 1[href=https://www.wicell.org#sec7.1].Clone the target sequence (Pol II CTD (RPB1, aa 1593-1970), CTCF DBD (ZNF 3-7: aa 322-460)) into pET-28a as \u201cHis tag- Sumo-Pol II CTD/CTCF DBD-mEGFP\u201d using the commercialized Gibson ligation kit (https://www.transgenbiotech.com/cloning_vector/peasy_basic_seamless_cloning_and_assembly_kit.html[href=https://www.transgenbiotech.com/cloning_vector/peasy_basic_seamless_cloning_and_assembly_kit.html]).\nSanger sequencing validate the insert, transfect the plasmid into Transetta (DE3) Chemically Competent cells and then examine the recombinant protein expression (Figure\u00a02[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2567-Fig2.jpg\nFigure\u00a02. The optimization program of the IDR and DBD domain proteins expression\nOptional: Generally, soluble Pol II CTD and CTCF DBD expression can be obtained by following this protocol (Figure\u00a02[href=https://www.wicell.org#fig2]). If more soluble proteins are needed, in addition to the suitable tag fusion, the expression host cell and the induction temperature are also very important for further optimization. If possible, different host cells are worth testing, and the induction at low temperature is better for protein folding. In this protocol test, 16\u00b0C\u201318\u00b0C is the best temperature range.\nRecombinant proteins purification. Troubleshooting 2[href=https://www.wicell.org#sec7.3].\nPurify the proteins through the general protocol of the recombinant proteins,1[href=https://www.wicell.org#bib1],2[href=https://www.wicell.org#bib2] as depicted in Figure\u00a03[href=https://www.wicell.org#fig3].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2567-Fig3.jpg\nFigure\u00a03. Purification of the His tag fused Pol II CTD/CTCF DBD, and control proteins\nCheck the purified proteins using SDS-PAGE and coomassie brilliant blue R250 staining.\nMeasure the protein concentration (mass.c (g/L) indicated the mass concentration) using NanoDrop and BCA method (Transgen Easy II Protein Quantitative Kit).\nBased on the predicted molecular weight (mol. wt.) of tag fused Pol II CTD: 80.5\u00a0kDa, CTDF DBD: 58.3\u00a0kDa, mEGFP 41.9\u00a0kDa, calculate the mol concentration (mol.c, mol/L) following the formula: mol.c.\u00a0= mass.c. / mol. wt., and split into 100\u00a0\u03bcL/ aliquot (200\u00a0\u03bcM) in storage buffer (20\u00a0mM Tris\u00b7HCl (pH8.0), 500\u00a0mM NaCl), store at \u221280\u00b0C.Critical: The purer the proteins, the better for the droplet formation assay. In this protocol, 70%\u201390% purity of the recombinant proteins (checked by coomassie brilliant blue staining) are able to form droplets normally. To get better results or eliminate the interference of the tag proteins, tag digestion can be tested. The buffer exchange and condensation are simultaneously performed using a hyperfiltration tube. During buffer exchange, protein concentration can be measured using NanoDrop simply, stopping condensation until the concentration is close to 200\u00a0\u03bcM, and finally confirmed using the BCA kit. The same batch of proteins were diluted to the expected concentration, then aliquoted and stored.\nPause point: The proteins can be stored at \u221280\u00b0C up to 1 year until use.\nDroplet formation of the Pol II CTD and CTCF DBD proteins\nTiming: 1\u00a0day\nPol II CTD and CTCF DBD droplet assay in\u00a0vitro (for different proteins, the pH, temperature, salt, and crowding molecules should be tested12[href=https://www.wicell.org#bib12]), for Pol II CTD and CTCF DBD droplet assay, the adoptive and comprehensive protocol were performed, respectively.1[href=https://www.wicell.org#bib1],2[href=https://www.wicell.org#bib2],6[href=https://www.wicell.org#bib6]\nPol II CTD droplet formation assay with crowding molecules.\nGet out the stored proteins (200\u00a0\u03bcM Pol II CTD-mEGFP or mEGFP served as a control) from \u221280\u00b0C, and place them on ice to thaw.\nProtein dilution: add 2\u00a0\u03bcL thawed proteins into 6\u00a0\u03bcL Dilution buffer 1 in the 8 tandem PCR tube, mix gently by using the 10\u00a0\u03bcL pipette 15 times but avoid bubbling, centrifuge briefly.\nOptional: If want to investigate whether the other proteins contribute to the Pol II CTD phase separation, the target proteins can be diluted here together with the Pol II CTD-mEGFP at the indicated concentration1[href=https://www.wicell.org#bib1] and then follow the next steps. Troubleshooting 5[href=https://www.wicell.org#sec7.9].Prepare the reaction mix (20\u00a0\u03bcL/reaction): take 16\u00a0\u03bcL 20% dextran into two new 8 tandem PCR tubes, and add 4\u00a0\u03bcL diluted Pol II CTD-mEGFP or mEGFP into the 20% dextran, respectively, to get the final 10\u00a0\u03bcM protein concentration. Then pipette up and down 20 times to mix the phase separation buffer thoroughly by using the 20\u00a0\u03bcL pipette, avoid bubbling and centrifuge briefly.\nTransfer the mixed protein solution (20\u00a0\u03bcL) into the bottom of 96 well glass plate, and spread it evenly by gently sliding the 20\u00a0\u03bcL tip to the entire well.\nIncubate the solution at room temperature (25\u00b0C) for 0.5 h, and avoid the light.\nAfter incubation, examine the droplet formation immediately using a confocal microscope (Nikon A1RSi+). Usually for EGFP, set laser strength to 20%, HV 40%, pinhole size (0.7), and field amplification (100\u00d7 oil-immersion objective lens, 1024\u00a0\u00d7\u00a01024 pixels, 0.12\u00a0\u03bcm/pixel). Capture no less than 5 different fields for each condition within 30\u00a0min after the droplet formed. Troubleshooting 3[href=https://www.wicell.org#sec7.5].\nCTCF DBD proteins can form droplets in\u00a0vitro without crowding molecules and are also significantly affected by the protein and salt concentration (10\u00a0\u03bcM protein in 20\u00a0mM Tris\u00b7HCl and 150\u00a0mM NaCl mix is recommended here).2[href=https://www.wicell.org#bib2] Troubleshooting 3[href=https://www.wicell.org#sec7.5].\nGet out recombinant CTCF DBD-EGFP proteins from \u221280\u00b0C, and place them on ice to thaw.\nProtein dilution: add 2\u00a0\u03bcL thawed proteins into 6\u00a0\u03bcL Dilution buffer 2 in the 8 tandem PCR tube. Mix gently using the 10\u00a0\u03bcL pipette 15 times but avoid bubbling, and centrifuge briefly.\nOptional: If aim to investigate whether other proteins regulate the CTCF DBD phase separation or not, the candidate proteins can also be prepared together with the CTCF DBD-EGFP proteins. Troubleshooting 5[href=https://www.wicell.org#sec7.9].Take 16\u00a0\u03bcL Dilution buffer 2 into two new 8 tandem PCR tubes, and add 4\u00a0\u03bcL diluted CTCF DBD-EGFP or EGFP into the dilution buffer, respectively, to get the final 10\u00a0\u03bcM protein concentration. Then pipette up and down 20 times to mix the phase separation buffer thoroughly by using the 20\u00a0\u03bcL pipette, avoid bubbling and centrifuge briefly.\nIncubate the protein solution at room temperature (25\u00b0C) in dark for 10\u00a0min, and immediately load on the glass bottom of the 96-well plate to ensure uniform distribution.\nObserve and image acquire more than 5 different fields using Nikon A1RSi+ scanning confocal microscope with identical microscopy settings, same to recorded as step 5.f.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2567-Fig4.jpg\nFigure\u00a04. Pol II CTD droplet assay with different crowd chemicals: e.g., Mock, 10% dextran, 16% dextran, 10% glycerol, 1% PEG8000, 2.5% PEG8000, 5% PEG8000\nPurified mEGFP and RUVBL1/2-mCherry proteins were used as controls. Images were captured under 100\u00d7 oli lens and scale bar is 25\u00a0\u03bcm.\nOptional: Besides the dextran, the other crowding molecules such as PEG8000, Ficoll, glycerol with different concentration can be tested for other proteins compared to the negative control (usually the fused tag proteins, exemplify mEGFP/EGFP in this protocol). Here we found that 16% dextran is best for the Pol II CTD phase separation (Figure\u00a04[href=https://www.wicell.org#fig4]), but crowding molecules are not needed for CTCF DBD in\u00a0vitro droplet formation assay.\nNote: The Pol II CTD and CTCF DBD droplet assays are performed separately. Do not set too many different concentration gradients and or other conditions repeats at one time. To finish imaging in time, 3\u20134 different droplet assays are carried out one time.\nPause point: After imaging, the captured pictures can be analyzed at any suitable time.", "Step-by-step method details\nStep-by-step method details\nRed blood cell lysis\nTiming: 10\u00a0min\n      A large proportion of whole blood is red blood cells (RBCs), thus the\n      first step in isolating neutrophils is an RBC lysis (Figure\u00a02[href=https://www.wicell.org#fig2]).\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2496-Fig2.jpg\n          Figure\u00a02. Schematic overview of the general steps involved in the\n          enrichment of neutrophils from fresh whole blood samples\n        \n        To begin, if the anticoagulant in the vacutainer is not EDTA, add 1\u00a0\u03bcL\n        of 0.5\u00a0M EDTA to the whole blood.\n      \n        Prepare the following solutions (see \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d):\n        \n1\u00d7 PBS+EDTA.\nTCL+2-mercaptoethanol lysis buffer.\n        For each individual sample, gently open the vacutainer tube and move\n        500\u00a0\u03bcL of whole blood to a 15\u00a0mL conical tube.\n      \n        Add 8\u00a0mL of ACK lysis buffer to each tube using a 10\u00a0mL serological\n        pipet, pipetting up and down 5 times to mix the sample and lyse the\n        RBCs.\n      \n        Centrifuge the samples at 300\u00a0\u00d7\u00a0g for 5\u00a0min at 25\u00b0C with max\n        acceleration and max deceleration.\n      \n        Aspirate the supernatant, being careful not to disturb the pellet.\n        \nThe pellet contains the neutrophils and all other PMBCs.\n        Resuspend the pellet of cells in 250\u00a0\u03bcL of 25\u00b0C 1\u00d7 PBS+EDTA by pipetting\n        up and down gently 5 times.\n      \nNeutrophil enrichment\nTiming: 40\u201360\u00a0min\n    \n      Once most RBCs have been removed and the pellet is resuspended, enrichment\n      for neutrophils via EasySep isolation can be performed (Figure\u00a02[href=https://www.wicell.org#fig2]).\n    \nVortex EasySep\u2122 RapidSpheres\u2122 for 30 s.\nAdd 50\u00a0\u03bcL of EasySep\u2122 Isolation Cocktail to each sample.\nAdd 50\u00a0\u03bcL of RapidSpheres to each sample.\nMix gently by pipetting up and down 10 times.\nIncubate samples for 5\u00a0min at 25\u00b0C.\nTop up the volume of each tube to 4\u00a0mL with 1\u00d7 PBS+EDTA.\nCap all of the 15\u00a0mL conical tubes tightly.Mix gently by tilting the samples back and forth 5 times.\nRemove the lid from each 15\u00a0mL conical tube.\n        Place samples in the EasyEights\u2122 EasySep\u2122 Magnet, ensuring that all\n        tubes sit all the way down.\n      \nIncubate samples on the magnet for 5\u00a0min at 25\u00b0C.\n        Use a 5\u00a0mL serological pipet to transfer the supernatant of the samples\n        into a new set of labeled 15\u00a0mL conical tubes.\n        \nDiscard tubes containing the RapidSpheres.\nNote: Angle the pipet such that the tip is\n      opposite the magnet to avoid touching the magnetic beads. The procedure is\n      a negative selection, meaning that neutrophils remain in the supernatant\n      while PBMCs are captured by the RapidSpheres.\n    \n        Add 25\u00a0\u03bcL of RapidSpheres to the tubes containing the supernatant from\n        the first selection step.\n      \n        Cap samples tightly and mix gently by tilting the samples back and forth\n        5 times.\n      \nIncubate samples for 5\u00a0min at 25\u00b0C.\n        Following the incubation, place tubes on the EasySep magnet and incubate\n        for 5\u00a0min.\n      \n        Use a 5\u00a0mL serological pipet to transfer the supernatant of the samples\n        into a new set of labeled 15\u00a0mL conical tubes.\n        \nDiscard tubes containing the RapidSpheres pellet.\n        Place the samples (supernatant) back on the EasySep magnet and incubate\n        for 5\u00a0min.\n      \nNote: This additional incubation on the\n      magnet is necessary in order to pull any remaining RapidSpheres from the\n      suspension.\n    \n        Use a 5\u00a0mL serological pipet to carefully transfer 3.5\u00a0mL of the\n        supernatant of the samples into a new set of labeled 15\u00a0mL conical\n        tubes.\n        \nDiscard tubes containing the RapidSpheres pellet.\n        Cap all samples and centrifuge at 300\u00a0\u00d7\u00a0g for 5\u00a0min at 25\u00b0C.\n      \n        Aspirate the supernatant, being careful not to disturb the pellet.\n      \nNote: The cell pellet should now be highly\n      enriched for neutrophils with minimal other cell species present.Resuspend the isolated neutrophils in 1\u00a0mL 1\u00d7 PBS\u00a0+ EDTA.\nMix thoroughly by pipetting up and down.\nCell counting and lysis\nTiming: 5\u201320\u00a0min\n      Once isolated, neutrophils should be counted in order to determine the\n      yield and to determine the correct volume of lysis buffer to use so that\n      all samples are at the same concentration for bulk RNA sequencing library\n      preparation.\n    \n        Mix 10\u00a0\u03bcL of neutrophil suspension with 10\u00a0\u03bcL of trypan blue by\n        pipetting.\n      \nAdd 10\u00a0\u03bcL to a counting chamber of a hemocytometer.\n        Using a hemocytometer or an automated cell counter, determine the total\n        number of neutrophils isolated from each sample.\n      \nNote: if an automated cell counter is\n      used, be sure to adjust the detection gates to count only the smaller\n      population of cells. Larger leukocytes may still be present at low levels\n      in the sample, and the goal is to determine the neutrophil count, not the\n      absolute cell count of each sample.\n    \n        Move 2\u00a0\u00d7\u00a0105 cells to a 1.5\u00a0mL Eppendorf tube by transferring\n        an appropriate volume of the cell suspension based on the determined\n        count/concentration.\n      \n        Top up the tube of cells to 1.5\u00a0mL total volume by adding additional 1\u00d7\n        PBS+EDTA.\n      \nCentrifuge at 300\u00a0\u00d7\u00a0g for 5\u00a0min at 25\u00b0C.\n        Aspirate the supernatant, being careful not to disturb the pellet.\n        \nRemove the last bit of supernatant with a 20\u2013200\u00a0\u03bcL pipette.\n        Resuspend the neutrophils in 200\u00a0\u03bcL of the prepared TCL\u00a0+ 1%\n        2-mercaptoethanol lysis buffer.\n        \n            If a lower number of cells than desired were retrieved, resuspend\n            the cells in a lower volume such that the final lysis concentration\n            is 1,000 cells/\u03bcL.\n          \nMix well to fully lyse all cells.\n        Transfer 100\u00a0\u03bcL per tube to two labeled cryopreservation tubes.\n        \nPlace tubes on dry ice immediately.After 5\u00a0min, the aliquoted lysates can be moved to storage at \u221280\u00b0C.\n      \nOptional: If enough of the original\n      sample remains, it may be helpful to check for successful enrichment by\n      flow cytometry, comparing the unenriched whole blood to the enriched\n      neutrophil product. A suggested antibody panel for discriminating between\n      the various PBMCs and neutrophils is suggested in LaSalle et\u00a0al.1[href=https://www.wicell.org#bib1]\n      and includes CD3, CD14, CD16, CD19, CD20, CD56, CD66b.\n    \nModified Smart-Seq2 cDNA generation\nTiming: 6\u20138 h\n      Once neutrophils have been enriched, cDNA can now be prepared via a\n      modified version of the Smart-Seq2 protocol11[href=https://www.wicell.org#bib11] and\n      then used for downstream library preparation. These steps involve\n      purifying RNA from the lysed cells, reverse transcription of mRNA\n      transcripts to cDNA, whole-transcriptome amplification, and purification\n      of amplified cDNA.\n    \n        Prepare neutrophil samples for cDNA generation.\n        \nBring RNA-SPRI beads to 25\u00b0C.\nThaw lysed neutrophils on ice.\n            Pipette 20\u00a0\u03bcL (at a concentration of 1,000 cells/\u03bcL) of each\n            neutrophil lysis sample into the well of a 96-well plate on ice.\n          \nCentrifuge plates at 1,500\u00a0rpm for 30\u00a0s at 4\u00b0C.\nPlace plate(s) back on ice.\n            Cover with the lid of a pipette tip box, until beginning the\n            following step.\n          \n        Prepare RNA-SPRI beads.\n        \nVortex the beads for 30 s.\n            Use a serological pipet to transfer the necessary volume (44\u00a0\u03bcL per\n            sample, plus excess) into a reagent reservoir.\n          \n        Using a multichannel pipette, add 44\u00a0\u03bcL of RNA-SPRI beads (2.2 times the\n        sample volume) to samples.\n      \nMix thoroughly by pipetting up and down.\n        Cover the plate(s), then incubate for 10\u00a0min at 25\u00b0C.\n        \n            During this incubation, prepare 50\u00a0mL of 80% EtOH: 40\u00a0mL EtOH and\n            10\u00a0mL ddH2O.\n          \n            Also prepare the appropriate volume of Smart-Seq2 Mix-1 (see\n            \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d),\n            plus excess (10%), and set aside on ice.Following the incubation, place the plate on the DynaMag\u2122-96 Side\n        Skirted Magnet and incubate for 5\u00a0min at 25\u00b0C.\n      \n        Remove the supernatant.\n        \n            Aspirate the liquid by angling the tips of the pipette opposite to\n            the beads.\n          \nCritical: Do not disturb the beads.\n      RNA from the neutrophils is bound to them, and any loss of beads at this\n      step will result in loss of genetic material input for subsequent steps.\n    \n        Use a serological pipet to fill a reagent reservoir with 80% EtOH.\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2496-Fig3.jpg\n              Figure\u00a03. Smart-Seq2 cDNA generation bead washing\n            \n              (A\u2013C) As the 96-well plate is moved side to side on the\n              side-skirted plate magnet as depicted in A\u2013C, note how the pellets\n              of beads move from the right side of the well to the left, and\n              then back to the right. This movement of the beads through the\n              ethanol allows for superior removal of non-nucleic acids and\n              better retrieval of RNA.\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2496-Fig4.jpg\n              Figure\u00a04. Appropriate way to cover SPRI beads during drying\n            \n              The lid must be slightly skewed in order to increase airflow. If\n              the plate is covered too tightly, the beads will dry unevenly and\n              take too long, enabling RNA degradation.\n            \n        Wash the beads in 100\u00a0\u03bcL of 80% EtOH twice.\n        \nAdd the EtOH, ensuring the beads are covered.\n            Agitate the beads, moving the plate side to side on the magnet such\n            that the beads move from one side of the well to the other side (Figure\u00a03[href=https://www.wicell.org#fig3]).\n            \nNote: Make sure to move in both\n              directions so that samples in columns A and H are washed properly.\n            \n            Place the plate on the magnet and incubate for 30\u201360\u00a0s to allow all\n            beads to come out of solution.\n          \nUse a P200 pipette to remove the ethanol.\nRepeat steps a-d (total 2 washes).After the final removal of the EtOH with the P200, use a P20 to\n            remove any additional EtOH left behind in the wells.\n          \n        After complete removal of the EtOH, keep the plate on the magnet and\n        allow the RNA-SPRI beads to dry for around 10\u00a0min, until cracks in the\n        beads are visible.\n        \n            Cover the plate to keep out contaminants but skew the lid to allow\n            for more airflow/easier evaporation, as shown in\n            Figure\u00a04[href=https://www.wicell.org#fig4].\n            \nCritical: Once cracks appear,\n              proceed to the next step. Overdrying of the RNA-SPRI beads can\n              negatively impact sample yields.\n            \n            Dilute 100\u00a0\u03bcL of 1\u00a0M MgCl2 in 900\u00a0\u03bcL nuclease-free water\n            to make a 100\u00a0mM stock solution.\n          \n            During this incubation, prepare the appropriate volume of\n            Smart-Seq2 Mix-2 (see \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d), plus excess, without adding Maxima H Minus\n            reverse transcriptase.\n            \nSet aside on ice.\n            Start the Denaturing protocol on the thermocycler.\n            \n                Immediately press pause so that the lid and block get up to\n                temperature.\n                table:files/protocols_protocol_2496_5.csv\n        Elute RNA from the dry RNA-SPRI beads by adding 4\u00a0\u03bcL of Smart-Seq2 Mix-1\n        and resuspending the samples thoroughly.\n      \nSeal the plate with a foil plate seal.\n        Centrifuge plate to get rid of any bubbles.\n        \nSet the centrifuge to run at 300\u00a0\u00d7\u00a0g for 1\u00a0min.\n            Start the spin but stop it once the speed has risen to 300\u00a0\u00d7\u00a0g\n            to eliminate bubbles but prevent pelleting of the magnetic beads.\n          \n        Place the samples in the thermocycler and resume the Denaturing\n        protocol.\n      \n        As soon as the 72\u00b0C step completes, immediately place samples on ice.\n        \nDo not wait for the thermocycler to ramp down to 4\u00b0C.\n            After placing the samples on ice, start the Reverse Transcription\n            (RT) protocol on the thermocycler and immediately press pause to\n            allow the block and the lid to come up to temperature.\n          \ntable:files/protocols_protocol_2496_6.csvAdd Maxima H Minus reverse transcriptase to Smart-Seq2 Mix-2, pipette to\n        mix.\n      \nSpin down for 5\u00a0s in a benchtop microcentrifuge.\n        With the plate of samples on ice, carefully remove the plate seal and\n        add 7\u00a0\u03bcL of Smart-Seq2 Mix-2 to each well.\n        \n            Mix by pipetting up and down 10 times or until RNA-SPRI beads are\n            resuspended.\n          \nRe-seal the plate with a fresh foil seal.\n        Centrifuge plate to get rid of any bubbles.\n        \nSet the centrifuge to run at 300\u00a0\u00d7\u00a0g for 1\u00a0min.\n            Start the spin but stop it once the speed has risen to 300\u00a0\u00d7\u00a0g\n            to eliminate bubbles but prevent pelleting of the magnetic beads.\n          \n        Place the plate in the thermocycler and resume the RT protocol.\n        \n            15\u201320\u00a0min before the program ends, take the KAPA HiFi HotStart\n            ReadyMix and the ISPCR primers out of the freezer to thaw on ice.\n          \n        When the RT program has ended, remove the samples from the thermocycler\n        and place the plate on ice.\n        \n            After placing the samples on ice, start the PCR Preamplification\n            program on the thermocycler and immediately press pause to allow the\n            block and the lid to come up to temperature.\n            table:files/protocols_protocol_2496_7.csv\n            Prepare Smart-Seq2 Mix-3 (see \u201cmaterials and equipment[href=https://www.wicell.org#materials-and-equipment]\u201d) and place on ice.\n          \nCarefully remove the foil plate seal.\n        Add 14\u00a0\u03bcL of Mix-3 to samples and mix by pipetting up and down 10 times\n        or until RNA-SPRI beads are resuspended.\n      \nRe-seal the plate with a fresh foil seal.\n        Centrifuge plate to get rid of any bubbles.\n        \nSet the centrifuge to run at 300\u00a0\u00d7\u00a0g for 1\u00a0min.\n            Start the spin but stop it once the speed has risen to 300\u00a0\u00d7\u00a0g\n            to eliminate bubbles but prevent pelleting of the magnetic beads.\n          \n        Place the samples in the thermocycler and resume the PCR\n        Preamplification program.Pause point: Samples can be held at\n      4\u00b0C for up to 24\u00a0h once the PCR Preamplification is completed.\n    \nCritical: Before moving to the next\n      step, make sure to equilibrate the AMPure XP SPRI beads to 25\u00b0C (\u223c30\u00a0min).\n    \n        Prepare AMPure XP SPRI beads.\n        \nVortex the beads for 30 s.\n            Use a serological pipet to transfer the necessary volume (40\u00a0\u03bcL\n            total per sample, plus excess) into a reagent reservoir.\n          \n        Purify PCR products.\n        \n            Using a multichannel pipette, add 20\u00a0\u03bcL of AMPure XP SPRI beads (0.8\n            times the sample volume) to samples.\n          \nMix thoroughly by pipetting up and down.\nCover the plate(s), then incubate for 5\u00a0min at 25\u00b0C.\n        Following the incubation, place the plate on the magnet and incubate for\n        5\u00a0min at 25\u00b0C.\n        \n            During this incubation, prepare 50\u00a0mL of 70% EtOH: 35\u00a0mL EtOH and\n            15\u00a0mL ddH2O.\n          \n        Remove the supernatant.\n        \n            Aspirate the liquid by angling the tips of the pipette opposite to\n            the beads.\n          \n        Use a serological pipet to fill a reagent reservoir with 70% EtOH.\n      \n        Wash the beads in 100\u00a0\u03bcL of 70% EtOH twice.\n        \nAdd the EtOH, ensuring the beads are covered.\n            Agitate the beads, moving the plate side to side on the magnet such\n            that the beads move from one side of the well to the other side.\n            \nNote: Make sure to move in both\n              directions so that samples in columns A and H are washed properly.\n            \n            Place the plate on the magnet and incubate for 30\u201360\u00a0s to allow all\n            beads to come out of solution.\n          \nUse a P200 pipette to remove the ethanol.\nRepeat steps a-d.\n            After removing the EtOH with the P200, use a P20 to remove any\n            additional EtOH left behind in the wells.\n          \n        After complete removal of the EtOH, keep the plate on the magnet andallow the AMPure XP SPRI beads to dry for around 10\u00a0min, until cracks in\n        the beads are visible.\n        \n            While the beads are drying, use a serological pipet to transfer an\n            excess of TE buffer to a reagent reservoir.\n          \n            Additionally, prepare/label a new 96-well plate which will be used\n            for elution.\n          \n        Elute amplified cDNA from the dry AMPure XP SPRI beads by adding 25\u00a0\u03bcL\n        of TE buffer and resuspending the samples thoroughly.\n      \nIncubate for 1\u00a0min at 25\u00b0C.\n        Place the plate on the magnet and wait until all magnetic beads have\n        come out of solution.\n        \nIn most cases, this will take approximately 1\u00a0min.\n        With a pipette set to 25\u00a0\u03bcL, transfer the supernatants (containing the\n        cDNA) to the new 96-well plate.\n      \n        Perform a second round of purification, repeating steps 70\u201380, for a\n        total of 2 cleanup cycles.\n      \n        Store samples at 4\u00b0C for 1\u20132\u00a0days, or at \u221220\u00b0C for longer durations.\n      \nPause point: Samples can be held at\n      4\u00b0C for 1\u20132\u00a0days (or at \u221220\u00b0C for up to 4\u00a0weeks, without freeze/thaw)\n      following cDNA purification and before moving on to cDNA quality control.\n    \ncDNA quality control\nTiming: 1\u20132 h\n      Prior to taking the cDNA generated via this modified Smart-Seq2 protocol\n      forward into library preparation for bulk RNA sequencing, the quality of\n      the cDNA must be determined. We use two distinct metrics to do so: a\n      measurement of the concentration of each individual sample using a\n      plate-based Qubit dsDNA HS assay, and a measurement of the size\n      distribution of cDNAs in representative samples via generation of\n      BioAnalyzer traces.\n    \n        Perform a Qubit assay using a Qubit\u2122 dsDNA HS kit.\n        \nCentrifuge plates of cDNA for 5\u00a0s and then place on ice.\n            Prepare enough Qubit working solution for all of the samples youwill be testing and for 8 standards (198\u00a0\u03bcL per sample/standard),\n            plus excess.\n            \n                Dilute the appropriate volume of Qubit\u2122 dsDNA HS Reagent 1:200\n                in dsDNA HS Buffer.\n              \n                Wrap the tube in aluminum foil to protect from light, and set\n                aside.\n              \n            Prepare standards.\n            \nPrepare an 8-well PCR strip tube and label wells 1\u20138.\n                Add 25\u00a0\u03bcL of standard-1 (provided with the kit) into all wells\n                except the first one.\n              \n                Add 50\u00a0\u03bcL of standard-2 to the first empty well, take 25\u00a0\u03bcL, add\n                to the second well containing 25\u00a0\u03bcL of standard-1 and mix.\n              \n                Continue with the serial dilution until the 7th well, leaving\n                the 8th well with only 25\u00a0\u03bcL of standard-1.\n              \n            Obtain one black-sided spectrophotometer assay plate per plate of\n            neutrophils cDNA as well as an additional plate for the standards.\n            \n                The standards should be prepared in triplicate, so 24 wells\n                (columns A-C) should be prepped.\n              \n            Pour the prepared Qubit working solution into a reagent reservoir.\n            \n                Use a multichannel pipette to add 198\u00a0\u03bcL of Qubit solution into\n                each well that will be used to measure samples/standards.\n              \n            Place the plate of cDNA on the side-skirted magnet.\n            \n                Use a multichannel pipet to transfer 2\u00a0\u03bcL of sample into the\n                corresponding wells in the plate containing Qubit working\n                reagent.\n              \n            Transfer 2\u00a0\u03bcL of standards to the appropriate wells on the standard\n            plate.\n          \n            Seal all plates thoroughly with a plastic plate seal, vortex for\n            5\u201310 s, then spin down.\n          \n            Remove plate seals and read on a plate-based\n            spectrophotometer/fluorometer.\n            \nExcitation: 485\u00a0nm; emission: 530\u00a0nm.\n            Use the standards to create a standard curve and linear formula to\n            relate fluorescence to dsDNA concentration.\n            \n                Use this formula to determine the concentration of each sample.\n              \n        Once neutrophil cDNA concentrations have been determined, select 11\n        samples from each plate to run on an Agilent Bioanalyzer using theHigh-Sensitivity DNA Bioanalyzer Kit.\n        \n            Choose the samples such that you will be able to observe the DNA\n            quality over the range of concentrations measured via the Qubit\n            assay.\n          \n            Prepare the Bioanalyzer chip and load the 11 samples according to\n            the\n            manufacturer\u2019s protocol[href=https://www.agilent.com/cs/library/usermanuals/public/G2938-90322_HighSensitivityDNAKit_QSG.pdf].\n          \n            Example traces are shown below in Figure\u00a05[href=https://www.wicell.org#fig5].\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2496-Fig5.jpg\n                  Figure\u00a05. Example Bioanalyzer traces from two different bulk\n                  neutrophil cDNA samples following preparation via SmartSeq2\n                  and clean-ups\n                \n                  Note the size distributions and peaks of both traces. The\n                  upper represents an ideal cDNA trace, with all fragments\n                  larger than 500\u00a0bp. The lower trace is still acceptable to\n                  take forward into library preparation, but the presence of\n                  cDNA fragments lower than 500\u00a0bp suggests that some mRNA\n                  degradation occurred prior to the reverse transcription\n                  reaction.\n                \nPause point: Samples can be held at\n      4\u00b0C overnight or at \u221220\u00b0C for up to 4\u00a0weeks (without freeze/thaw)\n      following cDNA purification and before moving on to library preparation.\n    \nModified Nextera library generation\nTiming: 2\u20133 h\n      Illumina\u2019s Nextera XT Library Prep Kit is a fast and easy to use library\n      preparation method. We have modified and optimized the protocol to work\n      with ultra-low DNA inputs of 0.1\u20130.2\u00a0ng in a 384-well plate format to\n      allow for easy multiplexing. This is ideal for use with cell-types such as\n      neutrophils that have relatively small transcriptomes and enables a\n      low-effort way to process many samples for RNAseq library generation at\n      one time.\n    \n        Following library preparation, all samples should be diluted to\n        0.1\u20130.2\u00a0ng/\u03bcL.\n        \nNote: We have found that the easiest\n          way to do this is to create a 96-well \u201cdilution plate\u201d that\n          corresponds to each plate of cDNA generated via SmartSeq2.\n        \n            First, 10\u00a0\u03bcL of each sample determined to already have\n            concentrations in the range of 0.1\u20130.2\u00a0ng/\u03bcL should be transferredto the corresponding sample wells in the dilution plate.\n            \n                10\u00a0\u03bcL of any samples below the desired concentration should also\n                be transferred at this time.\n              \n            All remaining samples should be diluted using ultra-pure,\n            double-distilled water.\n          \n            At least 1\u00a0\u03bcL of original cDNA should be used to create the diluted\n            sample, and the total dilution volume should be no less than 10\u00a0\u03bcL.\n          \n        Prepare the set-up for library generation.\n        \n            Obtain two large ice buckets, and place a metal 384-well plate\n            cooling block on one of them.\n          \n            Retrieve Tagmentation DNA Buffer (TD) and Amplicon Tagment Mix (ATM)\n            from storage at\u00a0\u221220\u00b0C and allow to thaw on ice.\n          \n            Retrieve the prepared index plates (Q1, Q2, Q3, Q4) and thaw at 25\u00b0C\n            before placing on\u00a0ice.\n          \n            If diluted plates were held at 4\u00b0C, place on ice.\n            \n                If they were stored at \u221220\u00b0C, allow them to thaw at 25\u00b0C and\n                then place on ice.\n              \n            Retrieve a fresh 384-well PCR plate for the library generation.\n          \nObtain a 96-well plate for use as a reagent plate.\n        Add TD, ATM, and diluted samples to the 384-well PCR plate.\n        \n            Aliquot 55\u00a0\u03bcL of TD into all 12 wells of row A of the reagent plate.\n          \n            Use two tip boxes (one row of tips per row of the 384-well plate) to\n            transfer 1.25\u00a0\u03bcL of TD into each well of the 384-well plate.\n          \n            Aliquot 30\u00a0\u03bcL of ATM into all 12 wells of row B of the reagent\n            plate.\n          \n            Use four tip boxes (two rows of tips per row of the 384-well plate)\n            to transfer 0.625\u00a0\u03bcL of ATM to each well of the 384-well plate.\n          \n            Use four tip boxes to transfer 0.625\u00a0\u03bcL of diluted samples into the\n            384-well plate, keeping track of the positions of samples in orderto match them with the unique indices that will be added in step 93.\n            \n                Seal the 384-well plate with a foil plate seal and mix samples\n                by tapping gently on the benchtop.\n              \n            Spin the plate at 20,000\u00a0\u00d7\u00a0g for 3\u00a0min to ensure that there\n            are no bubbles.\n          \nCritical: Make sure that the library\n      plate is held on ice while samples are added, and place the plate\n      immediately back on ice after centrifugation. This will limit premature\n      tagmentation of the cDNA.\n    \n        Perform the tagmentation reaction.\n        \n            Put the plate in a thermocycler equipped with a 384-well plate\n            block, and run the Make NTA protocol.\n          \ntable:files/protocols_protocol_2496_8.csv\n        While the tagmentation reaction is occurring, prepare for the next\n        steps.\n        \n            Retrieve the NT buffer (NT) from storage at 25\u00b0C, and aliquot 30\u00a0\u03bcL\n            of TD into all 12 wells of row C of the reagent plate.\n          \nReplace the TD and ATM at \u221220\u00b0C.\nReseal the cDNA plates and store at \u221220\u00b0C.\n        As soon as the 55\u00b0C step of the Make NTA program finishes and the\n        thermocycler begins to ramp down to 10\u00b0C, take the plate out and\n        immediately place it on ice.\n      \n        Carefully remove the foil plate seal and add NT to the tagmented\n        samples.\n        \n            Use four tip boxes to transfer 0.625\u00a0\u03bcL of NT to each well of the\n            384-well plate.\n          \n            Seal the plate and mix samples by tapping gently on the benchtop.\n          \n            Spin the plate at 20,000\u00a0\u00d7\u00a0g for 3\u00a0min to ensure that there\n            are no bubbles.\n          \n            Incubate the plate for 10\u00a0min at 25\u00b0C.\n            \n                Begin the timer for this incubation as soon as you take the\n                plate out of the centrifuge.\n              \nCritical: Place the library plate back\n      on the metal block on ice as soon as possible after the tagmentationreaction (55\u00b0C step) is complete and add the NT carefully but efficiently\n      to prevent over-tagmentation of the cDNA.\n    \n        While the samples are incubating with NT, prepare for the next steps.\n        \n            Retrieve the NPM reagent from storage at \u221220\u00b0C and place on ice.\n          \nEnsure that the index plates have thawed.\n            Gently vortex the index plates and then spin them down by allowing\n            the centrifuge to reach 20,000\u00a0\u00d7\u00a0g before stopping it.\n          \n        Remove the foil plate seal, then add NPM and indices.\n        \n            Aliquot 80\u00a0\u03bcL of NPM into all 12 wells of row D of the reagent\n            plate.\n          \n            Use four tip boxes to transfer 1.875\u00a0\u03bcL of NPM into each well of the\n            384-well plate.\n          \n            Use four tip boxes to add 1.25\u00a0\u03bcL of indices to each well of the\n            384-well plate. Q1 indices will be added to rows 1\u20134 of the 384-well\n            plate, Q2 to rows 5\u20138, etc.\n            \n                This layout is visualized in\n                Table\u00a0S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2496-Mmc1.xlsx]\n                (sheet \u201ci5_i7_index_and_layout\u201d).\n              \n            Seal the plate and mix the samples by tapping gently on the\n            benchtop.\n          \nSpin the plate at 20,000\u00a0\u00d7\u00a0g for 3\u00a0min.\n            Put the plate in a thermocycler equipped with a 384-well plate\n            block, and run the Nextera PCR protocol.\n          \ntable:files/protocols_protocol_2496_9.csv\n        Pool the individual samples.\n        \n            Once the Nextera PCR program has finished and the thermocycler is\n            holding at 4\u00b0C, remove the place and spin it at 20,000\u00a0\u00d7\u00a0g\n            for 3\u00a0min.\n          \n            Using the four boxes of low-retention tips, pool together 2.5\u00a0\u03bcL of\n            each sample from the 384-well plate into row H of the reagent plate.\n          \nSeal the plate with a foil seal.\n            Spin the plate at 20,000\u00a0\u00d7\u00a0g for 1\u00a0min to eliminate any\n            bubbles.\n          \n            Form the final pooled library by collecting all of the contents of\n            row H into a 2\u00a0mL DNA LoBind Eppendorf tube.Set the pipette to 80\u00a0\u03bcL to ensure that you are collecting all\n                of the liquid from each well of row H.\n              \n        Perform the first purification of the cDNA library.\n        \n            Ensure that the AMPure XP SPRI beads are at 25\u00b0C and vortex them for\n            30\u00a0s to resuspend.\n          \nPrepare fresh 80% EtOH.\n            Add 0.9 volumes of AMPure XP SPRI beads to the sample (for a library\n            generated from 384 samples the final volume will be 960\u00a0\u03bcL so 864\u00a0\u03bcL\n            of beads would be added, for example).\n          \nMix well by pipetting up and down.\n            Incubate bead/library mixture at 25\u00b0C for 5\u00a0min.\n            \nNote: This mixture will be\n              viscous. Ensure that the total volume of the mixture is dispensed\n              by pipetting slowly.\n            \n            Incubate the sample on a DynaMag\u2122-2 Magnet for 5\u00a0min.\n            \n                At the end of the 5\u00a0min, tighten the cluster of beads using the\n                96-well plate magnet.\n              \n            Remove the supernatant, being careful not to disturb the beads.\n          \n            Wash the beads twice with 1\u00a0mL of 80% EtOH, making sure that the\n            beads are covered.\n          \n            For each wash, rotate the Eppendorf tube at least twice on the\n            magnet.\n            \nNote: Due to the size of the\n              pellet, not all of the beads will move when the tube is turned.\n              This is okay.\n            \n            After the second wash, remove all of the EtOH. Ensure that all EtOH\n            is removed by aspirating any final drops left in the tube with a\n            20\u00a0\u03bcL tip.\n          \n            Dry the beads for 10\u00a0min, or until cracks start to appear in the\n            cluster. During the drying period, cover the tube loosely with the\n            lid of a pipette tip box.\n          \n            Elute the library in 200\u00a0\u03bcL of TE buffer, remove the sample from the\n            magnet, and pipette up and down to resuspend. Incubate for 5\u00a0min at\n            25\u00b0C.Critical: make sure that the\n              magnetic beads are wet before removing the Eppendorf tube from the\n              magnet. Not doing so can result in a loss of sample.\n            \nPlace the sample on the magnet for 5\u00a0min.\n            Remove the supernatant (contains the cDNA library) to a new 1.5\u00a0mL\n            DNA LoBind Eppendorf tube.\n            \n                Discard the 2\u00a0mL tube containing the bead pellet.\n                \nNote: It is okay if a small\n                  amount of beads are transferred with the sample.\n                \n        Perform the second purification of the cDNA library.\n        \n            Add 180\u00a0\u03bcL (0.9 times the sample volume) of AMPure XP SPRI beads to\n            the sample. Mix well by pipetting up and down, and then incubate at\n            25\u00b0C for 5\u00a0min.\n          \n            Incubate the sample on the tube magnet for 5\u00a0min. At the end of the\n            5\u00a0min, tighten the cluster of beads using the 96-well plate magnet.\n          \n            Remove the supernatant, being careful not to disturb the beads.\n          \n            Wash the beads twice with 1\u00a0mL of 80% EtOH, making sure that the\n            beads are covered.\n            \n                For each wash, rotate the Eppendorf tube at least twice on the\n                magnet.\n              \n            After the second wash, remove all of the EtOH.\n            \n                Ensure that all EtOH is removed by aspirating any final drops\n                left in the tube with a 20\u00a0\u03bcL tip.\n              \n            Dry the beads for 10\u00a0min, or until cracks start to appear in the\n            cluster.\n            \n                During the drying period, cover the tube loosely with the lid of\n                a pipette tip box.\n              \n            Elute the library in 200\u00a0\u03bcL of TE buffer, remove the sample from the\n            magnet, and pipette up and down to resuspend.\n          \nIncubate for 5\u00a0min at 25\u00b0C.\nPlace the sample on the magnet for 5\u00a0min.\n            Remove the supernatant (contains the purified library) to a new\n            1.5\u00a0mL DNA LoBind Eppendorf tube.\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2496-Fig6.jpgFigure\u00a06. Example Bioanalyzer traces from two different bulk\n                  neutrophil bulk RNAseq Nextera libraries\n                \n                  Note the broad peak, with a broad size distribution of around\n                  250\u20131,200\u00a0bp.\n                \n        Perform library quality control.\n        \n            Dilute the library 1:10 in water, and analyze the size distribution\n            on an Agilent Bioanalyzer using the Agilent DNA 1000 Kit according\n            to the\n            manufacturer\u2019s protocol[href=https://www.agilent.com/cs/library/usermanuals/public/G2938-90322_HighSensitivityDNAKit_QSG.pdf].\n            \n                Example traces are shown below in Figure\u00a06[href=https://www.wicell.org#fig6].\n              \n            Determine the concentration of the library using a\n            QubitTM dsDNA HS kit[href=https://www.thermofisher.com/document-connect/document-connect.html?url=https://assets.thermofisher.com/TFS-Assets%2FLSG%2Fmanuals%2FQubit_dsDNA_HS_Assay_UG.pdf].\n            \n                Prepare 700\u00a0\u03bcL of Qubit working solution by diluting the\n                appropriate volume of Qubit\u2122 dsDNA HS Reagent 1:200 in dsDNA HS\n                Buffer.\n              \n                For the cDNA library, combine 199\u00a0\u03bcL of Qubit working solution\n                and 1\u00a0\u03bcL of sample in a Qubit assay tube.\n                \nCritical: Hold the tube\n                  containing the cDNA library on the tube magnet while\n                  transferring the sample. Any bead contamination will mess up\n                  the results of the Qubit assay.\n                \n                Prepare standards 1 and 2 by combining 190\u00a0\u03bcL of Qubit working\n                solution with 10\u00a0\u03bcL of the appropriate standard in Qubit assay\n                tubes.\n              \n                Make sure to vortex samples and standards well, then spin for\n                5\u00a0s to get rid of any bubbles.\n              \n                Select the dsDNA High Sensitivity program on the Qubit\n                fluorometer.\n              \n                Calibrate with standards 1 and 2, and then read the\n                concentration of the cDNA library.\n                \nPause point: the\n                  library can be stored at \u221220\u00b0C until sequencing.\n                \n        Perform paired-end sequencing of cDNA libraries from enriched\n        neutrophils using the Illumina NovaSeq S4 flowcell and NovaSeq 6000\n        sequencer or an equivalent platform.\n        \n            For the NovaSeq S4, use the following read parameters for 300 total\n            cycles: 2\u00a0\u00d7\u00a0150\u00a0bp read 1/read 2 sequencing reads, and 8\u00a0bp i7/i5\n            index reads.\n          \nProcess sequencing data\nTiming: 1\u20132\u00a0days\n      The alignment and quantification methods used herein are based on theGTEx/TOPMed RNA-seq pipeline from the Broad Institute, with custom\n      modifications.\n    \nObtain paired-end FASTQ files from the sequencer.\nNote: Each sample should have one R1 and\n      one R2 fastq file. Depending on the sequencer or data delivery platform\n      used, it may be necessary to concatenate files from multiple lanes of a\n      flow cell that correspond to the same sample.\n    \n        Install components of the pipeline (https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md[href=https://github.com/broadinstitute/gtex-pipeline/blob/master/TOPMed_RNAseq_pipeline.md]).\n      \n        Install FastQC by downloading the software and unzipping it in a\n        suitable location.\n      \nRun FastQC on the FASTQ samples to test their quality.\n>fastqc \u2217.fastq\n        Align FASTQ files to the genome using STAR (Spliced Transcripts\n        Alignment to a Reference).\n        \n            Using the GTEx pipeline Docker image established in \u201cbefore you begin[href=https://www.wicell.org#before-you-begin]\u201d, run the following code.\n          \n            \u201cstar_index\u201d is the path to the STAR index with the appropriate\n            overhang generated in step 11 of \u201cbefore you begin[href=https://www.wicell.org#before-you-begin]\u201d, \u201cfastq1_abs\u201d and \u201cfastq2_abs\u201d are the paths to the paired FASTQ\n            files, and \u201cprefix\u201d is the name to be given to the BAM files.\n          \n>/src/run_STAR.py \\\n>\u00a0\u00a0star_index fastq1_abs fastq2_abs prefix \\\n>\u00a0\u00a0--output_dir star_out \\\n>\u00a0\u00a0--outFilterMultimapNmax 20 \\\n>\u00a0\u00a0--alignSJoverhangMin 8 \\\n>\u00a0\u00a0--alignSJDBoverhangMin 1 \\\n>\u00a0\u00a0--outFilterMismatchNmax 999 \\\n>\u00a0\u00a0--outFilterMismatchNoverLmax 0.1 \\\n>\u00a0\u00a0--alignIntronMin 20 \\\n>\u00a0\u00a0--alignIntronMax 1000000 \\\n>\u00a0\u00a0--alignMatesGapMax 1000000 \\\n>\u00a0\u00a0--outFilterType BySJout \\\n>\u00a0\u00a0--outFilterScoreMinOverLread 0.33 \\\n>\u00a0\u00a0--outFilterMatchNminOverLread 0.33 \\\n>\u00a0\u00a0--limitSjdbInsertNsj 1200000 \\\n>\u00a0\u00a0--outSAMstrandField intronMotif \\\n>\u00a0\u00a0--outFilterIntronMotifs None \\\n>\u00a0\u00a0--alignSoftClipAtReferenceEnds Yes \\\n>\u00a0\u00a0--quantMode TranscriptomeSAM GeneCounts \\\n>\u00a0\u00a0--outSAMattrRGline ID:rg1 SM:sm1 \\\n>\u00a0\u00a0--outSAMattributes NH HI AS nM NM ch \\\n>\u00a0\u00a0--chimSegmentMin 15 \\\n>\u00a0\u00a0--chimJunctionOverhangMin 15 \\\n>\u00a0\u00a0--chimOutType Junctions WithinBAM SoftClip \\\n>\u00a0\u00a0--chimMainSegmentMultNmax 1 \\\n>\u00a0\u00a0--threads 8\n        Use Picard MarkDuplicates to identify PCR artifacts and generate the BAM\n        index.\n        \n            Duplicates are defined as reads coming from the same fragment ofDNA. \u201cinput_bam\u201d is the output of the previous step.\n          \n>python3 -u /src/run_MarkDuplicates.py input_bam prefix \\\n>\u00a0\u00a0--memory 4 \\\n>\u00a0\u00a0--max_records_in_ram 50000 \\\n>\u00a0\u00a0samtools index output_bam\n        Use RSEM (RNA-Seq by Expectation-Maximization) to quantify transcript\n        abundance from the BAM file with duplicates marked.\n      \n>/src/run_RSEM.py \\\n>\u00a0\u00a0--max_frag_len 1000 \\\n>\u00a0\u00a0--estimate_rspd true \\\n>\u00a0\u00a0--threads 2 \\\n>\u00a0\u00a0rsem_reference output_bam sample_id\n>gzip \u2217.results\n>\u00a0\u00a0samtools index output_bam\n        Obtain quality control metrics of the alignment using RNA-SeQC2.\n        \n            RNA-SeQC2 takes the duplicate-marked BAM file, sample_id, and the\n            collapsed genes GTF generated in \u201cbefore you begin[href=https://www.wicell.org#before-you-begin]\u201d as inputs.\n          \n>rnaseqc genes_gtf output_bam . -s sample_id -vv\n>gzip \u2217.gct\n        Aggregate results for RSEM and RNA-SeQC2 separately.\n        \n            tpm_gcts, count_gcts, exon_count_gcts, and metrics_gcts are the\n            listed outputs of RNA-SeQC2, while rsem_isoforms and rsem_genes are\n            the listed outputs of RSEM.\n          \n>mkdir individual_outputs\n>mv tpm_gcts individual_outputs/\n>mv count_gcts individual_outputs/\n>mv exon_count_gcts individual_outputs/\n>mv metrics_tsvs individual_outputs/\n>python3 -m rnaseqc aggregate \\\n>\u00a0\u00a0--parquet \\\n>\u00a0\u00a0-o . \\\n>\u00a0\u00a0individual_outputs \\\n>\u00a0\u00a0prefix\n>\n>python3 /src/aggregate_rsem_results.py rsem_isoforms TPM IsoPct\n          expected_count prefix\n>python3 /src/aggregate_rsem_results.py rsem_genes TPM\n          expected_count prefix\nQuality control analysis\nTiming: 2 h\u20132\u00a0days\n    \n        Import the count matrix, transcripts-per-million (TPM) matrix, quality\n        control metrics, and other metadata into R.\n      \n        Log transform the TPM data and run a principal component analysis (PCA)\n        (Figure\u00a07[href=https://www.wicell.org#fig7]).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2496-Fig7.jpg\n              Figure\u00a07. Principal component analysis (PCA) plots displaying\n              Exonic Rate and Median 3\u2032 bias of samples pre- and post-quality\n              control filtration\n            \n            Plot the various quality control metrics over the PCA to see if any\n            parameters are driving outlier behavior and which should be filtered\n            on.\n          \n>genepc <-\n          read.delim(paste0(prefix,\"Ensembl_to_Symbol.txt\"))\n>logTPM <- log2(TPM\u00a0+ 1)\n>proteincoding <- genepc$Gene.stable.ID[genepc$Gene.type\u00a0==\n          \"protein_coding\"]\n>logPCG <- logTPM[which(rownames(logTPM) %in%\n          proteincoding),]\n>logPCG <- logPCG[which(rowSums(logPCG)\u00a0>\u00a00),]\n>pcaResults <- prcomp(t(logPCG), center\u00a0= T, scale.\u00a0= T)\n        Filter out low quality samples.The exact parameters for filtration will depend on the source and\n            inherent quality of the dataset.\n          \n            For this neutrophil dataset, the criteria we find to retain the most\n            high-quality samples is:\n            \nPercent of mitochondrial reads\u00a0<\u00a020%\nGenes Detected\u00a0>\u00a010,000.\nMedian Exon CV\u00a0<\u00a01.\nExon CV MAD\u00a0<\u00a00.75.\nExonic rate\u00a0>\u00a025%.\nMedian 3\u2032 bias\u00a0<\u00a00.9.\n                Other quality control metrics may be considered depending on the\n                context; for example, one could generate a \u201chousekeeping gene\u201d\n                score based on genes that viable cells should express and remove\n                samples below a threshold.\n              \n        Run the PCA once more, this time using only the high-quality samples (Figure\u00a07[href=https://www.wicell.org#fig7]). Check that technical bias does not have a major impact on the data\n        distribution.\n      \n        Check that the sample filtration procedure has not introduced bias into\n        the dataset.\n        \n            For example, it is possible that samples from patients with severe\n            disease could have lower quality and be systematically filtered,\n            biasing the data towards patients with mild disease.\n          \n            Similar bias could unintentionally be introduced on the basis of\n            age, gender, race, ethnicity, disease status, or comorbidities.\n          \n            To test whether there is an association between sample exclusion and\n            one of these variables, perform a Fisher test on the two-way table\n            comparing inclusion status versus the categories of the clinical\n            variable.\n          \nObtain cell type proportion estimates from CIBERSORTx\nTiming: 2\u20136 h\n        Upload the quality control-filtered gene count matrix to CIBERSORTx as a\n        \u201cMixture\u201d file.\n      \n        Select \u201c2. Impute Cell Fractions\u201d in \u201cCustom\u201d mode.\n        \n            Select the Signature Matrix file generated during \u201cbefore you begin[href=https://www.wicell.org#before-you-begin]\u201d, and pick the \u201cMixture\u201d file that has been uploaded in the\n            previous step.\n          \n            Run the algorithm in \u201crelative\u201d mode (i.e., not absolute mode).\n          \n        Download the cell type proportions as estimated by CIBERSORTx (Figure\u00a08[href=https://www.wicell.org#fig8]).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2496-Fig8.jpg\n              Figure\u00a08. Bar plots displaying the CIBERSORTx estimated cell typepercentages for each sample\n            \n              Bar plots are separated by the absolute neutrophil count (ANC)\n              range of the blood sample from which the neutrophils were\n              isolated.\n            \n            Check for associations between the various estimates and the\n            outcomes of interest to the study.\n          \nNote: For example, we identified an\n      association between higher estimated T/NK cell fraction and non-severe\n      COVID-19, which could potentially confound the results of a differential\n      expression analysis between severe and non-severe disease. To control for\n      this, the cell type proportion estimates can be used as covariates in\n      downstream analyses.", "Step-by-step method details\nStep-by-step method details\nTree-based MP analysis\nTiming: 1\u00a0day to a few days\nNote: Timing depends on the size and complexity of the data set matrix, and also of PAUP software execution times of 16 MP and 16 bootstrap runs.\nFor the MP analysis with the complete initial Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc1.zip], take into consideration three kinds of constraints in the execution of the runs: the type of characters, the presence of polymorphic characters in multistate taxa, and with regard to the weighing of characters use two options, i.e., initially apply same weights and reweigh characters after the initial runs.\nCritical: It is essential to discard preconceived subjective biases with regard to these constraints, and conduct the MP analysis with all possible numerical options available in order to objectively select the most parsimonious evolutionary scenario, without postulating a priori how morphological character changes evolve, what type of multistate taxa setting to use, and whether characters should be reweighed or not.\nOnce a most parsimonious evolutionary scenario is selected, the supporting derived characters (apomorphies and eventually informative homoplasies) at the nodes of the cladogram (LCAs) ought to be visualized such as for example in Figures S9 and S10 of Caparros and Prat (2021)[href=https://www.wicell.org#bib8] showing graphically the apomorphies listed in Tables S2 and S3 of that paper. The visualization will facilitate the discussion of the anatomic modifications of an adaptive nature or the result of epigenetic processes that come in support of the evolutionary changes.\nFinally, the most parsimonious evolutionary scenario must be validated by comparing a bootstrap simulation based on the same MP parsimony settings with an alternative statistical method such as for example BMCMC.Critical: Doubts have been expressed as to whether phylogeny reconstruction ought to be considered as a statistical question, with the suggestion that phylogenetic inference is best viewed in non-statistical terms (Farris, 1983[href=https://www.wicell.org#bib18]). In studies of phylogenetic reconstruction both the non-statistical method such as MP and the statistical ones, such as BMCMC and bootstrap analyses, must be used and compared for a better interpretation of the data. However, in light of contrary views concerning the adequacy of MP or statistical methods to make phylogenetic inferences (Wright and Hillis, 2014[href=https://www.wicell.org#bib50]; Kolaczkowski and Thornton, 2004[href=https://www.wicell.org#bib30]), as far as statistical methods are concerned caution must be exercised with regard to the question of statistical consistency, the complexity and empirical basis of the evolutionary models used, and what epistemological considerations support the use of these specific models of evolution.\nExecute 16 MP runs and 16 corresponding bootstrap runs With Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc1.zip] and the PAUP software execute 16 MP runs corresponding to 16 different evolutionary tree scenarios with two settings for weights (equal weight and Rescaled Consistency Index RC reweight), four character types (Fitch, Wagner, Camin-Sokal or Dollo), and two settings for multistate taxa (uncertainty or polymorphism). The runs can be executed with a search algorithm in two phases: one phase with equal weights and one with characters reweighed. We show below the relevant dropdown menus used to execute the 2 phases.\nNote: With regard to the computational MP tree search methods, there exists three algorithms available in PAUP:\nExhaustive Search: exact algorithm that evaluates every possible tree and can be used for a maximum of 12 terminal taxa.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig2.jpg\nBranch-and-Bound Search: also an exact search option that can be used for more than 12 taxa; however, this algorithm works computationally for data sets of a limited size.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig3.jpgHeuristic search: when the processing with the branch-and-bound algorithm is not feasible with a large data set, the only alternative to process the data is by heuristic search that operates by hill-climbing methods, and is computationally effective. In Caparros and Prat (2021)[href=https://www.wicell.org#bib8] all the runs were conducted by heuristic search with simple stepwise addition and TBR branch swapping.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig4.jpg\nFor further explanations of the search parameters consult the PAUP manual (http://phylosolutions.com/paup-documentation/paupmanual.pdf[href=http://phylosolutions.com/paup-documentation/paupmanual.pdf]).\nMP runs corresponding to 8 scenarios with initial equal character weights In a first phase, execute 8 equal weight runs with the appropriate search method and the following parameters:\nEqual character weights.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig5.jpg\nFour character-types.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig6.jpg\nRuns are executed with the four character types optimality criteria (Fitch-unordered, Wagner-ordered, Camin-Sokal-irreversible or Dollo-reversible). For Dollo and Irrev types two modes are available: Up (normal) and Down (reversed); these modes are related to the polarity of the coded observed states, with Up starting with the lowest observed ancestral state and Down the highest. We used Up polarity in Caparros and Prat (2021)[href=https://www.wicell.org#bib8].Note: There are four main character type options to choose from PAUP to conduct MP analysis runs . The four parsimony optimality criteria represent four models of evolutionary paths applied to character changes, subject to constraints as to the polarity or reversibility of the characters. If one does not want any constraint, one then uses unordered Fitch setting (Fitch, 1971[href=https://www.wicell.org#bib22]) which permits any state of a character to transform directly to any other state. If one wants to impose a constraint as to the direction (polarity) of the state changes, one uses Wagner ordered setting (Farris, 1970[href=https://www.wicell.org#bib16]; Swofford and Maddison, 1987[href=https://www.wicell.org#bib48]) which is bidirectional, i.e., in a three states 0,1,2 character there is no requirement that 0 be the ancestral state, only the resulting tree will determine which state is ancestral after orienting the tree (graph) with an outgroup. Wagner parsimony makes convergence and reversion possible, as in Fitch parsimony. If one considers that evolution is irreversible as suggested by Simpson (1953)[href=https://www.wicell.org#bib40], then one chooses the Camin-Sokal criterion (Camin and Sokal, 1965[href=https://www.wicell.org#bib6]) which is equivalent to ordered characters with the additional constraint of irreversibility, or unidirectionality from a plesiomorphic state or less derived state to a more derived state. Finally, if one assumes that a derived character can change only once from the plesiomorphic state to the derived state, and can revert more than once to the plesiomorphic state, then the Dollo criterion (Farris, 1977[href=https://www.wicell.org#bib17]), which only allows homoplasy under the form of reversal, is required. For a more comprehensive description of these four main choices we refer to Caparros and Prat (Caparros and Prat, 2021[href=https://www.wicell.org#bib8] - Supplemental Information/Transparent Methods 1. Maximum Parsimony (MP) optimality criteria), and show below their graphical representation.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig7.jpgCritical: In most MP published studies in Paleoanthropology, authors commonly use the unordered character type (Fitch parsimony) or ordered type (Wagner parsimony), or a combination of both (Collard and Wood, 2000[href=https://www.wicell.org#bib10]; Strait and Grine, 2004[href=https://www.wicell.org#bib46]), whereas the ones favoring convergent homoplasies (Camin-Sokal parsimony) or reversals (Dollo parsimony) are never selected. The use of Fitch and/or Wagner character types would indicate that authors make a priori assumptions about the process of evolution with regard to character-state changes. Thus, to avoid any bias with regard to which model of evolutionary path a character type should follow, we strongly recommend that the four character types be used in the analysis runs.\nUncertainty or polymorphism setting for multistate taxa For each of the 4 character types execute runs with uncertainty and polymorphism settings for multistate taxa.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig8.jpg\nCritical: Many authors tend to discard polymorphic characters in multistate taxa. Ignoring polymorphism results in a loss of information (Wiens, 1999[href=https://www.wicell.org#bib49]); this is the reason why we recommend that polymorphic characters in multistate taxa be coded and kept in the data set. For further discussion of the multistate taxa choices (uncertainty or polymorphism), we refer to Caparros and Prat (Caparros and Prat, 2021[href=https://www.wicell.org#bib8] - Supplemental Information/Transparent Methods 2. Treatment of multistate taxa).\nNote: Caution is required in interpreting the results with regard to uncertainty or polymorphism setting for multistate taxa. After running the parsimony algorithm, PAUP assigns one of the states of a multistate character to the terminal taxon, i.e., the algorithm treats the multistate character as an ambiguity, and by optimization assigns one of the features to the terminal in the output. Conceptually one may construe the selected chosen state in the output terminal as if the polymorphism had resulted in the monomorphic dominant fixation of the trait in the concerned species.Thus, using the distinct character types (4) and multistate settings (2), 8 runs will be executed in total with equal character weights.\nMP runs corresponding to 8 scenarios with reweighed characters In a second phase, execute 8 runs with characters reweighed by the respective Rescaled Consistency indices (RC) obtained from the previous corresponding equal weight runs, with the same character types and multistate settings. The relevant parameters are as follows:\nRC reweighed characters.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig9.jpgCritical: Weighing of characters in cladistics is a controversial issue, one argument being that reweighing characters after an initial run leads to unparsimonious hypotheses (Kluge, 1997[href=https://www.wicell.org#bib29]). However, Farris (1983)[href=https://www.wicell.org#bib18] considered that characters with more homoplasy are less reliable and suggested that parsimony is not equivalent to equal weights; he argued on the basis that some characters represent stronger evidence than others, that consequently they should be weighed. Reweighing of morphological data sets produces substantial increases in jackknife frequencies, and results compared to equal weighing become more stable with lower error rates (Goloboff et\u00a0al., 2008[href=https://www.wicell.org#bib25]). This supports the affirmation that \u201cit is not that parsimony does not preclude weighing, but rather that it requires weighing\" (Goloboff, 1993[href=https://www.wicell.org#bib23]). Successive weighing is successful when cladistically reliable characters such as apomorphies are outnumbered by homoplasies (Farris, 1969[href=https://www.wicell.org#bib15]), which is generally the case in most studies. Thus, should the characters originally with equal weight be reweighed after a first run? The answer is definitely yes as is evident in Caparros and Prat (Caparros and Prat, 2021[href=https://www.wicell.org#bib8], Table 1) showing that reweighing produces higher tree Retention Indices RIs. Higher tree RIs imply a more coherent degree of overall synapomorphy and a more consistent phylogenetic information signal. The use of the character rescaled consistency index rc\u00a0= retention index (ri) x consistency index (ci) from the first phase runs is recommended as a weighing criterion (Farris, 1989a[href=https://www.wicell.org#bib19]) of the respective characters.\nFour character-types (Fitch-unordered, Wagner-ordered, Camin-Sokal-irreversible or Dollo-reversible) are used as in step 1, a, ii.\nUncertainty or polymorphism settings for multistate taxa are used as in step 1, a, iii. Thus, using the distinct character types (4) and multistate settings (2), 8 runs will be executed in total with character RC reweighing.Execute 16 bootstrap runs To assess the branch support of the 16 MP tree scenarios individually, execute bootstrap runs concurrently to the MP runs with the same parsimony settings used in each MP run.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig10.jpg\nCritical: Bootstrapping, a random resampling method developed by Efron (1979)[href=https://www.wicell.org#bib14] to evaluate the accuracy of statistical estimates, was proposed by Felsenstein (1985)[href=https://www.wicell.org#bib21] to assess the confidence intervals in phylogenetic analyses. There seems to exist some confusion with regard to the use of bootstrap simulation in MP analysis. In some phylogenetic studies, the MP bootstrap consensus tree is used as a final result and considered as an end in itself, without relating it to or showing the relevant MP tree it is supposed to evaluate. For comparative purposes, it is imperative to show graphically the 50% majority rule bootstrap consensus tree with its relevant bootstrap branch replicate values along with the corresponding tree figure (single or consensus) of the MP scenario, as for example in Figures S3\u2013S6 of Caparros and Prat (2021)[href=https://www.wicell.org#bib8].\nNote: Bootstrap values in general are negatively affected by the number of characters, the number of non-informative characters, the number of missing characters in some taxa, and by the sample size and the nature of the search algorithms (Soltis and Soltis, 2003[href=https://www.wicell.org#bib44]). One should not exclude the possibility that a tree topography with very low support values of an extensive and complex data set might still be phylogenetically closer to the \"true\" tree than a tree topology with higher support values from a limited and related data set. It is thus paradoxical that the more extensive a data set is, the lower the support values may be.Note: Since the bootstrap analysis is a random resampling method, it is not certain that the resulting bootstrapping consensus tree will be exactly identical to the MP tree (single or consensus) obtained with the same settings, and various bootstrapping runs might generate slightly different results. Refer to troubleshooting problem 1[href=https://www.wicell.org#sec5.1] for a potential solution.\nOutput After an MP run PAUP provides two types of output:\nTree(s)-cladogram(s). The run may generate one most parsimonious tree or multiple trees that are also most parsimonious. A run producing more than one most parsimonious tree can be expressed for analytical purposes under the form of a consensus tree (see troubleshooting problem 2[href=https://www.wicell.org#sec5.3]). We recommend to use the 50% majority rule consensus tree of the resulting minimal trees, the most robust commonly used method that allows one to retain all groups found in over half of the rival trees.Analytical descriptive parameters. In addition to the most parsimonious tree(s) and eventual consensus tree resulting from the analysis, extensive descriptive output data are available for each run from the Describe Trees menu. The following information is available to conduct an in-depth analysis of the results: main result parameters (along with the cladogram, number of most parsimonious trees, number of steps, tree Retention Index and Consistency Index; these may be summarized in a table for all the 16 runs as for example in (Caparros and Prat, 2021[href=https://www.wicell.org#bib8], Table 1), branch lengths and linkages of terminal\u00a0taxa and nodes (LCAs) to other nodes, possible character-state assignments to internal nodes, data matrix and reconstructed states for internal nodes on the basis of the optimization algorithm selected to resolve ambiguities (ACCTRAN or DELTRAN), character change lists showing at which nodes/terminal taxa each character changes, apomorphy lists showing for every branch which characters change and how, and character diagnostics list providing for each character the Minimum, Actual and Maximum number of steps (changes) allowing the computation of character consistency indices (ci), retention indices (ri) and rescaled consistency indices (rc). From this last list you will select the apomorphies with character retention index ri\u00a0= 1 that will compose Data S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc2.zip] for the intermediate MP analysis.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig11.jpg\nSelection of the most parsimonious tree scenarioWhy do apomorphies matter? An apomorphy is the sharing of an anatomical novelty by two species as a sign of close relatedness (Hennig, 1966[href=https://www.wicell.org#bib26]). By contrast, a novelty not shared by an immediate common ancestor is designated as an homoplasy such as convergence or reversal (Simpson, 1961[href=https://www.wicell.org#bib41]). For further information on the concepts of apomorphy and homoplasy refer to Caparros and Prat (Caparros and Prat, 2021[href=https://www.wicell.org#bib8] - Transparent Methods. Foundations of the phylogenetic concept of Maximum Parsimony). As a general rule if a character shows n states, an apomorphous character will result in n-1 one-time character state changes in the cladogram, and for a homoplastic character there will be at least n character-states changes. The most parsimonious tree(s) that you are searching is(are) the one(s) that reveals the maximum number of apomorphies or equivalently minimizes the number of homoplasies inherent to the tree.\nHow to identify apomorphies? The discovery of apomorphies (synapomorphies shared downstream in a tree or autapomorphies specific to a branch) represents an important aim of the MP analysis because before conducting the analysis the researcher does not know which character-state changes are apomorphous. An apomorphy is not a declaration ex ante of a character-state derived change; an apomorphy emerges from the results of the run, and the easiest and most elegant way to identify apomorphies after a PAUP run is to select characters with a retention index ri\u00a0= 1 from the \"character diagnostics\" list of the Describe Trees output menu, as shown for example in Table S1 of Caparros and Prat (2021)[href=https://www.wicell.org#bib8].The character retention index The character retention index ri is \"the fraction of apparent synapomorphy in the character that is retained as synapomorphy on the tree\" (Farris, 1989b[href=https://www.wicell.org#bib20]). We illustrate graphically in Figure 2[href=https://www.wicell.org#fig2] reproduced from Figure S8 (Caparros and Prat, 2021[href=https://www.wicell.org#bib8]) the ri character retention index's behavior related to the coherence of the MP tree; it expresses the degree of a posteriori homology apparent in the character, i.e., its information content. If ri\u00a0= 1 the character change represents an apomorphy while values of 0\u00a0< ri\u00a0< 1 represent homoplasies. The lower this index the more homoplastic the character is; thus, the quantity 1- ri represents the degree of homoplasy inherent to this character in the MP tree.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Fig2.jpg\nFigure\u00a02. Character retention index ri formula and description of how it behaves as a function of changes in character state variables\nYellow color: three variables m, s and g provided in the character diagnostic output of the first PAUP run.\nPink color: homoplasy definitions based on variables m, s and g.\nGreen color: behavior standardized from 0 to 1 of character retention index ri and its complement d observed homoplasy index.\nBlue color: inference interpretations drawn from behavior from 0 to 1 of character retention index ri.\nReproduced from Figure S8 of Caparros and Prat (2021)[href=https://www.wicell.org#bib8].\nFrom the character retention index to the tree retention index A distinction should be made between a single character retention index ri and an overall tree retention index RI. The tree Retention Index RI is the sum over all the individual-character retention indices ri calculated as per the formula (\u2211 g \u2212 \u2211 s) / (\u2211 g \u2212 \u2211 m) with g, s and m as defined in Figure 2[href=https://www.wicell.org#fig2].Metric to select the most parsimonious scenario The tree RI is the best metric to select the most informative run because it measures accurately the overall degree of synapomorhy of the MP tree (Farris, 1989a[href=https://www.wicell.org#bib19], 1989b[href=https://www.wicell.org#bib20]; Caparros, 1997[href=https://www.wicell.org#bib7]), i.e., the scenario with the highest tree RI is the most coherent and phylogenetically informative. The tree RI is a better indicator of the level of synapomorphy than the Consistency Index CI, and represents a robust proxy for confidence and accuracy estimate in the selection of the most informative and coherent MP scenario (Caparros, 1997[href=https://www.wicell.org#bib7]; Farris, 1989a[href=https://www.wicell.org#bib19], 1989b[href=https://www.wicell.org#bib20]; Mounier and Caparros, 2015[href=https://www.wicell.org#bib35]; Mounier et\u00a0al., 2016[href=https://www.wicell.org#bib36]). In essence, a high value of the RI index is indicative of a high degree of overall synapomorphy inherent to the tree, and thus of its phylogenetic information content. In Caparros and Prat (Caparros and Prat, 2021[href=https://www.wicell.org#bib8] - Figure\u00a01, Table 1) we called MPMAX the scenario with the highest RI.\nNote: Some workers may object to the use of the tree RI to select the most parsimonious scenario. In such eventuality, refer to troubleshooting problem 3[href=https://www.wicell.org#sec5.5].\nNote: The total number of steps of a PAUP run is not a reliable parameter to select the most parsimonious scenario due to the fact that the constraints (models of character type, equal weight or reweighing, and multistate taxa settings) affect the scale of the number of steps as evidenced in Table 1 of Caparros and Prat (2021)[href=https://www.wicell.org#bib8].\nVisualization of the most parsimonious scenario results and character supportElliptic graphic representation The results of an MP run are normally expressed as a tree (single or consensus) where the nodes of the branches represent the hypothetical LCAs. Hennig (Hennig, 1966[href=https://www.wicell.org#bib26] Figure 18, p.71) was the first to present an elliptic phylogenetic graphic representation equivalent to a cladogram. For illustrative purposes we show in Figure 3[href=https://www.wicell.org#fig3] the similarities between an elliptic representation and the MP cladogram from Caparros and Prat (Caparros and Prat, 2021[href=https://www.wicell.org#bib8] - Figures 1 and S7). The identifying node numbers of the LCAs of the cladogram are expressed as\u00a0numbered stem species elliptic boundaries. The difference between the cladogram and the elliptic representation is the way the phylogenetic relationship is expressed between the stem species and its successor sister groups. In the cladogram, it is represented by the nodes (hypothetical ancestors) linking the dichotomous branches to its successors (two other nodes, or a node and a terminal, or two terminals) whereas in the elliptic representation, the boundary lines of the stem species are drawn around their successor species. The elliptic boundaries illustrate more accurately than the tree-based representation the formation of higher taxa monophyletic groups in conformity with the hierarchic system of Phylogenetic Systematics, and facilitate the interpretation of the ancestor-descendant relationships among the various species under study.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Fig3.jpg\nFigure\u00a03. MPMAX cladogram and its elliptic representation\nMPMAX cladogram (A) and its elliptic representation (B) reproduced from Figures 1 and S7 of Caparros and Prat (2021)[href=https://www.wicell.org#bib8]. The nodes of the tree are indicated in black numbers with arrows. In the elliptic representation the LCA nodes are expressed as equivalent elliptic boundaries. Refer to the legend of Caparros and Prat (2021)[href=https://www.wicell.org#bib8] Figure 1 for sister group definitions.Note: The elliptic representation applies only to a fully resolved tree, i.e., it cannot be used with a tree that has polytomies. No software exists for the elliptic representation which must be done manually, with the character state changes coming in support of the branches (as illustrated for example in Figure 4[href=https://www.wicell.org#fig4] reproduced from Figure S9 (Caparros and Prat, 2021[href=https://www.wicell.org#bib8])) obtained from the PAUP run Describe Trees output.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Fig4.jpg\nFigure\u00a04. Character-state changes (apomorphies) in support of stem species elliptic boundaries 40 to 26 (tree nodes - LCAs) of MPMAX scenario\nMeaning of symbols are as follows: numbered stem boundaries correspond to nodes of equivalent tree, arrows from boxes to stem boundaries indicate supporting apomorphous character-state changes, ==> unambiguous characters and --> ACCTRAN resolved ambiguous characters. ACCTRAN optimization (Agnarsson and Miller, 2008[href=https://www.wicell.org#bib2]) was used for the treatment of ambiguities. Reproduced from Caparros and Prat (2021)[href=https://www.wicell.org#bib8] Figure S9.\nVisualization of apomorphies in support of LCAs Contrary to probability-based methods of phylogeny reconstruction, MP identifies clearly the character-state changes that support the various clades, e.g., in Caparros and Prat (Caparros and Prat, 2021[href=https://www.wicell.org#bib8] - Tables S1 and S2) 74 apomorphies in support of the LCAs were identified from the MPMAX run output with character retention index ri\u00a0= 1. We show in Figure 4[href=https://www.wicell.org#fig4] the one-time character-state changes (apomorphies) in support of stem species elliptic boundaries 40 to 26 (tree nodes - LCAs) of this scenario that encompasses the Homo genus species definitions.\nCritical: We strongly recommend that the apomorphies in support of the various clades be graphically visualized as a prior step to the discussion of the anatomical significance regarding the appearance of morphological novelties.Note: In Figure 4[href=https://www.wicell.org#fig4] the character changes highlighted for nested elliptic boundaries (stem taxa) A, B, C, D, E, F, 34 and terminal Hsap (H.\u00a0sapiens) are synapomorphies, i.e., morphological novelties passed on to succeeding nested monophyletic groups, whereas the novelties of the remaining lower sister groups J, I, H, G and stem boundaries 26, 31 and 33 are autapomorphies specific to these groups.\nProjection of individual characters on to the cladogram The PAUP software after conducting a run provides extensive detailed output data from the Describe Trees menu which are very useful in analyzing the anatomical features of individual character state changes and their relevant phylogenetic significance. However, PAUP does not provide an easy way to visualize synthetically this rich output material. In our opinion, the best software to visualize these results is MacClade which happens to be compatible with PAUP. Both the MacClade software (Maddison and Maddison, 2005[href=https://www.wicell.org#bib32]) and the PAUP software (Swofford, 2020[href=https://www.wicell.org#bib47]) use the NEXUS format, e.g., one can set up a character data matrix in MacClade and execute the MP run in PAUP with this matrix, or vice-versa visualize with MacClade the tree output of a PAUP run from a given outsourced data set.Critical: The freely available MacClade software works well only on old PowerPC Mac computers under a classic operating environment and on Intel Macs under operating systems up to MacOS 10.6. Unfortunately, it does not work under MacOS X 10.7 or later versions of the MacOS, and cannot run under Windows; thus, to benefit from this very helpful visualization tool one needs to use an old Mac computer with one of the operating systems just mentioned. We are not aware of any other phylogenetic software having the versatile features of MacClade for visualizing and rendering with high precision the results obtained from PAUP.\nSetting up a data matrix in MacClade This is done easily with the data editor, with the possibility of entering additionally the character state names along with the coded symbols if necessary.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig12.jpg\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig13.jpgProjection of individual characters on to the cladogram One attractive feature of MacClade is the possibility to project individual characters on to the cladogram from the resulting PAUP analysis run. To use this feature, read in MacClade Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc1.zip] and the relevant resolved tree saved from the PAUP run. This projection allows one to verify graphically for each character its status either as an apomorphy or homoplasy, and in the case of homoplasy to ascertain its status as convergence or reversal provided one insures that the projection is unconstrained by selecting an unordered mode. These projections are useful to explain the anatomical changes that took place along the evolutionary path of the tree. Below are four examples of character state projections (Figure 5[href=https://www.wicell.org#fig5]) from the MPMAX output of Caparros and Prat (Caparros and Prat, 2021[href=https://www.wicell.org#bib8] - Figure\u00a0S8) with (B) example of synapomorphy of mandibular fossa position character, (C) example of autapomorphy in Paranthropus clade of lingual shape of maxillary canine character, (D) example of convergence homoplasy of suprameatal spine character, and (E) example of reversal homoplasy of external auditory meatus size character.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Fig5.jpg\nFigure\u00a05. Four examples of character state projections with MacClade based on the MPMAX cladogram\nReproduced from Caparros and Prat (2021)[href=https://www.wicell.org#bib8] Figure S8\nNote: To project the characters on to the cladogram with MacClade, use the Trace menu and scroll through them one at a time to visualize them. Should a character be ambiguous with several possible most parsimonious reconstructions (MPRs), as shown for H.\u00a0antecessor in the illustration below, it is possible to view each ambiguity using the \"Show MPRs Mode\" in the menu. To find a resolved state based on ACCTRAN or DELTRAN optimization option see troubleshooting problem 4[href=https://www.wicell.org#sec5.7].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig14.jpg\nValidation of the most parsimonious scenario by statistical comparisonComparison of non-statistical MP and statistical BMCMC results with same data set An empirical objective for a better interpretation of the data in studies of human phylogenetic reconstruction requires that MP, a non-statistical numerical method based on mathematical graph theory (Darlu and Tassy, 2019[href=https://www.wicell.org#bib12]), be compared with a statistical one such as for example BMCMC (Goloboff and Pol, 2005[href=https://www.wicell.org#bib24]; Cummings et\u00a0al., 2003[href=https://www.wicell.org#bib11]). However, the two methods are not directly comparable since the measure of coherence of MP such as the tree RI is unrelated to the posterior probabilities produced by the BMCMC analysis. An indirect way of comparing the two methods is to execute a bootstrap run with the same settings as the MP run, and compare the proportions of bootstrap replicates values Pboot of the bootstrap tree to the posterior probability values Ppost of the BMCMC tree resulting from the Bayesian analysis with the same data set.Are Pboot and Ppost comparable? The question is, are the values of Pboot and Ppost comparable? The proportion of bootstrap replicate value Pboot in which a clade is identified is subject to various interpretations in phylogenetic analysis (Soltis and Soltis, 2003[href=https://www.wicell.org#bib44]). Some view Pboot as a measure of repeatability, others as a measure of accuracy, i.e., closeness to the true tree, or a measure of statistical confidence. Most systematists view bootstrap values as a relative assessment of clade support rather than a strict statistical confidence statement of the nodes, or joint confidence for the entire tree. Simulations of maximum likelihood bootstrap and BMCMC analyses performed on the same sequence data set show that over various model spaces values Pboot and Ppost are not significantly different (Cummings et\u00a0al., 2003[href=https://www.wicell.org#bib11]). These findings are corroborated in a set of simulations showing that Ppost estimated by Mr Bayes software and Pboot calculated by PAUP software were almost exactly the same (Goloboff and Pol, 2005[href=https://www.wicell.org#bib24]). Thus, the search method with the highest support values will indicate relatively a higher coherence level in the relationships expressed phylogenetically by the clades.\nExample of scenario validation further to the comparison Caparros and Prat (2021)[href=https://www.wicell.org#bib8] compared the bootstrap values Pboot of MPMAX to the Ppost probabilities of a BMCMC analysis from Dembo et\u00a0al. (2016)[href=https://www.wicell.org#bib13] executed with the same data set. Figure 6[href=https://www.wicell.org#fig6] shows the results of the comparison by means of an elliptic phylogenetic representation. The MPMAX scenario had a systematically higher support in practically all of the sister groups with a higher resolution concordant with paleoanthropological paradigms, and the conclusion was that the bootstrap tree identical to the MPMAX tree with the same settings validated this most phylogenetically informative and coherent evolutionary scenario.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Fig6.jpgFigure\u00a06. Phylogenetic support comparison between MPMAX scenario and best BMCMC Dembo scenario\n(A) MPMAX bootstrap elliptic representation with replicate Pboot values (B) BMCMC Dembo elliptic representation with posterior probability values Ppost. Pboot and Ppost are shown in red and expressed in %. Reproduced from Caparros and Prat (2021)[href=https://www.wicell.org#bib8] Figure 2.\nIntermediate MP analysis\nTiming: a few hours\nIs the most parsimonious scenario MPMAX indicative of a process of reticulate evolution?\nCould a sequential hierarchical order in the most parsimonious MPMAX cladogram be indicative of a process of reticulate evolution among some of the species of this scenario? Prior to answering this question, you need to verify the phylogenetic signal related to the various sister groups by following the fundamental principle of Phylogenetic Systematics. This principle stipulates that the evolutionary history of taxa is reconstituted from characters by the sharing of apomorphies while homoplasies, resemblance not due to inheritance from a close common ancestor, represent features of an adaptive nature and are therefore phylogenetically less informative; in other words, \"The more characters certainly interpretable as apomorphous (not characters in general) are present in a number of different species, the better founded is the assumption that these species form a monophyletic group (Hennig, 1966[href=https://www.wicell.org#bib26])\".\nCritical: In essence, only apomorphies ought to be relevant in finding monophyletic groups \" not characters in general\". To obtain the response to the question whether the most parsimonious scenario is indicative of reticulate evolution, it is advisable to re-run the MP analysis with apomorphous characters.\nRe-run the parsimony analysis with apomorphous characters as prior step to the PN analysisIn keeping with the Hennigian principle that only apomorphies are relevant in finding monophyletic groups, re-run the MP analysis with apomorphies (and if necessary add informative homoplasies as per troubleshooting problem 5[href=https://www.wicell.org#sec5.9] below) resulting from the most parsimonious scenario MPMAX. Select apomorphous characters with retention index ri\u00a0= 1 to make up Data S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc2.zip] from the MPMAX character diagnostics output. With Data S2 execute an MP run with the same settings as the most parsimonious scenario from Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc1.zip] selected with the highest tree RI. In Caparros and Prat (Caparros and Prat, 2021[href=https://www.wicell.org#bib8], Table S1) apomorphies of the MPMAX scenario (Dollo type algorithm, RC reweighing and uncertainty setting for multistate taxa) were selected with character retention index ri\u00a0= 1, and generated a reduced Data S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc2.zip] of 74 apomorphous characters. There was no need to execute a second run with reweighed characters since the selected apomorphies of Data S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc2.zip] had an ri\u00a0= 1 and ci\u00a0= 1, with a reweighing parameter rc (ri x ci)\u00a0= 1 identical to the initial run equal weights. We reproduce below Table S1 of Caparros and Prat (2021)[href=https://www.wicell.org#bib8] showing character diagnostics from output of MPMAX run. The list shows for each informative character its consistency index ci, retention index ri and rescaled consistency index rc\u00a0= ci x ri. Apomorphies for the subsequent intermediate MP analysis were selected on the basis of their character ri\u00a0= 1.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig15.jpg\nExample - Intermediate MP analysis results from Caparros and Prat (2021)[href=https://www.wicell.org#bib8]The run with 74 apomorphous characters generated a set of 3213 MP trees called MMPT (multiple most parsimonious trees) which make up Data S3[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc3.zip], with perfect tree congruence indices RI\u00a0= 1 and CI\u00a0= 1 for all the trees. The 50% majority consensus tree of MMPT trees resulting from the analysis based on Data S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc2.zip] is reproduced in Figure 7[href=https://www.wicell.org#fig7]; it confirmed the existence of four highly resolved ancestral monophyletic sister groups identified in the MPMAX scenario (PreHs, Her, NalSed and Par in Figure 3[href=https://www.wicell.org#fig3] - elliptic representation) and showed four polytomies that regroup incompatible parts of the multiple trees.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Fig7.jpg\nFigure\u00a07. 50% majority consensus tree from MP run with apomorphous\u00a0Data S2\u00a0that generated 3213 Dollo MP trees (MMPT) with RI=1 and CI=1 for all trees\nReproduced from Caparros and Prat (2021)[href=https://www.wicell.org#bib8] Figure 3.\nNote: More polytomies become apparent when the number of homoplastic character decreases in the executed Data S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc2.zip], while the number of most parsimonious trees increases substantially, as observed in Caparros (1997)[href=https://www.wicell.org#bib7]. Polytomies may be construed as background noise defined as N\u00a0= (1 - RI\u2217), RI\u2217 being the tree retention index of the MPMAX scenario from analysis with the full Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc1.zip] and the value 1 representing the perfect tree retention index from the intermediate analysis with the apomorphous Data S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc2.zip]. Conceptually N represents the contribution of removing homoplastic characters to the diminution of the RI between the two types of analyses, and may be considered as a loss of phylogenetic information.\nAs shown below, polytomies thought to be generally uninformative resulted in being of utmost importance in the PN analysis, i.e., deciphering this apparent loss of information embedded in the polytomies allowed to conduct the PN analysis with Data S3[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc3.zip] composed of the MMPT multiple most parsimonious trees.\nPhylogenetic Networks analysisTiming: a few hours\nTransition from a tree to a network to explain human evolution\nPhylogenetic trees may not be the best tool to represent phylogenetic history in light of the complex biological processes such as hybridization that may generate evolutionary reticulation; networks are therefore the solution if one conceives that a network is a tree with reticulations. The question methodologically is how do we make the transition from a tree to a network to explain evolution. In a hybrid species, its genome comes partly from one parent and partly from the other, so the resulting hybrid genome will have different evolutionary histories; therefore, the analysis will endeavor to reconstruct and blend the different phylogenetic trees of the genomes. In the construction of a hybridization network the conflicting gene trees will be processed, and not the original genomic data. More generally, the question becomes how do we represent evolutionary relationships when the history of a set of taxa includes inheritance from multiple ancestors across hybridized taxa.\nEmpirical assumptions to link MP and PN analyses\nBased on the analogy of hybrid species sharing multiple ancestral evolutionary histories, in Caparros and Prat (2021)[href=https://www.wicell.org#bib8] we used a Phylogenetic Networks conceptual approach extensively employed in molecular biology to test the hypothesis of phenotypic hominin evolutionary reticulation. To establish a link between the MP and the PN analyses, we made two empirical assumptions: 1) apomorphous character-state changes represent analogous proxies of gene sequence changes evolving from common ancestors, and 2) the 3213 MP trees (MMPT) making up Data S3[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc3.zip] (resulting from the parsimony PAUP run with apomorphies of Data S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc2.zip]) are methodologically analogous to gene trees from a set of species, and are thus used as input in the PN analysis.Types of phylogenetic networks Huson and Bryant (2006)[href=https://www.wicell.org#bib27] distinguish three main types of phylogenetic networks:\nPhylogenetic trees used to summarize taxonomic relationships between biological entities.\nSplits networks used to represent incompatibilities in data sets reflected for example by polytomies in consensus trees.\nReticulate networks used to explain evolutionary histories with edges representing lineages of descent or reticulate events, and internal nodes representing hypothetical ancestors. In Caparros and Prat (2021)[href=https://www.wicell.org#bib8], to shed some further light on the taxonomic threshold between\u00a0the genus Homo and other hominin sister groups, we tested the hypothesis of phenotypic hominin evolutionary reticulation based on these three types of phylogenetic networks.\nNote: For a review of Phylogenetic Networks methods, we refer to the comprehensive syntheses of the field by Huson et\u00a0al. (2010)[href=https://www.wicell.org#bib28] along with the freely available application software SplitsTree (Huson and Bryant, 2006[href=https://www.wicell.org#bib27]), and Morrison (2011)[href=https://www.wicell.org#bib34].\nSteps used in Caparros and Prat (2021)[href=https://www.wicell.org#bib8] as example of PN analysis With the MMPT Data S3[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc3.zip] and using the SplitsTree software (Huson and Bryant, 2006[href=https://www.wicell.org#bib27]), we computed the majority consensus tree, the consensus network and the reticulate network which we reproduce here in Figure 8[href=https://www.wicell.org#fig8].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Fig8.jpg\nFigure\u00a08. Phylogenetic networks graphical results\nTree and network computations were executed with the program SplitsTree (Huson and Bryant, 2006[href=https://www.wicell.org#bib27]) based on Data S3[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc3.zip] (MMPT = 3213 trees) obtained from the Intermediate MP analysis with Data S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc2.zip].\n(A) Unrooted majority consensus tree with mean edge weights (pipeline Taxa=24 [Trees > ConsensusTree > EqualAngle]).\n(B) Consensus network computed with threshold = 0.10, mean edge weights and splits convex hull transformation (pipeline Taxa=24 [Trees > ConsensusNetwork > ConvexHull]).\n(C) Rooted reticulate network with first taxon as outgroup and 180 equal angle (pipeline Taxa=24 [Trees > ConsensusNetwork > ReticulateNetwork]).\nReproduced from Figure 4 in Caparros and Prat (2021)[href=https://www.wicell.org#bib8].Read the MMPT Data S3[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-Mmc3.zip] nexus file with the SplitsTree software and hit Apply. The sequence of steps with the dropdown menus to produce the 3 panels of Figure 8[href=https://www.wicell.org#fig8] are illustrated as follows:\nConsensus tree\nScreenshot for Consensus Tree run.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig16.jpg\nScreenshot of resulting Consensus Tree.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig17.jpg\nConsensus network\nScreenshot for Consensus Splits of trees.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig18.jpg\nScreenshot of resulting Consensus Network with EqualAngle.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig19.jpg\nScreenshot for splits graph obtained with ConvexHull.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig20.jpg\nScreenshot of resulting Consensus Network obtained with ConvexHull.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig21.jpg\nReticulate network\nScreenshot for Reticulate Network.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig22.jpg\nScreenshot of resulting Reticulate Network.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1507-InFig23.jpg", "Step-by-step method details\nStep-by-step method details\nGenome annotation\nTiming: days to weeks (depending on genome size and complexity; e.g., \u223c15\u00a0days for the 3-Gb beaver genome on a 10-node computer cluster with 12-GB memory in each node)\nIn this step, we predict and annotate genes for the de novo genome assembly of a target species, the focus of a particular genome annotation study. Besides Maker2 pipeline described here, researchers can also consider other pipelines as alternative choice, e.g., BRAKER2 (Bruna et\u00a0al., 2021[href=https://www.wicell.org#bib6]), the Ensemble gene annotation system (Aken et\u00a0al., 2016[href=https://www.wicell.org#bib2]) etc.\nMask repetitive elements in the genome.\nRepeatMasker.\nConstruct species-specific repetitive elements by RepeatModeler (version 1.0.10). Details about this step can be find at http://weatherby.genetics.utah.edu/MAKER/wiki/index.php/Repeat_Library_Construction--Basic[href=http://weatherby.genetics.utah.edu/MAKER/wiki/index.php/Repeat_Library_Construction--Basic].\n# Collect repetitive elements\n# Input: genomic sequences\n# Output: file \u201cconsensi.fa.classified\u201d, containing the receptive sequences\n# path_RM: path of RepeatModeler\n# -pa N: how many cores to run\n# -engine ncbi: refers to blast program for alignment\n\u00a0\u00a0<path_RM>/BuildDatabase -name seqfiledb -engine ncbi <genome.fa>\n\u00a0\u00a0<path_RM>/RepeatModeler -database seqfiledb -pa N\u00a0>\u00a0seqfile.out\nMask repeat elements by RepeatMasker (Tarailo-Graovac and Chen, 2009[href=https://www.wicell.org#bib21]), with RepBase repeat libraries (Bao et\u00a0al., 2015[href=https://www.wicell.org#bib3]), together with specifies-specific repeating elements constructed in the previous step. Repbase stores consensus sequences of repetitive elements among species. This step masks both common repeat elements among species and species-specific repetitive elements in the assembly.\nRepeatRunner. RepeatRunner is a part of the Maker2 pipeline, which also comes with a list of default transposable protein elements in the FASTA format. Maker2 will automatically search for more divergent transposable protein elements.\nNote: Repetitive elements are enriched throughout the genome. Such repetitive elements can cause non-specific gene hits during annotation. By masking repetitive elements, annotation tools can target gene encoding regions more easily.\nTrain the models.\nTrain Augustus.\n# Train Augustus\n# Input: genome assembly# Output: trained model\n# --long: performs full optimization for Augustus training\n\u00a0\u00a0python <directory_of_BUSCO>/BUSCO.py --cup <number_thread> --in <genome_assembly>.fa --out <output_name> --lineage <directory_BUSCO_lineage_data> --mode genome --long\nNote: The \u201c--long\u201d parameter turns on Augustus optimization mode for self-training, which can improve the accuracy for non-model organisms. The latest lineage data are available at https://busco-data.ezlab.org/v5/data/lineages/[href=https://busco-data.ezlab.org/v5/data/lineages/].\nTrain SNAP. The details of training SNAP can be found in (Campbell et\u00a0al., 2014[href=https://www.wicell.org#bib7]), which recommends three iterations of training. The trained parameter/HMM file from the current round can be used to seed the next round of training.\n# Train SNAP\n# (a) Generate MAKER control files\n# Generate three files with suffix \u201c.ctl\u201d, through which to provide user input\n\u00a0\u00a0maker -CTL\n# (b) Edit maker_opts.ctl file to provide input parameters\ngenome=<genome_assembly.fa>\n# choose either eukaryotic or prokaryotic\norganism_type=<eukaryotic|prokaryotic>\n# Expressed sequence tags (ESTs) or assembled mRNA\nest=<transcript_evidence.fa>\n# Protein sequences from other organisms (e.g., UniProt)\nprotein=<protein.fa>\n# Gene prediction method\n# (1st round training derive gene mode from EST, i.e., est2genome=1 or protein evidence, i.e., protein2genome=1)\nest2genome=1 | protein2genome=1\n# (c) Run MAKER\n# Run on a single processor by \u201cmaker\u201d or on \u201cN\u201d processors by \u201cmpirun -n\u201d\n\u00a0\u00a0maker | mpirun -n N maker\n# (d) Collect annotation result and merge into a single file\n\u00a0\u00a0cd <maker.ouput>\n\u00a0\u00a0gff3_merge -d <genome_datastore_index.log> -g\n# (e) Make a directory for the training\n\u00a0\u00a0mkdir <snapTrain1>\n\u00a0\u00a0cd <snapTrain1>\n# (f) Generate files required for training\n# Generate <genome.ann>, <genome.dna> required to train SNAP\nmaker2zff <../all.gff>\n# \u201cfathom\u201d separates annotation into categories\n# uni: single gene per sequence\n# alt: genes with alternative splicing\n# olp: genes overlap others\n# err: genes with errors\n# wrn: genes with warnings\n\u00a0\u00a0fathom -categorize 1000\u00a0<genome.ann> <genome.dna>\n# \u201cfathom\u201d exports the genes\n# Generate export.aa, export.ann, export.dna, export.txt\n\u00a0\u00a0fathom export 1000 uni.ann uni.dna# (g) Generate new parameters\n\u00a0\u00a0mkdir params\n\u00a0\u00a0cd params\n\u00a0\u00a0forge ../export.ann ../export.dna\n\u00a0\u00a0cd ..\n# (h) Generate new HMM\n\u00a0\u00a0hmm-assembler.pl <genome> params\u00a0>\u00a0<genome.hmm>\n\u00a0\u00a0cd ..\n# (i) Update maker_opts.ctl & retrain the model from step (c)\u00a0to (h)\nsnaphmm=<genome.hmm>\nest2genome=0\nprotein2genome=0\nGene annotation and functional annotation.\nGene structure annotation. In addition to gene prediction models, evidence from orthologous protein sequences and transcriptome assembly could be used to improve annotation quality. Protein sequences of orthologous genes can be obtained from UniProt (The UniProt, 2017[href=https://www.wicell.org#bib22]). Ones from Swiss-Port have been reviewed and thus are of higher quality. Transcriptome assembly may be available from previous studies or can be assembled de novo from RNA-seq reads by Trinity (Haas et\u00a0al., 2013[href=https://www.wicell.org#bib9]). High quality transcriptome assembly can be selected as described in (Zhang et\u00a0al., 2021[href=https://www.wicell.org#bib25]).\n# Gene structure annotation\n# (a) Generate MAKER control files\n# Generate three files with suffix \u201c.ctl\u201d, through which to provide user input\n\u00a0\u00a0maker -CTL\n# (b) Edit maker_opts.ctl file to provide input parameters\ngenome=<genome_assembly.fa>\n# choose either eukaryotic or prokaryotic\norganism_type=<eukaryotic|prokaryotic>\n# Expressed sequence tags (ESTs) or assembled mRNA\nest=<transcript_evidence.fa>\n# Protein sequences from other organisms (e.g., UniProt)\nprotein=<protein.fa>\n# Gene prediction models\nsnaphmm=<SNAP_trained_model>\naugustus_species=<augustus_trained_model>\n# (c) Run MAKER\n# Run on a single processor by \u201cmaker\u201d or on \u201cN\u201d processors by \u2018\u201dmpirun -n\u201d\n\u00a0\u00a0maker | mpirun -n N maker\n# (d) Collect annotation result and merge into a single file\n\u00a0\u00a0cd <maker.ouput>\n\u00a0\u00a0gff3_merge -d <genome_datastore_index.log> -g\nNote: Details about gene structure annotation (Holt and Yandell, 2011[href=https://www.wicell.org#bib10]) can be found at http://gmod.org/wiki/MAKER_Tutorial[href=http://gmod.org/wiki/MAKER_Tutorial], https://darencard.net/blog/2017-05-16-maker-genome-annotation/[href=https://darencard.net/blog/2017-05-16-maker-genome-annotation/], and the protocol (Campbell et\u00a0al., 2014[href=https://www.wicell.org#bib7]).Quality measurement and functional annotation. For each predicted gene, Maker2 provides the annotation edit distance (AED) score, which measures the goodness of fit between its predicted gene structure and its evidence support. The lower the score, the more accurate the prediction. If more than 90% genes with AED scores lower than 0.5, the genome can be considered well annotated. In addition to the AED score, a high proportion of recognizable domains contained in predicted protein \u2013 e.g., higher than 50% \u2013 also indicates a good annotation. Recognizable protein domains can by scanned by InterProScan (Jones et\u00a0al., 2014[href=https://www.wicell.org#bib12]), assigning potential function to predicted genes.\nNote: Besides the aforementioned quality measurement, we strongly recommend measuring the completeness of the genome assembly and annotation by checking the existence of a set of Benchmarking Universal Single-Copy Orthologs (BUSCO) (Simao et\u00a0al., 2015[href=https://www.wicell.org#bib19]). A high-level completeness of genome assembly and annotation is imperative for a better identification of gene expansion. Based on the result of this analysis, researchers can decide whether they need to further improve the genome assembly before predicting gene expansion. A detailed protocol of BUSCO is available at (Manni et\u00a0al., 2021[href=https://www.wicell.org#bib15]).\n# Run BUSCO\n# Input: genome sequence or protein sequence to be measured\n# Output: completeness of input regarding to near-universal single-copy orthologs\n# -i: input file, either a nucleotide fasta file or a protein fasta file\n# -l: lineage dataset\n# -o: folder to save results\n# -m: assessment mode (i.e., genome, protein, transcriptome)\n\u00a0\u00a0busco -i <DNA.fa|protein.fa> -l <lineage> -o <output> -m <mode>\nGene family construction\nTiming: hours to days (e.g., \u223c12\u00a0h for 26k predicted proteins)To identify gene expansion, we first need to assign genes into gene families and then count gene copies of each gene family among species in the study. Next, the table of gene counts will be used to identify gene expansion in a particular species. See Figure\u00a01[href=https://www.wicell.org#fig1].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2006-Fig1.jpg\nFigure\u00a01. Workflow for gene expansion identification and functional analysis\nAfter gene annotation, a gene count table is prepared through OrthoDB (Kriventseva et\u00a0al., 2019[href=https://www.wicell.org#bib14]), and then replicated genes are predicted by CAFE (Mendes et\u00a0al., 2020[href=https://www.wicell.org#bib16]). For computational validation, Apollo is used to further improve gene annotation quality if necessary. Protein sequence similarity, synteny, and evolution in gene structure and coding sequences can be used to identify and examine more reliable candidate genes. RNA-seq reads can differentiate gene copies generated by divergent evolution after expansion from false ones resulted from sequencing and/or assembly errors. Final candidates can be further validated by qPCR with primers designed by Primer-Blast (Ye et\u00a0al., 2012[href=https://www.wicell.org#bib24]). Their genome-wide specificity is checked by MFEprimer (Qu et\u00a0al., 2012[href=https://www.wicell.org#bib17]) and then further inspected by gel electrophoresis and the melting curve analysis. Amplification efficiency should also be checked. For the functional assessment, GeneWise (Birney et\u00a0al., 2004[href=https://www.wicell.org#bib4]) can be used to check potential pseudogenization of gene copies, while gene expression pattern of gene copies across different tissues can be measured by Kallisto (Bray et\u00a0al., 2016[href=https://www.wicell.org#bib5]) with RNA-seq data.\nMap annotated genes into gene families.OrthoDB (Kriventseva et\u00a0al., 2019[href=https://www.wicell.org#bib14]) can be used to map annotated genes into gene families. OrthoDB is a database of hierarchical catalog of orthologs, including 1,271 eukaryotes in the current release (v10.1). In addition to species included in the database, users can upload and analyze their own protein sequences and get corresponding ortholog information, which include \u201cortholog group name\u201d, \u201cgene name\u201d, \u201cscore of match\u201d, etc.\nNote: For a customized analysis of OrthoDB, up to 5 species can be selected as references to derive ortholog information for user's data. Well annotated and evolutionally close species are recommended as potential references. Details about customized analysis can be found at https://www.orthodb.org/orthodb_userguide.html#uploading-and-analyzing-your-own-sequences[href=https://www.orthodb.org/orthodb_userguide.html#uploading-and-analyzing-your-own-sequences].\nGene family expansion identification\nTiming: several hours (e.g., \u223c3\u00a0h for 19k sets of gene homologs from 18 species on a 10-node computer cluster with 12-GB memory in each node)\nThis step will report expanded gene families and corresponding species.\nPrepare the gene count table.\nWith gene families on its rows and species on its columns, this table records the count of each orthologous gene in each species. We recommend removing or further refining gene families with more than 100 genes if present in some species (see the link to a tutorial in Note below), because gene families with a large variance in the gene count can cause erroneous parameter estimation.\nPredict expanded gene families.\nCAFE first calculates an error model for the prediction to account for potential errors in genome assembly and/or annotation and then carries out corrections based on the error model before calculating the gene family size of ancestor nodes. It thus can estimate gene family evolutionary rate more accurately. CAFE 5 (Mendes et\u00a0al., 2020[href=https://www.wicell.org#bib16]), the latest version, can account for evolution rate variation among families with improved performance on parallelization.Note: A tutorial for the CAFE tool can be found at http://evomicsorg.wpengine.netdna-cdn.com/wp-content/uploads/2016/06/cafe_tutorial-1.pdf[href=http://evomicsorg.wpengine.netdna-cdn.com/wp-content/uploads/2016/06/cafe_tutorial-1.pdf] and https://github.com/hahnlab/CAFE5[href=https://github.com/hahnlab/CAFE5]. It is a dilemma to further filter less reliable gene copies. Stringent thresholds will help to obtain most reliable predictions, while it may lead to lose some true positives. A reasonable threshold may depend on a particular project. After identifying most reliable gene families showing potential expansion, we recommend to further check all filtered predictions manually, as shown below, in those targeted gene families.\nAdditional filtering.\nGene copies with low protein sequence similarity to other genes within a gene family could be removed (Keane et\u00a0al., 2015[href=https://www.wicell.org#bib13]; Zhang et\u00a0al., 2021[href=https://www.wicell.org#bib25]).\nThe validity of gene prediction can be verified by RNA-seq data generated from either separate or pooled samples of many tissues. Marked by a lack of gene expression support, questionable predictions should be filtered, so more reliable ones can be prioritized for experimental validation (Zhang et\u00a0al., 2021[href=https://www.wicell.org#bib25]).\nNote: Assembly and/or annotation errors could cause false prediction of gene expansion. It is time-consuming to manually check all predictions. To obtain reliable predictions, further filtering should be considered. Kallisto (Bray et\u00a0al., 2016[href=https://www.wicell.org#bib5]) was recommended to measure gene expression of genes with paralogs, as it shows better performance on paralogs.\nGene family expansion validation\nTiming: several days (e.g., \u223c14\u00a0days for computational validation of \u223c200 candidate gene families and qPCR experimental validation of \u223c20 selected ones)\nAbove, we show how to filter out less reliable predictions through computational analysis. Nevertheless, false predictions could still present. More detailed checking is required to ascertain true ones. Here we described some steps to further validate the potential gene expansions (Figure\u00a01[href=https://www.wicell.org#fig1]).\nComputational and manual checking.Improve the gene annotation. For some predicted gene copies, the annotation of their gene structure could be problematic (e.g., wrong prediction of splicing sites). We recommend improving the annotation quality of gene copies in targeted potentially expanded gene families. Apollo (Dunn et\u00a0al., 2019[href=https://www.wicell.org#bib8]) is a convenient tool to adjust and improve gene structure annotation.\nNote: BAM files of RNA-seq read alignments provide good evidence to improve gene structure annotation. In addition, alignment of coding sequences between studied species and well annotated species (e.g., human and mouse) could provide useful information and help to identify local regions with poor annotation quality (Abascal et\u00a0al., 2010[href=https://www.wicell.org#bib1]).\nSynteny analysis of gene loci. Synteny analysis provides information about the conservation of homologous genes and gene orders between genomes of different species. At gene loci of targeted genes, it provides clues about whether the extra gene copies could come from genome assembly errors. Any genome browser \u2013 e.g., IGV (Thorvaldsdottir et\u00a0al., 2013[href=https://www.wicell.org#bib23]) \u2013 can be used for this analysis.\nGene structure, coding sequence analysis of predicted genes. After gene duplication, each copy of the gene may evolve independently and shows differences in their gene structure and coding sequences. Existing of such evolutionary changes among different gene copies could indicate true positives rather than false identification due to errors in genome assembly and annotation. Some tools are helpful for such analysis.\nGSDS (Hu et\u00a0al., 2015[href=https://www.wicell.org#bib11]) can generate plots of gene structure with the input of gene structure information in BED format or GTF format. It is accessible at http://gsds.gao-lab.org/[href=http://gsds.gao-lab.org/].\nTranslatorX (Abascal et\u00a0al., 2010[href=https://www.wicell.org#bib1]) can align multiple sequences guided by amino acid translations. Accessible at http://www.translatorx.co.uk/[href=http://www.translatorx.co.uk/], it can be used to view evolutionary changes in coding sequences among different copies of the expanded gene.RNA-seq supports of predicted genes. It is possible that the difference in coding sequences may come from sequencing errors rather than evolutionary changes. The aligned RNA-seq reads are useful to distinguish between them. Sequence differences are most likely real evolutionary changes if they are present in both genomic sequence and RNA-seq reads. Otherwise, they are probably sequencing errors in the genome assembly as they are not verified by corresponding transcript sequences.\nNote: IGV (Thorvaldsdottir et\u00a0al., 2013[href=https://www.wicell.org#bib23]) can be used to view RNA-seq reads coverage at unique sites among copies by using BAM file of RNA-seq read alignment. By default, STAR will only report alignments of reads with at most 10 mapped loci, which can be adjusted by the parameter \u201c--outFilterMultimapNmax\u201d, in case predicted gene copy number of target gene family is greater than 10.\nGene copy number validation by qPCR.\nSelection of reference genes. To quantify the gene copy number by qPCR, it is important to select reference genes with normal copy numbers. It is reasonable to select genes with no predicted expansion in the studied genome and no known gene expansion in other closely related species. Orthologous databases are useful to identify such genes. For example, for each gene family, OrthoDB (Kriventseva et\u00a0al., 2019[href=https://www.wicell.org#bib14]) shows number of species with certain number of copies. Genes with consistent copy numbers across all the species are good candidates as reference genes. It is also recommended to select reference genes from both autosomes and the X chromosome, with 2 or 3 candidates each. If the genomic DNA of a male individual was used for the experiments, candidates from autosome and the X chromosome can be validated for each other due to expected double copy number on autosome compared with that on X chromosome.Primer design. Design primers for genes under expansion could be challenging, because different gene copies evolve differently. Highly conserved coding sequences are good candidates for designing primers. Below shows the procedure to design and select primers.\nWe use Primer-Blast (Ye et\u00a0al., 2012[href=https://www.wicell.org#bib24]) to design primer candidates in coding sequences of target genes, and set conserved regions among different copies as the searching regions.\nSpecificity of primers \u2013 i.e., no other targets except for the copies of the expanded gene \u2013 can be first checked by Primer-Blast against all coding sequences of the genome, and then by MFEprimer (Qu et\u00a0al., 2012[href=https://www.wicell.org#bib17]) on the genomic DNA level.\nFinal candidates can be further inspected by gel electrophoresis and the melting curve analysis.\nqPCR validation. The first step is to check whether the amplification efficiency for each candidate primer is the same, which can be achieved by a standard curve analysis with different amount of DNA input for the replicates. The relative gene copy number can then be calculated by   \u0394  C t    (Zhang et\u00a0al., 2021[href=https://www.wicell.org#bib25]).\nNote: When genomic DNA of a male individual is used, reference genes from both autosome and X chromosome should be used for qPCR validation. For each gene, 2 or 3 candidate primers should be designed, and among them the more accurate ones should be selected.\nFunctional explore of expanded genes\nTiming: several hours (e.g., \u223c3\u00a0h for 25 RNA-seq samples on a 10-node computer cluster with 12-GB memory in each node)\nIt is important to check whether different copies of an expanded gene function in similar way as the ancient copy. Thus, it is important to check pseudogenization and transcriptional level of each copy of the expanded gene. See Figure\u00a01[href=https://www.wicell.org#fig1].\nPseudogene identification.Check whether a copy of an expanded gene could be a processed intronless pseudogene through mRNA retrotransposition. Some genes, however, do have only a single exon. To identify processed pseudogenes, one can search whether the gene is known to have a single exon in evolutionarily closely species in a gene ortholog database, such as OrthoDB (Kriventseva et\u00a0al., 2019[href=https://www.wicell.org#bib14]).\nThen check whether a gene copy was pseudogenized through mutations, such as frameshift indels. GeneWise (Birney et\u00a0al., 2004[href=https://www.wicell.org#bib4]) can identify such mutations, when it is used to scan the gene locus \u2013 e.g., gene body with upstream and downstream 5-kb \u2013 against the coding sequence of the functional orthologous gene (e.g., a human ortholog) as the reference. GeneWise is used in our pipeline. Alternatively, researchers can use any other tools, e.g., PseudoPipe (Zhang et\u00a0al., 2006[href=https://www.wicell.org#bib26]). The genome sequence at the gene locus can be retrieved by the bedtools (Quinlan and Hall, 2010[href=https://www.wicell.org#bib18]).\nNote: The pipeline to identify pseudogene is straightforward and relatively simple here. However, it is sufficient for analyzing a small number of genes. More complicated methods for genome-wide identification of pseudogenes can be found in other studies (Sisu et\u00a0al., 2020[href=https://www.wicell.org#bib20]).\n# Run bedtools\n# Input: genome sequence, gene coordinate (\u00b15 kb) in bed format\n# Output: genome sequence in gene coordinate (\u00b15 kb)\n# fi: the genomic sequence\n# -bed: gene coordinate in bed format (extend to upstream/downstream 5 kb)\n# -s: force strandedness. Return reverse complement if the gene is on the antisense strand.\n# -name: use \u201cname\u201d column in bed file as fasta headers of output\n\u00a0\u00a0bedtools getfasta -fi <genomic.fa> -bed <geneCoordinate.bed> -s -name\u00a0>\u00a0genomic-region.fa\n# Run GeneWise\n# Input: protein sequence of homolog, DNA sequence of genomic region of a predicted gene copy\n# Output: report mutations that pseudogenized the gene copy# -sum: show summary output\n# -pretty: show pretty ascii output\n# -pseudo: mark genes with frameshifts as pseudo genes\n# -genes: show gene structure\n# -cdna: show predicted cDNA sequence\n# -trans: show protein translation\n# -pep: show predicted peptide\n# -para: show parameters\n# -both: check both strand\n# -quiet: no report on stderr\n\u00a0\u00a0genewise <protein.fa> <genomic-region.fa> -sum -pretty -pseudo -genes -cdna -trans -pep -para -both -quiet > out.gw[href=http://out.gw/]\nGene expression.\nThe transcription level of each copy of an expanded gene can be checked to explore its functionality. RNA-seq data from different tissues can be used to examine gene expression pattern across tissues and identify tissue-specific expression of some copies. Kallisto (Bray et\u00a0al., 2016[href=https://www.wicell.org#bib5]) shows better performances on paralogs.", "Step-by-step method details\nStep-by-step method details\nViral-like particles (VLPs) isolation\nTiming: 6\u20137 h per sample\n      This step is helpful to isolate and concentrate the VLPs from the fecal\n      samples.\n    \n      Before beginning the experiments, thaw the fecal samples on ice. Also, it\n      is essential to filter all reagents through sterile syringe filter\n      polyethersulfone membrane (PES) units. We recommend using a 0.45\u00a0\u03bcm pore\n      size to remove larger cell debris followed by a 0.22\u00a0\u03bcm pore size to\n      remove remaining cell debris and bacterial-size particles. This two-step\n      filtration protocol applies to all reagents and the supernatant of the\n      fecal samples.\n    \n        Preparation of phage suspension (15\u00a0min per sample).\n        \n            Pour 40\u00a0mL of SM Buffer, previously filtered, into a 50\u00a0mL\n            centrifuge tube.\n          \n            Gently pipet the 250\u00a0mg of fecal sample and resuspend in RNA later\n            several times until obtaining a homogeneous mixture.\n          \n            Add the whole RNA later\u00a0+ fecal sample mixture to the SM Buffer\n            tube.\n          \n        Remove bacterial debris (40\u00a0min).\n        \nCentrifuge at 4700\u00a0g for 30\u00a0min at 4\u00b0C.\n            Carefully transfer the supernatant (it contains the VLPs) to a 5\u00a0mL\n            syringe attached to a 0.45\u00a0\u03bcm PES filter, using a micropipette.\n            Filter the supernatant and collect the filtrate into a new 50\u00a0mL\n            centrifuge tube.\n            \nNote: When taking the supernatant\n              with the micropipette, make sure not to disturb the pellet. We\n              recommend using a fine tip and removing the volume very slowly.\n            \n            Filter all the supernatant obtained from the previous step through a\n            0.22\u00a0\u03bcm PES filter.\n            \nNote: Regularly change the filters\n              to avoid clogging them with cell debris that was not removed by\n              the centrifugation step.\n            \nNote: Do not push too hard with\n              the syringe to avoid breaking the filter.\n            \nPause point: The filtered\n              buffer with VLPs can be stored at 4\u00b0C until needed.Note: Some reports have shown good\n              stability of phages when stored at 4\u00b0C in SM buffer for more than\n              six months (Jepson and March, 2004[href=https://www.wicell.org#bib10];\n              Jo\u0144czyk et\u00a0al., 2011[href=https://www.wicell.org#bib11]). Similarly, storing at\n              2\u00b0C\u20135\u00b0C has shown no significant reduction in phage titer (5%\u201310%)\n              after six months (Mullan 2001[href=https://www.wicell.org#bib16]).\n            \nNote: Check that the SM buffer\n              contains gelatin. It stabilizes VLPs membranes during storage.\n            \n        Enrichment and washing the VLPs (2\u20133 h).\n        \n            Add 15\u00a0mL of the previous buffer with VLPs into an Amicon Ultra-15\n            centrifugal filter (100 KDa).\n          \n            Centrifuge at 5000\u00a0g in a fixed-angle centrifuge for 15\u201360\u00a0min at\n            4\u00b0C, until the remaining volume reaches \u223c200\u00a0\u03bcL on the top of the\n            Amicon filter unit.\n          \n            Discard the flow-through and add fresh filtrate again to the top of\n            the filter unit.\n          \nRepeat steps a, b and c until the entire buffer is filtered.\n            Add 5\u00a0mL of SM Buffer to the top chamber of the Amicon and spin at\n            5000\u00a0g at 4\u00b0C for 2\u20135\u00a0min to wash the VLPs until the remaining\n            volume reaches \u223c200 uL on the top of the Amicon filter unit. Repeat\n            this VLPs washing step (at least five times) until the buffer\n            containing the VLPs is clear.\n            \nNote: Wash several times the VLPs\n              to diminish cellular debris that could inhibit the nucleic acid\n              extraction and their visualization in the microscope. Eight washes\n              of the VLPs allow a more precise visualization of viral particles\n              by electron microscopy (TEM).\n            \n            Discard the collector tube of the Amicon filter unit and pipet the\n            \u223c200\u00a0\u03bcL of VLPs several times to resuspend the VLPs retained on the\n            filter walls.\n          \n            Transfer the total volume \u223c200\u00a0\u03bcL containing the VLPs to a 1.5\u00a0mL\n            microfuge tube.\n            \nNote: If the volume in the Amicon\n              filter unit is less than 200\u00a0\u03bcL, adjust the volume using SMbuffer.\n            \nPause point: This 200\u00a0\u03bcL of\n              concentrated VLPs in SM buffer can be stored at 4\u00b0C until needed.\n            \nNote: Check that the SM buffer\n              contains gelatin. The gelatin in SM buffer stabilizes VLPs\n              membranes during storage.\n            \n        Eliminate the remaining bacterial and human cell debris (15\u00a0min).\n        \n            Add 40\u00a0\u03bcL of chloroform to the 200\u00a0\u03bcL of VLPs and incubate at room\n            temperature (20\u00b0C\u201325\u00b0C) for 10\u00a0min.\n          \n            Centrifuge at room temperature for 5\u00a0min at 20,000g to separate the\n            aqueous phase. Take the aqueous phase carefully (it contains the\n            VLPs) with a micropipette and transfer it to a new 1.5\u00a0mL microfuge\n            tube.\n          \n        Remove residual bacterial and human DNA (3 h).\n        \n            Eliminate the Non-virus-protected DNA by adding 2.5 units of DNase I\n            per milliliter of VLPs, and incubate at 37.8\u00b0C for two hours,\n            following the manufacturer\u2019s procedure.\n          \nInactivate the DNase at 65\u00b0C for 10\u00a0min.\n            Cool down the reaction on the laboratory table until room\n            temperature (20\u00a0min).\n            \nCritical: Do not inactivate\n              the DNase at 75\u00b0C; this temperature can damage the VLPs.\n            \nNote: Wash two times the VLPs to\n              clean up remnants of the DNAse reaction.\n            \n            Transfer the previous reaction into an Amicon Ultra-15 centrifugal\n            filter (100 KDa) and add 15\u00a0mL of SM buffer.\n          \n            Centrifuge at 5000\u00a0g in a fixed-angle centrifuge for 15\u201360\u00a0min at\n            4\u00b0C, until the remaining volume reaches \u223c200 uL on the top of the\n            Amicon filter unit.\n          \n            Discard the flow-through, add fresh SM buffer to the top of the\n            filter unit, and repeat step e.\n          \n            Discard the collector tube of the Amicon filter unit and pipet the\n            \u223c200\u00a0\u03bcL of VLPs several times to resuspend the VLPs retained on the\n            filter walls.\n          \n            Transfer the total volume \u223c200\u00a0\u03bcL containing the VLPs to a 1.5\u00a0mL\n            microfuge tube.\n            \nPause point: The VLPs canbe stored at 4\u00b0C until needed.\n            \nNote: At this point it is\n              recommended to use TEM microscopy to confirm the presence of VLPs\n              and count the number of particles using fluorescence microscopy.\n            \nMicroscopy visualization and VLPs counts\nTiming: 2\u20132.5 h per sample\n      This step is to corroborate the presence and quantify the VLPs in the\n      samples.\n    \n        Quantify the number of isolated VLPs by epifluorescence microscopy (1.5\n        h).\n        \n            Preparation of the SYBR Green staining solution (5\u00a0min).\n            \n                Dilute 5\u00a0\u03bcL of SYBR Green in 45\u00a0\u03bcL of nuclease-free water\n                (previously passed through the 0.22\u00a0\u03bcm PES filtration units).\n              \n            Staining the VLPs of the sample (10\u00a0min).\n            \n                Place 10\u00a0\u03bcL of the VLPs sample in a 0.6\u00a0mL Eppendorf tube.\n              \nAdd 2\u00a0\u03bcL of the SYBR Green solution.\n                Add 10\u00a0\u03bcL of 4% (wt/vol) paraformaldehyde.\n                \nCritical: Paraformaldehyde\n                  is a toxic chemical, and it only should be used in a chemical\n                  fume hood. Waste should be collected and discarded\n                  appropriately.\n                \n            Mounting the VLPs sample on a microscopy slide (5\u00a0min).\n            \nPlace 10\u00a0\u03bcL of stained VLPs sample on a slide.\n                Place a coverslip over the sample, avoiding the formation of air\n                bubbles.\n              \n            Microscopy visualization and quantification of VLPs (Olympus FV1000\n            Multi-photonic confocal microscope) (1h).\n            \n                Place the slide under the microscope and focus on the field at\n                the confocal microscope. Set the laser\u2019s parameters to the\n                optimal setup.\n              \n                Take five micrographs of the sample, each in triplicate. This\n                protocol uses 0.212\u00a0\u00d7 0.212-millimeter images (Figure\u00a01[href=https://www.wicell.org#fig1]A).\n                \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1430-Fig1.jpg\n                      Figure\u00a01. Microscopy visualization and counting of VLPs\n                    \n                      The image was modified from\n                      Bikel et\u00a0al. (2021[href=https://www.wicell.org#bib2]).\n                    \n                      (A) SYBR Green I-stained virus-like particles (VLPs) by\n                      epifluorescence microscopy. Red arrows show an example of\n                      the VLPs. The scale bar represents an approximate size of\n                      10 \u00b5m.\n                    \n                      (B) TEM microscopy of VLPs. Red arrows show an example ofthe VLP morphology. The scale bar represents an\n                      approximate size of 0.5 \u00b5m.\n                    \n                Use the FIJI software (Schindelin et\u00a0al., 2012) for image\n                analysis. Any image analysis program can be used.\n              \n                Count the number of fluorescents VLPs from 5 random fields per\n                slide by triplicate and average. Calculate the absolute VLPs\n                numbers based on the average VLPs counts per field, taking into\n                account the area of one field, and back calculating based on the\n                dilution factor, as follows:\n              \ntable:files/protocols_protocol_1430_1.csv\nX\n\u00a0\n=\n\u00a0\np\na\n(\nb\n)\np\u00a0= number of VLPs per sample\na\u00a0= square area for the observed field in the microscope\nb\u00a0= total loaded area with VLPs on the microscope slide\nY\n\u00a0\n=\n\u00a0\nX\nc\n(\nd\n)\nc\u00a0= volume loaded into slide\nd\u00a0= total volume of extracted VLPs for 250\u00a0mg of feces\nZ\n\u00a0\n=\n\u00a0\n(\nY\n)\n(\n4\n)\nNote: The optimal microscope setup depends\n      on the microscope type and operating system.\n    \nNote: Fluorescence is not stable over\n      time. Thus the images should be taken immediately after slide preparation.\n    \nNote: Some dyes can stain the phage capsid\n      rather than staining the DNA inside it. Both stains can be used depending\n      on the experimental need. Keep in mind that the SYBR stain does not modify\n      the exterior of the phage.\n    \nNote: It is expected to obtain between\n      108 and 1010 VLPs /g in stools (Kim et\u00a0al., 2011[href=https://www.wicell.org#bib12]; Hoyles et\u00a0al., 2014[href=https://www.wicell.org#bib7];\n      Shkoporov et\u00a0al., 2019[href=https://www.wicell.org#bib19];\n      Bikel et\u00a0al., 2021[href=https://www.wicell.org#bib2]). In our hands, we obtained from\n      8.5\u00a0\u00d7 108 to 4.8\u00a0\u00d7 1010 VLPs per gram of feces (Bikel et\u00a0al., 2021[href=https://www.wicell.org#bib2]).\n    \n        Corroborate the phage morphology using Transmission Electron Microscopy\n        (TEM) (1 h).\n        \n            Place 8\u00a0\u03bcL of a concentrated VLP sample onto carbon-coated Formvar\n            grids.\n          \nIncubate at room temperature for 1\u00a0min.Drain off the sample excess of the grid with the aid of filter\n            paper.\n          \nStain the grids with 8\u00a0\u03bcL of 2% uranyl acetate.\nIncubate at room temperature for 2\u20133\u00a0min.\nDrain off the excess stain with the aid of filter paper.\nDry the grids at room temperature until analysis.\n            Take the image from the grids at magnifications of 14,000\u00d7 to\n            80,000\u00d7 (Figure\u00a01[href=https://www.wicell.org#fig1]B).\n          \nCritical: The image fields could be\n      obstructed in a few instances by extensive, amorphous, dark-staining\n      material. (See potential problem 1 in the\n      troubleshooting[href=https://www.wicell.org#troubleshooting] section).\n    \nViral DNA extraction and purification\nTiming: 1 h per sample\n      In this step, the DNA from the isolated VLPs is extracted following the\n      QIAamp MinElute Virus Spin Kit Handbook (https://www.qiagen.com/us/resources/resourcedetail?id=8798cda6-4c55-4c0e-a302-966521c81aec&lang=en[href=https://www.qiagen.com/us/resources/resourcedetail?id=8798cda6-4c55-4c0e-a302-966521c81aec&lang=en]). The QIAamp MinElute Virus Spin procedure comprises four steps (lyse,\n      bind, wash, and elute). Perform this step using QIAamp MinElute columns in\n      a standard microcentrifuge. This procedure is optimized for use with a\n      starting volume of 200\u00a0\u03bcL of VLPs containing liquid.\n    \n        Preparation of QIAGEN Protease.\n        \n            Add 1.4\u00a0mL of AVE Buffer to the vial of lyophilized QIAGEN Protease,\n            and mix carefully to avoid foaming.\n          \nMake sure to dissolve the QIAGEN Protease completely.\n            Label and store the resuspended QIAGEN Protease at 2\u00b0C\u20138\u00b0C, only for\n            12\u00a0months or until the kit\u2019s expiration date.\n          \nNote: This kit provides two alternative\n      buffers for dissolving QIAGEN Protease \u2014 Buffer AVE (recommended) or\n      Protease Resuspension Buffer. We recommend using the AVE Buffer.\n    \nNote: Storage at \u201320\u00b0C will prolong its\n      life, but avoid repeated freezing and thawing. Dividing the solution into\n      aliquots and freezing at \u201320\u00b0C is recommended.\n    \n        Preparation of Buffer AW1.\n        \n            Add 25\u00a0mL of ethanol (96%\u2013100%) to a bottle containing 19\u00a0mL of\n            Buffer AW1 concentrate, as described on the bottle.\n          \n            Tick the check box on the label to indicate that ethanol has been\n            added.Store reconstituted Buffer AW1 at room temperature (15\u00b0C\u201325\u00b0C).\n          \nNote: Reconstituted Buffer AW1 is stable\n      for up to 1 year or until the kit expiration date when stored at room\n      temperature.\n    \nNote: Always mix reconstituted Buffer AW1\n      by shaking before starting the procedure.\n    \nCritical: Buffer AW1 contains\n      chaotropic salt. Take appropriate laboratory safety measures and wear\n      gloves when handling. Not compatible with disinfectants containing bleach.\n    \n        Preparation of Buffer AW2\n        \n            Add 30\u00a0mL of ethanol (96%\u2013100%) to a bottle containing 13\u00a0mL of\n            Buffer AW2 concentrate, as described on the bottle.\n          \n            Tick the check box on the label to indicate that ethanol has been\n            added.\n          \n            Store reconstituted Buffer AW2 at room temperature (15\u00b0C\u201325\u00b0C).\n          \nNote: Reconstituted Buffer AW2 is stable\n      for up to 1 year or until the kit expiration date when stored at room\n      temperature.\n    \nNote: Always mix reconstituted Buffer AW2\n      by shaking before starting the procedure.\n    \n        Viral DNA extraction Before starting, let the samples reach room\n        temperature. All centrifugation steps were carried out at room\n        temperature (15\u00b0C\u201325\u00b0C).\n        \n            Pipet 25\u00a0\u03bcL QIAGEN Protease into a 1.5\u00a0mL microcentrifuge tube\n          \n            Add the 200\u00a0\u03bcL of the VLPs sample into the microcentrifuge tube.\n            \nNote: If the sample volume is less\n              than 200\u00a0\u03bcL, add the appropriate volume of SM Buffer solution to\n              bring the volume of the protease and the sample to a total of\n              225\u00a0\u03bcL.\n            \n            Add 200\u00a0\u03bcL Buffer AL. Close the cap and mix by pulse-vortexing for\n            15 s.\n            \nNote: To ensure efficient lysis is\n              essential that the sample and Buffer AL are mixed thoroughly to\n              yield a homogeneous solution.\n            \nCritical: Do not add QIAGEN\n              Protease directly to Buffer AL.\n            \nCritical: Buffer AL contains\n              guanidine hydrochloride, forming highly reactive compounds when\n              combined with bleach. Do not add bleach or acidic solutions\n              directly to waste containing Buffer AL.Incubate at 56\u00b0C for 15\u00a0min in a heating block.\n            Briefly centrifuge the 1.5\u00a0mL tube to remove drops from the inside\n            of the lid.\n          \n        Viral DNA purification Binding conditions are adjusted by adding ethanol\n        to allow optimal binding of the viral DNA to the membrane. Lysates are\n        then transferred onto a QIAamp MinElute column and viral nucleic acids\n        are adsorbed onto the silica-gel membrane as the lysate is drawn through\n        by centrifugation.\n        \n            Add 250\u00a0\u03bcL of ethanol (96\u2013100%) to the sample, close the cap and mix\n            thoroughly by pulse-vortexing for 15 s.\n          \n            Incubate the lysate with the ethanol for 5\u00a0min at room temperature\n            (15\u00b0C\u201325\u00b0C).\n            \nNote: If the ambient temperature\n              exceeds 25\u00b0C, ethanol should be cooled on ice before adding to the\n              lysate.\n            \n            Briefly centrifuge the 1.5\u00a0mL tube to remove drops from the inside\n            of the lid.\n          \n            Carefully apply all lysates onto the QIAamp MinElute column without\n            wetting the rim. Close the cap and centrifuge at 6000\u00d7g\n            (8000\u00a0rpm) for 1\u00a0min.\n          \n            Place the QIAamp MinElute column in a clean 2\u00a0mL collection tube,\n            and discard the collection tube containing the filtrate.\n            \nNote: If the lysate has not\n              entirely passed through the column after centrifugation,\n              centrifuge again at a higher speed until the QIAamp MinElute\n              column is empty.\n            \n            Carefully open the QIAamp MinElute column, and add 500\u00a0\u03bcL of Buffer\n            AW1 without wetting the rim.\n          \n            Close the cap and centrifuge at 6000\u00d7g (8000\u00a0rpm) for 1\u00a0min.\n          \n            Place the QIAamp MinElute column in a clean 2\u00a0mL collection tube,\n            and discard the collection tube containing the filtrate.\n          \n            Carefully open the QIAamp MinElute column, and add 500\u00a0\u03bcL of Buffer\n            AW2 without wetting the rim.\n          \n            Close the cap and centrifuge at 6000\u00d7g (8000\u00a0rpm) for 1\u00a0min.\n          \n            Place the QIAamp MinElute column in a clean 2\u00a0mL collection tube,and discard the collection tube containing the filtrate.\n          \n            Carefully open the QIAamp MinElute column and add 500\u00a0\u03bcL of ethanol\n            (96\u2013100%) without wetting the rim.\n          \n            Close the cap and centrifuge at 6000\u00d7g (8000\u00a0rpm) for 1\u00a0min.\n          \nDiscard the collection tube containing the filtrate.\n            Place the QIAamp MinElute column in a clean 2\u00a0mL collection tube.\n            Centrifuge at full speed (20,000\u00d7g; 14,000\u00a0rpm) for 3\u00a0min to\n            dry the membrane completely.\n            \nNote: To evaporate any remaining\n              liquid, it is recommended to place the QIAamp MinElute column into\n              a new 2\u00a0mL collection tube (not provided), open the lid, and\n              incubate the assembly at 56\u00b0C for 3\u00a0min to dry the membrane\n              completely.\n            \n            Place the QIAamp MinElute column in a clean 1.5\u00a0mL microcentrifuge\n            tube, and discard the collection tube with the filtrate.\n          \n            Carefully open the lid of the QIAamp MinElute column, and apply\n            30\u00a0\u03bcL of Buffer AVE or RNase-free water to the center of the\n            membrane.\n          \n            Close the lid and incubate at room temperature for 1\u00a0min.\n            \nNote: Incubating the QIAamp\n              MinElute column loaded with Buffer AVE or water for 5\u00a0min at room\n              temperature before centrifugation generally increases DNA yield.\n            \n            Centrifuge at full speed (20,000\u00d7g; 14,000\u00a0rpm) for 1\u00a0min.\n          \n            Eluted DNA can be collected in standard 1.5\u00a0mL microcentrifuge\n            tubes.\n            \nNote: Ensure that the elution\n              buffer is at room temperature. If elution is done in small volumes\n              (<50\u00a0\u03bcL), the elution buffer must be dispensed onto the center\n              of the membrane for complete elution of bound DNA.\n            \nNote: Elution volume is flexible\n              and can be adapted according to the requirements of the downstream\n              application. However, the recovered elution volume will be\n              approximately 5\u00a0\u03bcL less than the volume of elution buffer applied\n              onto the column.\n            \nNote: If the purified viral DNA isused within 24 h, storage at 2\u00b0C\u20138\u00b0C. For periods longer than 24\n              h, storage at \u201320\u00b0C.\n            \nNote: With the application of this\n              protocol, we obtained an average of 160.7\u00a0\u00b1 105.0\u00a0ng of total DNA\n              from 200\u00a0\u03bcL of VLPs extracted from 250\u00a0mg of feces (Bikel et\u00a0al., 2021[href=https://www.wicell.org#bib2]).\n            \nDNA library preparation and purification\nTiming: 1.5\u20132 h per sample\n      In this step, the sequencing libraries of the extracted DNA from the VLPs\n      are prepared using the Nextera XT DNA Library Preparation Guide (https://support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/samplepreps_nextera/nextera-xt/nextera-xt-library-prep-reference-guide-15031942-05.pdf[href=https://support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/samplepreps_nextera/nextera-xt/nextera-xt-library-prep-reference-guide-15031942-05.pdf]). Prepare the sequencing libraries in a PCR laminar airflow cabinet to\n      avoid contamination.\n    \n        Assessing DNA Quality.\n        \n            Use an electrophoresis gel and a UV absorbance method to assess the\n            quality of the DNA sample. Absorbance ratio values of 1.8\u20132.0 are\n            considered adequate for this protocol.\n          \n        Quantification of input DNA.\n        \n            To accurately quantify the input DNA and library concentration, use\n            a fluorometric-based method such as Qubit dsDNA Assay.\n          \n            Dilute the sample DNA to a final concentration of 0.3\u00a0ng/\u03bcL in\n            nuclease-free water.\n          \nCritical: Avoid using DNA\n      quantification methods that measure total nucleic acid content, such as\n      Nanodrop or other UV absorbance methods. Contaminants such as ssDNA, RNA,\n      and oligonucleotides are not substrates for the Nextera XT assay.\n    \nCritical: A DNA concentration of\n      0.3\u00a0ng/\u03bcL is essential because the Nextera XT Protocol is optimized for\n      \u223c1\u00a0ng of input DNA contained in 5\u00a0\u03bcL. If the total concentration of DNA\n      extracted from the VLP samples results low, an additional DNA\n      concentration step might be needed.\n    \nNote: Maintain the DNA sample on the ice\n      during the process or store it at \u221220\u00b0C until use.\n    \n        DNA Tagmentation. The Nextera XT transposase simultaneously fragments\n        the input DNA and adds adapter sequences during this step. The reactions\n        can be assembled in sterile PCR tubes.Thaw the diluted DNA sample and the following Kit reagents on ice.\n            \nAmplicon Tagment Mix (ATM) and Tagment DNA Buffer (TD).\n                Maintain the Neutralize Tagment Buffer (NT) at room temperature.\n              \n            Vortex and visually inspect all reagents to make sure there is no\n            precipitated. Suppose there is, vortex gently until the precipitate\n            is resuspended.\n            \nNote: Assemble the reaction in the\n              order described for optimal kit performance. The reaction does not\n              need to be assembled on ice.\n            \nAdd 10\u00a0\u03bcL TD buffer to a new PCR tube.\n            Add 5\u00a0\u03bcL input DNA 0.3\u00a0ng/\u03bcL (1.5\u00a0ng total) to the tube with the TD\n            buffer and gently pipette up and down five times to mix.\n            \nCritical: Use the same amount\n              of input DNA for all libraries.\n            \nNote: The user can assemble more\n              than one library simultaneously. However, we recommend only\n              preparing together libraries that belong to the same experimental\n              group to avoid cross-contamination among tested groups.\n            \n            Add 5\u00a0\u03bcL ATM buffer to the reaction tube containing the TD buffer\n            and the DNA. Gently pipette up and down five times to mix.\n          \n            Centrifuge the reaction tube at 280\u00d7g at room temperature for\n            1\u00a0min.\n          \n            Incubate the reaction tube in a thermal cycler at 55\u00b0 for 5\u00a0min and\n            then hold at 10\u00b0C. Maintain the thermal cycler lid heated during the\n            incubation.\n          \n            When the incubation reaches 10\u00b0C, immediately add 5\u00a0\u03bcL of the NT\n            buffer to the reaction tube to neutralize the reaction. Pipette up\n            and down five times to mix.\n            \nNote: To immediately neutralize\n              the tagmentation reaction, adding the NT buffer while the reaction\n              tube is still in the thermal cycler is more convenient.\n            \n            Centrifuge the reaction tube at 280\u00d7g at room temperature for\n            1\u00a0min.\n          \nMaintain the samples at room temperature for 5\u00a0min.PCR Amplification. During this step, the tagmented DNA is amplified for\n        a limited number of PCR cycles to add the index i7 and i5.\n        \n            Select a pair of one i7 plus one i5 index primer for each library.\n            \nCritical: Two libraries cannot\n              have the same pair of index primers, as they would be interpreted\n              and sequenced as the same sample.\n            \n            Thaw the selected primers and the Nextera PCR Master Mix (NPM) on\n            ice.\n            \n                After all reagents are thawed mix each tube by inverting 3\u20135\n                times and centrifuge at 280\u00d7g for 1\u00a0min.\n              \nAdd 15\u00a0\u03bcL of NPM to the tagmented DNA sample.\n            Add 5\u00a0\u03bcL of index i7 and 5\u00a0\u03bcL of index i5 to each sample. Gently\n            pipette up and down five times to mix.\n            \nCritical: Change tips between\n              index primers to avoid cross-contamination.\n            \nCentrifuge at 280\u00d7g at room temperature for 1\u00a0min.\n            Perform PCR using the following program and maintain the lid heated\n            during the process:\n            table:files/protocols_protocol_1430_2.csv\nCritical: Use the same number\n              of PCR cycles for all libraries to avoid over-estimation in a\n              particular library. Adding extra cycles of PCR could produce\n              low-quality sequence results.\n            \nPause point: The user can\n              safely stop before proceeding to PCR clean-up. The user can either\n              keep the samples at 10\u00b0C overnight (12\u201314 h) inside the thermal\n              cycler or store at 2\u00b0C\u20138\u00b0C for up to 2\u00a0days.\n            \n        PCR Clean-Up. This step uses AMPure XP beads to purify the DNA library\n        and removes short library fragments.\n        \nNote: Before start, bring AMPure XP\n          beads to room temperature.\n        \nVortex the AMPure XP beads to evenly disperse them.\n            Add 30\u00a0\u03bcL of AMPure XP beads to each amplified library. Gently\n            pipette up and down ten times to mix.\n            \nNote: The volume of 30\u00a0\u03bcL of\n              AMPure beads selects inserts of >500\u00a0bp. Refer to the NexteraXT DNA Library Preparation Guide to select the desired insert\n              sizes.\n            \nIncubate at room temperature for 5\u00a0min.\n            Place the tube on a magnetic stand until the supernatant has\n            cleared.\n          \n            While the tube is on the magnetic stand, carefully remove and\n            discard the supernatant.\n            \nNote: If any beads are aspirated,\n              dispense back to the tube and wait until the supernatant is clear\n              before removing again.\n            \n            While the tube is on the magnetic stand, add 200\u00a0\u03bcL of 80% ethanol\n            to wash the beads.\n            \nCritical: Prepare fresh 80%\n              ethanol from absolute ethanol. Ethanol can absorb water from the\n              air altering the concentration and impacting the results.\n            \n            Maintain the tube on the magnetic stand and incubate for 30 s.\n          \nRemove and discard the supernatant carefully.\n            Maintain the tube on the magnetic stand and allow the beads to\n            air-dry for 15\u00a0min.\n            \nCritical: Do not over-dry the\n              beads. This could reduce the amount of library recovered.\n            \nRemove any excess ethanol with a clean tip.\n            Remove the tube from the magnetic stand and add 50\u00a0\u03bcL of\n            nuclease-free water. Gently pipette up and down ten times to mix.\n          \nIncubate at room temperature for 2\u00a0min.\n            Place the tube on the magnetic stand and wait until the supernatant\n            has cleared.\n          \n            While the tube is on the magnetic stand, transfer the supernatant\n            carefully to a clean tube.\n          \n            Quantify the final library with a fluorometric-based method such as\n            Qubit dsDNA assay, and check the size distribution in a High\n            Sensitivity DNA Bioanalyzer.\n          \n            All final libraries can be pooled together and sequenced in any\n            Illumina platform.\n            \nNote: To achieve an even\n              sequencing depth among all samples is essential that all libraries\n              are pooled in equimolar proportions. As a reference, we adjusted\n              each library to a final concentration of 2nM (Bikel et\u00a0al., 2021[href=https://www.wicell.org#bib2]).\n            \nNote: The Nextera librariescontain Illumina adapters; thus, they could be sequenced in any\n              Illumina platform. As a reference, we used the NextSeq500 system\n              in a 2\u00a0\u00d7 150 pair-end mode and obtained 74,859,356 reads (an\n              average of 2,673,548 reads per sample) (Bikel et\u00a0al., 2021[href=https://www.wicell.org#bib2]).\n            \nNote: According to Illumina's\n              recommendations, the sequencing read length for De novo sequencing\n              ranges from 2\u00a0\u00d7 150 to 2\u00a0\u00d7 300\u00a0bp. As a reference, when sequencing\n              with 2\u00a0\u00d7 150\u00a0bp, we assembled 18,602 viral contigs with \u2265500 nt of\n              length (Bikel et\u00a0al., 2021[href=https://www.wicell.org#bib2]).", "Training resources (both events and materials) may be searched in TeSS in several ways. A general search can be performed based on keywords, which will return separate lists of events and materials. Alternatively, events or materials can be searched for independently of each other. The second approach allows more precise filtering on several parameters (e.g., event type, country, and target audience) alone or in combination. In this protocol, we provide examples of searches for specific events or materials.\nNecessary Resources\nDesktop or a laptop computer or a mobile device with fast Internet connection\nUp-to-date web browser such as Google Chrome, Mozilla Firefox, Apple Safari, or Microsoft Edge\nSearch for specific events\n1a. Select \u201cEvents\u201d in the top menu.\n2a. Enter \u201cSingle-cell\u201d as the keyword in the search text box and click on the magnifier icon.\n3a. In the side menu, scroll down to the \u201cCountry\u201d facet. Select \u201cUnited Kingdom\u201d.\n4a. In the side menu, scroll to the \u201cTarget audience\u201d facet. Select \u201cGraduate students\u201d.\nThe result is a list of training events about single-cell data analysis available in the United Kingdom for graduate students.\nSearch for specific materials\n1b. Select \u201cMaterials\u201d in the top menu.\n2b. Enter a keyword in the search text box. We will enter \u201cData retrieval\u201d.\n3b. In the side menu, select a difficulty level. We will select \u201cBeginner\u201d.\n4b. Scroll to the \u201cResource type\u201d facet and select \u201ce-learning\u201d.\nThe result is a list of training materials about data retrieval intended for a beginner level audience and tagged as e-learning material.\nWhether searching for event or materials, any of the filters can be removed at any time to reduce the stringency of the search. Past events can also be included by clicking the \u201cShow past events\u201d button.Widgets are chunks of JavaScript code that can be copied into the website source code to display TeSS content. Several different styles and functionalities are available from our configurable widget suite. Widgets can be used to enhance your site and offer your community lists of relevant events or training resources. Examples of code can be found at https://elixirtess.github.io/TeSS_widgets/[href=https://elixirtess.github.io/TeSS_widgets/].\nNecessary Resources\nDesktop or a laptop computer or a mobile device with fast Internet connection\nUp-to-date web browser such as Google Chrome, Mozilla Firefox, Apple Safari, or Microsoft Edge\n1. For a tabular layout of events with filtering options, choose the specific widget type Events table with a sidebar.\nA working example is shown on the GitHub pages from the ELIXIR TeSS team at https://elixirtess.github.io/TeSS_widgets[href=https://elixirtess.github.io/TeSS_widgets]. For training material, investigate the example for training materials on the same website.\n2. Manually annotate the HTML website (e.g., index.html) with the code snippet for the TeSS widget. More specifically, enter the code snippet as shown below as a sibling of a div element.\n         \n<link rel=\"stylesheet\" property=\"stylesheet\" href=\"https://elixirtess.github.io/TeSS_widgets/css/tess-widget.css\"/>\n<div id=\"tess-widget-events-table\" class=\"tess-widget tess-widget-faceted-table\"></div>\n<script>\nfunction initTeSSWidgets() {\n\u2003\u2003TessWidget.Events(document.getElementById('tess-widget-events-table'),\n'FacetedTable',\n{\nopts: {\ncolumns: [{name: 'Date', field: 'start'},\n{name: 'Name', field: 'title'},\n{name: 'Location', field: 'location'}],\nallowedFacets: ['scientific-topics', 'country', 'city', 'target-audience'],\nfacetOptionLimit: 5\n},\nparams: {\nq: 'Python',\ncountry: ['Belgium', 'United Kingdom']\n}\n});\n}\n</script>\n<script async=\"\" defer=\"\" src=\"https://elixirtess.github.io/TeSS_widgets/js/tess-widget-standalone.js\" onload=\"initTeSSWidgets()\"></script>\n3. Save the HTML page and serve the HTML page for visual inspection.\n4. Visually validate that the rendered HTML page includes a table with the events extracted from TeSS.The table will show a filtered list according to the values in the params section (see snippet from step 2). In this case, all future courses aggregated in TeSS will be live-filtered on the general search term \u201cPython\u201d and display only the courses from Belgium and the United Kingdom.\n5. Remove the term \u201cPython\u201d from the general search.\nThe table should now be updated with all upcoming courses from Belgium and the United Kingdom. By adapting other facets such as \u201cScientific Topics\u201d and \u201cTarget audience\u201d, other filtering schemes can be established.In order to be able to register resources in TeSS, you need to log in to the registry. The following steps are specific for the login procedure using a Life Science RI account. Alternatively, you can use another existing account like Google, Apple, or ORCID. In this case, you will be redirected to the respective login portal, but we cannot provide a detailed procedure for all the federated authentication mechanisms involved.\nNecessary Resources\nDesktop or a laptop computer or a mobile device with fast Internet connection\nUp-to-date web browser such as Google Chrome, Mozilla Firefox, Apple Safari, or Microsoft Edge\nAccess to an email account\n1. Open your browser and go to https://tess.elixir-europe.org/[href=https://tess.elixir-europe.org/].\n2. Click the \u201cLog In\u201d dropdown button on the top-right corner of the page.\n3. Choose \u201cLog in with LS Login\u201d from the dropdown menu.\nThis will redirect you to the Life Science RI authentication page (Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.682#cpz1682-fig-0001]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/212bf1d3-b76e-434b-97ce-0044af9397cd/cpz1682-fig-0001-m.jpg</p>\nFigure 1\nScreenshot of the Life Science RI login portal.\n4. Start typing the name of your institution (e.g., \u201cCNRS\u201d) into the text box, choose the appropriate option when it appears, and proceed with your usual institutional login procedure.\nYou should be taken to the TeSS homepage. In the top-right corner of the page you should see your username (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.682#cpz1682-fig-0002]), which you can click to view and edit your TeSS profile. You will also see a button to log out of TeSS.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6b197328-272c-4f34-9c17-2c5f4f78cd34/cpz1682-fig-0002-m.jpg</p>\nFigure 2\nScreenshot of the TeSS training portal welcome page.In order to register resources in TeSS as a unaffiliated provider, you can create a local account that will allow you to log in to the registry. The following steps show how this is done.\nNecessary Resources\nDesktop or a laptop computer or a mobile device with fast Internet connection\nUp-to-date web browser such as Google Chrome, Mozilla Firefox, Apple Safari, or Microsoft Edge\nAccess to an email account\n1. Open your browser and go to https://tess.elixir-europe.org/[href=https://tess.elixir-europe.org/].\n2. Click the \u201cLog In\u201d dropdown button on the top-right corner of the page.\n3. Click the \u201cRegister\u201d option.\n4. Fill out the username, email, and password fields, complete the captcha if present, and click the data processing consent checkbox after reading the privacy policy.\n5. Wait for a confirmation email to arrive, then click the \u201cConfirm my account\u201d link.\nYou should be taken to the TeSS homepage (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.682#cpz1682-fig-0002]) with a message stating \u201cYour email address has been successfully confirmed\u201d.\n6. Click the \u201cLog In\u201d dropdown button on the top-right corner of the page and then choose the \u201cLogin\u201d option.\n7. Fill out your username (or email address) and password, then click the \u201cLog in\u201d button.\nYou should be taken to the TeSS homepage. In the top-right corner of the page you should see your username (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.682#cpz1682-fig-0002]), which you can click to view and edit your TeSS profile. You will also see a button to log out of TeSS.An event in TeSS is a link to a single training event sourced by a provider along with a description and related meta information (e.g., date, location, audience, ontological categorization, keywords). Training events can be added manually or harvested automatically from a provider's website. Here is the procedure to register training events manually.\nNecessary Resources\nDesktop or a laptop computer or a mobile device with fast Internet connection\nUp-to-date web browser such as Google Chrome, Mozilla Firefox, Apple Safari, or Microsoft Edge\n1. Log in to TeSS at https://tess.elixir-europe.org[href=https://tess.elixir-europe.org].\n2. Click on the \u201cEvents\u201d menu.\n3. Click on \u201cRegister event\u201d to be directed to the form.\nAs an example, we use an RNAseq training event in all of the following steps.\n4. In the \u201cType\u201d field, select \u201cFace-to-face\u201d for a face-to-face event.\nIf your event is online, choose the option \u201cOnline\u201d to modify the form for that type of event (for instance, no address field is needed).\n5. In the \u201cTitle\u201d field, enter the title of the event: \u201cBulk RNASeq: from counts to differential expression\u201d.\n6. In the optional \u201cSubtitle\u201d field, enter any additional information for the event, such as \u201cSpring edition\u201d for a regularly repeated course.\n7. In the \u201cURL\u201d field, enter the URL where the announcement and registration for the event can be found: \u201chttps://training.vib.be/all-trainings/bulk-rnaseq-counts-differential-expression-2[href=https://training.vib.be/all-trainings/bulk-rnaseq-counts-differential-expression-2]\u201d.\n8. In the \u201cDescription\u201d field, provide general information and relevant context about the event in the form of short text:\n\u201cThe course consists of introductory online material on producing count matrices and two face-to-face sessions on differential expression analysis in R and all the questions that arise when trying the analysis on your own data.\u201d\nThis field supports markdown syntax.9. Specify the start date and door time of the event in the \u201cStart\u201d field via the pop-up \u201cDate and Time Picker\u201d (in our case, 29 September, 2022).\n10. Specify the end date in the \u201cEnd\u201d field via the pop-up Date and Time Picker (4 October, 2022).\n11. In the \u201cTimezone\u201d field, choose the time zone of the location where the event will occur: \u201c(GMT:+01:00) Amsterdam\u201d for our event in Leuven, Belgium.\n12. In the \u201cDuration\u201d field, indicate how long the event will last in hours, days, or whatever applies best. In our example: \u201c2 days\u201d.\n13. In the \u201cPrerequisites\u201d field, enter the necessary prerequisites for the training event:\n\u201cBackground knowledge of NGS data formats and the first steps in the analysis workflow (fastqc -> bam files). If you are a newbie in the field, you have to follow the NGS introduction training first. Experience in R programming is highly recommended. If you never worked with R, you should attend the R introduction training first.\u201d\nThis field supports markdown syntax.\n14. In the \u201cLearning objectives\u201d field, list the learning objectives of the event, such as:\n         \nLearn how RNA-seq reads are converted into counts\nUnderstand QC steps that can be performed on RNA-seq reads\nGenerate interactive reports to summarize QC information with MultiQC\nUse the Galaxy Rule-based Uploader to import FASTQs from URLs\nCreate a Galaxy Workflow that converts RNA-seq reads into counts\nWe recommend applying the Bloom hierarchy of cognitive skills to formulate the learning objectives. For more detailed information about our pedagogical model, please browse the course material of the ELIXIR Train the Trainer course at https://github.com/TrainTheTrainer/ELIXIR-EXCELERATE-TtT[href=https://github.com/TrainTheTrainer/ELIXIR-EXCELERATE-TtT].\nThis field supports markdown syntax.15. In the \u201cAddress\u201d and related fields, start to type the address. Once you select an address, the Google maps view will be shown and the fields Venue, City, Region, Country, and Postcode will be filled in automatically. For our example: \n         \ntable:\n\ufeff0,1\nAddress:,Herestraat 49\nVenue:,Campus Gasthuisberg\nCity:,Leuven\nRegion:,Flemish region\nCountry:,Belgium\nPostcode:,3000\n16. In the \u201cEligibility\u201d field, enter \u201cFirst come first served\u201d.\nThe other two options are \u201cRegistration of interest\u201d and \u201cBy invitation\u201d. More than one type can be provided, if applicable.\n17. In the \u201cOrganiser\u201d field, enter the organizer of the event: \u201cELIXIR-Belgium\u201d.\nThis is a free text field.\n18. In the \u201cContact\u201d field, enter the name of the training coordinator.\nThis is also a free text field, and one or more contact persons can be added, preferably with an email address.\n19. In the \u201cHost institutions\u201d field, start typing \u201cVIB\u201d to bring up a dropdown menu to choose from.\nIn steps 19-21, if your institutions, keywords, or target audiences are not listed in the menu, you can add them. You may add and select as many as you like.\n20. In the \u201cKeywords\u201d field, enter \u201ctranscriptomics\u201d.\n21. In the \u201cTarget audience\u201d field, enter \u201cLife Science Researchers\u201d.\n22. In the \u201cScientific topics\u201d field, enter \u201cRNA-Seq, Sequencing\u201d.\nScientific topics can be added according to the EDAM ontology (www.edamontology.org[href=http://www.edamontology.org]). If you do not see a matching entry, you can use the keywords field.\n23. In the \u201cOperations\u201d field, enter \u201cDifferential gene expression profiling\u201d.\nOperations can also be added according to the EDAM ontology. If you do not see a matching entry, you can use the keywords field.\n24. In the \u201cCapacity\u201d field, enter the maximum number of attendants (e.g., 20).\n25. In the \u201cEvent type\u201d field, choose \u201cWorkshops and courses\u201d.The other three options are \u201cAwards and prizegivings\u201d, \u201cMeetings and conferences\u201d, and \u201cReception and networking\u201d. More than one event type can be provided, if applicable.\n26. In the \u201cTech requirements\u201d field, enter \u201cR, Rstudio, various R packages\u201d.\nAny technical requirements can be added. Ideally, a link to installation instructions should also be provided.\nThis field supports markdown syntax.\n27. For the type of credit or recognition of attendance that will be delivered, enter \u201cCertificate of attendance recognized by Doctoral School\u201d.\nOther types of recognition can be entered as free text.\n28. In the \u201cExternal resources\u201d field, search for \u201cDESeq2\u201d in the tools section. Once the tool has been found in bio.tools, click on the \u201c+\u201d sign to add it to the entry.\nYou can associate policies, standards, and databases (provided by FAIRsharing) with your training event. In each case, a list of existing resources will be listed when you start typing keywords (click on the \u201c+\u201d sign to select).\n29. In the \u201cCost basis\u201d field, select \u201cCost to non-members\u201d.\nThe other two options are \u201cFree to all\u201d and \u201cCost incurred by all\u201d.\n30. In the \u201cContent Provider\u201d field, enter \u201cVIB Training\u201d.\nIf you do not see your organization, register it first as a Provider in TeSS.\n31. In the \u201cMaterials search\u201d box, search for \u201cTranscriptomics data analysis made easy\u201d and select suggested training materials to associate with the course.\nStart typing keywords and a list of existing training materials will be listed to select from (click on the \u201c+\u201d sign to select).\n32. Select the \u201cNode\u201d the organizer belongs to from the dropdown list: \u201cELIXIR Belgium\u201d.\nA node is a country or EMBL-EBI in the ELIXIR context.\n33. Enter the Sponsors/Funders of your training event: \u201cVIB\u201d.\n34. Click the \u201cAdd event\u201d button to finalize the registration.In the context of TeSS, a training material is a link to a single online training material sourced by a content provider (such as text on a webpage, presentation, video, etc.) along with a description and related meta information (e.g., ontological categorization, keywords). Materials can be added manually or harvested automatically from a provider's website. Here is the procedure to register training materials manually.\nNecessary Resources\nDesktop or a laptop computer or a mobile device with fast Internet connection\nUp-to-date web browser such as Google Chrome, Mozilla Firefox, Apple Safari, or Microsoft Edge\n1. Log in to TeSS at https://tess.elixir-europe.org[href=https://tess.elixir-europe.org].\n2. Click on the \u201cMaterial\u201d menu.\n3. Click on \u201cRegister training material\u201d to be directed to the form.\nAs an example, the following steps use training material about the use of AlphaFold for structure prediction of proteins and binary protein complexes.\n4. In the \u201cTitle\u201d field, enter \u201cHow to use AlphaFold for prediction of single proteins or protein complexes using the HPC\u201d.\n5. In the \u201cURL\u201d field, enter \u201chttps://elearning.bits.vib.be/courses/alphafold/[href=https://elearning.bits.vib.be/courses/alphafold/]\u201d.\n6. In the \u201cDescription\u201d field, enter:\n\"Architectural details, code and trained AlphaFold models were released by DeepMind in 2021. Given the high computational cost of deep learning algorithms, specialized hardware and software are required. Online solutions are available but come with considerable disadvantages. Therefore, the Flemish Supercomputer Center (VSC) provides high performance computing facilities, on which AlphaFold is installed and fully operational. This course gives a solid introduction on how AlphaFold can be easily and swiftly accessed using the HPC.\nThis tutorial material was created by Jasper Zuallaert (VIB-UGent), with the help of Alexander Botzki (VIB) and Kenneth Hoste (UGent). For questions and remarks, feel free to contact [email\u00a0protected][href=https://currentprotocols.onlinelibrary.wiley.com/cdn-cgi/l/email-protection#c5afa4b6b5a0b7ebbfb0a4a9a9a4a0b7b185b3aca7e8b0a2a0abb1eba7a0] or [email\u00a0protected][href=https://currentprotocols.onlinelibrary.wiley.com/cdn-cgi/l/email-protection#196d6b78707770777e596f707b377b7c].\"\nThis field supports markdown syntax7. In the \u201cKeywords\u201d field, enter \u201cStructure prediction\u201d, \u201cAlphaFold Database\u201d, and \u201cProtein complex prediction\u201d.\nA list will be displayed when you start typing keyword(s). If your desired keywords are not listed, you can add them. You may add and select as many as you like.\n8. In the \u201cLicense\u201d field, select \u201cCreative Commons Attribution Non commercial Share Alike 4.0 International\u201d.\nThe dropdown list includes an option for \u201cLicense not specified\u201d in case you do not hold a license for your material.\n9. In the \u201cStatus\u201d field, select \u201cActive\u201d.\nThe three options are Active, Under development, and Archived.\n10. In the \u201cContact\u201d field, enter \u201c[email\u00a0protected][href=https://currentprotocols.onlinelibrary.wiley.com/cdn-cgi/l/email-protection#c6b2b4a7afa8afa8a186b0afa4e8a4a3]\u201d.\nAs this is a free text field, one or more contact persons can be added, preferably with an email address.\n11. In the \u201cDOI\u201d field, enter the URL of the DOI if you have created one for your training material.\nIn our example, we do not provide a DOI.\n12. In the \u201cVersion\u201d field, enter \u201c1.0\u201d.\nYou can provide a version number for the material, if available.\n13. In the \u201cContent Provider\u201d field, select the content provider from the dropdown list: \u201cVIB Training\u201d.\n14. In the \u201cAuthors\u201d field, enter the name(s) of the author(s) of the material: \u201cJasper Zuallaert\u201d and \u201cAlexander Botzki\u201d.\n15. In the \u201cContributors\u201d field, enter the name(s) of contributor(s) to the material: \u201cKenneth Hoste\u201d.\nPreferentially, specify your ORCID.\n16. In the \u201cEvents\u201d field, select \u201cStructural Prediction of Proteins using AlphaFold on the HPC\u201d from the dropdown list.\nWe recommend doing the linking from the event side (when the event is registered) according to Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.682#cpz1682-prot-0005], step 30.\n17. In the \u201cTarget audience\u201d field, select \u201cLife scientists with programming skills\u201d from the dropdown list.\nAdd as many applicable options as you want\n18. In the \u201cPrerequisites\u201d field, add:\u201cYou are encouraged to use your own laptop. For those who do not have a laptop, the YASARA software can be run in a remote Linux environment (access to cloud via web browser). Knowledge of command line and basic Python skills are recommended.\u201d\nThis field supports markdown syntax.\n19. For \u201cCompetency\u201d level, select \u201cIntermediate\u201d.\nThe other options are \u201cBeginner\u201d and \u201cAdvanced\u201d.\n20. For \u201cLearning Objectives\u201d, enter:\n\u201cUnderstand the technical methodology of AlphaFold2, Understand the technical setup at the Flemish SuperComputer, Predict three-dimensional protein models with AlphaFold2 using the HPC at the VSC UGent.\u201d\nWe recommend applying the Bloom hierarchy of cognitive skills to formulate the learning objectives. For more detailed information about our pedagogical model, please browse the course material of the ELIXIR Train the Trainer course at https://github.com/TrainTheTrainer/ELIXIR-EXCELERATE-TtT[href=https://github.com/TrainTheTrainer/ELIXIR-EXCELERATE-TtT].\nThis field supports markdown syntax.\n21. Enter the \u201cDate created\u201d (1 April, 2022).\n22. Enter \u201cno value\u201d in \u201cDate modified\u201d.\nIf applicable, enter the date the material was last modified.\n23. Enter the \u201cDate published\u201d (28 April, 2022).\n24. In the \u201cResource types\u201d field, enter \u201cTutorial\u201d and \u201cSlides\u201d from the dropdown menu.\nOther options include video, slides, pdf, etc.\n25. In the \u201cScientific topics\u201d field, select \u201cStructure prediction, Protein Structure Analysis, Machine Learning\u201d.\nScientific topics can be added according to the EDAM ontology (www.edamontology.com[href=http://www.edamontology.com]). If you do not see a matching entry, you can use the keywords field.\n26. In the \u201cExternal resources\u201d field, click on \u201cSuggested tools to associate with this resource\u201d. In the \u201cSearch\u201d field, enter \u201cAlphaFold\u201d. Wait a couple of seconds until the result from the request to the ELIXIR Tools and Services Registry is shown. Select the entry \u201cAlphaFold 2\u201d by clicking on the \u201c+\u201d sign.You can associate policies, standards, and databases (provided by FAIRsharing) or tools (provided by bio.tools) with your training event. In each case, a list of existing resources will be listed when you start typing relevant words (click on the \u201c+\u201d sign to select).\n27. In the \u201cNodes\u201d field, use the dropdown list to select \u201cBelgium\u201d as the node to which the authors and contributors belong.\n28. In the \u201cOperations\u201d field, select \u201cMultiple Sequence Alignment, Structure Visualisation\u201d.\nOperations can be added according to the EDAM ontology. If you do not see a matching entry, you can use the keywords field.\n29. Click on \u201cRegister training material\u201d to finalize the registration.TeSS features content providers, which are entities such as academic institutions, non-profit organizations, or portals that provide training materials of relevance to the life sciences and ELIXIR. In order to have a training event automatically harvested in TeSS (see Basic Protocol 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.682#cpz1682-prot-0008]), a content provider must first be registered. This also adds visibility for content providers. Here is the procedure to register a new content provider in TeSS.\nNecessary Resources\nDesktop or a laptop computer or a mobile device with fast Internet connection\nUp-to-date web browser such as Google Chrome, Mozilla Firefox, Apple Safari, or Microsoft Edge\n1. Log in to TeSS at https://tess.elixir-europe.org[href=https://tess.elixir-europe.org].\n2. Under \u201cDirectory\u201d, click on the \u201cProviders\u201d menu.\n3. Click on \u201cRegister content provider\u201d to be directed to the form.\nAs an example, we create the content provider NanoCommons.\n4. In the \u201cTitle\u201d field, enter \u201cNanoCommons\u201d.\n5. In the \u201cURL\u201d field, enter the URL of the content provider: \u201chttps://www.nanocommons.eu/[href=https://www.nanocommons.eu/]\u201d.\n6. In the \u201cContact\u201d field, enter \u201cJean Dupont\u201d as the contact person.\n7. In the \u201cDescription\u201d field, provide general information and relevant context about the content provider in the form of short text:\n\u201cNanoCommons will deliver a sustainable and openly accessible nanoinformatics framework (knowledgebase and integrated computational tools, supported by expert advice, data interpretation and training), for assessment of the risks of NMs, their products and their formulations. NanoCommons combines Joint Research Activities to implement the nanoinformatics Knowledge Commons, Networking Activities to facilitate engagement with the research community, industry and regulators, and provision of funded Access to the nanoinformatics tools via funded calls for Transnational Access.\u201d\nThis field supports markdown syntax.\n8. To provide a logo for the Content Provider, in the \u201cImage\u201d field under \u201cor enter the URL\u201d, enter the URL of the image location: https://www.nanocommons.eu/wp-content/uploads/2018/04/NanoCommons-Logo-Sphere-Trans-White-circle-512px.png[href=https://www.nanocommons.eu/wp-content/uploads/2018/04/NanoCommons-Logo-Sphere-Trans-White-circle-512px.png]Upload an image by choosing an image file if you cannot provide a URL where the image file is located.\n9. In the \u201cType\u201d field, enter \u201cProject\u201d as type of content provider.\nAlternative options are Organisation or Portal.\n10. In the \u201cApproved Editors\u201d field, enter \u201cAlexander\u201d and select \u201c[email\u00a0protected][href=https://currentprotocols.onlinelibrary.wiley.com/cdn-cgi/l/email-protection#bedfd2dbc6dfd0dadbcc90dcd1cac4d5d7fec8d7dc90dcdb] (Alexander Botzki)\u201d.\nIf approved editors are not listed, they must first be registered as in the Alternate Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.682#cpz1682-prot-0004] and acknowledged by the TeSS team. More than one registered user can be added as an approved editor. Selected users can be removed by clicking on the cross.\n11. In the \u201cKeywords\u201d field, enter \u201cnanotechnology\u201d, \u201ccommons\u201d, \u201cproduct safety\u201d, and \u201ctoxicology\u201d.\n12. In the \u201cNodes\u201d field, select an associated ELIXIR node, if applicable.\nIn our example, there is no direct association with ELIXIR nodes, so it is left empty.\n13. Click on \u201cRegister content provider\u201d to confirm the creation of the content provider.TeSS features several options for automatically harvesting resources from external sources. This can be helpful if you maintain a large collection of events and materials that changes frequently. To register resources in TeSS automatically, data must be able to be reliably extracted from target sources. To this end, it is helpful if the data are structured according to a globally used standardized format. The following are examples of the kinds of structured data that are compatible with TeSS.\nIf your website currently includes no structured data and you want your resources added to TeSS, we recommend using Bioschemas to structure your site. Schema.org is a project run by a consortium of search engines. Schema.org has created an extensive library of schemas that webmasters can use to explicitly mark up website content in order to improve search engine visibility and interoperability. Bioschemas is an initiative to supplement the work of Schema.org to help improve the findability of online resources in the life sciences. TeSS supports the following Bioschemas profiles: (1) CourseInstance and Course for events that are courses, (2) Event for other events, and (3) TrainingMaterial for training materials. Figure 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.682#cpz1682-fig-0003] provides a schematic overview of the protocol steps.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/3ab954eb-6975-4c14-bb62-200542996b74/cpz1682-fig-0003-m.jpg</p>\nFigure 3\nSchematic overview of steps for automatic harvesting of training events and materials.\nNecessary Resources\nDesktop or a laptop computer or a mobile device with fast Internet connection\nUp-to-date web browser such as Google Chrome, Mozilla Firefox, Apple Safari, or Microsoft Edge\n1. Select the profile Learning resource https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE[href=https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE] for the automated harvesting of a specific training material from the NanoCommons initiative (https://nanocommons.github.io/user-handbook/[href=https://nanocommons.github.io/user-handbook/]) located at https://nanocommons.github.io/tutorials/enteringData/index.html[href=https://nanocommons.github.io/tutorials/enteringData/index.html].\nFor training events, select Course/CourseInstance by using the profile described at https://bioschemas.org/profiles/CourseInstance[href=https://bioschemas.org/profiles/CourseInstance].2. Manually annotate the HTML website containing the training material (here, index.html; https://nanocommons.github.io/tutorials/enteringData/index.html[href=https://nanocommons.github.io/tutorials/enteringData/index.html]) using JSON-LD markup. More specifically, enter a JSON object as shown below in a script element with the attribute type=\"application/ld+json\". This script element needs to be placed as a child of the head element of the HTML page. This results in the following script HTML element.\n         \n<script type=\"application/ld+json\">\n{\n\"@context\": \"https://schema.org/\",\n\"@type\": \"LearningResource\",\n\"http://purl.org/dc/terms/conformsTo\": { \"@type\": \"CreativeWork\", \"@id\": \"https://bioschemas.org/profiles/TrainingMaterial/1.0-RELEASE\" },\n\"name\": \"Adding nanomaterial data\",\n\"version\": \"0.9.3\",\n\"description\": \"This tutorial describes how nanomaterial data can be added to an eNanoMapper server using an RDF format.\",\n\"license\": \"https://creativecommons.org/licenses/by/4.0/\",\n\"keywords\": \"ontologies, enanomapper, RDF\",\n\"url\": \"https://nanocommons.github.io/tutorials/enteringData/\",\n\"provider\": {\n\"@type\": \"Organization\",\n\"name\": \"NanoCommons\",\n\"url\": \"https://www.nanocommons.eu/\"\n},\n\"audience\": {\n\"@type\": \"EducationalAudience\",\n\"educationalRole\": \"Graduates\"\n},\n\"inLanguage\": {\n\"@type\": \"Language\",\n\"name\": \"English\",\n\"alternateName\": \"en\"\n},\n\"author\": [\n{\n\"@context\": \"https://schema.org\",\n\"@type\": \"Person\",\n\"name\": \"Egon Willighagen\",\n\"identifier\": \"https://orcid.org/0000-0001-7542-0286\",\n\"orcid\": \"https://orcid.org/0000-0001-7542-0286\"\n}\n]\n}\n</script>\n3. Validate the individual page with the Schema.org validator at https://validator.schema.org[href=https://validator.schema.org] by pasting the URL https://nanocommons.github.io/tutorials/enteringData/index.html[href=https://nanocommons.github.io/tutorials/enteringData/index.html] into the Fetch URL tab (Fig. 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.682#cpz1682-fig-0004]).\nThe validation procedure will indicate if you have used non-existing properties of the Bioschemas profile. If error messages are returned, see Troubleshooting below.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/028fef46-4bc1-451f-9ae6-d1f2a8538c46/cpz1682-fig-0004-m.jpg</p>\nFigure 4\nScreenshot of the Schema.org validator that checks whether you have used non-existing properties of the Bioschemas profile.\n4. Create a sitemap listing the material page URL and save it as sitemap.xml. The sitemap.xml file needs to be publicly browsable on the internet. In our example, the manually created sitemap.xml is published at https://nanocommons.github.io/sitemap.xml[href=https://nanocommons.github.io/sitemap.xml].\nMore complex mechanisms for sitemap creation are available by using content management systems such as Drupal and using specific sitemap plugins.\n5. Enter \u201cNanoCommons\u201d as the \u201cContent Provider\u201d.\nIf the provider is not already registered in TeSS, this should be done first as described above (see Basic Protocol 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.682#cpz1682-prot-0007]).6. Make note of the Content Provider's exact Title and URL. In our example, this is NanoCommons and https://www.nanocommons.eu/[href=https://www.nanocommons.eu/].\n7. Go to https://github.com/ElixirTeSS/bioschemas_sources/edit/main/sources.yml[href=https://github.com/ElixirTeSS/bioschemas_sources/edit/main/sources.yml] to edit the sources.yml file and add your Content Provider details along with the URL to your sitemap:\n         \ntitle: NanoCommons\nurl: https://www.nanocommons.eu/\nsource: https://nanocommons.github.io/sitemap.xml\n8. To commit your change, click the button to open a pull request, which will then be reviewed. After review, the new training material will appear in TeSS.\nYour content should appear in TeSS the following day. Each source will be scraped once per day at \u223c3 AM UTC.", "Step-by-step method details\nStep-by-step method details\nRecombinant AID protein expression and purification\nTiming: 4\u20135\u00a0days\n      In this section, recombinant AID is expressed in Expi293F cells and\n      purified using amylose resin. The total yield of purified AID is about\n      1.25\u00a0mg from 500\u00a0mL culture.\n    \n        Expi293F cell culture and transfection.\n        Troubleshooting 1[href=https://www.wicell.org#troubleshooting].\n        \n            Day 1. Grow starter cultures in 125\u2013250\u00a0mL conical cell culture\n            flasks with vented caps, in volumes between 30 and 60\u00a0mL.\n            \nMaintain cells at 0.5\u20135\u00a0\u00d7\u00a0106 cells/mL.\nPassenger cells the day before transfection.\n                Incubate the cells in an orbital shaker incubator at 37\u00b0C,\n                120\u00a0rpm, 5% CO2.\n              \n            Day 2. Transfect Expi293F cells with pcDNA3.4-MBP-hAID plasmid using\n            PEI-MAX and culture the cells in serum-free medium at 37\u00b0C with 5%\n            CO2 for 60 h.\n            \n                Before transfection, change the cell culture medium to fresh\n                medium and keep the final density at 2.0\u00a0\u00d7\u00a0106\n                cells/mL.\n              \n                For a 500-mL culture, add 500\u00a0\u03bcg of filter-sterilized\n                pcDNA3.4-MBP-hAID plasmid into 12.5\u00a0mL fresh medium and gently\n                mix by inverting 5\u20136 times.\n              \n                Pipette 250\u00a0\u03bcL (PEI-MAX:plasmid\u00a0= 3:1, mass ratio) of the\n                filter-sterilized PEI-MAX solution to the other 12.5\u00a0mL fresh\n                medium and gently mix by inverting 5\u20136 times.\n              \n                Mix the plasmid and PEI-MAX master mix in the above two steps,\n                then incubate for 20\u00a0min at 20\u00b0C\u201325\u00b0C.\n              \n                Add the 25\u00a0mL of plasmid:PEI-MAX:medium mixture to the 500\u00a0mL of\n                cell culture and shake the bottle for 5 s.\n              \n                Incubate the transfected cells in an orbital shaker incubator at\n                37\u00b0C, 120\u00a0rpm, 5% CO2 for 60 h.\n              \nCritical: Serum-free Expi293F medium\n      should be pre-warmed in a 37\u00b0C water bath prior to any cell culture work.\n      The Expi293F serum-free medium is a ready-to-use complete medium without\n      any further additives and can be repaced by other media of the same class,e.g., Expi293 expression medium (A1435101; ThermoFisher Scientific).\n    \nNote: Cells are harvested at 2\u20133\u00a0days\n      post-transfection without changing or adding new medium. The\n      pCDNA3.4-MBP-AID plasmid will be deposited to Addgene. The plasmid was\n      generated by inserting an N-terminal His-MBP tag and a PreScission\n      protease cleavage site following the human AID coding sequence downstream\n      of the CMV promoter based on the pCDNA3.4 vector (Thermo Fisher\n      Scientific).\n    \nNote: The amount of plasmid and PEI-MAX\n      can be adjusted proportionally according to the cell number.\n    \n        AID protein purification.\n        Troubleshooting 2[href=https://www.wicell.org#troubleshooting].\n        \n            Day 5. Harvest cells by centrifugation at 2,000\u00a0\u00d7\u00a0g for\n            20\u00a0min at 4\u00b0C.\n            \nCritical: All the buffer for\n              purification should be autoclaved or filtered with a 0.22\u00a0\u03bcm\n              filter before use. Keep the whole operation on ice or at 4\u00b0C.\n            \n            Discard supernatant and thoroughly resuspend cells in 30\u201340\u00a0mL of\n            pre-chilled lysis buffer.\n            \nNote: Add PMSF, DTT and DNase I to\n              the lysis buffer prior to use.\n            \n            Lyse the cells using a high-pressure crusher (manufacture model) or\n            similar instrument at 100\u00a0Pa for 1\u00a0min followed by 300\u00a0Pa for 5\u00a0min.\n          \n            Centrifuge the lysate at 45,000\u00a0\u00d7\u00a0g for 40\u00a0min at 4\u00b0C to\n            pellet the cell debris.\n          \n            Equilibrate 1\u00a0mL of amylose resin with 40\u00a0mL of pre-chilled\n            ddH2O and then with 20\u00a0mL of pre-chilled\u00a0protein\n            purification wash buffer in a gravity chromatography column.\n            \nNote: Add DTT to the wash buffer\n              prior to use.\n            \n            Load the supernatant onto the column, and allow it to flow slowly\n            under gravity (1\u00a0mL per minute). Repeat this loading step two more\n            times to increase the final protein yield.\n          \n            Wash the resin with 10\u201320 column volumes of protein purification\n            wash buffer.\n          \n            Elute the protein from the column using 10\u00a0\u00d7\u00a0500\u00a0\u03bcL protein elute\n            buffer.\n            \nNote: Add DTT to the protein elutebuffer prior to use. Add 500\u00a0\u03bcL protein elute buffer each time. In\n              general, the protein concentration is higher from elute 2 to elute\n              4.\n            \n            Measure the concentration and purity of the recombinant AID protein\n            by Bradford assay and gel electrophoresis (Figure\u00a01[href=https://www.wicell.org#fig1]B).\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3015-Fig1.jpg\n                  Figure\u00a01. Preparation of long ssDNA substrates and recombinant\n                  AID protein\n                \n                  (A) Overview of the preparation of long ssDNA substrates. The\n                  ssDNA was prepared with a biotin-conjugated non-C primer via\n                  linear-amplification mediated (LAM)-PCR.\n                \n                  (B) Recombinant purified AID protein separated by the SDS-PAGE\n                  followed by Coomassie blue staining are shown at left.\n                  Prepared dsDNA and ssDNA substrates were separated on the\n                  agarose gel followed by Ethidium bromide staining are shown at\n                  right.\n                \n            Aliquot the protein and snap freeze with liquid nitrogen, then store\n            at \u221280\u00b0C.\n          \nLong ssDNA substrates preparation\nTiming: 2\u20133\u00a0days\n      In this section, long ssDNA substrates (\u223c200 nt) are generated using a\n      linear-amplification mediated PCR (LAM-PCR) approach followed by\n      biotin-streptavidin purification. With this protocol, 1,500\u00a0ng of ssDNA\n      can be prepared.\n    \n        dsDNA template preparation.\n        \n            Day 1. Design primers to amplify the specific DNA sequence.\n            \nNote: As an example, we chose a\n              124-bp substrate that covers the FR3-CDR3 sequence of the mouse\n              VHB1-8 exon, which has biased WRC mutations in the CDR3\n              in\u00a0vivo.13[href=https://www.wicell.org#bib13],14[href=https://www.wicell.org#bib14] To facilitate\n              downstream PCR reactions, an additional non-C sequence is added to\n              both ends of the dsDNA template, resulting in a final length of\n              198\u00a0bp (Figure\u00a01[href=https://www.wicell.org#fig1]A).\n            \n            Set up eight 50-\u03bcL PCR reactions for each sequence as below: PCR\n            reaction master mix\n            table:files/protocols_protocol_3015_17.csv\nNote: non-C or non-G sequences are\n              underlined, gene-specific sequences are shown in blue. To amplify\n              the sense strand substrate:\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3015-Fx5.jpg\n            Set a PCR program to amplify the DNA fragments as below: PCR cycling\n            conditions\n            table:files/protocols_protocol_3015_18.csv\nNote: For different templates, theannealing temperature, extension time and number of cycles can be\n              adjusted.\n            \nPause point: Amplified DNA\n              products can be stored at \u221220\u00b0C for months.\n            \n            Separate the PCR products by agarose gel electrophoresis.\n            \nPool the eight tubes of PCR products.\nAdd 80\u00a0\u03bcL 6\u00d7 DNA loading dye directly to the pool.\nRun the sample by a 2% agarose gel in 1\u00d7 TAE buffer.\nCut the 198-bp DNA fragment from the gel.\n            Add 2\u20133\u00a0mL of PC buffer included in the Universal DNA Purification\n            Kit to the excised gels and allow to completely dissolve at\n            20\u00b0C\u201325\u00b0C.\n          \n            Add 500\u00a0\u03bcL BL buffer to a Universal DNA Purification column and spin\n            at 13,523\u00a0\u00d7\u00a0g for 1\u00a0min at 20\u00b0C\u201325\u00b0C, discard the\n            flow-through.\n          \n            Add the solubilized gel mixture to the balanced column and spin at\n            13,523\u00a0\u00d7\u00a0g for 1\u00a0min at 20\u00b0C\u201325\u00b0C, discard the flowthrough.\n          \n            Add 600\u00a0\u03bcL PW buffer to the column, spin at 13,523\u00a0\u00d7\u00a0g for\n            1\u00a0min at 20\u00b0C\u201325\u00b0C, discard the flowthrough. Repeat this step one\n            more time.\n          \n            Spin the column at 13,523\u00a0\u00d7\u00a0g for 2\u00a0min at 20\u00b0C\u201325\u00b0C,\n            transfer the column to a new 1.5\u00a0mL microtube.\n          \n            Add 50\u2013100\u00a0\u03bcL ddH2O to the column, incubate for 2\u00a0min at\n            20\u00b0C\u201325\u00b0C, spin at 13,523\u00a0\u00d7\u00a0g for 2\u00a0min at 20\u00b0C\u201325\u00b0C.\n          \n            Check the DNA concentration using a NanoDrop or similar instrument.\n            \nPause point: Purified DNA\n              products can be stored at \u221220\u00b0C for months.\n            \n        LAM-PCR and size selection.\n        Troubleshooting 3[href=https://www.wicell.org#troubleshooting].\n        \n            Day 2. Prepare four 50-\u03bcL PCR reactions for sense or antisense\n            strand as follows:\n            Primers for LAM-PCR:\n            Biotinylated primer: 5\u2032 Biotin-AGATGTGGATGAGGAAGGTT-3\u2032. PCR reaction\n            master mix\n            table:files/protocols_protocol_3015_19.csv\n            Set the PCR program to linear-amplify ssDNA as below: PCR cycling\n            conditions\n            table:files/protocols_protocol_3015_20.csv\nNote: Do not leave PCR products in\n              the PCR machine for too long at 4\u00b0C after the PCR amplification,as the TransStart FastPfu DNA polymerase may resect the 3\u2032 ends of\n              the ssDNA products.\n            \n            Size selection.\n            \n                Add 60\u00a0\u03bcL Dynabeads MyOne Silane (2.4\u00a0mg) to each 50\u00a0\u03bcL PCR\n                products.\n              \nMix by gentle pipetting 5\u20136 times.\n                Incubate at 20\u00b0C\u201325\u00b0C for 10\u00a0min to remove excess biotinylated\n                primer.\n              \n            Retrieve the beads on a magnetic stand for 2\u00a0min, and discard the\n            supernatant.\n          \nWash the beads three times with 200\u00a0\u03bcL 70% ethanol.\n            Remove the 70% ethanol and dry the beads for 3\u20135\u00a0min at 20\u00b0C\u201325\u00b0C.\n          \nResuspend the beads in 40\u00a0\u03bcL Tris-HCl (10\u00a0mM, pH7.4).\nRetrieve the beads on a magnetic stand for 2\u00a0min.\n            Combine the supernatants of 4 reactions in a new tube, final volume\n            160\u00a0\u03bcL.\n            \nPause point: The ssDNA\n              fragments can be stored at \u221220\u00b0C for up to one week. Longer\n              storage is not recommended as ssDNA products are not stable.\n            \nNote: Completely resuspend the\n              streptavidin beads by vortexing for at least 30\u00a0s prior to\n              pipetting. Different concentrations of Dynabeads can be used to\n              capture DNA of different sizes. The final concentration can be\n              adjusted according to specific conditions.\n            \n        Streptavidin affinity purification.\n        \n            Day 3. Add 40\u00a0\u03bcL 5\u00a0M NaCl (1\u00a0M final) and 2\u00a0\u03bcL 0.5\u00a0M EDTA (pH 8.0,\n            5\u00a0mM final) to the 160\u00a0\u03bcL ssDNA products.\n          \n            Transfer 20\u00a0\u03bcL Dynabead MyONE C1 Streptavidin Beads (200\u00a0\u03bcg) to a\n            new 1.5-mL microtube, add 600\u00a0\u03bcL 1\u00d7 Binding and Washing (B&W)\n            buffer and mix by pipetting.\n            \nNote: Before pipetting\n              streptavidin beads, fully resuspend the beads by vortexing for at\n              least 30 s.\n            \n            Retrieve the beads on a magnetic stand for 2\u00a0min, and discard the\n            supernatant. Repeat this step three times.\n          \n            Resuspend the beads with pooled ssDNA products from step 5a, andincubate the mixture on a rotary shaker at 20\u00b0C\u201325\u00b0C for at least 2\n            h.\n            \nNote: Although 2\u00a0h is sufficient\n              for the beads to capture most of the biotinylated PCR products,\n              longer incubation times (up to 4 h) can be used.\n            \n            Retrieve the DNA-beads complex on the magnetic stand and wash the\n            DNA-beads complex with 200\u00a0\u03bcL 1\u00d7 Binding and Washing (B&W)\n            buffer, 200\u00a0\u03bcL 10\u00a0mM NaOH, 200\u00a0\u03bcL 1\u00d7 Binding and Washing (B&W)\n            buffer and 200\u00a0\u03bcL Tris-HCl (10\u00a0mM, pH7.4), respectively.\n          \n            Retrieve the DNA-beads on a magnetic stand for 2\u00a0min, and discard\n            the supernatant.\n          \n            Resuspend the DNA-beads complex with 30\u201350\u00a0\u03bcL ddH2O. Heat\n            the final ssDNA at 95\u00b0C for 10\u00a0min and flash-cool the sample on ice.\n          \nRetrieve the ssDNA supernatant on the magnetic stand.\n            Determine the ssDNA concentration using a NanoDrop or a similar\n            machine.\n          \nAliquot the ssDNA and store at \u221220\u00b0C.\n            Evaluate the purity of the ssDNA by running 200\u00a0ng of ssDNA on a 3%\n            agarose gel in 1\u00d7 TAE buffer (Figure\u00a01[href=https://www.wicell.org#fig1]B).\n          \nssDNA oligo pools preparation\nTiming: 3\u20134\u00a0weeks\n    \n      In this section, we design oligo pools as deamination substrates. As an\n      example, IGHV oligo pools containing 1,084 IGHVs from 27 species, and\n      4,096 random N6 oligo pool are designed.\n    \nCritical: Oligo pool design takes\n      about a few days and oligo synthesis takes 2\u20133\u00a0weeks. The non-C primers\n      and non-C barcodes should be added to both sides of each oligo. For the\n      IGHV oligo pool, the sense and antisense strand oligo pools are designed\n      separately. We use the sense strand as an example. The antisense strand\n      oligo pool contains the same set of primers and barcodes as the sense\n      strand, but only reverse-complement tested sequences. All custom scripts\n      used in this section are available at\n      https://github.com/ZhangSenxin/Deamination-HTS-Pipeline[href=https://github.com/ZhangSenxin/Deamination-HTS-Pipeline]\n      (https://doi.org/10.5281/zenodo.8271540).\n    \n        IGHV oligo pool design.Week1. Download the functional L-PART1-IGHV-EXON segments from 16\n            species and the functional V-REGION without leader sequences from 11\n            species from the IMGT database.15[href=https://www.wicell.org#bib15]\nNote: Chicken has only 1\n              functional V gene, so we include other pseudogenes as well. For\n              the V-REGION without leader information, we arbitrarily add 50 of\n              N on the 5\u2032 side.\n            \n            Select the \u221701 allele of each V gene, resulting in a total of 1,084\n            Vs.\n            table:files/protocols_protocol_3015_21.csv\n            Divide each V gene into a 50-nt oligonucleotide pool at 25-nt\n            resolution.\n            \nNote: Since the average length of\n              each V is about 350\u00a0bp, each V is tiled into 13\u201315 oligos (Figure\u00a02[href=https://www.wicell.org#fig2]A).\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3015-Fig2.jpg\n                  Figure\u00a02. Design of short ssDNA oligo pools containing complex\n                  sequences\n                \n                  (A) Illustration of short ssDNA oligo pools. Two oligo pools,\n                  corresponding to the sense and antisense strands, were\n                  synthesized to cover the IGHVs and control regions.\n                  Each sequence was tiled into a 50-nt oligo pool at 25-nt\n                  resolution. L/N notes the leader sequence or 50\u00a0N added to the\n                  5\u2032 side of V-REGION without leader. The final substrates are\n                  5\u2019 - \u201812 nt adapter\u2019 - \u20186 nt barcode\u2019 - 50-nt - \u20186 nt barcode\u2019\n                  - \u201812 nt adapter\u2019 - 3\u2019.\n                \n                  (B) Illustration of 4096 random N6 ssDNA oligo\n                  pools. The final substrates are 5\u2019 - \u201812 nt adapter\u2019 - \u20188 nt\n                  barcode\u2019 - 28-nt - \u20188 nt barcode\u2019 - \u201812 nt adapter\u2019 -\u00a03\u2019.\n                \n            Pool all of the 50-nt sequences together and remove duplicates,\n            resulting in a total of 9,753 unique sequences.\n          \n            Select 52 AID off-targets (OT) in mouse CSR-activated B cells and\n            the first exon of highly-transcribed genes (HiG)16[href=https://www.wicell.org#bib16]\n            in mouse GC B cells as controls.\n          \n            For OT or HiG regions, select 50-nt sequences with a WRCH motif in\n            the middle (23-nt-WRCH-23-nt), resulting in a total of 1,355 uniqueOT sequences and 892 unique HiG sequences, respectively. (g-i)\n            Unique barcode design.\n          \n            Extract and cluster the 9-nt sequences on both sides of the IGHVs\n            and control sequences using custom scripts as follows:\n            Troubleshooting 9[href=https://www.wicell.org#troubleshooting].\n            \n>python3 cluster_inbarcode.py -i SEQ_FILE -o\n                  CLUSTER_PATH\n>python3 barcode_assignment.py -i CLUSTER_PATH -o\n                  SAVE_FILE -m\n            Determine the largest sequence number in the cluster of identical\n            9-nt sequences.\n            \nNote: For example, in the current\n              IGVH pool, the largest cluster contains \u223c500 sequences because\n              many IGVHs from different species share some degree of similarity.\n            \n            Select 30 of the 6-nt non-C barcodes with at least 2 nt difference,\n            and add the barcodes to both sides of the 50-nt sequence, resulting\n            in ssDNA 62 nt long.\n            6-nt non-C barcode\n            table:files/protocols_protocol_3015_22.csv\nNote: There are 900 different\n              combinations of 30\u00a0\u00d7\u00a030 barcodes, which is enough to cover the\n              largest cluster in the IGVH pool (\u223c500 sequences). Different\n              clusters use the same set of barcodes. In the IGVH pool, the 15-nt\n              in-barcodes on both side of ssDNA sequence are used to demultiplex\n              the reads in the following steps. The length of the non-C barcode\n              can be extended accordingly.\n            \n            Run a simulation test using a custom script to demultiplex each\n            sequence according to the in-barcodes as follows:\n            \n>python3 fastq_simulation.py -I SEQ_FILE -o\n                  FASTQ_PATH\n                Generate raw data files with 10 paired-reads for each sequence,\n                named as R1_10.fq.gz and R2_10.fq.gz\n              \n                Prepare the metafiles.\n                \nNote: Each metafile contains\n                  500 sequences. The format is as follows: (ID, \u2018barcode1+9\u00a0bp\n                  sequence\u2019-\u2018the reverse complementary sequence of barcode2+9\u00a0bp\n                  sequence\u2019).\n                \ntable:files/protocols_protocol_3015_23.csv\ntable:files/protocols_protocol_3015_24.csv\n                \u2026\n              \n                Perform the simulation test using this code:\n                \n>fastq-multx -x -B meta.test1.txt -m 1 -d 1 -b\n                      R1_10.fq.gz R2_10.fq.gz -o test1/%_R1.fq.gz\n                      test1/%_R2.fq.gz\n> out.test1_m1.txt\n                If each sequence is accurately split into 10 reads, the\n                simulation is successful, indicating that the in-barcodes can be\n                used to distinguish each sequence.Add the 12-nt non-C PCR primers to both sides of the 62-nt\n            sequences, resulting in a final length of 86 nt.\n            \nNote: The final oligo sequence is\n              5\u2032- GTAAGGGTGAGG-barcode1-\u201950-nt test\n              sequence\u2019-barcode2-GATAGGGTGGTG-3\u2019 (Figure\u00a02[href=https://www.wicell.org#fig2]A).\n            \n        4,096 N6 oligo pool design.\n        \n            Week1. Generate 4096 sequences of\n            A6N6AGCTA12 in silico. N\n            represents A, G, C or T.\n          \n            Add a unique 8-nt non-C barcode pair on each side of the 28-nt\n            sequences, for a total length of 44 nt.\n          \n            Add 12-nt non-C PCR-primers to each side of the above sequence,\n            resulting in a final length of 68 nt.\n            \nNote: The oligo sequence is 5\u2032-\n              GTAAGGGTGAGG-barcode1-\n              A6N6AGCTA12-barcode2-GATAGGGTGGTG-3\u2019\n              (Figure\u00a02[href=https://www.wicell.org#fig2]B).\n            \n            As in Step 6j, run a simulation test to demultiplex each sequence\n            according to the in-barcodes.\n          \n        Week2-Week4. Synthesize the oligo pool using a commercial vendor (Twist\n        Bioscience or similar).\n      \nIn\u00a0vitro AID deamination assay\nTiming: 1\u20131.5 h\n      This section describes how to perform the in\u00a0vitro AID deamination\n      reaction.\n    \n        Reaction setup. Troubleshooting 4[href=https://www.wicell.org#troubleshooting],\n        5[href=https://www.wicell.org#troubleshooting].\n        \n            Thaw AID protein on ice from the \u221280\u00b0C refrigerator.\n            \n                Centrifuge the enzyme at 9,391\u00a0\u00d7\u00a0g for 10\u00a0min at 4\u00b0C\n                prior to use.\n              \n                Retrieve the supernatant carefully. Discard the \u223c5\u00a0\u03bcL of liquid\n                at the bottom to remove any aggregated proteins.\n              \n            Heat the ssDNA at 95\u00b0C for 10\u00a0min, and quickly place the tube into\n            ice to completely denature the substrates.\n          \n            Perform the reaction in a 10-\u03bcL format.\n            \nAdd the pre-cooled reaction buffer.\n                Add the ssDNA and AID.\n                \nNote: The deamination reaction\n                  buffer contains 20\u00a0mM HEPES pH7.4, 10/150\u00a0mM NaCl and 1\u00a0mM\n                  DTT.\n                \nPipette the mixture up and down several times.\nQuick-spined on a desktop centrifuge for 5 s.\n                Incubate the tube at 37\u00b0C for 5\u201360\u00a0min.\n                \nNote: In our hands, the\n                  reaction reaches a plateau after 60\u00a0min.\n                \nAfter incubation, stop the reaction by heating (95\u00b0C, 10\u00a0min).Note: The salt concentration in the AID\n      protein buffer should be counted in the final reaction salt concentration.\n      Reaction time, substrate/enzyme ratio should be tested for each batch of\n      purified enzyme, as exampled in Figures\u00a03[href=https://www.wicell.org#fig3]A and 3B.\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3015-Fig3.jpg\n          Figure\u00a03. HTS-data analysis of long ssDNA substrates and short ssDNA\n          oligo pools\n        \n          (A) Mutation profiles of a FR3-CDR3 region from the VHB1-8 allele in in\u00a0vitro deamination assay with\n          different molar ratio of ssDNA:AID. Deamination frequency, i.e.,\n          substitution in total sequencing reads, at each nucleotide along the\n          124-nt region is plotted as a bar graph with green error bars,\n          representing mean with SEM from 3 repeats. WRC/GYW motifs are labeled\n          in yellow, and AGCT is labeled in orange.\n        \n          (B) Deamination products of IGHV oligo pools were subjected to\n          mutation spectrum (upper) and seqlogo analyses (lower). The seqlogo\n          was obtained from the top16 tetramer with high mutation frequency in\n          all 64 NNCN combinations.\n        \nPause point: The final reaction\n      product can be stored for up to one week at \u221220\u00b0C. Longer storage is not\n      recommended because ssDNA is not stable.\n    \nAmplicon-seq library preparation\nTiming: 1\u00a0day\n      This step describes how to prepare the amplicon sequencing library from\n      deamination products for high-throughput sequencing (HTS).\n    \n        U-friendly PCR. Troubleshooting 6[href=https://www.wicell.org#troubleshooting].\n        \n            Set up a 10-\u03bcL PCR reaction for each sample as below:\n            table:files/protocols_protocol_3015_25.csv\nNote: Primers for long ssDNA\n              substrates (underlined sequences are used as the primer sites for\n              the next step PCR, in-barcodes are in red, non-C or non-G\n              sequences are in blue). A pair of primers used for\n              VB1-8 FR3-CDR3 amplicon is shown.\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3015-Fx6.jpg\n            Primers for short ssDNA oligo pools: (underlined sequences are used\n            as the primer sites for the next step PCR, in-barcodes are in red,\n            12-nt non-C or non-G sequences are in blue).\n            imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3015-Fx7.jpg\n            Set up the following PCR program:\n            table:files/protocols_protocol_3015_26.csvNote: This 1st step of\n              PCR can also use other U-friendly enzymes, such as Q5U Hot Start\n              High-Fidelity DNA Polymerase, or Phusion U DNA polymerase. The\n              in-barcode is used to distinguish the difference batch of\n              experiments, which is different from the in-barcode designed in\n              step 6 or 7.\n            \n        Amplicon-seq HTS library preparation.\n        \n            Set up a 25-\u03bcL PCR reaction for each sample as below:\n            table:files/protocols_protocol_3015_27.csv\nNote: Primers for 2nd\n              step PCR (underlined sequences overlap with 1st step\n              PCR primers, barcode is in red):\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3015-Fx8.jpg\n            Set the following PCR program to amplify the DNA fragments:\n            table:files/protocols_protocol_3015_28.csv\nPause point: Amplified DNA\n              products can be stored at \u221220\u00b0C for months.\n            \n        HTS-library purification.\n        \n            Add 5\u00a0\u03bcL 6\u00d7 DNA loading dye directly into the PCR products and run\n            the sample on a 2% agarose gel in 1\u00d7 TAE buffer.\n          \nExcise the target DNA fragments from the gel.\n            Add 2\u20133\u00a0mL of PC buffer provided in the Universal DNA Purification\n            Kit into the excised gels and allow to fully dissolve at 20\u00b0C\u201325\u00b0C.\n          \n            Add 500\u00a0\u03bcL BL buffer to a Universal DNA Purification column and spin\n            at 13,523\u00a0\u00d7\u00a0g for 1\u00a0min at 20\u00b0C\u201325\u00b0C, discard the flow\n            through.\n          \n            Load the dissolved gel mixture onto the balanced column and spin at\n            13,523\u00a0\u00d7\u00a0g for 1\u00a0min at 20\u00b0C\u201325\u00b0C, discard the flow-through.\n          \n            Add 600\u00a0\u03bcL buffer PW to the column, spin at 13,523\u00a0\u00d7\u00a0g for\n            1\u00a0min at 20\u00b0C\u201325\u00b0C, discard the flow-through. Repeat this step one\n            more time.\n          \n            Spin the column at 13,523\u00a0\u00d7\u00a0g for 2\u00a0min at 20\u00b0C\u201325\u00b0C,\n            transfer the column to a new 1.5\u00a0mL tube.\n          \n            Add 50\u2013100\u00a0\u03bcL ddH2O to the column, incubate for 2\u00a0min at\n            20\u00b0C\u201325\u00b0C, spin at 13,523\u00a0\u00d7\u00a0g for 2\u00a0min at 20\u00b0C\u201325\u00b0C.\n          \nMeasure the concentration using a Qubit Quantification Kit.\nHTS data analysis\nTiming: 1\u20132\u00a0daysThis section describes how to perform the\n      HTS data analysis[href=https://www.wicell.org#sec3.6]. Data from long-ssDNA and\n      oligo-pool substrates are processed differently.\n    \nCritical: All custom scripts used in\n      this section are available at\n      https://github.com/ZhangSenxin/Deamination-HTS-Pipeline[href=https://github.com/ZhangSenxin/Deamination-HTS-Pipeline]\n      (https://doi.org/10.5281/zenodo.8271540).\n    \n        Create a metafile.txt file for Hiseq run.\n        Troubleshooting 7[href=https://www.wicell.org#troubleshooting],\n        8[href=https://www.wicell.org#troubleshooting].\n      \nNote: The metafile contains the\n      configuration information needed to process sequence reads for a given\n      sample. Incorrect information can lead to errors at various stages of the\n      pipeline or produce incorrect results. The metadata file is a\n      tab-delimited plain text file containing the following header line with\n      each subsequent line describing the design of a single sample.\n    \ntable:files/protocols_protocol_3015_29.csv\n      \u201cexperiment\u201d- the unique name of the sample, this ID will be used to name\n      most files generated by the pipeline.\n    \n      \u201cgenotype\u201d, \u201callele\u201d, \u201cmouse/cell\u201d, \u201ctissue/clone\u201d- information to\n      describe the sample.\n    \n\u201cPE-P7-index\u201d- the unique PE-P7 index ID.\n\u201cbarcode\u201d- sequence of the PE-P7-index.\n\u201cforward_primer\u201d- sequence of the gene-specific primer on P5 side.\n\u201creverse_primer\u201d- sequence of the gene-specific primer on P7 side.\n      \u201cin-barcode-F\u201d- barcode on the P5 side that you will add to your\n      forward_primer.\n    \n      \u201cin-barcode-R\u2033- barcode on the P7 side that you will add to your\n      reverse_primer.\n    \n      \u201creference\u201d- The pipeline uses this name to find the reference sequence\n      and bowtie2 index on the filesystem.\n    \n      \u201cstart\u201d- position of the next nucleotide at the end of the forward_primer.\n    \n      \u201cend\u201d- position of the next nucleotide at the end of the reverse_primer.\n    \n        For long ssDNA substrates, run the SHM pipeline13[href=https://www.wicell.org#bib13]\n        to generate the result files. Multiple samples can be processed in the\n        same time.\n      \n>perl SHMPipeline.pl --meta meta.wyy.txt --in raw --out\n          result_wyy\n--ref ./ref --threads 8--ow\u00a0&> wyy.nohup.txt\nNote: The SHM pipeline reads the metafile\n      and calls bowtie2 to align the forward and reverse reads to the reference\n      sequence.\n    \n        The WYY\u2217_filt_profile.txt can be used directly to plot the mutation\n        profile using the custom scripts as follows:>For WYY in \u2217txt\n>do RScript ./SHMPlot2_zhou1_WRC.R ./$WYY/$WYY.pdf tstart=xx\n          tend=xxx plotrows=1 ymax=0.35 figureheight=2 showsequence=T or F;\n          done\nNote: Where tstart and tend are the start\n      and end filled in the metafile, plotrows defines the number of rows\n      displayed in the plot, ymax means the maximum value of the mutation\n      frequency, figureheight defines the height of the plot, showsequence\u00a0= T\n      or F means to show or hide the sequence information, as illustrated on\n      Figure\u00a03[href=https://www.wicell.org#fig3]A.\n    \n        For short ssDNA oligo pools, demultiplex the raw reads and remove the\n        12-bp adapters at the 5\u2032 and 3\u2032 ends of the oligos using cutadapt v1.9\n        as follows:\n      \n>bash cutadapter.sh\n        Filter out reads with Illumina Sequencing Quality Scores less than 30.\n      \n>source activate fastp\n>fastp -i WYY\u2217_recut_R1.fq.gz -I WYY\u2217_recut_R2.fq.gz -o\n          WYY\u2217_recut_R1_q30.fq.gz -O WYY\u2217_recut_R2_q30.fq.gz -q 30 -u 10\n          -A\n        Divide each substrate according to the in-barcodes.\n        \n            Create multiple metafile_cut.txt files with no header to split the\n            reads.\n            \nNote: Since the fastq-multx\n              program has an upper limit for each run, each metafile contains\n              500 rows and 2 columns, the first column is the sample ID, the\n              second column is the internal barcode information, the format is\n              5\u2032barcode-3\u2032barcode (reverse complement).\n            \n            Divide each sequence into different files using the custom script as\n            follows:\n            \n>bash multx.sh\n        Annotate and generate mutation profiles using the custom script as\n        follows:\n      \n>python3 read_fq.gz.py -m ./array1_ref.txt -f ./array1/\n-s ./array1/out2/ -c 1\u00a0>\u00a0array1.txt\nNote: Where -m: oligo pool reference file,\n      -f: the data path generated in step 19b, -s: save path, -c defines the\n      sense strand (-c 1, default) or the antisense strand (-c 0).\n    \n        Mutation spectrum analysis.\n        \n            Mutation spectrum statistics for A, G, C, T using the custom script\n            as follows (Figure\u00a03[href=https://www.wicell.org#fig3]B):\n            \n>python3 mut_summary.py -m ./IGHV_ref_kabat.txt\n-f ./array1/out2/ -s\n                  ./array1/out3_kabat/\u00a0>\u00a0mutation_summary01_out.txt\nNote: Where -m: oligo poolreference file containing region information (one sequence spans\n              up to 3 regions), -f: the data path generated in step 20 -s: save\n              path.\n            \n            Mutation spectrum statistics for C in WRC, SYC, WGCW, AGCT using the\n            custom script as follows:\n            \n>python3 mut_summary2.py -m ./IGHV_ref_kabat.txt\n-f ./array1/out2/ -s\n                  ./array1/out3_kabat/\u00a0>\u00a0mutation_summary02_out.txt\nNote: Where -m: oligo pool\n              reference file containing region information (one sequence spans\n              up to 3 regions), -f: the data path generated in step 20, -s: save\n              path.\n            \n        Combine the tiling assay into a full-length V gene.\n        Troubleshooting 10[href=https://www.wicell.org#troubleshooting].\n      \n>python3 mapping2full_length_sequence.py\n-m ./full_length_reference.csv\n-r ./IGHV_ref_kabat.txt -f ./array1/out2/\n-s ./array1/full_length_out/\n-i ./array1/full_length_mismatch.txt\u00a0>\u00a0full_length_out.txt\nNote: Where -m: sequence reference file\n      containing full length V, -r: oligo pool reference file containing region\n      information (one sequence spans up to 3 regions), -f: the data path\n      generated in step 20, -s: save path, -i file path to save mismatch\n      information (if exist).\n    \nNote: This step combines each 50-nt\n      segment from a V gene and generate the full-length V mutation information.\n    \n        Plot mutation frequency for V genes.\n        Troubleshooting 11[href=https://www.wicell.org#troubleshooting].\n        \n            Pre-process the data:\n            \n>python3 data_produce.py -m\n                  ./full_length_reference.csv\n-r ./V_region_kabat.csv -f ./array1/full_length_out/\n-s ./array1/plot_data/ -p\n                  ./species.txt\u00a0>\u00a0data_produce_out.txt\nNote: Where -m: sequence reference\n              file containing full length sequence, -r: region reference file\n              containing V gene region info, -f: the data path generated in step\n              22, -s: save path, -p species information.\n            \n            Plot mutation frequency based on specie pre-processed data (as shown\n            in Wang et\u00a0al.1[href=https://www.wicell.org#bib1]):\n            \n>python3 figure_plot.py -f ./array1/plot_data/\n-s ./array1/plot_figure/ -p ./species.txt\nNote: Where -f: the data path\n              generated in step 23a, -s: save path, -p species reference file.", "Step-by-step method details\nStep-by-step method details\nStep 1: Find sgRNAs\nTiming: 5\u00a0min\nAlthough CRISPRi can target both coding sequences and promoters (Gilbert et\u00a0al., 2013[href=https://www.wicell.org#bib5]; Cui et\u00a0al., 2018[href=https://www.wicell.org#bib4]), in prokaryotes, transcriptional interference is most effective when blocking RNA polymerase elongation (Rishi et\u00a0al., 2020[href=https://www.wicell.org#bib18]). Targeting within the protein coding sequence of a gene guarantees an elongating RNAP and does not require knowledge of promoters. For dCas9, it has furthermore been shown that knockdown efficacy is higher when targeting the non-template strand of a gene (Figures 3[href=https://www.wicell.org#fig3]A and 3B). The computational pipeline therefore screens for sgRNAs by identifying PAM sequences (NGG) on the template strand of genes. The 20 nucleotides upstream of a PAM sequence corresponds to the spacer sequence of an sgRNA that would target the complementary non-template strand, causing effective expression knockdown (Figure\u00a01[href=https://www.wicell.org#fig1]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/636-Fig3.jpg\nFigure\u00a03. Impact of targeting strand and offset on expression knockdowns and relative fitness\n(A and B) Expression knockdown of sgRNAs targeting the template (T) or non-template (NT) strands (box\u00a0= interquartile range, horizontal line\u00a0= median, wishers\u00a0= data within 1.5 times interquartile range above or below box, dots\u00a0= outliers). (A) Relative GFP expression in strains expressing gfp-targeting sgRNAs from a constitutive promoter in B.\u00a0subtilis (red) and E.\u00a0coli (blue) (Hawkins et\u00a0al., 2020[href=https://www.wicell.org#bib7]). (B) Relative fitness of sgRNAs targeting essential genes using the template or non-template strand in two different E.\u00a0coli strains with high (green) or low (purple) dCas9 expression (Cui et\u00a0al., 2018[href=https://www.wicell.org#bib4]).(C and D) Impact of sgRNA offset \u2013 distance to start codon (nt) \u2013 on expression knockdown. (C) Expression knockdown of sgRNAs targeting the non-template strand of gfp in B.\u00a0subtilis (red) and E.\u00a0coli (blue). (D) Relative fitness of sgRNA targeting non-template strand of essential genes in E.\u00a0coli with high (green, upper regression: p\u00a0< 0.01, R2=0.003) and low (purple, lower regression: P\u00a0< 0.001, R2=0.005) dCas9 expression.\nTo identify all sgRNAs targeting the non-template strand of the protein coding sequence of the gene(s) of interest, one can run the first step of the computational pipeline in the following way:\n>python <path to \u201cgenerate_sgrnas.py\u201d script>\n--genbank <path to genbank file>\n--locus_tag <locus_tag>\n--step find\nHere, the --genbank and --locus_tag are the only mandatory arguments and specify the path to the GenBank file and the locus tag(s) (not gene names) of the gene(s) of interest, respectively.\nThe --step argument is used to only run the first step of the computational pipeline (i.e., find). Without specifying this argument, the script will automatically go through all steps. For a complete list of arguments and the default settings see Table 1[href=https://www.wicell.org#tbl1].\ntable:files/protocols_protocol_636_1.csv\nNote: one can also run the help-command:\n>python <path to \u201cgenerate_sgrnas.py\u201d script> -h\nStep 1 will generate a CSV file (\u201csgRNA.csv\u201d) in the same folder as the python script, which lists all sgRNAs targeting the gene(s) of interest, as well as their properties (Figure\u00a02[href=https://www.wicell.org#fig2] and Table 2[href=https://www.wicell.org#tbl2]). This includes:\ntable:files/protocols_protocol_636_2.csv\nIndex of sgRNA (n)\nOffset between the sgRNA and the start codon\nGC content of sgRNA spacer\nsgRNA spacer sequence (20nt upstream of the PAM)\nPAM sequence\nSequence downstream of the PAM\nFull sequence: sgRNA spacer\u00a0+ PAM\u00a0+ downstream sequence\nSeed sequence from sgRNA spacer used to identify potential off-target hitsNumber of potential off-target hits based on seed sequence\nThese properties will be used in step 2 to filter against sgRNAs with unwanted properties. Below we give some examples on what input arguments could be used for running the first step of the pipeline.\nExamples\nTo generate the list of all possible sgRNAs for the essential gene, dnaA (locus tag\u00a0= BSU_00010), in Bacillus subtilis (GenBank\u00a0= GCF_000009045.1_ASM904v1_genomic.gbff). One can run the following command:\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step find\nTo examine multiple genes, provide a list of locus tags. For example, to identify the sgRNAs of both DnaA and DnaN one can run the following command:\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010,BSU_00020\n--step find\nOne can examine all genes using the following command:\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag all\n--step find\nOne can also specify the path and file name of the CSV file generated in step 1. By default the CSV file is saved in the same folder as your python script and is named \u201csgRNA.csv\u201d. In order to change the default settings run the following command:\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag all\n--step find\n--file_find new_name.csvThe CSV file stores information for each of the sgRNAs (Figure\u00a02[href=https://www.wicell.org#fig2]). For example, the computational pipeline determines the number of potential off-target hits in the genome based on sgRNA seed sequence. By default, the seed sequence equals the 9 PAM-proximal nucleotides of an sgRNA spacer. The length of the seed sequence is based on a study showing that DNA complementarity of 9 nucleotides proximal to the PAM is sufficient to cause off-target knockdown effects (Cui et\u00a0al., 2018[href=https://www.wicell.org#bib4]). Within our pipeline, one could however adjust the size of the seed sequence using --off_target_seed. For example, the following command line limits the seed sequences to 8 nucleotides only and thus finds all off-target hits for this shorter sequence:\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step find\n--off_target_seed 8\nFinally, one could specify the number of nucleotides downstream from the PAM that are included for further analyses (--downstream). As we will detail below, the downstream sequence of the PAM can affect the efficacy of CRISPRi-mediated expression knockdown, and therefore plays\u00a0an\u00a0important role in filtering out potentially weak sgRNAs. Based on a previous analysis (Calvo-Villama\u00f1\u00e1n et\u00a0al., 2020[href=https://www.wicell.org#bib3]), we include a default downstream sequence of 15 nucleotides, but one could adjust this to, for example, 20 downstream nucleotides using the following command:\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step find\n--downstream 20\nStep 2: Filter sgRNAs\nTiming: 1\u00a0minSeveral factors have been suggested to affect the sgRNA-mediated knockdown efficacy, although in most cases we have little mechanistic understanding as to how these effects come about. Qi et\u00a0al. (2013)[href=https://www.wicell.org#bib17] reported a relatively strong decline in knockdown efficiency with the distance of a sgRNA from the start codon, but this effect has not been reproduced in more recent studies. For instance, in Hawkins et\u00a0al. (2020)[href=https://www.wicell.org#bib7], the offset between the sgRNAs and start codon had no effect on the knockdown of gfp in both E.\u00a0coli and B.\u00a0subtilis (Figure\u00a03[href=https://www.wicell.org#fig3]C). Similarly, when re-analyzing the relative fitness of sgRNA targeting essential genes in Cui et\u00a0al. (2018)[href=https://www.wicell.org#bib4], we only find a weak dependency of the relative fitness on the offset (R2<0.01; Figure\u00a03[href=https://www.wicell.org#fig3]D). Reanalyzing the same data, Calvo-Villama\u00f1\u00e1n et\u00a0al. (2020)[href=https://www.wicell.org#bib3] also did not note an effect of offset. Besides offset, the nucleotide sequence has been suggested to affect knockdown efficacy. Using the knockdown library of Cui et\u00a0al. (2018)[href=https://www.wicell.org#bib4], Calvo-Villama\u00f1\u00e1n et\u00a0al. (2020)[href=https://www.wicell.org#bib3] showed that a small number of nucleotide positions in the sgRNA, PAM and downstream region affect transcriptional interference by perfectly matched sgRNAs. Inspired by this work, we re-analyzed the E.\u00a0coli and B.\u00a0subtilis datasets of Hawkins et\u00a0al. (2020)[href=https://www.wicell.org#bib7] as well as the previously analyzed data from Cui et\u00a0al. (2018)[href=https://www.wicell.org#bib4]. For each dataset, we fitted a linear model on one-hot encoded primary sequence data to predict knockdown efficacy. For the primary sequence, we used the sgRNA spacer sequence (20 nucleotides), the PAM sequence (3 nucleotides, of which only one variable) and 15 nucleotides downstream of the PAM. Following Calvo-Villama\u00f1\u00e1n et\u00a0al. (2020)[href=https://www.wicell.org#bib3], we prevent overfitting of the highly parameterized linear models, by penalizing absolute coefficient values using L1 regularization (based on the penalty term with the lowest mean squared error in a 10-fold cross validation; Figure\u00a04[href=https://www.wicell.org#fig4]).imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/636-Fig4.jpg\nFigure\u00a04. Impact of individual nucleotides on knockdown efficacy\nInspired by a recent study (Calvo-Villama\u00f1\u00e1n et\u00a0al., 2020[href=https://www.wicell.org#bib3]), the impact of sgRNAs targeting the non-template strand of essential genes in both the datasets of Cui et\u00a0al. (2018)[href=https://www.wicell.org#bib4] (top panel) and Hawkins et\u00a0al. (2020)[href=https://www.wicell.org#bib7] (bottom panels) using an L1 linear model is shown.\n(A) Sequence logo (Wagih, 2017[href=https://www.wicell.org#bib22]) of coefficients associated with nucleotides in the sgRNA, PAM and downstream sequences. Positive coefficients correspond to improved knockdown efficacy and negative with reduced efficacy.\n(B) Relation between penalty term,   log  \u03bb  , in the L1 linear model and the mean squared error of a 10-fold cross validation. Red dots show mean squared error and error bars indicate standard deviation. Vertical dotted line shows penalty term associated with lowest mean squared error. Lower penalties result in overfitting of the data and higher penalties in underfitting.Figure\u00a04[href=https://www.wicell.org#fig4] shows how each nucleotide in the sgRNA-PAM-downstream region affects knockdown efficacy. Positive coefficients indicate stronger knockdowns. As expected, for the dataset of Cui et\u00a0al. (2018)[href=https://www.wicell.org#bib4] we retrieve nearly the same results as Calvo-Villama\u00f1\u00e1n et\u00a0al. (2020)[href=https://www.wicell.org#bib3]: a few nucleotides upstream (19, 20) and downstream from the PAM (+1,+2), together with the first nucleotide of the PAM, largely determine the knockdown efficacy. Similar data for E.\u00a0coli from Hawkins et\u00a0al. (2020)[href=https://www.wicell.org#bib7] shows a much weaker pattern, due to substantially more noise, which likely results from the high-copy number plasmid used in these experiments. Nonetheless, key residues are consistent with the data from Cui et\u00a0al. (2018)[href=https://www.wicell.org#bib4]. For example, an adenine in the first nucleotide of the PAM strongly reduces knockdown efficiency. The dataset from B.\u00a0subtilis (which used a chromosomally integrated system) showed a very similar noise levels as that from Cui et\u00a0al. (2018)[href=https://www.wicell.org#bib4] and also a similar dependency on nucleotides close to the PAM. However, in addition, we observe a strong negative effect on knockdown efficacy from thymine in the first three positions of the sgRNA (1,2,3), corresponding to the transcription start site (TSS) at the 5\u2019-end of the sgRNA (PAM-distal). This is likely due to inefficient transcriptional initiation from these nucleotides or enhanced degradation. We also observe that a cytosine or guanine in the\u00a0+8 or\u00a0+9 position downstream from the PAM negatively impacts knockdown efficiency. It is yet unclear if these effects are specific to B.\u00a0subtilis or are present in other gram-positive bacteria as well.Off-target effects are also a concern when designing sgRNAs. Where possible, sgRNAs with high specificity \u2013 few off-target hits \u2013 should be chosen. Analyses of mismatch sgRNAs suggest that higher binding affinity resulting from high GC percentage results in lower sensitivity to imperfect complementarity between sgRNA and DNA, which could imply that these sgRNAs are more likely to have off-target knockdown effects as well (Hawkins et\u00a0al., 2020[href=https://www.wicell.org#bib7]; Jost et\u00a0al., 2020[href=https://www.wicell.org#bib9]; Mathis et\u00a0al., 2021[href=https://www.wicell.org#bib14]). Cui et\u00a0al. (2018)[href=https://www.wicell.org#bib4] furthermore showed that off-target knockdown effects can occur when only 9 nucleotides in the seed sequence of a sgRNA complement a gene sequence.\nFinally, Cui et\u00a0al. (2018)[href=https://www.wicell.org#bib4] identified the presence of toxic seed sequences that are consistently associated with negative fitness effects in E.\u00a0coli. These seed sequences comprise five nucleotides at the 3\u2019 end of the sgRNA, proximal to the PAM. Although dCas9 expression levels were shown to affect this bad-seed effect, the mechanistic basis of this toxicity has not been elucidated (Cui et\u00a0al., 2018[href=https://www.wicell.org#bib4]). The top 10 bad-seed sequences are: AGGAA, TGACT, ACCCA, AAAGG, GAGGC, CGGAA, ATATG, AACTA, TGGAA, CACTC (Vigouroux and Bikard, 2020[href=https://www.wicell.org#bib21]).\nIn step 2 of the computational pipeline, the list of sgRNAs is filtered based on a number of pre-specified criteria, thereby removing sgRNAs with potentially low efficacy, off-target hits or toxic side-effects. By default, the pipeline uses lenient filter criteria, only removing sgRNAs with\u00a0more than 10 potential off-target hits and with an offset (distance to the start codon) of\u00a0more than 1000 nucleotides. One can however use a number of additional filter criteria,\u00a0including the sgRNA spacer sequence, PAM sequence, downstream sequence and GC content. To run the second step of the computational pipeline, use the following command line:\n>python <path to \u201cgenerate_sgrnas.py\u201d script>\n--genbank <path to genbank file>\n--locus_tag <locus_tag>--step filter\n--sgrna_remove <string of sgRNA sequence(s) to remove>\n--pam_remove <string of PAM sequence(s) to remove>\n--downstream_remove <string of downstream sequence(s) to remove>\n--gc_lower <minimal GC-content sgRNA spacer>\n--gc_upper <maximum GC-content sgRNA spacer>\n--offset_upper <maximum distance to the start codon>\n--off_target_upper <maximum number of off-target hits>\nAll filter criteria are optional. When no filter criteria are specified, only the default filter settings will apply. For the filter step, the Python script will load the CSV file generated in the first step of the pipeline (e.g., \u201csgRNA.csv\u201d). Importantly, filtering only works when this CSV file is present. When an alternative file name is used in the first step (using the --file_find argument), this should also be specified for the filtering step.\nLike in step 1, a CSV file is also generated in the second step (\u201csgRNA_filtered.csv\u201d). It contains the same information as the first file, but includes only the sgRNAs that pass the filtering criteria (Figure\u00a02[href=https://www.wicell.org#fig2]). The first two steps of the computational pipeline can also be used to design sgRNAs that target the same gene(s) across different strains or species. To this end, one should first generate and filter sgRNAs for each individual strain or species, targeting the same orthologous gene(s), and subsequently compare their \u201csgRNA_filered.csv\u201d output files. By selecting sgRNAs shared across strains/species, one can identify common CRISPRi targets. Note that these targets might not necessarily yield the same knockdown efficacies across strains/species, for example because of differences in the region upstream of the PAM.\nExamplesBased on the sequence analysis in Figure\u00a04[href=https://www.wicell.org#fig4], in B.\u00a0subtilis, we expect higher knockdown efficiencies when filtering against: (1) sgRNAs starting with a T, (2) PAM sequences starting with an A, (3) and downstream sequences associated with either a C or G at position\u00a0+8 or\u00a0+9. The following command would filter the list of sgRNAs accordingly:\n>python Generate_sgRNAs.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step filter\n--sgRNA_remove TNNNNNNNNNNNNNNNNNNN\n--PAM_remove ANN\n--off_target_upper 5\n--downstream_remove NNNNNNNGNNNNNNN,NNNNNNNNGNNNNNN,NNNNNNNNCNNNNNN,NNNNNNNCNNNNNNN\nAll sgRNAs that match the sequence of sgrna_remove will be removed; all sgRNAs associated\u00a0with a PAM sequence matching pam_remove will be removed; and all sgRNAs\u00a0associated with a downstream sequence matching downstream_remove will be removed.\nThe following command removes sgRNAs that have a spacer sequence starting with three consecutive Ts or ending with an A:\n>python Generate_sgRNAs.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step filter\n--sgRNA_remove TTTNNNNNNNNNNNNNNNNN,NNNNNNNNNNNNNNNNNNNA\nOne can also run the find and filter steps in one go:\n>python Generate_sgRNAs.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step find,filter\nWhen you would like to filter sgRNAs based on a downstream nucleotide sequence of only 10 nucleotides (default is 15), one should specify the length of the downstream sequence before specifying the filter criteria:\n>python Generate_sgRNAs.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step find,filter\n--downstream 10\n--downstream_remove NNNNNNNGNN,NNNNNNNNGN,NNNNNNNNCN,NNNNNNNCNN\nOne can also remove sgRNAs with either a low or high GC content. The following command filters out all sgRNAs with a GC content below 0.4 and above 0.6:\n>python Generate_sgRNAs.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step find,filter\n--gc_lower 0.4\n--gc_upper 0.6\nYou can also adjust the default filter criteria, for example the following command removes all sgRNAs with more than 5 off-target hits or an offset larger than 200nt:\n>python Generate_sgRNAs.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step find\n--off_target_upper 5\n--offset_upper 200When needed, it is possible to perform multiple consecutive rounds of filtering by both specifying the input and output files. For example:\nFirst round: remove sgRNA starting with T:\n>python Generate_sgRNAs.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step find,filter\n--sgRNA_remove TNNNNNNNNNNNNNNNNNNN\nSecond round: remove sgRNA of PAM sequences that start with A:\n>python Generate_sgRNAs.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step filter\n--PAM_remove ANN\n--file_find sgRNA_filter.csv\n--file_filter sgRNA_filter_new.csv\nIn the second round of filtering, the filtered sgRNAs from the first step (\u201csgRNA_filter.csv\u201d) is used as input file (--file_find sgRNA_filter.csv) to generate a new CSV file (\u201csgRNA_filter_new.csv\u201d; --file_filter sgRNA_filter_new.csv). If the new file is not specified, the old one (\u201csgRNA_filter.csv\u201d) will be overwritten. Consecutive rounds of filtering can be used to determine how each additional filter criterion prunes the list of sgRNAs.\nStep 3: Generate mismatch sgRNAs\nTiming: 5\u00a0minIn the third and final part of the computational pipeline, mismatch sgRNAs are generated for all sgRNAs that passed the filtering criteria. As detailed by Hawkins et\u00a0al. (2020)[href=https://www.wicell.org#bib7], imperfect complementarity between the sgRNA spacer and target DNA sequence can be used to predictably tune sgRNA activity. By examining over 1500 single-nucleotide mismatch sgRNAs in both E.\u00a0coli and B.\u00a0subtilis, corresponding to 33 sgRNA spacers targeting gfp, Hawkins et\u00a0al. (2020)[href=https://www.wicell.org#bib7] trained a species-independent linear model that predicts knockdown efficacy, based on the location and type of nucleotide mismatch and the GC percentage of the parent sgRNA (E.\u00a0coli: p<10\u221216, R2=0.48 and B.\u00a0subtilis: p<10\u221216, R2=0.57). PAM proximal mismatches more strongly affect sgRNA activity, while PAM distal mismatches have little effect (Figure\u00a05[href=https://www.wicell.org#fig5]A). These results corroborate previous findings that demonstrate the importance of the sgRNA seed sequence for binding of the dCas9-sgRNA complex to the DNA (Cui et\u00a0al., 2018[href=https://www.wicell.org#bib4]; Hsu et\u00a0al., 2013[href=https://www.wicell.org#bib8]; Jost et\u00a0al., 2020[href=https://www.wicell.org#bib9]; Mathis et\u00a0al., 2021[href=https://www.wicell.org#bib14]; Qi et\u00a0al., 2013[href=https://www.wicell.org#bib17]). Besides predicting knockdown efficacy for single mismatched sgRNAs, the predictive model was also successfully applied to predict knockdown efficacies of double mismatched sgRNAs using a multiplicative model, as suggested in Qi et\u00a0al., 2013[href=https://www.wicell.org#bib17]. As a next step, Hawkins et\u00a0al. (2020)[href=https://www.wicell.org#bib7] generated a library of mismatched sgRNAs targeting the essential genes in E.\u00a0coli and B.\u00a0subtilis to examine how graded knockdowns affect fitness. As expected, reduced expression knockdowns, associated with PAM proximal mismatches, were associated with higher relative fitness (Figure\u00a05[href=https://www.wicell.org#fig5]B).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/636-Fig5.jpg\nFigure\u00a05. Efficacy of mismatch CRISPRiEffect of single-nucleotide mismatches on (A) knockdown of GFP expression and (B) relative fitness of essential genes. Left panels show how mismatches in different positions of sgRNA affect GFP knockdown and relative fitness in datasets of E.\u00a0coli and B.\u00a0subtilis. Middle panels show relation between predicted knockdowns and (A) observed knockdowns in E.\u00a0coli (p<10\u221216, R2=0.48) and B.\u00a0subtilis (p<10\u221216, R2=0.57) and the (B) relative fitness in E.\u00a0coli and B.\u00a0subtilis. Right panels show distribution of observed GFP knockdowns and relative fitness.\nThe coefficients of the species-independent linear model are stored in the file \u201cmodel_param.csv\u201d and can be manually updated based on new experimental data. When running the final step of the computational pipeline, both the \u201csgRNA_filtered.csv\u201d and \u201cmodel_param.csv\u201d files are loaded to determine the predicted expression knockdowns of all 60 possible sgRNA spacer mismatches per sgRNA. This final step of the Python script does not rely on any additional arguments and can be run using the following command:\n>python <path to \u201cgenerate_sgrnas.py\u201d script>\n--genbank <path to genbank file>\n--locus_tag <locus_tag>\n--step mismatch\nLike in the previous two steps of the pipeline (Figure\u00a02[href=https://www.wicell.org#fig2]), a CSV file will be generated, \u201csgRNA_mismatched.csv\u201d, which shows the complete list of mismatched sgRNAs with their predicted knockdown efficacy. This file contains the following information:\nIndex of sgRNA, as shown in sgRNA.csv and sgRNA_filtered.csv, with the index of the associated mismatch in between parenthesis\nNucleotide position in sgRNA spacer that is substituted\nOriginal base\nSubstituted base\nSequence of parental (fully complementary) sgRNA spacer\nSequence of mismatch sgRNA spacer\nPredicted knockdown efficacy: 1 or higher indicating full knockdown (as seen in fully complementary sgRNA) and 0 no knockdown.\nExamples\nIn order to run the mismatch step for our B.\u00a0subtilis example, use the following command line:\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\n--step mismatchRunning the entire pipeline in one go\nAlthough the computational pipeline can be conducted step-by-step, one can also run the entire pipeline, by leaving out the step argument (default, --step find,filter,mismatch):\n>python <path to \u201cgenerate_sgrnas.py\u201d script>\n--genbank <path to genbank file>\n--locus_tag <locus_tag>\nExamples\nIn the case of our B.\u00a0subtilis example, one can generate all sgRNA targeting DnaA (step 1), filter these sgRNA based on default filter criteria (step 2) and generate all mismatches (step 3) using the following command:\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag BSU_00010\nTo generate sgRNA for all genes in the genome, simply add --locus_tag all (e.g., this takes \u223c10\u00a0minutes on a regular laptop for all 4325 genes of B.\u00a0subtilis):\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag all\nOne could also specify filter criteria when running the entire pipeline. As explained above, these filter criteria are applied in the second step of the pipeline:\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag all\n--sgrna_remove TNNNNNNNNNNNNNNNNNNN\nWhen you would like to include additional filter criteria, after running the entire pipeline, you can simply redo the last two steps of the pipeline and specify the new output files:\n>python generate_sgrnas.py\n--genbank GCF_000009045.1_ASM904v1_genomic.gbff\n--locus_tag all\n--step filter,mismatch\n--pam_remove ANN\n--offset_upper 200\n--file_filter sgRNA_filtered_new.csv\n--file_mismatch sgRNA_mismatched_new.csv", "UniLectin3D is a curated database that classifies and describes 3D structures of lectins by origin and fold, with cross-links to literature and other databases in glycoscience. Among the 2330 entries (sept 2021), more than 60% are complexes between a protein and a carbohydrate ligand, bringing invaluable information on the molecular basis of specificity and affinity. Many lectins recognize glycans present on human tissues, playing a role in cancer, development, immunity, and infection. Structural variations of some of these glycans differentiate individuals, defining for example the basis of the ABO human blood group system (Marionneau et\u00a0al., 2001[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0025]). Some pathogens, such as Vibrio cholerae or noroviruses, infect sub-populations with given blood group phenotypes more efficiently, and the lectins involved in their recognition have been well studied (Heggelund, Varrot, Imberty, & Krengel, 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0017]). This protocol, using bacterial lectins binding the blood group O oligosaccharide (also called the H epitope) as an example, will demonstrate the search function of UniLectin3D for the analysis of the structural details of this interaction.\nNecessary Resources\nHardware\nComputer with internet connection\nSoftware\nUp-to-date web browser such as Chrome or Firefox. Using Safari or Edge may not be as reliable.\nAccess to UniLectin3D and search interface\n1. Go to the UniLectin platform webpage: https://www.unilectin.eu/[href=https://www.unilectin.eu/].\n2. Select the UniLectin3D module by clicking on the corresponding blue button on the homepage. This action opens https://www.unilectin.eu/unilectin3D/[href=https://www.unilectin.eu/unilectin3D/]. The UniLectin3D landing page, shown in Figure 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0002], offers different search modes (keywords, fields, or glycan ligands). It also displays visual summaries of the module content in the form of sunbursts enabling data browsing. Finally, it shows a hierarchical chart that can be expanded to detail the underlying lectin classification built on folds branching to classes branching to families. In all cases, these charts are interactive and open new windows for accessing the lectin pages.<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4000a04d-5c21-42b8-96de-5a052e64c9df/cpz1305-fig-0002-m.jpg</p>\nFigure 2\nUniLectin3D homepage. Left: the main searching options and sunbursts for browsing. Right: the clickable classification at the fold level.\nSearching UniLectin3D with a glycan-based query for H-type 1 epitope\n3. Oligosaccharide structures can be searched through the \u201cGlycan Search\u201d option of the UniLectin3D interface. Clicking on this button opens https://www.unilectin.eu/unilectin3D/glycan_search[href=https://www.unilectin.eu/unilectin3D/glycan_search] and displays the table of monosaccharides shown in Figure 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0003].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/68ca2538-fadc-4c0a-928a-3587e97219c0/cpz1305-fig-0003-m.jpg</p>\nFigure 3\nRepresentation of the two types of blood group O(H) antigens. From top: IUPAC condensed representation, schematic chemical drawing, corresponding ChEBI accession numbers, and SNFG representation. SNFG details: red triangle: fucose; yellow disk: galactose; and blue square: N-acetyl-glucosamine. Carbons are numbered from 1 to 6 to specify the linkages between monosaccharides.\n4. To select the O(H) blood group antigen, the user needs to know it is a trisaccharide composed of one fucose (L-Fucp), one galactose (D-Galp), and one N-acetylglucosamine (D-GlcNAcp). Two types of this molecule are known, corresponding to a subtle difference in the linkage between the galactose and the N-acetylglucosamine. These are described by IUPAC sequences along with the matching 2D structures recorded in ChEBI and in the SNFG symbolic notation in Figure 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0004]. With this information, it is then possible to click on either of the three monosaccharides that are part of the blood group O(H) epitope or to simultaneously select the three of them, as displayed in Figure 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0003] (green boxes). This results in outputting all oligosaccharides containing these three monosaccharides and present in binding sites of lectin structures in UniLectin3D.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/adda3694-8cb3-4857-84f9-1100e4c2d451/cpz1305-fig-0004-m.jpg</p>\nFigure 4Top: Collection of monosaccharides available for search in the Glycan search option. Successive mouse clicks on the three boxes (L-Fucp, D-Galp and D-GlcNAcp) turns their background from blue to green. Bottom: Excerpt of the output selection of 30 oligosaccharides matching the query. H-Type 1 (Fuc(a1-2)Gal(b1-3)GlcNAc) and H-Type 2 (Fuc(a1-2)Gal(b1-4)GlcNAc) are shown.\n5. Click on the Fuc(a1-2)Gal(b1-3)GlcNAc motif to display all lectins for which a three-dimensional structure is available in complex with this ligand or with a longer oligosaccharide comprising this epitope. The result by default shows lectins in all species, resulting in a total of 11 entries, two of bacterial and nine of viral origin, mainly noroviruses that attach to blood group O in intestine during infection. The query can be limited to bacteria only by selecting \u201cBacterial lectins\u201d in the \u201cOrigin\u201d box. This search results in two structures from two different species of Burkholderia, as represented in Figure 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0005]. These two lectins belong to different classes, one adopting a \u03b2-propeller fold (AAL-like class) and the other one a \u03b2-sandwich/TNF-like fold (TNF\u03b1-like fold). Both are opportunistic bacteria from the Burkholderia cepacia complex responsible for lung infection in immunocompromised or cystic fibrosis patients (Mahenthiralingam, Baldwin, & Dowson, 2008[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0023]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d302547e-530b-4cd2-9be6-7da7b1b43f0f/cpz1305-fig-0005-m.jpg</p>\nFigure 5\nResults of the search based on H-type 1 oligosaccharide in bacterial lectins.\nSearching UniLectin3D by field search query for H-type 2 epitope6. For more expert users mastering the IUPAC condensed representation of glycans, a shortcut is exemplified here using the H-type 2 oligosaccharide. In this case, the \u201cFuc(a1-2)Gal(b1-4)GlcNAc\u201d motif is entered in the \u201cIUPAC condensed\u201d box of the \u201cSearch by field\u201d menu and can be directly associated with \u201cBacterial lectins\u201d in the \u201cOrigin\u201d box. As shown in Figure 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0006], this results in two lectins from two different bacteria, i.e., Burkholderia ambifaria (as seen in step 4) and Photorhabdus asymbiotica, an insect pathogen also able to infect humans (Waterfield, Ciche, & Clarke, 2009[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0034]). Both adopt a \u03b2-propeller fold, but they belong to different classes, i.e., AAL-like (6-bladed \u03b2-propeller) for BambL and PLL-like (7-bladed \u03b2-propeller) for PHL.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/c49dee6a-552b-4893-ba27-e9ac72ea69bb/cpz1305-fig-0006-m.jpg</p>\nFigure 6\nResults of the search based on H-type 2 oligosaccharide in bacterial lectins.\nVisualization of the structural basis for blood group O recognition by bacterial lectins7. The previous search results show that the BambL lectin of Burkholderia ambifaria binds both H-type 1 and H-type 2 oligosaccharides. Detailed information on BambL lectin interacting with the Fuc(a1-2)Gal(b1-3)GlcNAc type 1 ligand can be obtained by clicking on the \u201cView the 3D structure and information\u201d button on the upper right corner of the listed item. This action will open https://unilectin.eu/unilectin3D/display_structure?pdb=3ZW2[href=https://unilectin.eu/unilectin3D/display_structure?pdb=3ZW2]. Information on BambL is accessible either by clicking on any green button that links to corresponding entries in UniProt (UniProt, 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0033]), PDB (European or American sites; Burley et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0009]), and GlyConnect (Alocci et\u00a0al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0002]), or by exploring the structural information displayed on the page with two different viewers. Litemol software (Sehnal et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0032]) is integrated to show the lectin 3D structure, which can be manipulated by holding the mouse to turn the molecule around and, for example, bring the ligand to the fore. PLIP software (Salentin, Schreiber, Haupt, Adasme, & Schroeder, 2015[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0030]) is integrated to detail atomic interactions between the lectin and the ligand. Mousing over one of the listed interactions on the left updates the view on the right to locate where that particular interaction is located in the structure. In the same way, details of the same BambL interacting with the Fuc(a1-2)Gal(b1-4)GlcNAc type 2 ligand are found at https://unilectin.eu/unilectin3D/display_structure?pdb=3ZZV[href=https://unilectin.eu/unilectin3D/display_structure?pdb=3ZZV], and comparison of both ligands is displayed in Figure 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0007].\nIn the end, a user can shape new assumptions on blood group O recognition based on a clearer understanding of specificity. Moreover, structural data can be downloaded for further computational studies.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/7f101c5c-a88b-4cf8-afb7-dc24aa1edb8a/cpz1305-fig-0007-m.jpg</p>\nFigure 7\nScreen captures of the PLIP window illustrating the basis for the recognition of H-type 1 and H-type 2 epitopes by BambL.LectomeXplore is a module of UniLectin dedicated to the exploration of predicted lectins from each of the 109 classes of UniLectin3D. Translated genomes (proteomes) stored in UniProtKB and RefSeq sequence databases have been screened to generate all putative lectins (lectome) for each available species. LectomeXplore can be searched by assessing the extent of a class of lectins in the kingdoms of life. In this protocol, we demonstrate how to analyze the lectome of a specified organism and how to compare it with other species of the same genus, but living in different environments. This is applied to investigating the lectomes of two Pseudomonas species that have been widely characterized, P. syringae, a plant pathogen, and P. fluorescens, a ubiquitous bacterium present in soil with much less pathogenic behavior. More specifically, P. syringae is a plant pathogen that causes damage in a wide range of plants, for example, with large impact on kiwi fruit production in New Zealand and in Italy (Donati et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0013]). In contrast, P. fluorescens is an environmental bacterium that grows in soil and is in general non-pathogenic. The database also contains strains of P. cichorii, a bacterium first isolated on endives and known to cause green midrib rot of greenhouse-grown lettuces (Cottyn et\u00a0al., 2009[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0012]).\nThe protocol also shows how to investigate a specific lectin class common to Pseudomonas species. The OAA-like lectins were chosen. These organisms were structurally described in the cyanobacteria Oscillatoria agardhii and considered of pharmaceutical significance due to an antiviral activity related to their high affinity for mannose (Koharudin & Gronenborn, 2011[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0020]), and later characterized in Burkholderia and Pseudomonas species.\nNecessary Resources\nHardware\nComputer with internet connection\nSoftware\nUp-to-date Web browser such as Chrome or Firefox. Using Safari or Edge may not be as reliable\nAccess and search LectomeXplore with a query1. LectomeXplore is accessible via the UniLectin portal via its corresponding button (https://www.unilectin.eu/predict/[href=https://www.unilectin.eu/predict/]). On the LectomeXplore homepage, click on the \u201csearch by field\u201d button. The corresponding interface displays search fields spanning lectin characteristics (Fold, class) or phylogenetic properties (Kingdom, species \u2026) to be filled individually or in combination (Fig. 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0008]). Note that a second bioinformatics-dedicated box offers the option to query LectomeXplore with protein or protein properties using accession numbers of different databases.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/fe214c43-abc8-40ab-be9d-a332a1bb5e20/cpz1305-fig-0008-m.jpg</p>\nFigure 8\nLectomeXplore query interface.\nData mining for species-specific genomes\n2. Fill in the species field of the search window to launch the P. syringae lectome search, and then click on the \u201cLoad predicted lectin\u201d button. This action results in showing that 65 available strains have been predicted and span six different classes of lectins. The same search is then performed replacing the species name by P. fluorescens, yielding 76 strains spread across 13 classes. These distinct distributions are shown in Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0009]. This cross-species comparison seems to indicate that in the narrower niche of plant pathogens, the variety of lectins is reduced. To strengthen this statement, a third Pseudomonas species is searched by typing P. cichorii in the species search field. Only 11 strains and two lectins in two distinct classes are found. In the end, the result of these three searches shows that the lectome of Pseudomonas species living in soil with a more saprophytic and opportunistic behavior, such as P. fluorescens, is more diverse. It could potentially be involved in adhesion to various surfaces.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e5a22332-89fe-4aef-827e-6f1701bc8c3f/cpz1305-fig-0009-m.jpg</p>\nFigure 9\nLectome comparison obtained by entering Pseudomonas syringae (left) and Pseudomonas fluorescens (right) in the \u201cSpecies\u201d search box of LectomeXplore.\nSearching for a specific lectin in selected genomes3. Search can be then focused on a lectin of interest, such as OAA-like lectin that was predicted in both P. syringae and P. fluorescens lectomes (see Fig. 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0009]). In the search window, type OOA-like in the \u201cclass\u201d field and Pseudomonas in the \u201cgenus\u201d field. This results in the listing of 90 genes, in exactly 90 Pseudomonas genomes in the database, as shown in Figure 10[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0010]. The monogenic character of this lectin is rather unusual. Interestingly, 20 of these 90 genes are not correctly annotated, and the lectin is not functionally identified in Pfam (Mistry et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-bib-0027]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d5e65733-30d7-4219-ae5f-d6c45025bf49/cpz1305-fig-0010-m.jpg</p>\nFigure 10\nResults of searching for the occurrence of OAA-like lectins in the Pseudomonas genus.\nAnalyzing the quality of the prediction\n4. As mentioned above, 20 out of 90 lectins are poorly annotated. Clicking on the magnifying glass icon next to \u201cuncharacterized protein\u201d will display all corresponding entry summaries. Then, clicking on the top right corner of the listed P. putida entry opens a new page with further information on the alignment of this putative lectin with the reference of the class, here OAA-like. This \u201cAmino acid conservation\u201d also singles out the specific amino acids involved in carbohydrate binding. Figure 11[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.305#cpz1305-fig-0011] details this information and shows the match of five out of six binding residues. The sequence logo is another guide for assessing the quality of the alignment. As a matter of fact, many entries in LectomeXplore are named \u201cuncharacterized,\u201d \u201cputative,\u201d or \u201chypothetical\u201d proteins (approximately 20% of lectins predicted with a score above 0.25). The present example illustrates how lectins of potential interest can be \u201cmined\u201d in genomes where they were not precisely identified.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6a848659-1b4b-49a0-bc27-f36ea16eefdf/cpz1305-fig-0011-m.jpg</p>\nFigure 11\nEntry page of the OAA-like lectin predicted to occur in the P. putida genome.", "Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001] describes the scenario in which BUSCO is used to assess a single input, either a genome, gene set, or transcriptome, with a dataset manually specified by the user. This protocol assumes that the taxonomic origin of the input sequence is known. Before running BUSCO assessments, you need to first set up the BUSCO software and its dependencies (see Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0005]). Users are also encouraged to visit the website and read the user guide for further details and up-to-date information (http://busco.ezlab.org[href=http://busco.ezlab.org]).\nNecessary Resources\nHardware\nA Unix-based workstation. BUSCO has been tested on several Linux platforms, and it is recommended to use a Linux machine. BUSCO and its dependencies may work on Macintosh machines and operating systems, but this has not been thoroughly tested.\nOptional, but recommended: The machine needs to have access to the Internet for downloading the BUSCO datasets and files.\nSoftware\nThe BUSCO package and its dependencies (see Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0005] for various ways of setting up BUSCO). For plotting scores: R, the ggplot2 library, and the script generate_plot.py from the BUSCO repository.\nFiles\nAn assembled genome or transcriptome, or an annotated protein-coding gene set in standard FASTA format.\nPreparing the input files and choosing a dataset\n1. Download the content of the testing repository (https://gitlab.com/ezlab/busco_protocol), where you can find the inputs used for the examples. You can clone the repository using Git. If not available on your system, you can easily install Git by following the instructions at https://git-scm.com/[href=https://git-scm.com/]. Run:\n         \n$ git clone https://gitlab.com/ezlab/busco_protocol.git\nIn general, a command to run a BUSCO assessment with a dataset manually specified looks like the following:\n         \n$ busco -i <SEQUENCE_FILE> -l <LINEAGE> -o <OUTPUT_NAME> -m <MODE>\n<OTHER OPTIONS>\nwith four main mandatory arguments:-i (or --in): a path to your FASTA file, which is either a nucleotide fasta file or a protein fasta file, depending on the BUSCO mode of assessment. From v5.1.0 the input argument can also be a directory containing fasta files to run the analysis on multiple inputs (see Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0003]).\n-l (or --lineage): the manually picked dataset for the assessment. It can be a dataset name, i.e., bacteria_odb10, or a relative (e.g., ./bacteria_odb10) or full (e.g., /home/user/bacteria_odb10) path. It is recommended to use the dataset name alone, as in this way BUSCO will automatically download and check the version of the dataset. In the case of a path, the dataset found in the given path will be used. Note that this argument is ignored when running with the auto-lineage workflow (see Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0002]).\n-m (or --mode): which analysis mode to run, either genome (or geno), proteins (or prot), or transcriptome (or tran).\n-o (or --out): the name of the output directory that identifies the analysis run and that contains all results, logs, and intermediate data.\nThe BUSCO command can take several additional arguments (also see Critical Parameters and Advanced Parameters in Commentary. The arguments can also be specified in a config file, which can be passed to BUSCO using the --config argument. If all parameters are set in the config.ini file, the command for launching BUSCO will look like:\n$ busco --config /path/to/config/config.ini\nThe arguments passed through the command line will overwrite the arguments in the config file. Remember to uncomment the lines in the config file to enable them (i.e., remove the semicolon in front of the line). For additional instructions on how to use the config file, see Advanced Parameters.In the following examples, we are going to analyze the genome assembly and annotated gene set of the yeast Torulaspora globosa (assembly accession: GCF_014133895.1). We first need to select an appropriate dataset according to the taxonomy of this species. The lineage datasets used for BUSCO assessments are not packaged with the software. BUSCO will automatically download the required dataset when running the assessment. On the first BUSCO run, a busco_downloads/ folder will be created, containing the necessary lineage datasets within the subdirectory lineages/ (see Critical Parameters for more details on BUSCO datasets).\n2. Manually select a dataset for the analysis. The most specific BUSCO dataset available for the yeast Torulaspora globosa is saccharomycetes_odb10. You can explore all available odb10 datasets by running:\n         \n$ busco --list-datasetsHere we are using the most specific dataset to perform the assessment with the highest possible resolution. You could also assess this genome with all the other datasets matching the lineage of the organism. In this case, according to NCBI taxonomy (Schoch et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0033]), the full taxonomic lineage of Torulaspora globosa is \u201ccellular organisms; Eukaryota; Opisthokonta; Fungi; Dikarya; Ascomycota; saccharomyceta; Saccharomycotina; Saccharomycetes; Saccharomycetales; Saccharomycetaceae; Torulaspora\u201d. On the basis of the currently available BUSCO odb10 datasets, besides the saccharomycetes_odb10 dataset, you could use the ascomycota_odb10, fungi_odb10, or eukaryota_odb10 datasets for the assessment. As a rule of thumb, it is always better to use the most specific dataset because it allows the highest-resolution analysis (i.e., more clade-specific markers are covered and scored). For example, saccharomycetes_odb10 is made of 2137 markers, while the ascomycota_odb10, fungi_odb10, and eukaryota_odb10 datasets are made of 1706, 758, and 255 markers, respectively. However, in some cases, it is preferable or necessary to use a lower-resolution dataset, e.g., for obtaining a set of markers shared among multiple species for building a phylogenomic tree (see Support Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0007]). In such case, a less specific dataset might need to be selected, as the marker genes for building the alignments must be shared across all the species included in the tree. For example, for building a phylogenomic tree of fungal species that includes five budding yeasts (for which the most specific BUSCO dataset is saccharomycetes_odb10) and five boletales species (for which the most specific BUSCO dataset is boletales_odb10), the more general fungi_odb10 dataset could be used.\nRunning BUSCO on a single genome assembly with known taxonomy\nIn this example, we are going to assess the genome assembly of T. globosa using the saccharomycetes_odb10 dataset. In the downloaded repository, you can find this gene set at BUSCO_protocol/protocol1/Tglobosa_GCF_014133895.1_genome.fna.3. Enter the busco_protocol testing folder:\n         \n$ cd /{path_to_busco_protocol_folder}/\nwhere {path_to_busco_protocol_folder} is the path to the busco_protocol/ testing folder you downloaded.\n4. To launch an assessment of a genome assembly (which can be in the form of contigs, scaffolds, or chromosomes) the genome mode needs to be specified. Run the following command:\n         \n$ busco -i ./protocol1/Tglobosa_GCF_014133895.1_genome.fna -l saccharomycetes_odb10 -m geno -o busco_out_Tglob_genome -c 12\nYou can specify a relative or full path to the input file, e.g., if the file is located in /usr/user_name/busco_protocol/protocol1/Tglobosa_GCF_014133895.1_genome.fna, and you are in the busco_protocol/ folder, you can specify the relative path with -i ./protocol1/Tglobosa_GCF_014133895.1_genome.fna, where ./ stands for the current working directory. Otherwise, you can specify the full path with -i /usr/user_name/busco_protocol/protocol1/Tglobosa_GCF_014133895.1_genome.fna. Additional parameters can be specified to change the default values of various settings. Here, for example, we specify the number of CPUs (central processing units) to be used with the --cpu (or -c) argument. By default, the --download_path argument, with which you can specify the location on your machine where you wish to store the downloaded datasets and additional files required to run the software, and the --out_path argument with which you can specify the location on your machine where you wish to store the output folder, are set to the current working directory. Feel free to change these paths to your preferred location on your machine. For directions on using additional parameters, see the Critical Parameters and Advanced Parameters sections in Commentary.The BUSCO software and dataset versions are mutually dependent: odb10 datasets can be used with BUSCO v4 and v5. The old BUSCO odb9 datasets do not work with these more recent BUSCO versions. BUSCO automatically downloads the manually specified dataset, and thus your machine needs to have access to the Internet. You can also download a dataset or a set of datasets independently from a BUSCO run using the --download option (see the first step in Alternate Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0004] 1). If you need to download many or all datasets (e.g., when running the auto-lineage procedure, see Basic Protocols 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0002] and 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0003]), make sure you have enough disk space on your system. All extracted datasets occupy approximately 127 GB of disk space. In case your system does not allow Internet connections, you can add the --offline flag to the BUSCO command. In this case, you will need to download the dataset(s) on another machine with an Internet connection and transfer the files to the system where you want to run BUSCO. You can use the BUSCO --download option (see Critical Parameters for more details) or the Unix command wget on a machine with an Internet connection, and then transfer the files to the machine where you want to run BUSCO. For example, to download the arthropoda_odb10 dataset from the https://busco-data.ezlab.org/v5/data/lineages/[href=https://busco-data.ezlab.org/v5/data/lineages/] site with \u201cwget\u201d, enter:\n         \n$ wget https://busco-data.ezlab.org/v5/data/lineages/arthropoda_odb10.2020-09-10.tar.gz'\nYou then need to unpack and decompress the dataset before running BUSCO, e.g., with:\n         \n$ tar -xfv arthropoda_odb10.2020-09-10.tar.gzRunning this assessment on 12 CPUs with otherwise default options should take approximately 1 min. After BUSCO has completed the run, the summary scores are printed to the standard output and reported in the short_summary*.txt file that you can find in the main output folder, in this example busco_out_Tglob_genome/. The summary file is named after the dataset used for the assessment and the output name. In this case, it will be named short_summary.specific.saccharomycetes_odb10.busco_out_Tglob_genome.txt, and will be similar to the following:\n         \n# BUSCO version is: 5.2.2\n# The lineage dataset is: saccharomycetes_odb10 (Creation date: 2020-08-05, number of genomes: 76, number of BUSCOs: 2137)\n# Summarized benchmarking in BUSCO notation for file /data/manni/busco_protocol/protocol1/Tglobosa_GCF_014133895.1_genome.fna\n# BUSCO was run in mode: genome\n# Gene predictor used: metaeuk\n***** Results: *****\nC:99.6%[S:99.5%,D:0.1%],F:0.1%,M:0.3%,n:2137\n2129\u2003\u2003\u2003\u2003Complete BUSCOs (C)\n2126\u2003\u2003\u2003\u2003Complete and single-copy BUSCOs (S)\n3\u2003\u2003\u2003\u2003\u2003\u2003Complete and duplicated BUSCOs (D)\n3\u2003\u2003\u2003\u2003\u2003\u2003Fragmented BUSCOs (F)\n5\u2003\u2003\u2003\u2003\u2003\u2003Missing BUSCOs (M)\n2137\u2003\u2003\u2003\u2003Total BUSCO groups searched\nDependencies and versions:\nhmmsearch: 3.1\nmetaeuk: 4.a0f584d\nThis text file contains the classification of the identified BUSCO markers into categories of Complete (C), Complete and single-copy (S), Complete and duplicated (D), Fragmented (F), and Missing (M) BUSCOs as percentages and counts, and additional information such as the dataset used and the versions of the dependencies. In our example, BUSCO evaluates this genome assembly as of high quality, i.e., containing almost all the expected single-copy genes with a low duplication score. In the output folder, you can also find the logs/ subfolder containing the logs of BUSCO and its dependencies, and the run_<odb_dataset_name>/ folder (in this case run_saccharomycetes_odb10) containing all the other outputs and intermediate files. See Guidelines for Understanding Results for details on all BUSCO outputs and their interpretation.When assessing a prokaryotic species (i.e., by using a prokaryotic dataset) or virus, BUSCO uses the gene predictor Prodigal (Hyatt et\u00a0al., 2010[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0016]) to predict genes. For eukaryotic species, as in this example, two alternative workflows, BUSCO_Metaeuk and BUSCO_Augustus, employing two different gene predictors, are available. From BUSCO v5 the default genome mode uses the BUSCO_Metaeuk workflow which employs the gene predictor MetaEuk (Levy Karin, Mirdita, & S\u00f6ding, 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0021]) for predicting genes. In the command above, we used the BUSCO_Metaeuk workflow by default. To use the alternative BUSCO_Augustus workflow, which employs the gene predictor AUGUSTUS (Hoff & Stanke, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0012]), you need to add the --augustus flag:\n         \n$ busco -i ./protocol1/Tglobosa_GCF_014133895.1_genome.fna -l saccharomycetes_odb10 -m geno -o busco_out_Tglob_genome_Augustus -c 12 --augustus\nNote that this takes much longer to run (\u223c11 min) compared to the default workflow (\u223c1 min) using 12 CPUs. Feel free to use more CPUs if available on your system. The BUSCO_Augustus workflow requires a working installation of tBLASTn and AUGUSTUS (see Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0005] for details on these dependencies), and BUSCO needs to know the location of the AUGUSTUS configuration directory. So check that the correct path is set for this variable by typing:\n         \n$ echo ${AUGUSTUS_CONFIG_PATH}\nIf the variable is not set or you need to change it, you can declare it as follows:\n         \n$ export AUGUSTUS_CONFIG_PATH=\"/path/to/AUGUSTUS/augustus-x.x.x/config/\"\nWhen using the BUSCO_Augustus workflow, one important optional argument to consider is the choice of AUGUSTUS pretrained species-specific gene prediction parameters. Each BUSCO lineage dataset has a predefined default selection, e.g., for the diptera_odb10 dataset the default species is \u201cfly\u201d, meaning AUGUSTUS gene prediction parameters pretrained on the fruit fly, Drosophila melanogaster. Also see Critical Parameters and Advanced Parameters in Commentary for further details on AUGUSTUS-specific parameters.The output folder obtained from the BUSCO_Augustus workflow contains the same main output files as seen for the default BUSCO_Metaeuk workflow, and only differs for the intermediate output files generated by the different tools in the workflow. See Guidelines for Understanding Results for a detailed description of all the output files and folders generated with different workflows.\nOn medium to large eukaryotic genomes, and especially on very large genomes, e.g., greater than a few gigabase pair (Gbp), the BUSCO_Metaeuk workflow is much faster than the BUSCO_Augustus workflow. For example, running a genome assessment on the 10-Gbp genome assembly of the wheat Triticum dicoccoides with the poales_odb10 dataset (4896 markers) using 50 CPUs takes approximately 9 hr with the BUSCO_Metaeuk workflow, whereas it needs several days using the BUSCO_Augustus alternative. The two workflows yield comparable but not identical results, as they are based on different methodologies; for details see Manni et\u00a0al. (2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0024]). Depending on the input size and your specific needs, you may opt for the BUSCO_Augustus workflow, e.g., if you need to obtain AUGUSTUS retraining parameters (also see Suggestions for Further Analyses).\nRunning BUSCO on a single annotated protein-coding gene set with known taxonomyThe command for analyzing an annotated gene set is similar to that used for genome assemblies, except for the --mode argument, which is set to proteins (or prot). The input file is a FASTA file of the amino acid translations of the protein-coding genes. To properly evaluate the amount of BUSCO gene duplications (which can be due to technical artifacts or true duplications), isoforms must be filtered from the gene set before running the assessment. Otherwise, each BUSCO gene with isoforms will score as a duplicated BUSCO. There are different criteria to select isoforms; for our purposes we selected the longest isoform per gene (steps in Support Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0007] also describe how to select the longest isoform per gene using a GFF file and the corresponding genome).\n5. For analyzing the gene set of the yeast T. globosa using the manually specified dataset, run:\n         \n$ busco -i ./protocol1/Tglobosa_GCF_014133895.1_geneset.faa -l saccharomycetes_odb10 -m prot -o busco_out_Tglob_geneset -c 12\nThe result folder looks similar to the one obtained for assessing genome assemblies, without the intermediate files related to the gene prediction steps. The gene set analysis is faster than the analysis on the corresponding genome assembly.If you have run the BUSCO command in the previous section on the genome assembly, the saccharomycetes_odb10 dataset has already been downloaded. BUSCO will just check that the current dataset available locally is the most updated version. If not, BUSCO will run the assessment issuing a warning that a newer dataset is available, suggesting to update the data. To always update the datasets if a new version is available, you can add the --update-data flag to the command. If a new version of the dataset is available, it will be downloaded and the older one renamed by adding the .old prefix. You can remove these old datasets to save disk space if these are no longer needed. Note that updates occurring within the same dataset version (e.g., odb10) are normally minor changes, e.g., for fixing minor errors/typos in the files that do not change the outcome of the assessments (i.e., the collection of single-copy markers remains the same among these updates).\nRunning BUSCO on a single transcriptome assembly with known taxonomyTranscriptome assessments are launched with the same four mandatory options for assessing genome assemblies and gene sets. You just need to change the value of the --mode (or -m) argument to transcriptome (or tran) and provide a transcriptome assembly as input file. The transcriptome mode requires MetaEuk when assessing eukaryotes and tBLASTn when assessing prokaryotes (see Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0005] for more details on BUSCO dependencies). As for the gene set, to obtain true estimates of the numbers of duplicated BUSCOs for transcriptomes, these should be pre-processed to select just one representative transcript per gene. Here we analyze an assembled transcriptome from the female reproductive tract of the Asian tiger mosquito Aedes albopictus (NCBI BioProject: PRJNA223166; Boes et\u00a0al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0003]) using the diptera_odb10 dataset. For this analysis, the transcriptome was not pre-processed to remove isoforms, and thus we may obtain a high number of duplicated BUSCOs which just reflects the presence of different isoforms.\n6. To launch the transcriptome assessment, run:\n         \n$ busco -i ./protocol1/Aalbopictus_transcriptome.fna -l diptera_odb10 -m tran -o busco_out_Aalbopictus_transcriptome -c 12\nThe main content of the result folder is similar to the one obtained for assessing genome assemblies, with some differences (see the BUSCO output subsection in the Guidelines for Understanding Results). If the transcriptome comes from a specific organ/tissue or time point, it may contain only a fraction of the full BUSCO sets. Our test transcriptome harbors approximately 38% of the 3285 diptera_odb10 markers, which is not surprising given the specialized function of the tissue under consideration. See Guidelines for Understanding Results for details on the output files and their interpretation.\nIn v5, BUSCO takes advantage of the MetaEuk program also to find BUSCO markers in transcriptomes.\nPlotting BUSCO scoresIt is common to plot BUSCO scores as a bar chart from a single run, or from different runs side-by-side to visualize like-for-like comparisons, e.g., of different species/strains or assembly versions. To encourage the use of a standard and distinctive color scheme in publications, BUSCO includes a dedicated R (R Core Team, 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0031]) script to produce a figure and its source code that can be further edited, allowing one to customize the resulting bar chart (labels, fonts, axes, etc.). To run the script, Python is required. To produce the image, R and the ggplot2 library need to be available on the system. To plot the scores of one or multiple runs, it is sufficient to provide in a single folder the short_summary.txt files of each BUSCO run.\n7. Collect the short_summary*.txt files in one folder, e.g.:\n         \n$ mkdir BUSCO_summaries/\n$ cp OUT1/short_summary.*.lineage_odb10.OUT1.txt ./BUSCO_summaries/\n$ cp OUT2/short_summary.*.lineage_odb10.OUT2.txt ./BUSCO_summaries/\n8. Then, simply run the generate_plot.py script available in the BUSCO repository (https://gitlab.com/ezlab/busco/scripts[href=https://gitlab.com/ezlab/busco/scripts]), providing the path to the directory containing the summary files:\n         \n$ python3 generate_plot.py -wd BUSCO_summariesA *.png image file and the corresponding R source code file, which can be further edited to make cosmetic adjustments to the plot, will be produced in the same folder containing the BUSCO summaries. By default, the run name is used as the label for each plotted result, and this is automatically extracted from the short summary file name: so, for short_summary.generic.lineage_odb10.OUT1.txt, the label would be \u201cOUT1\u201d. You can modify this in the file name as long as you keep the naming convention, e.g.: short_summary.generic.lineage_odb10.[edit_name_here].txt, or you can simply edit the R source code file to change any plotting parameters and produce a personalized bar chart by running the code manually in your R environment. However, we suggest keeping the same color scheme. By adding the --no_r flag to the command, the script will simply produce the R script required to reproduce the plot, which can then run on any system with R and ggplot2 installed. Figure 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-fig-0001] shows an example of the resulting default plot.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/ade3c6e1-fc57-4f47-bc8d-ccc4384a6ec6/cpz1323-fig-0001-m.jpg</p>\nFigure 1\nIllustration of the BUSCO bar plot as produced by the plotting script. Three genomes evaluated with 1519 BUSCO markers are depicted with varying degrees of completeness.This protocol describes the case in which you do not specify a BUSCO dataset for the assessment. Instead, BUSCO attempts to automatically select the most appropriate dataset on the basis of a phylogenetic placement onto precomputed trees of marker genes extracted from the input file. This workflow is activated by using the --auto-lineage flag, and can be applied to all data types (genomes, gene sets and transcriptomes). With this option, no assumptions are made on the taxonomic origin of the input sequences, and it can be useful for inputs with unknown taxonomic origin, as in the case of metagenome-assembled genomes (MAGs), or on large sets of inputs where specifying each dataset manually can be tedious (see Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0003] for running this workflow on multiple inputs). If the user knows the domain of origin of the input, the --auto-lineage-prok or --auto-lineage-euk flags can be specified for prokaryotic and eukaryotic species, respectively. BUSCO will run faster when one of these two options is selected instead of the full --auto-lineage. In general, if the taxonomy of the species is known, it is preferable to specify the dataset manually, as the time and resources needed increase substantially with the phylogenetic placement. Also, the placement procedure may resort to a broader, less-specific dataset for the assessment if not enough markers are placed on the tree to select the most specific one. In this case, the selection may roll back to one of the general domain datasets (i.e., archaea_odb10, bacteria_odb10, or eukaryota_odb10). See the Guidelines for Understanding Results for more details on the automatic selection of the datasets.\nNecessary Resources\nHardwareAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001]. Note that, usually, the RAM required for using the --auto-lineage option will not exceed 13 GB when assessing prokaryotic species; thus, the assessment can also run on a laptop. If the placement involves eukaryotic species, you might need more memory to run the assessment.\nSoftware\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001]. Note that for running the auto-lineage workflow you need a working installation of SEPP and pplacer.\nFiles\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001]. The auto-lineage workflow can be applied to genome assemblies, gene sets, and transcriptomes. To perform the phylogenetic placement procedure, additional files are required, e.g., the precomputed super-alignments and trees. These are automatically downloaded by BUSCO during the analysis if a connection to the Internet is available. If not, you will have to download and put these files manually in the busco_downloads/ folder as described in the annotation to step 4 of Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001].\nRunning BUSCO on one input without specifying a lineage dataset\nIn this case, you do not need to specify a lineage dataset. BUSCO will first run the three root datasets (bacteria_odb10, archaea_odb10, and eukaryota_odb10) to figure out the domain of the input file, and then attempt to phylogenetically place the markers extracted in this first run on a precomputed superalignment and phylogenetic tree.\n1. In this example, we are going to assess the same genome assembly of T. globosa assessed in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001], but this time without specifying the saccharomycetes_odb10 dataset. Enter the BUSCO_protocol testing folder downloaded in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001]:\n         \n$ cd {path_to_busco_protocol_folder}/\n2. Run the command as:\n         \n$ busco -i ./protocol1/Tglobosa_GCF_014133895.1_genome.fna -m geno -o busco_out_Tglob_genome_auto -c 12 --auto-lineageRunning this assessment on 12 CPUs with otherwise default options should take approximately 4 min, which is longer than the time needed to run the assessment described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001] when the dataset was manually specified (\u223c1 min), as additional steps are required here for the phylogenetic placement procedure. You can enter the output directory busco_out_Tglob_genome_auto/ to check the results and inspect which dataset was automatically selected for the assessment. If the run was successful, you should find the summary file short_summary.specific.saccharomycetes_odb10.busco_out_Tglob_genome_auto.txt, named by the same rule described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001]. As you can see, BUSCO was able to select the correct and most specific dataset for the assessment (saccharomycetes_odb10), corresponding to the manual choice we made in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001]. Additionally, you will find the file short_summary.generic.eukaryota_odb10.busco_out_Tglob_genome_auto.txt, which corresponds to the short_summary file obtained by running the parent \u201croot\u201d domain dataset (in this case eukaryota_odb10) as the first step of the auto-lineage workflow. See Guidelines for Understanding Results for more information on the outputs produced by the auto-lineage workflow and how to interpret the results. In this example, you could specify the --auto-lineage-euk option instead of --auto-lineage, as we already know that our input file belongs to a eukaryotic species. You can apply the auto-lineage workflow to all the input types selecting the different modes covered in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001].BUSCO v5 and higher can be run in \u201cbatch\u201d mode on a collection of input files. These must be present in a single folder and must be of the same type, i.e., being either all genomes assemblies, transcriptomes or gene sets. As for analyzing a single input file, there are two ways to run BUSCO on multiple files: a) with a BUSCO dataset manually specified (described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001]), and in this case the same dataset will be applied to all the input files, e.g., a set of insect genomes analyzed with the insecta_odb10 dataset; or b) in combination with the auto-lineage workflow (also see Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0002]). In this case each input file is analyzed independently with a specific dataset automatically selected by BUSCO, and thus can be applied on a set of taxonomically heterogeneous inputs, such as metagenomic data that include MAGs of both eukaryotic and prokaryotic origin.\nNecessary Resources\nHardware\nAs described in Basic Protocols 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001] and 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0002].\nSoftware\nAs described in Basic Protocols 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001] and 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0002].\nFiles\nAs described in Basic Protocols 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001] and 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0002].\nRunning BUSCO on multiple inputs using the same dataset specified manually\nCollect all the input files you want to analyze in a folder, and simply specify this folder as input. Here we analyze a set of bacterial genomes with the mycoplasmatales_odb10 dataset. You can find the folder containing the input genomes in the subfolder ./protocol3/bact_genomes/.\n1. Open the terminal and enter the following command:\n         \n$ busco -i ./protocol3/bact_genomes -l mycoplasmatales_odb10 -m geno -o busco_out_mycoplas_genomes -c 12BUSCO will figure out automatically that it must run on multiple files. For each run, a standard BUSCO result folder named after the input file will be created in the main output folder. In the main output folder, an additional text file summarizing the score of all the runs is provided. This batch mode can be applied to the other two data types by changing the attribute of the -m option.\nRunning BUSCO on multiple inputs without specifying a lineage dataset\nAs with the previous command, collect all the input files you want to analyze in a folder. This protocol is suitable for estimating the quality of metagenomic bins or \u201cmetagenome-assembled genomes\u201d (MAGs) of unknown taxonomic origin, e.g., obtained from binner programs such as MetaBat (Kang et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0018]), MaxBin2 (Wu & Singer, 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0040]), and MEGAN (Ba\u011fc\u0131, Patz, & Huson, 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0001]; Huson et\u00a0al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0014]). In such cases, the input file is a folder containing one FASTA file per bin. Here we analyze a mix of prokaryotic and eukaryotic genomes with known taxonomy, so we can compare the dataset selected by BUSCO with the ground truth represented by the taxonomic lineage of the inputs. As each input sequence can be either coming from a prokaryotic or eukaryotic species, we use the more general --auto-lineage flag to cover both cases.\n2. Uncompress the fasta files:\n         \n$ cd ./protocol3/genomes_mix/ && gunzip *.gz && cd ../../\n3. Run the auto-lineage workflow by entering:\n         \n$ busco -i ./protocol3/genomes_mix -m geno -o busco_out_mix -c 12 --auto-lineageAs with the previous example, the result folders for each run are written to the specified output directory (here busco_out_mix) and named after the corresponding input file name. An additional text file summarizing the scores of all the runs is also written in the main output directory. Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-tbl-0001] shows an example of the summary file listing the BUSCO results for all the inputs analyzed. The first column reports the input file name; the second reports the most specific dataset that was selected by BUSCO on the basis of the placement procedure; the third to eighth columns report the standard BUSCO scores as percentages; and the ninth column reports the total number of markers for the corresponding dataset. In our example, the inputs are a mix of bacterial genomes, one green algal and two fungal genomes. The datasets selected by BUSCO are in agreement with the taxonomic lineages of the inputs. The table additionally reports the score obtained by running the three main domain datasets used to establish the most likely domain of the input file during the first step of the workflow. These scores might be useful for spotting heavy cross-domain contamination issues. However, note that these \u201cby-products\u201d scores need to be carefully interpreted (see Guidelines for Understanding Results for how to interpret these scores). This workflow can be run on the other data types (gene sets and transcriptomes) by changing the attribute of the -m argument (see Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001]). This analysis should take approximately 30 min to complete when using 12 CPUs. On small to medium-sized genomes, increasing the number of CPUs will not speed up the analysis substantially, as the current implementation of parallelization in BUSCO is not optimized on small genomes.To speed up the analysis when analyzing a large number of prokaryotic or small eukaryotic genomes, you can take advantage of a workflow management system as described in Alternate Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0004].Table 1.\n                Example of the batch_summary.txt file Obtained by Running BUSCO Auto-Lineage Workflow on Multiple Inputs\ntable:\n\ufeffInput_file,Dataset,C,S,D,F,M,n,Scores_archaea_odb10,Scores_bacteria_odb10,Scores_eukaryota_odb10\nGCF_002215565.1_ASM221556v1_genomic.fna,sulfolobales_odb10,95.5,95.1,0.4,0.6,3.9,1244,\"C:95.4%[S:95.4%,D:0.0%],F:2.1%,M:2.5%,n:194\",\"C:14.5%[S:14.5%,D:0.0%],F:10.5%,M:75.0%,n:124\",\"C:7.9%[S:7.5%,D:0.4%],F:5.1%,M:87.0%,n:255\"\nGCA_900452565.1_34347_D01_genomic.fna,legionellales_odb10,95.6,95.6,0.0,2.6,1.8,772,\"C:17.0%[S:16.5%,D:0.5%],F:4.1%,M:78.9%,n:194\",\"C:91.1%[S:91.1%,D:0.0%],F:6.5%,M:2.4%,n:124\",\"C:4.7%[S:4.3%,D:0.4%],F:2.0%,M:93.3%,n:255\"\nGCA_003366055.1_ASM336605v1_genomic.fna,bacteria_odb10,63.7,63.7,0.0,8.9,27.4,124,\"C:7.2%[S:7.2%,D:0.0%],F:1.5%,M:91.3%,n:194\",\"C:63.7%[S:63.7%,D:0.0%],F:8.9%,M:27.4%,n:124\",\"C:0.8%[S:0.8%,D:0.0%],F:1.2%,M:98.0%,n:255\"\nGCA_003367175.1_ASM336717v1_genomic.fna,burkholderiales_odb10,99.5,98.5,1.0,0.4,0.1,688,\"C:18.0%[S:17.0%,D:1.0%],F:5.7%,M:76.3%,n:194\",\"C:100.0%[S:100.0%,D:0.0%],F:0.0%,M:0.0%,n:124\",\"C:5.5%[S:4.7%,D:0.8%],F:3.5%,M:91.0%,n:255\"\nGCA_003353085.1_ASM335308v1_genomic.fna,alteromonadales_odb10,99.5,99.1,0.4,0.2,0.3,820,\"C:21.1%[S:21.1%,D:0.0%],F:4.6%,M:74.3%,n:194\",\"C:99.2%[S:96.0%,D:3.2%],F:0.8%,M:0.0%,n:124\",\"C:4.7%[S:3.9%,D:0.8%],F:2.0%,M:93.3%,n:255\"\nGCF_000182965.3_ASM18296v3_genomic.fna,saccharomycetes_odb10,98.6,98.0,0.6,0.8,0.6,2137,\"C:58.8%[S:55.2%,D:3.6%],F:6.2%,M:35.0%,n:194\",\"C:30.6%[S:29.0%,D:1.6%],F:16.9%,M:52.5%,n:124\",\"C:94.9%[S:93.3%,D:1.6%],F:2.4%,M:2.7%,n:255\"\nGCF_002968355.1_ASM296835v1_genomic.fna,entomoplasmatales_odb10,95.5,94.6,0.9,1.5,3.0,332,\"C:3.6%[S:3.6%,D:0.0%],F:2.6%,M:93.8%,n:194\",\"C:81.5%[S:81.5%,D:0.0%],F:1.6%,M:16.9%,n:124\",\"C:1.2%[S:1.2%,D:0.0%],F:0.4%,M:98.4%,n:255\"\nGCF_000226975.2_ASM22697v3_genomic.fna,natrialbales_odb10,98.7,98.3,0.4,0.3,1.0,1368,\"C:99.5%[S:99.5%,D:0.0%],F:0.5%,M:0.0%,n:194\",\"C:21.0%[S:19.4%,D:1.6%],F:9.7%,M:69.3%,n:124\",\"C:7.5%[S:7.1%,D:0.4%],F:3.1%,M:89.4%,n:255\"\nGCA_001560045.1_GG12_C01_07_genomic.fna,sulfolobales_odb10,94.5,94.2,0.3,0.7,4.8,1244,\"C:93.3%[S:93.3%,D:0.0%],F:3.1%,M:3.6%,n:194\",\"C:13.7%[S:13.7%,D:0.0%],F:11.3%,M:75.0%,n:124\",\"C:7.9%[S:7.5%,D:0.4%],F:5.1%,M:87.0%,n:255\"\nGCA_900036045.1_Methanoculleus_sp_MAB1_genomic.fna,methanomicrobiales_odb10,87.3,87.2,0.1,5.8,6.9,882,\"C:88.6%[S:88.1%,D:0.5%],F:7.2%,M:4.2%,n:194\",\"C:17.7%[S:16.9%,D:0.8%],F:12.9%,M:69.4%,n:124\",\"C:5.1%[S:5.1%,D:0.0%],F:3.9%,M:91.0%,n:255\"\nGCA_003018975.1_ASM301897v1_genomic.fna,synechococcales_odb10,88.1,86.5,1.6,4.1,7.8,788,\"C:18.0%[S:17.5%,D:0.5%],F:5.7%,M:76.3%,n:194\",\"C:83.8%[S:80.6%,D:3.2%],F:12.1%,M:4.1%,n:124\",\"C:6.3%[S:5.9%,D:0.4%],F:3.9%,M:89.8%,n:255\"\nGCF_002220235.1_ASM222023v1_genomic.fna,chlorophyta_odb10,97.1,96.7,0.4,0.7,2.2,1519,\"C:54.1%[S:50.5%,D:3.6%],F:6.2%,M:39.7%,n:194\",\"C:68.6%[S:61.3%,D:7.3%],F:14.5%,M:16.9%,n:124\",\"C:76.9%[S:76.5%,D:0.4%],F:7.1%,M:16.0%,n:255\"This protocol describes the use of a workflow management system, specifically Snakemake, to increase the speed of a BUSCO auto-lineage analysis on multiple genomes, and it is an alternative to Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0003]. Currently, BUSCO parallelization is not optimized for small genomes; therefore, using a workflow management system such as Snakemake with multiple CPUs can considerably reduce the runtime for analyses involving a large number of inputs, e.g., when assessing metagenomic bins or MAGs. Note that this Alternate Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0004] is intended for analyzing prokaryotic genomes, small eukaryotic genomes, or a combination of both. It is not intended to be used on medium/large genomes, as for these the parallelization is already optimized in the standard BUSCO run.\nNecessary Resources\nHardware\nAs described in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0003].\nSoftware\nAs described in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0003]. Conda and Snakemake (\u2265 v5.30), available at https://snakemake.readthedocs.io/en/stable/getting_started/installation.html[href=https://snakemake.readthedocs.io/en/stable/getting_started/installation.html]. A clone of the BUSCO \u201cplugins\u201d GitLab repository is available at https://gitlab.com/ezlab/plugins_buscov5.git[href=https://gitlab.com/ezlab/plugins_buscov5.git].\nFiles\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001].\nRunning multiple BUSCO assessments with the auto-lineage workflow and Snakemake\nTo prevent collisions of simultaneous BUSCO runs attempting to write to the same location when using this alternate protocol, BUSCO will be run with the --offline flag, which does not allow BUSCO to connect to the Internet. Therefore, before running this workflow, you need to make sure to have all the required BUSCO datasets and files downloaded and updated on your machine.\nIn this example, we assess a mix of prokaryotic genomes, so we can just download the prokaryotic datasets.\n1. To download the datasets in bulk, enter the location on your machine where you want to store the them:\n         \n$ cd {/path_where_to_store_BUSCO_downloads/}\n2. And run:\n         \n$ busco --download prokaryotaIf you plan to analyze a mix of prokaryotic and eukaryotic genomes, you will need all the datasets. These can be downloaded with busco --download all, which will take approximately 25 min to complete. The size of all extracted datasets is \u223c127 GB (\u223c11 GB for the prokaryotic datasets only).\n3. In the part of your system where you want to run BUSCO, clone the BUSCO \u201cplugins\u201d repository. Here we clone the repository into the directory of the repository you downloaded in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001]:\n         \n$ cd /{path_to_busco_protocol_folder}/\n$ git clone https://gitlab.com/ezlab/plugins_buscov5.git\n4. Then enter the BUSCO_batch_analysis_with_snakemake/ folder:\n         \n$ cd plugins_buscov5/BUSCO_batch_analysis_with_snakemake\n5. You first need to change some parameters in the YAML configuration file that, from the current working directory, you can find in ./config/config.yaml. You need to change the paths to the inputs and the busco_downloads/ folder using Vim or your preferred text editor, e.g.:\n         \n$ vim config/config.yaml # and manually edit the file\nIf you have exactly followed the previous commands, the relative paths will be as follows:\n         \nwdir_path: \".\"\ndata_path: \"../../alternate_protocol1/genomes_mix\" # specify the path to the folder\nfile_suff: \".fna\" # specify the suffix of your input files\nout_path: \"busco_out_mix_alter_protocol\" # specify the output folder\n# configuration\nBUSCO_threads: 5\nautolineage: \"--auto-lineage-prok\" # or \"--auto-lineage\" or \"--auto-lineage-euk\"\ndownload_path: \"{path/where/you/stored/BUSCO_downloads}\" # specify the path of the busco_downloads folder where the datasets and files are stored\n6. In this example, we are going to use a job submission management system to submit our Snakemake workflow to a cluster. In the following example, we use SLURM, but a similar command can be launched with other submission management systems such as PBS. From the current working directory (i.e., BUSCO_batch_analysis_with_snakemake/), you may run:\n         \n$ snakemake --cluster \"sbatch --ntasks-per-node=1 --cpus-per-task=5 --job-name=busco_runs\" --jobs 6Here the --jobs argument specifies the total number of jobs to launch simultaneously. If in the config.yaml file we specify 5 CPUs for each BUSCO analysis, the overall command will use 30 CPUs in total. We suggest restricting the number of CPUs to 5-8 for each BUSCO analysis, as it is the optimal setting for small genomes (e.g., <10 Mbp) in the majority of cases.\nIf you specify the --use-conda flag in the Snakemake command, Snakemake will download (if needed) and use the BUSCO conda version specified in the envs/busco.yaml config file. Otherwise, the BUSCO version available on your system will be used.\nAlternatively, you can run the workflow without using a job submission management system, as in the next command (where we also specify the --use-conda flag), e.g., with:\n         \n$ snakemake --cores 30 --use-conda\nHere -\u2013cores defines the maximum number of CPUs that can be used by the workflow. With 5 CPUs per BUSCO analysis, as defined in the config.yaml file, the workflow should run six BUSCO analyses at the time.\nThe outputs of the analysis will be written to the busco_out_mix_alter_protocol/ folder. For the 25 prokaryotic genomes of this example, the analysis submitted through SLURM on a cluster using 30 CPUs completes in approximately 4 min.This protocol describes the different ways to install BUSCO and its dependencies. BUSCO was implemented using Python 3 and tested on Linux operating systems. It is therefore recommended to use a Linux machine for running BUSCO. There are currently three options to obtain BUSCO: using the Docker container, through Bioconda, or by installing BUSCO and its dependencies manually.\nNecessary Resources\nHardware\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001].\nSoftware\nPython 3.3 or higher, the BUSCO tool, and its dependencies (refer to step 4, below). Optionally, a working installation of Docker or Singularity if using the Docker container option. Optionally, a working installation of conda if using the Bioconda option.\nInstalling BUSCO from Bioconda\nTo install BUSCO through Bioconda you need a working installation of conda. See how to install conda at https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html[href=https://docs.conda.io/projects/conda/en/latest/user-guide/install/index.html]. Ensure you have a conda version >=4.8.4. Enter conda -V to check the version. If necessary, update conda by entering:\n         \n$ conda update -n base conda\n1. Install the latest BUSCO version. It is recommended to install BUSCO in a separate conda environment instead of your conda base environment. Enter:\n         \n$ conda create -n <your_env_name> -c conda-forge -c bioconda busco=x.x.x\nConda can be a bit slow sometimes. You can instead use the mamba package manager (https://github.com/mamba-org/mamba[href=https://github.com/mamba-org/mamba]), which is an extremely fast and popular conda replacement. You can install mamba with conda install -n base -c conda-forge mamba. To install BUSCO using mamba, simply replace conda with mamba in the previous command: mamba create -n <your_env_name> -c conda-forge -c bioconda busco=x.x.x.where your_env_name is the name you want to assign to your new environment, and x.x.x refers to the BUSCO version, which is 5.2.2 as of writing of this manuscript. For more updated versions, change the version number to the latest one. You can check the last BUSCO version available on Bioconda by searching for the \u201cBUSCO\u201d recipe on the Bioconda search bar.\nNow, in order to start working in the new environment, type:\n         \n$ conda activate <your_env_name>\nDouble check the BUSCO version with:\n         \n$ busco -v\nYou can exit the environment by typing:\n         \n$ conda deactivate\nUsing the BUSCO Docker container\nFor each new release, a BUSCO container that wraps everything required to run a BUSCO analysis is also created. Currently, the container is made available on Docker Hub. Before getting the BUSCO container, you need to have Docker installed on your system. See the Docker user guide for details on installing and using Docker (Merkel, 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0026]) (https://www.docker.com/[href=https://www.docker.com/]). Using the Docker container has the advantage of streamlining the setup and installation, and it also allows one to easily track all software versions used in the analyses (the version number, e.g., v5.2.1_cv1, identifies all components at once). It also guarantees that only dependency versions compatible with the most up-to-date BUSCO code are used.\n2. With Docker installed, just fetch the BUSCO container with:\n         \n$ docker pull ezlabgva/busco:vx.x.x_cv1\nwhere x.x.x refers to the BUSCO version and _cv1 to the container version for a given BUSCO version. There should be only one container per version unless an issue with the container was fixed without changing the BUSCO code.3. To run the BUSCO container, you will need to control the user that is run within the container. You can pass your own user id with the -u argument, e.g., -u $(id -u). Note that if you do not specify a user, the container will create files under a different user you do not control. With the mount option -v you specify the location on your filesystem on which you can write and transfer files between your filesystem and the container filesystem. In our example, we will use the current working directory from where you are launching the assessment. You could use another folder, but be careful not to specify a host folder that does not exist, as Docker will create it using the root account and without asking for confirmation. It is safer to use the current directory as in our example. Run the container as follows:\n         \n$ docker run -u $(id -u) -v $(pwd):/busco_wd ezlabgva/busco:vx.x.x_cv1 busco -i genome.fna <OTHER_OPTIONS>\nTo use a custom config.ini file to set run parameters, you need to extract this file from the container as follows:\n         \n$ docker run -v $(pwd):/busco_wd ezlabgva/busco:v5.2.1_cv1 cat /busco/config/config.ini > config.ini\nYou can then edit this newly created config.ini file with your preferred text editor and pass it to the BUSCO command using the --config argument, e.g.:\n         \n$ docker run -v $(pwd):/busco_wd ezlabgva/busco:v5.2.1_cv1 busco -i genome.fna --config=/busco_wd/myconfig.ini\nIf you want to run the BUSCO container in an HPC system that does not support Docker but supports Singularity, which is another containerization program commonly used on HPC clusters, you can convert the Docker container into a Singularity Image Format (see https://quay.io/repository/singularity/docker2singularity[href=https://quay.io/repository/singularity/docker2singularity]). Always consult the documentation of your specific HPC environment first.\nManually installing BUSCO and its dependencies\n4. First, manually install the required BUSCO dependencies.Note that you do not necessarily need to install all the following dependencies if you want to run just specific workflows. For example, if you plan to run a genome assessment with the default BUSCO_Metaeuk workflow, you do not need to install Augustus or NCBI BLAST+. Table 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-tbl-0002] specifies the dependencies required to run specific workflows.\nA fully functional BUSCO setup will require:\n         \nPython 3.3+\nBioPython (https://biopython.org/[href=https://biopython.org/]) and pandas (https://pandas.pydata.org/[href=https://pandas.pydata.org/]) Python modules.\nHMMER: To evaluate amino acid sequences using profile HMMs, all modes of BUSCO require HMMER, version 3.1b2 or higher, which can be obtained from http://hmmer.org/[href=http://hmmer.org/].\nMetaEuk: Required to assess eukaryotic genome assemblies with the default BUSCO_Metaeuk workflow. MetaEuk can be obtained from https://github.com/soedinglab/metaeuk[href=https://github.com/soedinglab/metaeuk].\nProdigal: Required to assess prokaryotic genomes. This can be obtained from https://github.com/hyattpd/Prodigal[href=https://github.com/hyattpd/Prodigal].\nSEPP (Mirarab, Nguyen, & Warnow, 2011[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0028]) and pplacer (Matsen, Kodner, & Armbrust, 2010[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0025]): Required to run BUSCO with the auto-lineage workflow, i.e., to perform the phylogenetic placement of input sequences to automatically select a BUSCO dataset for the assessment. SEPP can be retrieved from https://github.com/smirarab/sepp/[href=https://github.com/smirarab/sepp/] and pplacer from https://github.com/matsen/pplacer[href=https://github.com/matsen/pplacer].\ntBLASTn from NCBI BLAST+ (Camacho et\u00a0al., 2009[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0005]): Only needed if you want to perform genome assessments of eukaryotic species using the BUSCO_Augustus workflow or transcriptome assessments on prokaryotic sequences. It can be downloaded from https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+[href=https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+]. There is an issue with tBLASTn versions 2.4-2.10.0 when using more than one CPU. The issue was fixed in version 2.10.1+, so make sure you have at least version 2.10.1+ installed.AUGUSTUS (v3.3.3 or above): Required if you want to use the BUSCO_Augustus workflow (by specifying the --augustus flag in the command). BUSCO supports versions 3.3.3 or higher, and the software can be obtained from http://bioinf.uni-greifswald.de/augustus/[href=http://bioinf.uni-greifswald.de/augustus/]. It includes multiple PERL scripts (https://www.perl.org/[href=https://www.perl.org/]), and you should refer to the most up-to-date AUGUSTUS documentation for its PERL requirements. The executables required by BUSCO are: augustus, etraining, gff2gbSmallDNA.pl, new_species.pl, and optimize_augustus.pl. Additional environment variables have to be set as follows:\n$ export PATH=/path/augustus-3.x.x/bin:$PATH\n$ export PATH=/path/augustus-3.x.x/scripts:$PATH\n$ export AUGUSTUS_CONFIG_PATH=/path/augustus-3.x.x/config/\nAUGUSTUS makes its predictions based on parameters that are species specific. It comes with predefined values corresponding to well-annotated genomes. Each BUSCO dataset is configured to use the parameters of one available species (e.g., fly for the insecta_odb10 dataset). These species are listed in the $AUGUSTUS_CONFIG_PATH/species/ folder, and it is possible for the user to indicate a different species that is more closely related to the species under analysis (for details on this see the Advanced Parameters section in Commentary).\nR and ggplot2 library: These are required for plotting BUSCO results using the commands described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001].\nMake sure that each software package listed above works independently of BUSCO before attempting to run any BUSCO assessment. The minimal versions of the dependencies compatible with the current BUSCO code are shown in Table 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-tbl-0002]. You can obtain information about future version compatibility on the BUSCO website (https://busco.ezlab.org/[href=https://busco.ezlab.org/]).\nTable 2.\n                Dependencies Required for Running the Different Workflows to Assess Genome Assemblies\ntable:\n\ufeffUnnamed: 0,Genome assessment (eukaryotes),Genome assessment (prokaryotes and viruses),Auto-lineage workflow,Auto-lineage prok workflow,Auto-lineage euk workflow\nBiopython and pandas modules,x,x,x,x,x\nHMMER v3.1b2+,x,x,x,x,x\nMetaEuk v4-*+,x,,x,,x\nProdigal v2.6.3+,,x,x,x,\nSEPP v4.3.10+/ Pplacer v1.1.alpha13+,,,x,x,x\nNCBI BLAST+ v2.10.1+,x (only if using the --augustus flag),,x (only if using the --augustus flag),,x (only if using the --augustus flag)Augustus v3.3.3+,x (only if using the --augustus flag),,x (only if using the --augustus flag),,x (only if using the --augustus flag)\n5. It is recommended to install BUSCO in a separate virtual environment. To do this, first create a Python virtual environment using Python \u22653.3 and the \u201cvenv\u201d module (from version 3.3, Python should already include the venv module). Create a virtual environment with:\n         \n$ python3 -m venv <ENV_NAME>\nwhere <ENV_NAME> is the name you want to assign to the environment. You might name the environment after the BUSCO version you are installing, e.g., busco5.2.2.\n6. Enter the folder that has been created:\n         \n$ cd <ENV_NAME>\n7. Activate the environment with:\n         \n$ source ./bin/activate\n8. Install the Biopython and Pandas modules:\n         \n$ pip3 install biopython pandas\n9. Retrieve the BUSCO source code from the GitLab repository. You can download and extract the BUSCO repository manually from the GitLab release page (https://gitlab.com/ezlab/busco/-/releases[href=https://gitlab.com/ezlab/busco/-/releases]), or using Git, clone the repository with:\n         \n$ git clone https://gitlab.com/ezlab/busco.git\n10. Enter the busco/ folder:\n         \n$ cd busco\n11. Execute the setup.py script using python to install BUSCO:\n         \n$ python3 setup.py install\n12. Check that BUSCO has been installed and verify its version by entering the busco command with the help argument:\n         \n$ busco -h\nIf you do not want to create a separate virtual environment, you can install BUSCO using the same procedure described above excluding the commands from steps 2 to 4. In this case, from the busco/ folder, you need to execute the setup.py script with:\n         \n$ python3 install setup.py \u2013user #(with only user privileges)\nor\n         \n$ sudo python3 setup.py install #(with root privileges)This protocol describes how to use some of the BUSCO results to create charts for displaying the location of markers on the input genome and visualizing syntenies between genomes.\nNecessary Resources\nHardware\nAny machine supporting an installation of Python and R.\nSoftware\nPython, conda (and optionally mamba), Snakemake, R, Rstudio, and the RIdeogram library.\nFiles\nOne or multiple full_table.tsv files for plotting markers and visualizing syntenies.\nVisualizing BUSCO markers on genomes\nSometimes it can be useful to visualize the location of BUSCO markers on the input genome, for example to show the distribution of markers, or highlight problematic regions, e.g., those with a high density of duplicated markers. This can be performed by extracting the BUSCOs\u2019 coordinates from the full_table.tsv file and using a program for plotting genomic coordinates. There are several options available to plot these; here we use the R package RIdeogram (Hao et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0010]).\n1a. If not done already, download the busco_protocol:\n         \n$ git clone https://gitlab.com/ezlab/busco_protocol\n2a. Enter the support_protocol2/plot_markers subfolder:\n         \n$ cd busco_protocol/support_protocol2/plot_markers\nTo generate the plot, you need to provide the path to the full_table.tsv file generated from a BUSCO run performed on a genome assembly. A file, {sample_name}_karyotype.txt, with karyotype information is also required. You can generate this file yourself, following this example (see also the RIdeogram documentation):\n         \ntable:\n\ufeff0,1,2,3,4,5\nChr,Start,End,species,size,color\nNC_004354.4,1,23542271,Dmel,12,25252\nNT_033779.5,1,23513712,Dmel,12,25252\nNT_033778.4,1,25286936,Dmel,12,25252\nNT_037436.4,1,28110227,Dmel,12,25252\nNT_033777.3,1,32079331,Dmel,12,25252\nNC_004353.4,1,1348131,Dmel,12,25252\nNC_024512.1,1,3667352,Dmel,12,25252Alternatively, you can provide the path to the genome assembly used in the assessment, and the workflow will generate this file automatically from the assembly. Here we are using a full_table.tsv obtained by running the diptera_odb10 dataset on the D. melanogaster genome. You can find this file at support_protocol2/plot_markers/data/full_table/Dmel_full_table.tsv in the BUSCO protocol repository. Note that when using the workflow on your inputs, you need to name the full_table.tsv using the convention {sample_name}_full_table.tsv, and keep the same {sample_name}keyword across all the corresponding files related to the same sample. For example, if the full_table file is named Dmel_full_table.tsv, the karyotype file (if provided by the user) must be named Dmel_karyotype.txt, and the genome (if provided), Dmel.fna. You will need to edit the config.yaml file, which you can find in the plot_markers/ folder, to specify the paths to your own inputs. In this config file, you can also provide the path to a text file with the IDs of a subset of sequences you wish to plot, named as {sample_name}_selected_sequences.txt. If not provided, the script will attempt to plot all the sequences present in the input genome. Bear in mind that for a clean visualization, it is not possible to plot too many sequences, e.g., this approach will not work on a fragmented genome with hundreds or thousands of scaffolds. Alternatively, you can select some key sequences of interest to plot.In the following Snakemake command, by specifying the --use-conda flag, Snakemake will install the dependencies through conda, which can be a bit slow. A faster way of using Snakemake's conda integration is by using the mamba package manager (https://github.com/mamba-org/mamba[href=https://github.com/mamba-org/mamba]), which is an extremely fast and popular conda replacement. Therefore, we recommend first installing mamba with conda install -n base -c conda-forge mamba. If you prefer to use Conda, you can enforce that by adding --conda-frontend conda to the Snakemake command.\n3a. To run the workflow on the test data, just enter:\n         \n$ snakemake \u2013cores 2 \u2013use-conda\nThis workflow will: (a) extract the status, sequence IDs, and coordinates of each marker from the full_table.tsv; (b) extract the sequence lengths from the genome file to build a karyotype.txt file, if this file is not provided by the user.\n4a. The plotting step is performed in the Rstudio environment where you can easily customize the appearance according to your needs and readily visualize the changes (you can also generate the plot from the command line using the Rscript command). Open Rstudio and set the working directory to the support_protocol2/plot_markers/ folder of the BUSCO plugins repository. From the Rstudio console run:\n         \n> setwd(\"/path/to/support_protocol2/plot_markers/\")\n5a. Open the plot_markers/scripts/plot_markers2.R script within Rstudio. The paths to source the necessary files are already set for the test data. In this script, you may customize the R code to change colors and labels. Run the commands of this script within Rstudio to produce the plot. You can do this by selecting all the commands of the script and clicking on the \u201cRun\u201d button.A *.png and a *.svg file (here Dmel.svg and Dmel.png) are written to the plot_markers/ folder. Figure 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-fig-0002] shows the resulting plot for this example. As you can see all D. melanogaster chromosomes are covered by complete and single-copy BUSCO markers, except for the Y chromosome.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/3ba75a32-39e9-4c6b-ad29-62c1dfdd2072/cpz1323-fig-0002-m.jpg</p>\nFigure 2\nExample of plotting the location of BUSCO markers on a genome sequence. The figure shows D. melanogaster nuclear chromosomes along with the position of complete (light blue), duplicated (black), and fragmented (yellow) BUSCO markers from the diptera_odb10 dataset. The Y sex chromosome does not harbour BUSCO markers.\nVisualizing syntenies between genomes using BUSCO markers\nThe genomic position of each single-copy ortholog identified by BUSCO on two or more genomes can be used to infer syntenies between genomes. For visualizing syntenies between two or more genomes, you will need the full_table.tsv files obtained by running BUSCO on the input genomes. Note that you need to use the same dataset for all the assessments. In this example, we infer syntenic relationships between Drosophila melanogaster and D. pseudoobscura chromosomes using the diptera_odb10 single-copy orthologs. There are various programs to plot syntenies; here we use the R package RIdeogram as in the previous example.\n1b. If not done already, download the busco_protocol repository from the GitLab:\n         \n$ git clone https://gitlab.com/ezlab/busco_protocol\n2b. Enter the support_protocol2/plot_syntenies subfolder:\n         \n$ cd busco_protocol/support_protocol2/plot_syntenies\nYou can find the two full_table.tsv files obtained by running BUSCO with the diptera_odb10 dataset on D. melanogaster and D. pseudoobscura assemblies in the plot_syntenies/data/full_table/ folder. To plot your own data, remove these files and add your full_table.tsv files.\n3b. Then, run the script for creating the files required for generating the plot with:\n         \n$ snakemake \u2013cores 2 --use-condaThe script will pre-process the full_table.tsv files to create the file required for plotting the syntenies with RIdeogram. A karyotype.txt file can be provided to the command, in case you want to plot only a subset of sequences from the input files. You need to create this file with following format:\n         \ntable:\n\ufeff0,1,2,3,4,5,6\nChr,Start,End,fill,species,size,color\n1,1,23542271,8AFFBB,Dmela,12,7adb3d\n2,1,23513712,8AFFBB,Dmela,12,7adb3d\n3,1,25286936,8AFFBB,Dmela,12,7adb3d\n4,1,28110227,8AFFBB,Dmela,12,7adb3d\n5,1,32079331,8AFFBB,Dmela,12,7adb3d\n6,1,1348131,8AFFBB,Dmela,12,7adb3d\n7,1,3667352,8AFFBB,Dmela,12,7adb3d\n1,1,32422566,95FF7D,Dpseudo,12,6c72f0\n2,1,23510042,95FF7D,Dpseudo,12,6c72f0\n3,1,30706867,95FF7D,Dpseudo,12,6c72f0\n4,1,1881070,95FF7D,Dpseudo,12,6c72f0\n5,1,68158638,95FF7D,Dpseudo,12,6c72f0\nIf not provided, the script will extract this information directly from the genomes for all the sequences in the inputs. You may edit this file to keep only a subset of sequences of interest and rerun the workflow by specifying the edited file.\n4b. Open Rstudio, and set the working directory to the support_protocol2/plot_syntenies/ folder inside the busco_protocol directory. From the Rstudio console, run:\n         \n> setwd(\"/path/to/support_protocol2/plot_syntenies/\")\n5b. Open the plot_syntenies/scripts/plot_syntenies2.R script within Rstudio. The paths to source the necessary files are already set. Here you can customize the R code to change colors and labels. Now run the commands of this script within Rstudio to produce the plot. You can do this by selecting all the commands of the script and clicking on the \u201cRun\u201d button. A *.png and *.svg file should be written in the plot_syntenies/ folder. Figure 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-fig-0003] shows the chart of the syntenic relationships between the D. melanogaster and D. pseudoobscura chromosomes obtained with the test data.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/975d7940-b82d-4827-9710-ca632e05340a/cpz1323-fig-0003-m.jpg</p>\nFigure 3\nSyntenic relationships between the D. melanogaster and D. pseudoobscura sequenced chromosomes, computed by mapping the location of the diptera_odb10 markers obtained by running BUSCO on both genomes.BUSCOs, being near-universal single-copy genes, can represent reliable markers to be used in phylogenomics studies. BUSCO assessments on multiple organisms can be used to quickly and easily identify single-copy markers to generate multiple sequence alignments (MSA) from which to infer the species phylogeny. BUSCO datasets can identify large sets of genes from genomic data of variable quality, avoiding the tedious and possibly biased manual selection of orthologs. In this protocol we illustrate a workflow to build a phylogenomic tree from annotated gene sets (with a GFF and genome assembly available for each species). In this case, we concatenate the MSAs into a superaligment and use a Maximum Likelihood (ML) approach to build the phylogenomic tree. The analyses presented here are by no means exhaustive, and you should choose the most suitable methods and parameters (e.g., using partitions, different models etc.) according to your specific goals and inputs.\nNecessary Resources\nHardware\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001].\nSoftware\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-prot-0001] and additionally: conda (and optionally mamba), Snakemake, AGAT, MAFFT, TrimAl, IQ-TREE2, AMAS. Note that these tools will be automatically installed by running the Snakemake workflow (otherwise you can install these manually if you prefer).\nFiles\nA set of genome assemblies and their corresponding GFF files.\nBuilding phylogenomic tree from a set of genome assemblies and GFF files\n1. If not done already, download the busco_protocol repository from GitLab:\n         \n$ git clone https://gitlab.com/ezlab/busco_protocol\n2. Enter the support_protocol3/ folder:\n         \n$ cd support_protocol3/3. Collect the GFF and the genome assembly files you want to analyze in the GFFs/ and assemblies/ folders, respectively. In this example, we are going to build a small species tree using six insect species selected from various published sources: Drosophila melanogaster (GCF_000001215.4; Hoskins et\u00a0al., 2015[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0013]), Bombyx mori (GCF_014905235.1); Apis mellifera (GCF_003254395.2; Wallberg et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0038]); Tribolium castaneum (GCF_000002335.3) (Herndon et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0011]), Acyrthosiphon pisum (GCF_005508785.1; Li, Park, Smith, & Moran, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0023]) and Zootermopsis nevadensis (GCF_000696155.1; Terrapon et\u00a0al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0036]). Using the assembly accessions, you can download these with the NCBI \u201cdatasets\u201d command-line tool (https://www.ncbi.nlm.nih.gov/datasets/[href=https://www.ncbi.nlm.nih.gov/datasets/]), e.g., datasets download genome accession GCF_014905235.1 or manually from the NCBI portal or the ftp site using the wget command, e.g.:\n         \n$ cd genomes/\n$ wget -O Drosophila_melanogaster.fna.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/215/GCF_000001215.4_Release_6_plus_ISO1_MT/GCF_000001215.4_Release_6_plus_ISO1_MT_genomic.fna.gz\n$ wget -O Bombyx_mori.fna.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/014/905/235/GCF_014905235.1_Bmori_2016v1.0/GCF_014905235.1_Bmori_2016v1.0_genomic.fna.gz\n$ wget -O Apis_mellifera.fna.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/003/254/395/GCF_003254395.2_Amel_HAv3.1/GCF_003254395.2_Amel_HAv3.1_genomic.fna.gz\n$ wget -O Tribolium_castaneum.fna.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/335/GCF_000002335.3_Tcas5.2/GCF_000002335.3_Tcas5.2_genomic.fna.gz\n$ wget -O Acyrthosiphon_pisum.fna.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/005/508/785/GCF_005508785.1_pea_aphid_22Mar2018_4r6ur/GCF_005508785.1_pea_aphid_22Mar2018_4r6ur_genomic.fna.gz\n$ wget -O Zootermopsis_nevadensis.fna.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/696/155/GCF_000696155.1_ZooNev1.0/GCF_000696155.1_ZooNev1.0_genomic.fna.gz\n$ gunzip *.gz && cd ../GFFs\n$ wget -O Drosophila_melanogaster.gff.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/001/215/GCF_000001215.4_Release_6_plus_ISO1_MT/GCF_000001215.4_Release_6_plus_ISO1_MT_genomic.gff.gz\n$ wget -O Bombyx_mori.gff.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/014/905/235/GCF_014905235.1_Bmori_2016v1.0/GCF_014905235.1_Bmori_2016v1.0_genomic.gff.gz\n$ wget -O Apis_mellifera.gff.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/003/254/395/GCF_003254395.2_Amel_HAv3.1/GCF_003254395.2_Amel_HAv3.1_genomic.gff.gz\n$ wget -O Tribolium_castaneum.gff.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/002/335/GCF_000002335.3_Tcas5.2/GCF_000002335.3_Tcas5.2_genomic.gff.gz\n$ wget -O Acyrthosiphon_pisum.gff.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/005/508/785/GCF_005508785.1_pea_aphid_22Mar2018_4r6ur/GCF_005508785.1_pea_aphid_22Mar2018_4r6ur_genomic.gff.gz\n$ wget -O Zootermopsis_nevadensis.gff.gz\nhttps://ftp.ncbi.nlm.nih.gov/genomes/all/GCF/000/696/155/GCF_000696155.1_ZooNev1.0/GCF_000696155.1_ZooNev1.0_genomic.gff.gz\n$ gunzip *.gz && cd ../\n4. Edit the config/config.yaml file in which you need to specify the paths to the GFFs and genomes folders, and additional parameters such as which BUSCO dataset you want to use to identify the single-copy orthologs that encompass all species. If you exactly follow the steps in this protocol, all parameters in the config.yaml file are already set for this example. Analyses with higher-resolution datasets identify more markers for inferring the species phylogeny. In our example, we are using the insecta_odb10 dataset.\n5. Run the Snakemake workflow, which comprises all the steps required for obtaining the species phylogeny. Briefly, the workflow consists of the following steps:For each species, isoforms are first filtered to keep only the longest protein-coding sequence for each gene according to the corresponding GFF file. This is achieved using the AGAT toolkit (Dainat J.), which also produces a useful report that can be used to verify there are no issues with the GFF entries.\nBUSCO is then run in protein mode on the resulting filtered gene sets using the dataset specified in the config.yaml file.\nThe single-copy genes identified from each run are extracted and selected based on user-defined parameters specified in the config.yaml file, e.g., to select only genes that are shared across 100% of the species and with no duplicates across all species.\nFor each orthologous group, proteins are aligned using MAFFT (Katoh & Standley, 2013[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0019]) and trimmed with trimAl (Capella-Guti\u00e9rrez, Silla-Mart\u00ednez, & Gabald\u00f3n, 2009[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0006]).\nAlignments are concatenated with AMAS (Borowiec, 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0004]).\nThe resulting superalignment is used to infer a maximum likelihood phylogeny using IQ-TREE 2 (Minh et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0027]). To run the workflow, enter:\n$ snakemake \u2013cores 16 --use-conda\nSeveral intermediate files and the final phylogenetic.nwk tree file are written in the output folder. Assessments of the six insect species with the insecta_odb10 dataset identified 1154 complete and single-copy BUSCOs found in all species. Note that in the config.yaml file, you can also specify to use markers found as duplicated (the best scoring duplicate will be selected) and markers missing in a certain number of species, e.g., missing in 10% of the inputs. To visualize the Newick file, you can use any program for viewing phylogenetic trees, e.g., Dendroscope (Huson & Scornavacca, 2012[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-bib-0015]). Figure 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.323#cpz1323-fig-0004] shows the resulting tree obtained in this example.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d6f9a063-548a-4bf7-b021-a5d7fd0708fe/cpz1323-fig-0004-m.jpg</p>\nFigure 4Example of a phylogenomic tree obtained by using single-copy orthologs identified by BUSCO on a test data of six insect species. The phylogeny was inferred from a set of 1154 concatenated BUSCOs under the Q.insect+R5 substitution model using IQ-TREE 2.", "Step-by-step method details\nStep-by-step method details\nPart 1. Installation\nTiming: 15\u00a0min\nDownloading VirMutSig The installation of VirMutSig is done by downloading the GitHub repository in a local directory, which includes the VirMutSig scripts and the example files. After the installation, a new folder named VirMutSig will be generated. Please install VirMutSig by moving to your local directory and using GitHub with the following command in the terminal:\n>git clone https://github.com/BIMIB-DISCo/VirMutSig.git\nVirMutSig includes 4 directories with the following names:\n\u201cpreprocessing\u201d This directory contains all the files and directories required to perform Part 2, such as:\nSRR_to_SNVlist.nf: the Nextflow pipeline file\nnextflow.config: the config file with the preprocessing settings and parameters\nreference: a directory with the SARS-CoV-2-ANC reference file\nbin: a directory with an R script used to create the SNVs list for Parts 3\u20134.\n\u201cdenovo\u201d This directory contains the corresponding R script to perform the de novo discovery of mutational signatures (Part 3). The files included are:\ndenovo.R This script performs the discovery of viral mutational signatures from the list of selected SNVs taken as input, by employing a NMF approach described in the above section or in (Lal et\u00a0al., 2021[href=https://www.wicell.org#bib16]). The user must also specify the number of contexts, i.e., the flanking bases to the genome positions. They may either be 6 or 96, please refer to (Lal et\u00a0al., 2021[href=https://www.wicell.org#bib16]) for further details.\ndenovo_config.yaml This file contains the parameters explained above. It could be modified using any text\u00a0editor.\ndenovo_utils.R This R file contains some functions used by the main denovo.R scripts.\nNote: please do not modify this file.\n\u201cassignment\u201d This directory contains the corresponding R script to perform the assignment of the activity of mutational signatures to each sample (Part 4). The files included are:assignment.R This script takes as input the signatures.txt file generated by the denovo.R script or provided by the user, and the list of SNVs to perform the assignment of the signatures to each sample. Bootstrap can be employed to assess the statistical confidence of the signature assignments.\nassignment_config.yaml This file contains the parameters explained above. It could be modified using any text\u00a0editor.\nassignment_utils.R This R file contains some functions used by the main assignment.R scripts.\nNote: please do not modify this file.\n\u201cexample\u201d This directory includes an example of the VirMutSig analysis performed on 150 FASTQ files obtained from samples of SARS-CoV-2 via RNA sequencing experiments.\nDownload docker image\nThe preprocessing pipeline executes all the steps using a Docker image in which all the requested software is already installed to avoid compatibility issues.\nDocker can be obtained following the instruction at this link: (https://www.docker.com/get-started[href=https://www.docker.com/get-started]).\nOnce the installation is completed, please get the protocol image called \u2018dcblab/virmutsig_img\u2019 by digiting the following command in a terminal:\n>docker pull dcblab/virmutsig_img:latest\nVerify docker image installation\nTo test if the image is correctly installed in your system, please execute the following code:\n>docker image ls\nwhich should display the following lines:\nREPOSITORY \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0TAG\u00a0\u00a0\u00a0\u00a0IMAGE ID\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0CREATED\u00a0\u00a0\u00a0\u00a0SIZE\ndcblab/virmutsig_img\u00a0\u00a0latest\u00a0\u00a0\u00a0\u00a0223f27c12b45\u00a0\u00a0\u00a0\u00a04 hours ago\u00a0\u00a01.56GB\nNote: in this case, it is possible to proceed to the next steps.\nPart 2. Data preprocessing pipeline\nTiming: 10\u00a0min download\u00a0+ 70\u00a0min pipeline execution for each sample (can be parallelized)\nThe data preprocessing pipeline included in the VirMutSig protocol is provided as a Nextflow pipeline file, named \u201cSRR_to_SNVlist.nf\u201d and included in the \u201cpreprocessing\u201d folder of the GitHub repository: https://github.com/BIMIB-DISCo/VirMutSig[href=https://github.com/BIMIB-DISCo/VirMutSig].\nThe folder also includes a file named: \u201cnextflow.config\u201d, which must be opportunely modified according to the specific experimental settings (see below for the parameter description).The preprocessing pipeline includes the following processes (detailed in the following): data acquisition, trimming, alignment, remove duplicated reads (optional), get depth information, variant calling, and variant filtering.\nAs output, the preprocessing pipeline returns a file named: \u201cSNV_list.txt\u201d in which:\nrows correspond to all the single nucleotide variants (SNVs) detected in all the samples of the dataset\ncolumns correspond to: sample ID, variant ID, genome position, reference allele, alternative allele, reference three nucleotides (flanking bases), supporting reads, coverage, variant frequency (VF), and p-value (returned by the variant caller).\nTo run the preprocessing processes of the protocol, simply move to the \u201cpreprocessing\u201d folder and execute the following command from the terminal:\n>nextflow run SRR_to_SNVlist.nf\nIn the following, we describe the preprocessing processes and the related settings in detail.\nData acquisition From now on, we will consider 'preprocessing' as a working directory from the one specified during the installation: \u2018user_local_directory/VirMutSig/preprocessing' Input data can be either (i) downloaded from public repositories, or (ii) provided from local folders, if available.\nInput data acquisition from public repositories Input data can be downloaded from public repositories such as, e.g., the NCBI Sequence Read Archive (SRA) (https://www.ncbi.nlm.nih.gov/sra[href=https://www.ncbi.nlm.nih.gov/sra]). In this case, one can use the SRA Run Selector web interface (https://www.ncbi.nlm.nih.gov/Traces/study/[href=https://www.ncbi.nlm.nih.gov/Traces/study/]) to search for a dataset and download the \u201cAccession List\u201d by using the related button. The obtained list of SRR IDs (one per line) can be passed as input to download the files via SRA-toolkit, by editing the following parameter of the \u201cnextflow.config\u201d:\nparams.FASTQ_input\u00a0= '../example/SRAlist_paired.txt'\nAlso in this case, the library preparation layout (single-end or paired-end) must be specified by editing the following parameter in \u201cnextflow.config\u201d file:\nparams.library_preparation\u00a0= \u2018paired\u2019 // or \u2018single\u2019By default, the downloaded FASTQ files will be stored in the following path: '/intermediate/FASTQ' (relative to SRR_to_SNVlist.nf file). It possible to change the directory by editing the following parameter in in \u201cnextflow.config\u201d file:\nparams.FASTQdir\u00a0= 'intermediate/FASTQ'\nInput data from local folders\nIn this case, the user must indicate the directory where the FASTQ files are located, editing the following parameter of the \u201cnextflow.config\u201d:\nparams.FASTQ_input\u00a0= '/Path/To/Directory/'\nThe library preparation layout (single-end or paired-end) must be specified by editing the following parameter in \u201cnextflow.config\u201d file:\nparams.library_preparation\u00a0= \u2018paired\u2019 //or \u2018single\u2019\nNotice that with the \u201cpaired\u201d configuration two FASTQ files are expected for each sample, formatted as (sampleID_1.fastq.gz and sampleID_2.fastq.gz).\nConversely, with the \u201csingle\u201d configuration one sampleID.fastq.gz file is expected for each sample.\nTrimming\nThis step aims at removing from the sequence the nucleotides with low sequencing quality. To perform this step, the Nextflow pipeline exploits the tool Trimmomatic (http://www.usadellab.org/cms/?page=trimmomatic[href=http://www.usadellab.org/cms/?page=trimmomatic]).\nTo tune the additional parameters used by Trimmomatic please edit the following parameter in \u201cnextflow.config\u201d file:\n\u00a0\u00a0params.trimmomatic_setting\u00a0= 'LEADING:20 TRAILING:20\nSLIDINGWINDOW:4:20 MINLEN:40'\nIn brief, LEADING and TRAILING parameters cut the bases that display a quality below a certain threshold, respectively at the start and the end of a read (20).\nThe SLIDINGWINDOW parameter sets the size of a sliding window (4 in our example) and deletes all the bases from the leftmost position of the window to the end of the read, when the average quality detected in the window drops below a given threshold (e.g., 20).\nFinally, the MINLEN parameter specifies the minimum length of a read to be kept (40 bases in our example). Please, refer to the Trimmomatic documentation for more details.\nAlignment Reads need to be aligned to a reference genome, which can be selected by the user, e.g.,\nSARS-CoV-2-ANC (Ramazzotti et\u00a0al., 2021[href=https://www.wicell.org#bib20]),EPI_ISL_405839 / GeneBank ID: MN975262.1 (https://www.ncbi.nlm.nih.gov/nuccore/MN975262.1[href=https://www.ncbi.nlm.nih.gov/nuccore/MN975262.1]) (Bastola et\u00a0al., 2020[href=https://www.wicell.org#bib4]),\nEPI_ISL_402125 / NCBI ID: NC_045512.2 (https://www.ncbi.nlm.nih.gov/nuccore/1798174254[href=https://www.ncbi.nlm.nih.gov/nuccore/1798174254]) (Andersen et\u00a0al., 2020[href=https://www.wicell.org#bib2]).\nIn the subfolder \u201cpreprocessing/reference\u201d of the GitHub repository, we provide the SARS-CoV-2-ANC genome in FASTA format.\nThe user can specify the reference genome file by editing following parameter in \u201cnextflow.config\u201d\u00a0file:\nparams.fasta\u00a0= 'reference/SARS-CoV-2-ANC.fasta'\nThe alignment is performed with BWA-MEM (https://github.com/lh3/bwa[href=https://github.com/lh3/bwa]), which generates a SAM file including the aligned reads. All the associated files used by the BWA aligner will be automatically generated and placed in the same reference genome folder.\nEach SAM file will be sorted and compressed into a BAM file with Samtools (http://www.htslib.org/[href=http://www.htslib.org/]).\nBy default, the BAM files will be stored in the following path: '/intermediate/BAM' (relative to SRR_to_SNVlist.nf file). It is possible to change the directory by editing the following parameter in in \u201cnextflow.config\u201d file:\nparams.BAMdir\u00a0= 'intermediate/BAM'\nRemove duplicated reads (optional)\nOften, after obtaining an aligned BAM file, is useful to mark and remove duplicated reads to reduce the impact of the amplification bias, especially with RNA-seq experiments. Otherwise, when dealing with Amplicon data, several duplicated reads are expected and should not be removed. For this reason, we made this step optional.\nTo include or skip this step in the preprocessing pipeline, please set accordingly the following parameter in \u201cnextflow.config\u201d file:\nparams.remove_duplicates\u00a0= \u201ctrue\u201d // or \u201cfalse\u201d\nNote: the tool used to perform this task is Picard (https://broadinstitute.github.io/picard/[href=https://broadinstitute.github.io/picard/]).\nGet depth information\nThe aim of this step is to obtain the depth information for each sample in every genome position (i.e.,\u00a0number of reads mapped on each position of the viral genome).\nTo perform this task, we used Samtools.By default, the coverage files will be stored in the following path: '/intermediate/COVERAGE\u2019 (relative to SRR_to_SNVlist.nf file). It is possible to change the directory by editing the following parameter in in \u201cnextflow.config\u201d file:\nparams.COVERAGEdir\u00a0= 'intermediate/COVERAGE'\nVariant calling\nVariants can be called comparing the aligned reads with the reference genome. To do so, we used Samtools (http://www.htslib.org/[href=http://www.htslib.org/]) and VarScan (http://varscan.sourceforge.net/[href=http://varscan.sourceforge.net/]). More in detail, we used the mpileup command included in Samtools which converts the BAM file into the pileup format required by VarScan.\nThen we used the VarScan pileup2snp for variant calling to produce a VCF file for each BAM file. The VarScan setting can be adjusted by editing the following parameter included in \u201cnextflow.config\u201d file.\nparams.varscan\u00a0= '--min-var-freq 0.01 --p-value 1'\nNote: For more information, please refer to the VarScan documentation.\nBy default, the VCF files will be stored in the following path: '/intermediate/VCF\u2019 (relative to SRR_to_SNVlist.nf file). It is possible to change the directory by editing the following parameter in in \u201cnextflow.config\u201d file:\nparams.VCFdir\u00a0= 'intermediate/VCF'\nNote: We suggest performing the alignment step according to the manufacturer\u2019s recommendations for all sequencing technologies.\nCritical: All the above steps can be performed also by applying different pipelines for variant calling, such as the one proposed in [https://github.com/andersen-lab/ivar[href=https://github.com/andersen-lab/ivar]], which\u00a0was specifically designed for handling viral amplicon data obtained via the artic protocol.\nVariant filtering\nIn order to reduce the impact of noise in the data (due, e.g., to sequencing issues), we adopted multiple quality control (QC) filters to select only the reliable SNVs. The last step of the preprocessing pipeline applies different filtering criteria on the detected SNVs. The following parameters determine the filtering threshold, and they can be set by changing the corresponding numeric value included into a string assigned to VirMutSig_QCfilter parameter present in the \u201cnextflow.config\u201d file.params.SNV_filters\u00a0= 'PV_THR:0.01 VAR_FREQ_THR:0.05 MIN_COV:20 ALT_READ_THR:3'\np-value on variant calling significance (PV_THR). The filter removes all the variants called with a significance p-value larger than a given threshold (default\u00a0= 0.01).\nFrequency threshold (VAR_FREQ_THR). The filter keeps only variants observed in the data with a variant frequency that exceed the specified threshold (default\u00a0= 0.05).\nMinimum coverage (MIN_COV). The filter keeps only variants with the specified minimum coverage (default\u00a0= 20).\nMinimum alternative read count (ALT_READ_THR). The filter keeps only variants with a minimum number of reads showing the alternative allele equal to the given threshold. (default\u00a0= 3)\nNote: Indels are not considered in the analysis.\nBy default, the SNV_list.txt file will be stored in the 'example\u2019 directory. It is possible to change the directory by editing the following parameter in in \u201cnextflow.config\u201d file:\nparams.SNVlistdir\u00a0= '../example\nThis step uses a R script included in the \u201cpreprocessing/bin\u201d directory of the GitHub repository named makeSNVlist.R[href=https://github.com/BIMIB-DISCo/VirMutSig/blob/main/preprocessing/bin/makeVirMutSigInput.R]. It takes as input different arguments with the following fixed order:\nA string with the path of the directory containing the VCF files generated by the variant caller.\nA string with the path of the directory containing the depth files generated by step 8. Such files must end with '.depth.txt'.\nReference file in fasta format. (e.g., SARS-CoV-2-ANC.fasta)\nNote: If another pipeline has been used to perform variant calling, the R script can be used to aggregate the vcf files into a SNV list. Instead, if VirMutSig preprocessing steps have been used, the script is automatically executed via Nextflow.\nCritical: Parameters must be set according to the specific features of the datasets (see, e.g., the guidelines proposed on the website: https://virological.org/[href=https://virological.org/]).\nFurther informationThe Trimming and Alignment step require greater computational resources than the other processes. For this reason, it is possible to specify the maximum number of cores to be used, by changing the cpus value in the following setting of the \u201cnextflow.config\u201d file:\nprocess {\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0withName: 'Trimming_single' {cpus\u00a0= 4}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0withName: 'Trimming_paired' {cpus\u00a0= 4}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0withName: 'Alignment_and_sorting_single' {cpus\u00a0= 8}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0withName: 'Alignment_and_sorting_paired' {cpus\u00a0= 8}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\nAn increase of the cpus number assigned to the processes reduces the computational time required to trim and align the reads, but it also reduces the number of samples that can be analyzed in parallel. For these reasons, one should select a proper value based on the available computational resources and number of samples.\nAll parameters (\u201cparams.\u201d) can be specified by overriding when the pipeline is launched. To do so, please specify the corresponding parameter name (the string following \u201cparams.\u201d) and specify the opportune argument of the Nextflow command.\nIn the example, the SRR list file path and the output directory path of the SNVs list file will be specified without editing the \u201cnextflow.config\u201d file.\n>nextflow run SRR_to_SNVlist.nf \\\n--FASTQ_input \u2018SRRfile/custom/path\u2019 \\\n--SNVlistdir \u2018SNVlist/output/path\u2019\nFor a summary of the settings and processes executed with the example configuration please see Figure\u00a01[href=https://www.wicell.org#fig1].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1132-Fig1.jpg\nFigure\u00a01. Summary of the settings and processes executed with the example configuration\nWe analyzed 150 samples with a paired-end library layout, downloaded from the SRA database. The related processes are automatically executed via Nextflow.\nPart 3. De\u00a0novo discovery of mutational signatures (denovo.R)\nTiming: \u223c20\u00a0min (approximate time required to perform the step on a dataset with \u223c100 samples and \u223c10,000 mutations, with a significant variability related to the signature rank range)\nInput files and setup The denovo.R script requires two input files to be executed.List of selected SNVs [SNV_list.txt] This file is generated following the preprocessing step. It is a semicolon-separated file with a SNV for each line. It includes (at least) the following column headers:\nSampleId: The ID of the sample.\nPosition: The genome position.\nReference: the reference allele (A, T, C, G).\nAlternative: the alternative allele (A, T, C, G).\nVariantCount: the number of the reads including the alternative allele.\nTotalCount: the total number of reads covering the genome position. For the 96-contexts analysis (see below) the following column is also required:\nReferenceTrinucleotide: the triplet with the reference bases before and after the variant position (e.g., ATC where T is the reference). Please see the file SNV_list.txt contained in the \u201c/example\u201d directory for an example of input formatting (see Table 1[href=https://www.wicell.org#tbl1]).\ntable:files/protocols_protocol_1132_2.csv\nConfiguration file [denovo_config.yaml] The parameters to run denovo.R are set in a text file called as default \u2018denovo_config.yaml\u2019.\u00a0The file must contain a parameter in each line formatted as:\nPARAMETER NAMES: value\nThe parameters that can be modified are the following: [variant selection]\nCLONAL_SNV_THR: Double [0,1]. default\u00a0= 0.9. This parameter defines the variant frequency threshold that determines whether a given variant is clonal.MINOR_SNV_SEL: String {\u2018always\u2019, \u2018all\u2019}. default\u00a0= \u2018always\u2019. To reduce the bias induced by mutations transmitted and inherited in the population during the epidemic spread, for the signature analysis we select and employ only the SNVs that are never observed with a frequency higher than CLONAL_SNV_THR in any sample. The selected SNVs are defined as \u2018always minor\u2019. This parameter together with the parameter CLONAL_SNV_THR are critical as they determine the variants that are selected for the signatures analysis; the default parameters that we suggest here (i.e., CLONAL_SNV_THR\u00a0= 0.90 and MINOR_SNV_SEL\u00a0=\"always\") are very conservative and they should be determined based on the aim of the study. For further details, please refer to (Ramazzotti et\u00a0al., 2021[href=https://www.wicell.org#bib20]).\nMAX_SNV_SAMPLE: Integer [1, Inf]. default\u00a0= 100. In order to reduce the impact of highly mutated samples, likely due to degradation of their biological isolation, we suggest removing the samples exhibiting more than a user selected number of variants.\nMIN_SNV_SAMPLE: Integer [1, Inf]. default\u00a0= 6. To assess the existence of statistically significant mutational signatures, we remove all the samples with number of detected SNVs lower than this value. [analysis parameters]\nNUM_CONTEXTS: Integer {6, 96}. default\u00a0= 6. This parameter specifies the number of different nucleotide substitution types (based on the flanking bases) that are considered. 6 indicates that no flanking bases are considered. In this case, the possible substitution types are: C>A (or G>T), C>G (or G>C), C>T (or G>A), T>A (or A>T), T>C (or A>G), T>G (or A>C). 96 indicates that the two flanking bases are considered. In this case, the possible substitution types include, e.g.,: ACA>AAA (or AGA>ATA), ACG>AAG (or AGG>ATG), ATA>ACA (or AAA>AGA), etc. For further details, please refer to (Lal et\u00a0al., 2021[href=https://www.wicell.org#bib16]).MIN_NUM_SIG: Integer [1, NUM_CONTEXTS]. default\u00a0= 1. This parameter sets the lower bound of the range for the number of distinct signatures to be searched.\nMAX_NUM_SIG: Integer [MIN_NUM_SIG, NUM_CONTEXTS]. default\u00a0= 6 This parameter sets the upper bound of the range for the number of distinct signatures to be searched.\nNMF_ITER: Integer [1, inf]. default\u00a0= 100. This parameter sets the number of Negative Matrix Factorization iterations performed during the inference of A and B matrices\nSEED: Integer [0, Inf]. default\u00a0= 0 (with 0 the seed will be randomly set). This parameter initializes the random number generator. It is used to obtain reproducible results by keeping the same SEED.\nN_CORE: Integer [0, Inf]. default\u00a0= 0 (with 0 the number of cores available will automatically be detected). This parameter indicates the maximum number of computational units (cpus) available for the inference.\nCritical: MAX_NUM_SIG and MIN_SNV_SAMPLE must be set accordingly with the NUM_CONTEXTS parameter. To obtain robust results, we suggest setting the former equal to half the number of contexts (i.e., 3 or 48) and the latter larger than the number of contexts.\nRun denovo.R\ndenovo.R can be run with the following command:\n>Rscript denovo.R \\\n--SNV_list path/to/SNV_list.txt \\\n--config_file path/to/denovo_config.txt \\\n--output_dir path/to/output/dir\nThe script will generate in the output directory (specified by the user) the following three subdirectories:\n\u201cSignatures\u201d: This directory will contain a file for each number of searched signatures named \u2018x_signatures.txt\u2019. Each file will be formatted as a table with the context as header and a row for each signature.\n\u201cGraphical\u201d: This directory will contain the graphical representation of each signature set found in the data.\n\u201cRank\u201d: This directory will contain a set of files to determine the optimal number of signatures.Unfortunately, no automated procedures are available to find the correct number of signatures, so we here provide a set of metrics to determine it. The denovo.R script provides four metrics as output and the relative plots:\nexplained variance (Hutchins et\u00a0al., 2008[href=https://www.wicell.org#bib13])\ncophenetic coefficient (Brunet et\u00a0al., 2004[href=https://www.wicell.org#bib5]),\ndispersion coefficient (Kim and Park, 2007[href=https://www.wicell.org#bib15]),\nsilhouette consensus coefficient.\nThe four metrics are shown in Figure\u00a02[href=https://www.wicell.org#fig2] and Table 2[href=https://www.wicell.org#tbl2].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1132-Fig2.jpg\nFigure\u00a02. Metrics to evaluate the best number of signatures\n(A) shows the explained variance at different ranks; in this case we observed a bend at 3.\n(B\u2013D) show stability-based coefficients which report the consistency of NMF solutions across multiple iterations.\ntable:files/protocols_protocol_1132_3.csv\nThe first one measures how good is the fit of each given number of signatures, considering the observed mutational profiles. This metric is useful to estimate when the number of signatures is too high (i.e., overfitting). To this end, we suggest choosing the optimal number of signatures based on the bend rule that selects the number of signatures corresponding to the elbow in the explained variant plot (see the example in Figure\u00a02[href=https://www.wicell.org#fig2]A).\nOften there is uncertainty to select a unique elbow point on the plot, so the latter three metrics\u00a0can be used as additional evaluation of the optimal rank. Roughly, they provide a measure\u00a0of stability of the NMF solutions over multiple runs (NMF_ITER parameter). These coefficients\u00a0range from 0 to 1 and higher values indicate higher consistency among NMF solutions.\nConsidering all the above, our suggestion is to select one or few elbow points and among them select the one corresponding to higher cophenetic, dispersion, or silhouette coefficient. If still there is ambiguity it is reasonable to take the lower one.Considering the example, and the metrics shown in Figure\u00a02[href=https://www.wicell.org#fig2], we conservatively selected 3 as the optimal number of signatures, as at this rank we have a first bend in the explained variance and high values of the other metrics.\nNote: Please get from the \u2018Signatures\u2019 directory the corresponding file that should be then used in the following assignment step.\nNote: Each signature file contains a different number of signatures (identified with SIG_NUM) found in the data. In the \u2018graphical\u2019 directory, denovo.R generates pdf files with the same name as the corresponding signatures set. Those files contain a plot of the categorical distributions.\nPart 4. Assignment of existing mutational signatures (assignment.R)\nTiming: 15\u00a0min (approximate time required to perform the step on a dataset with \u223c100 samples and \u223c10 , 000 mutations, with significant variability related to bootstrap iterations)\nInput files and setup The assignment.R script requires three input files to be executed.\nList of selected SNVs [SNV_list.txt] This file is the same used in Part 3, please see above.\nSignatures file [x_signatures.txt] This file contains for each signature the frequency of substitutions. Their values are grouped by context and normalized up to 1. The file must be formatted as a table with the context as header and a row for each signature. The header must be formatted as: G>T:C>A;G>C:C>G;G>A:C>T;A>T:T>A;A>G:T>C;A>C:T>G. Notice that, for instance, variants from G to T and C to A are aggregated together in the cases of 6-context. For further details, especially for the 96-contexts representation, please refer to (Alexandrov et\u00a0al., 2013[href=https://www.wicell.org#bib1]). This file can be generated via Part 3 or can be directly passed by the user.Configuration file [assignment_config.yaml] The parameters to run assignment.R are set in a text file called as default \u2018assignment_config.txt\u2019. The file must contain a parameter in each line formatted as PARAMETER_NAMES: value The parameters that can be modified are the following: [variant selection]\nCLONAL_SNV_THR: Double [0,1]. default\u00a0= 0.9. This parameter defines whether a given SNV is considered as clonal (default\u00a0= 0.9).\nMINOR_SNV_SEL: String {\u2018always\u2019, \u2018all\u2019}. default\u00a0= \u2018always\u2019. To reduce the bias induced by the mutation transmitted and inherited among the population during the epidemic spread we select for the signature analysis only SNVs never observed with a frequency higher than CLONAL_SNV_THR in any sample. The remaining SNVs are defined as \u2018always\u00a0minor\u2019. For further details, please refer to (Graudenzi et\u00a0al., 2021[href=https://www.wicell.org#bib11]).\nMAX_SNV_SAMPLE: Integer [1, Inf]. default\u00a0= 100. In order to reduce the impact of highly mutated samples likely due to degradation of their biological isolation. We suggest removing the samples exhibiting more than a user selected number of variants.\nMIN_SNV_SAMPLE: Integer [1, Inf]. default\u00a0= 6. In order to assess the existence of statistically significant mutational signatures we have to remove all the samples with less than a given number of detected SNVs. [analysis parameters]\nBOOTSTRAP: String {\u2018yes\u2019, \u2018no\u2019}. default\u00a0= \u2018yes\u2019.\nGOODNESS_FIT_THR: Double [0.5,1]. default\u00a0= 0.95. This threshold indicates the minimum level of goodness of fit until when keep adding signatures (among the signatures.txt file) to the fit, in a given sample. The goodness of fit is measured with the cosine similarity between observed and predicted counts in each sample.\nMIN_SIG_FREQ: Double [0,1]. default\u00a0= 0.05. Each signature is considered to be present\u00a0in a given sample if its activity (alpha value) is greater than the value of this parameter.P_VALUE: threshold to be used when assessing significance of the exposure of samples to signatures. To this extent, Mann\u2013Whitney U test is performed whose results are evaluated with the given P_VALUE threshold. default\u00a0= 0.05\nNUM_ITER: Integer [1, Inf]. default\u00a0= 100\nSEED: Integer [0, Inf]. default\u00a0= 0 (with 0 the seed will be set randomly)\nN_CORE: Integer [0, Inf]. default\u00a0= 0 (with 0 the number of core available will automatically detected)\nCritical: We suggest setting the parameters in accordance with the number of contexts. Similar to Part 3 MIN_SNV_SAMPLE should be larger than the number of contexts to provide reasonable results.\nRun assignment.R\nassignment.R can be run by the following command:\n>Rscript assignment.R \\\n--SNV_list path_to_SNV_list.txt \\\n--signature path_to_signature_x.txt \\\n--config_file path_to_assignment_config.txt \\\n--output_file path_to_output_file\nThe script will generate a table in a text file specified by the user (default: assignment_result.txt). This table will have a row for each selected sample and one column for each signature, called \u201cSn_exposure\u201d. This column reports the alpha values of each signature found in each sample (i.e., a numeric value measuring the level of activity of the signature in that sample).\nIf the bootstrap procedure is performed, another column for each signature is added into file. This column, called \u2018Sn_pvalue\u2019, shows the p-value of significance of observing each signature in a given sample obtained with the harmonic mean of the one-sided (greater) Mann\u2013Whitney U-test.", "Step-by-step method details\nStep-by-step method details\nIn cellulo cross-linking\nTiming: 3 h\nThis step consists of detachment, collection of the cells and in-cellulo cross-linking.\nCell harvesting\nOnce the cells reach \u223c80%\u201390% of confluency, remove the medium and wash the cells with 5\u00a0mL of DPBS.\nDetach the cell from the flask using 1\u00a0mL of 0.05% Trypsin-EDTA (1\u00d7), phenol red.\nIncubate for 5\u00a0min at 37\u00b0C, 5% CO2.\nAdd 2\u00a0mL of complete cell culture media to inactivate Trypsin-EDTA (0.05%), phenol red.\nTransfer the cells into a centrifuge tube and centrifugate at 100\u00a0\u00d7\u00a0g for 5\u00a0min at 20\u00b0C.\nRemove supernatant and wash the cells with 5\u00a0mL of DPBS.\nPellet the cells by centrifuging at 100\u00a0\u00d7\u00a0g for 5\u00a0min at 20\u00b0C and remove the supernatant by aspiration.\nRepeat the DPBS wash two more times.\nCount the cells and aliquot to 3 million cells.\nPellet the cells by centrifuging at 100\u00a0\u00d7\u00a0g for 5\u00a0min at 20\u00b0C and aspirate the supernatant.\nKeep the dry pellet on ice.\nNote: In our experience the optimal number of cells is 3 million, this number has to be adapted and tested for other kinds of cells.\nCross-linking reaction.\nResuspend 3 million cells in 196\u00a0\u03bcL of DPBS.\nPrepare two 10\u00a0\u03bcM BSA solution as positive and negative control.\nPrepare a 50\u00a0mM stock solution of disuccinimidyl sulfoxide (DSSO) in DMSO.\nAdd 4\u00a0\u03bcL of 50\u00a0mM DSSO to the suspended cells and positive BSA control (Final DSSO concentration of 2\u00a0mM).\nIncubate at 37\u00b0C with continuous shaking (10 RPM) for 1 h.\nTo quench the reaction, add 10\u00a0\u03bcL of Tris-HCl 1.5 M.\nIncubate for 30\u00a0min at 20\u00b0C with continuous shaking (10 RPM).\nCentrifuge at 2,000\u00a0\u00d7\u00a0g for 10\u00a0min at 4\u00b0C. Remove the supernatant.Wash the cells with 200\u00a0\u03bcL of ice-cold DPBS.\nPellet the cells by centrifuging at 100\u00a0\u00d7\u00a0g for 5\u00a0min at 4\u00b0C and remove as much supernatant as possible.\nNote: It is recommended to use a BSA cross-linking positive control (96\u00a0\u03bcL of 10\u00a0\u03bcM BSA). Negative control is performed using 4\u00a0\u03bcL of DMSO instead of DSSO. The optimal working final concentration of DSSO is between 1\u20135\u00a0mM.\nCritical: Cross-linkers are moisture sensitive. Prepare these cross-linkers immediately before use. Use amine-free buffers (PBS, 20\u00a0mM HEPES, 100\u00a0mM carbonate/biocarbonate, or 50\u00a0mM borate). Cross-linking reactions (acylation) are favored near neutral pH (pH 6\u2013\u200a9) and with concentrated protein solutions.\nSubcellular protein fractionation of cross-linked and non-cross-linked cells.\nTiming: 3\u00a0days\nThe following methodology describes the steps of subcellular protein fractionation after the cross-linking reaction. This methodology is based on the instructions from the Subcellular Protein Fractionation Kit for Cultured Cells (Thermo Scientific, Cat# 78840). For more details and troubleshooting, please refer to the manual on Thermo website[href=https://www.thermofisher.com/document-connect/document-connect.html?url=https://assets.thermofisher.com/TFS-Assets%2FLSG%2Fmanuals%2FMAN0011667_Subcellular_Protein_Fraction_CulturedCells_UG.pdf].\nDMSO control of cells are treated according to the same protocol. These controls are key to determining the experimental subcellular location of the AltProts.\nOther subcellular fractionation kits in the market are Abcam\u2019s Cell Fractionation Kit - Standard (Cat# ab109719) and Cell Signaling\u2019s Cell Fractionation Kit (Cat# 9038). These kits are designed to fractionate the cells in three subcellular fractions which will not decrease the complexity of the samples as much.\nNote: Thaw all buffers using a 20\u00b0C water bath. Keep CEB, MEB, and NEB buffers on ice until use. Use a rotary shaker to avoid clumping of insoluble material during incubations.\nCritical: Immediately before use, add Thermo Scientific Halt Protease Inhibitor Cocktail at a 1:100 dilution into each volume of buffer required. Keep all protein extracts on ice.Subcellular Protein Fractionation of 3 million cells (Figure\u00a01[href=https://www.wicell.org#fig1]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2778-Fig1.jpg\nFigure\u00a01. Subcellular protein fractionation workflow\nPelleted cells, both cross-linked and non-cross-linked, are resuspended in CEB buffer. After incubation and centrifugation, the resulting cytoplasmic extract is removed and stored. The remaining pellet is then retaken in MEB\u00a0buffer, incubated, and centrifuged to obtain the membrane extract, which is also removed and stored. Next, the\u00a0pellet is resuspended in NEB buffer, incubated, and centrifuged to obtain the nuclear extract, which is similarly\u00a0removed and stored. The pellet from the previous step is then retaken in CBEB buffer, incubated, and centrifuged to obtain the chromatin-bound extract, which is also removed and stored. Finally, the remaining pellet is taken back in PEB buffer, incubated, and centrifuged to obtain the cytoskeletal extract, which is also removed and stored.\nLyse the cells by adding 300\u00a0\u03bcL of Cytoplasmic Extraction Buffer (CEB). Incubate at 4\u00b0C for 10\u00a0min with gentle shaking (10 RPM).\nCentrifuge at 2,000\u00a0\u00d7\u00a0g for 5\u00a0min. Aspirate by pipetting and immediately transfer the supernatant (cytoplasmic extract) to a clean, pre-chilled (4\u00b0C in ice) 1.5\u00a0mL microcentrifuge tube.\nResuspend the pellet in 300\u00a0\u03bcL of ice-cold Membrane Extraction Buffer (MEB). Vortex for 5\u00a0s and incubate at 4\u00b0C for 10\u00a0min with gentle shaking (10 RPM).\nCentrifuge at 5,000\u00a0\u00d7\u00a0g for 5\u00a0min. Aspirate and immediately transfer the supernatant (membrane extract) to a clean, pre-chilled 1.5\u00a0mL microcentrifuge tube.\nAdd 150\u00a0\u03bcL of ice-cold Nuclear Extraction Buffer (NEB). Roughly vortex (highest vortex setting) for 15\u00a0s and incubate at 4\u00b0C for 30\u00a0min with gentle shaking (10 RPM).Note: During the 30-min incubation time, prepare the Chromatin-Bound Extraction Buffer (CBEB) by adding 15\u00a0\u03bcL of 100\u00a0mM CaCl2 and 9\u00a0\u03bcL of Micrococcal Nuclease (300 units) in 150\u00a0\u03bcL of 20\u00b0C NEB.\nCentrifuge at 7,000\u00a0\u00d7\u00a0g for 5\u00a0min. Aspirate and immediately transfer the supernatant (nuclear extract) to a clean, pre-chilled 1.5\u00a0mL microcentrifuge tube.\nResuspend the pellet in 150\u00a0\u03bcL of 20\u00b0C CBEB. Roughly vortex for 15\u00a0s and incubate at 20\u00b0C for 15\u00a0min with gentle shaking (10 RPM).\nAfter incubation, roughly vortex 15\u00a0s and centrifuge at 16,000\u00a0\u00d7\u00a0g for 5\u00a0min. Aspirate and immediately transfer the supernatant (chromatin-bound extract) to a clean, pre-chilled 1.5\u00a0mL microcentrifuge tube.\nAdd 150\u00a0\u03bcL of Pellet Extraction Buffer (PEB) to the remaining pellet. Roughly vortex for 15\u00a0s and incubate at 20\u00b0C for 10\u00a0min with gentle shaking (10 RPM).\nAfter incubation, roughly vortex for 15\u00a0s and centrifuge at 16,000\u00a0\u00d7\u00a0g for 5\u00a0min. Aspirate and immediately transfer the supernatant (cytoskeletal extract) to a clean pre-chilled 1.5\u00a0mL microcentrifuge tube.\nAliquot 10\u00a0\u03bcL of each extract for SDS-PAGE and western blotting.\nNote: Performing protein quantification employing Thermo Scientific Pierce BCA Protein Assay (Cat# 23225) is recommended to calculate the correct protein: protease ratio, for the sequential enzymatic digestion.\nKeep the extracts in ice or for long-term storage keep them at \u221280\u00b0C.\nCross-linking and subcellular fractionation confirmation.\nMix 10\u00a0\u03bcL of 2\u00d7 Laemmli buffer with the 10\u00a0\u03bcL protein aliquot.\nLoad each sample of subcellular protein fraction and BSA on to a 4%\u201312% SDS-PAGE gel.\nMigrate the gels for 15\u00a0min at 70\u00a0V and for 90\u00a0min at 120\u00a0V in Tris-glycine-SDS buffer.\nAfter migration, stain the gels with PageBlue\u2122 Protein Staining Solution (Coomassie blue) for 1 h.Destain the gels by discarding the excess staining solution. Rince the gels two times with water.\nWash the gel for 16\u00a0h in an orbital shaker at 60 RPM. Placing a folded Kimwipes Tissue in the container to absorb excess dye will accelerate the destaining process.\nVisualize the destained gels using your preferred system (Figure\u00a02[href=https://www.wicell.org#fig2]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2778-Fig2.jpg\nFigure\u00a02. Cross-linking reaction and subcellular fractionation confirmation\n(A) The Coomassie blue stained SDS-PAGE displays each cross-linked subcellular fraction, compared to a non-cross-linked fraction. BSA cross-linked and not cross-linked are used as controls. Red arrows display cross-linked signals demonstrating that the reaction takes place.\n(B) The subcellular fractionation was confirmed by Western blot with compartment specific markers. The cytoplasm fraction showed the presence of HSPA1A signal. Calreticulin signal was detected at chromatin, cytoskeleton, and exhibited a stronger signal at the membrane-bounded fraction. SP1 was observed in the nucleus and cytoskeleton, while Histone H3 was found in chromatin and cytoskeleton. Similarly, Cytokeratin 18 was detected in the nucleus and cytoskeleton. These findings are consistent with the results reported in UniProtKB, COMPARTMENTS, and the literature.\nNote: Only non-cross-linked samples will continue to the western blot analysis.\nCritical: Use one membrane with the five subcellular protein fractions for one compartment specific primary antibody.\nTransfer the gels onto a 0.45\u00a0\u03bcm nitrocellulose membrane. Employ a tank transfer system for 2\u00a0h at 290 mA in Towbin buffer.\nWash the membranes for 5\u00a0min in an orbital shaker three times with 20\u00a0mL of 1\u00d7 TBS-T buffer.\nBlock the membranes for 1\u00a0h in an orbital shaker with 20\u00a0mL of milk blocking solution.\nMeanwhile, prepare the primary antibody dilution in milk blocking solution. The concentration of antibody has been adjusted as following: Cytokeratin 18 (1/1000), SP1 (1/200), Histone H3 (1/1000), Hsp70 (1/1000) and Calreticulin (1/200).Critical: Depending on the antibody\u2019s supplier, the dilution of it must be adjusted.\nIncubate each membrane for each antibody for 16\u00a0h at 4\u00b0C in an orbital shaker.\nAfter incubation, wash the membrane for 5\u00a0min in an orbital shaker three times with 20\u00a0mL of 1\u00d7 TBS-T buffer.\nIncubate for 1\u00a0h with the matched HRP anti-Chicken (1/5000) or anti-Mouse (1/5000) secondary antibody.\nAfter incubation, wash the membrane for 5\u00a0min in an orbital shaker three times with 1\u00d7 TBS-T buffer.\nPerform the horseradish-peroxidase reaction, by preparing the SuperSignal\u2122 West Dura Extended Duration Substrate (1\u00a0mL of Luminol/Enhancer Solution and 1\u00a0mL of Stable Peroxide Solution).\nIncubate the membrane with the substrate working solution for 5\u00a0min in the dark.\nRemove the membrane from the substrate working solution and place it in a plastic sheet protector.\nRemove the excess liquid with an absorbent tissue pressing out bubbles.\nScan the membranes using the Invitrogen iBright Imaging System or other compatible imaging system (Figure\u00a02[href=https://www.wicell.org#fig2]B).\nMode: Chemi Blots.\nExposure mode: Normal.\nAfter the autoexposure, the exposure time adjusted for each membrane: Cytokeratin 18 (5085\u00a0ms), SP1 (30 s), Histone H3 (4106\u00a0ms), Hsp70 (8213\u00a0ms) and Calreticulin (10 s).\nResolution: 4\u00a0\u00d7\u00a04.\nOptical zoom: 1\u00d7.\nDigital zoom: 2\u00d7.\nFocus Level: 220.\nSensitivity: Frame 1:100.\nSequential enzymatic digestion\nTiming: 2\u00a0days\nThe subsequent steps described the filter aided sample preparation (FASP)11[href=https://www.wicell.org#bib11] sequential enzymatic digestion using LysC/Trypsin followed by Chymotrypsin for cross-linked samples. A 50\u00a0kDa cut-off Amicon filter is suggested to eliminate as many as possible non-cross-linked proteins.\nFASP and sequential digestion\nConcentrate the five subcellular fractions in the 50\u00a0kDa Amicon filter by centrifugation at 4\u00b0C for 15\u00a0min at 14,000\u00a0\u00d7\u00a0g. As a result, a 20\u00a0\u03bcL protein concentrate will remain as dead volume in the filter.Add 80\u00a0\u03bcL of denaturing buffer to the filter and pipette up and down gently inside the filter.\nAdd 100\u00a0\u03bcL of reduction buffer.\nIncubate at 56\u00b0C for 40\u00a0min.\nNote: Don\u2019t incubate at 95\u00b0C. At above 60\u00b0C urea can produce protein carbamylation. Additionally, the filter could melt.\nCentrifuge 15\u00a0min at 14,000\u00a0\u00d7\u00a0g.\nAdd 200\u00a0\u03bcL of denaturing buffer and centrifugate 15\u00a0min at 14,000\u00a0\u00d7\u00a0g.\nRepeat step 5e at least two times.\nAdd 100\u00a0\u03bcL of alkylation buffer.\nIncubate for 20\u00a0min at 20\u00b0C in the dark.\nCentrifugate 15\u00a0min at 14,000\u00a0\u00d7\u00a0g.\nAdd 200\u00a0\u03bcL of ammonium bicarbonate buffer and centrifugate 15\u00a0min at 14,000\u00a0\u00d7\u00a0g.\nRepeat the previous step at least two times.\nAdd Trypsin/Lys-C Mix Mass Spec Grade to the vendor recommended 25:1 protein: protease ratio (w/w). Incubate for 16\u00a0h at 37\u00b0C.\nAfter incubation, add Chymotrypsin, Sequencing Grade at a 100:1 protein: protease ratio (w/w). Incubate for 4\u00a0h at 20\u00b0C.\nPlace the Amicon filter into a new clean tube.\nAdd 50\u00a0\u03bcL of ammonium bicarbonate buffer and centrifugate 15\u00a0min at 14,000\u00a0\u00d7\u00a0g.\nRepeat the previous step.\nDiscard the Amicon filter.\nAcidify the filtered peptides with TFA 1% until pH\u00a0<\u00a07.\nVacuum dry the samples in a SpeedVac concentrator and store them at \u221220\u00b0C if needed.\nThe membrane fraction contains a large amount of polymer. It requires the use of a HiPPR\u2122 Detergent Removal Resin column (Thermo Scientific, Cat# 88305) following the vendor\u2019s protocol[href=https://www.thermofisher.com/document-connect/document-connect.html?url=https://assets.thermofisher.com/TFS-Assets%2FLSG%2Fmanuals%2FMAN0011743_HiPPR_Detergent_Remov_Resin_UG.pdf] to be compatible for MS analysis.\nNote: \u221280\u00b0C is recommended for long term sample storage.\nNanoLC-MS/MS analysis\nTiming: 1\u00a0week\nThe following section describes the parameters used in the nanoLC-MS/MS sample analysis and shotgun protein interrogation of non-cross-linked samples. We recommend the use of Sequest HT12[href=https://www.wicell.org#bib12] search algorithm at Thermo Fisher\u2019s Proteome Discoverer.NanoLC-MS/MS\nResuspend the dried samples in 0.1% TFA.\nDesalt the peptides using C18 resin ZipTips.\nNote: we recommend following the protocol described in the product insert (Merck Cat# ZTC18S096). Another alternative is to use Affinisep AttractSPE\u00aeTips - C18, 200\u03bcL (Cat #Tips-C18.T1.200.96); Thermo Fisher Pierce\u2122 C18 Tips (Cat # 87782), or home-made stage tips based on C18 membrane.\nVacuum dry the desalted peptides.\nResuspend in 20\u00a0\u03bcL of ACN/0.1% FA (2:98, v/v) and then transfer to a clean autosampler vial.\nInject 5\u00a0\u03bcL of sample onto the nanoLC-MS/MS system.\nAnalyze samples on a nanoAcquity (Waters) coupled to a Q Exactive mass spectrometer (Thermo Fisher Scientific).\nThe injected sample is trapped on a ACQUITY UPLC M-Class Symmetry C18 Trap Column (100\u00a0\u00c5, 5\u00a0\u03bcm, 180\u00a0\u03bcm\u00a0\u00d7\u00a020\u00a0mm, 2G, V/M, Waters Part No: 186007496).\nThe peptides are separated on a ACQUITY UPLC M-Class Peptide BEH C18 Column (75\u00a0\u03bcm\u00a0\u00d7\u00a0250\u00a0mm, 1.7\u00a0\u03bcm, 130\u00a0\u00c5, Waters Part No: 186007484) with a flow of 300 nL/min.\nMobile phase A is ultrapure water, 0.1% formic acid, while mobile phase B is acetonitrile, 0.1% formic acid.\nThe eluent gradient is set to go from 5% to 20% of mobile phase A in 100\u00a0min, then from 20% to 30% in 20\u00a0min, and finally to 90% in 20\u00a0min.\nSettings for MS and MS/MS acquisitions (Figure\u00a03[href=https://www.wicell.org#fig3]): range is m/z 300\u20131,600, resolution 70,000 at FWHM (m/z 400), positive mode, AGC target of 3\u00a0\u00d7\u00a0106 and stepped NCE of 21, 24 and 30.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2778-Fig3.jpg\nFigure\u00a03. Settings for MS and MS2[href=https://www.wicell.org#bib2] acquisition method\nParameters used for data dependent acquisition method.MS/MS spectra are acquired using a Top-10 DDA (data-dependent acquisition) method, with a resolution of 35,000 FWHM. Dynamic exclusion is enabled, and only MS/MS spectra from peptide ions with charge states between\u00a0+2 and\u00a0+8 are selected.\nShotgun data analysis of non-cross-linked samples\nNote: as previously described, the databased OpenProt[href=https://www.openprot.org/]5[href=https://www.wicell.org#bib5] can be oversized for some identification nodes, we recommend the use of the limited AltProt database[href=https://www.openprot.org/download/files/1.6/human-openprot-r1_6-refprots+altprots+isoforms_min_1_pep-+uniprot2019_03_01.fasta.zip] with at least 1 identification in other MS data or Riboseq analysis.5[href=https://www.wicell.org#bib5] According to size limitation we recommend the use of SequestHT (Thermo ProteomeDiscoverer V2.5) which is not size limited.\nTwo different databases and consensus steps are employed to analyze AltProts (in a FASTA file combined AltProt, new isoforms and RefProt) and RefProts alone. The common consensus and processing parameters are enumerated at 7a. Specific parameters for RefProts (7b) and AltProts (7c) are displayed below.\nAnalyze the RAW LC-MS/MS data using Proteome Discoverer V2.5 (Thermo Fisher Scientific) with the Sequest HT search engine.\nSelect LysC-trypsin and chymotrypsin as cleaving enzymes and 2 possible missed cleavages.\nVariable modifications: methionine oxidation and protein N-terminus acetylation.\nStatic modifications: carbamidomethylation of cysteines.\nMinimum peptide length: six amino acids.\nMinimum precursor tolerance: 10 ppm.\nPSM and peptide validator: between 0.01 and 0.05 FDR.\nFragment mass tolerance: 0.02 Da.\nValidation is done with Percolator using strict FDR\u00a0= 0.01 and relaxed FDR\u00a0= 0.05\nFor RefProts identification.\nProtein database: UniProtKB[href=https://www.uniprot.org/uniprotkb?facets=model_organism%3A9606&query=%2A] v.2022_02 reviewed and unreviewed. (77,895 sequences, downloaded from Uniprot website, 25 feb 2022)\nAt least two peptides per sequence.\nFor AltProts identification.\nProtein database: Homo sapiens OpenProt v1.6 (184,706 sequences), containing RefProts and predicted AltProts detected in mass spectrometry experiments with at least one unique peptide.\nAt least one peptide per sequence.\nCritical: To eliminate false positives, use protein BLASTP[href=https://blast.ncbi.nlm.nih.gov/Blast.cgi]13[href=https://www.wicell.org#bib13]The basic parameters of BlastP are preserved (or the automated adaptation for small protein) and the database used for the comparison is \"non-redundant protein sequences\". An AltProt is considered \"false positive\" if it presents a sequence homology (identity+coverage)\u00a0>\u00a080%, ideally no identification should be detected, if an alignment <80% homology is identified, it should be checked that the peptide/PSM identified in MS is specific to the AltProt.\nAdditionally, check the identified PSMs of the AltProt with the NextProt Peptide uniqueness checker[href=https://www.nextprot.org/tools/peptide-uniqueness-checker]14[href=https://www.wicell.org#bib14] tool. Parameters from the NextProt uniqueness tool have been kept unchanged and the expected result is no sequence homology for the specific peptide of the AltProt previously identified and tested.\nCross-link data analysis and interaction modeling\nTiming: 1\u00a0week\nTo identify the cross-links, we employed the XlinkX15[href=https://www.wicell.org#bib15],16[href=https://www.wicell.org#bib16],17[href=https://www.wicell.org#bib17] node in Proteome Discoverer. Additionally, the validation of the cross-links can be performed by docking the protein-protein interactions (PPIs) and measuring the distances between the residues involved in each cross-link. Here, we describe the modeling of the 3D structure of AltProts and docking them to the RefProts to which they were cross-linked.\nCross-linking identification\nAnalyze the RAW LC-MS/MS data using Proteome Discoverer V2.5 (Thermo Fisher Scientific) with the Sequest HT search engine at the processing step (Figure\u00a04[href=https://www.wicell.org#fig4]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2778-Fig4.jpg\nFigure\u00a04. Proteome Discoverer cross-link identification workflow\n(A and B) (A) Displays the processing step used to identify cross-links. The parameters used in the Sequest HT are displayed in the (B)\u00a0panel. (B) The parameters used at Sequest HT and XlinkX/PD search are displayed.\n(C) Shows the consensus step for cross-link identification.\nProtein database: Homo sapiens OpenProt v1.6, which contains RefProts and predicted AltProts detected in mass spectrometry experiments with at least one unique peptide.\nSelect LysC-trypsin and chymotrypsin as cleaving enzymes and allow for 2 possible missed cleavages.Set minimum peptide length to six amino acids and at least one peptide per sequence.\nSet minimum precursor tolerance to 10 ppm.\nSet fragment mass tolerance to 0.02 Da.\nMaximum equal modifications per peptide: 3\nMaximum dynamic modifications per peptide: 4\nVariable modifications: methionine oxidation and N-terminus acetylation, DSSO amidated, hydrolyzed, and Tris form.\nStatic modification: carbamidomethylation of cysteines.\nSet the Target Decoy PSM Validator:\nTarget/decoy selection: concatenated\nFDR set between 0.01 and 0.05.\nSet a spectrum confidence filter: worse than high.\nDetect the cross-links using the XlinkX/PD Detect node in Proteome Discoverer V2.5 with the following parameters (Figure\u00a04[href=https://www.wicell.org#fig4]B):\nSet acquisition strategy as MS2.\nSet DSSO (158.0037 Da) as cross-linker.\nSet the following parameters at the XlinkX/PD Search node:\nSet same parameters as the Sequest HT node.\nPrecursor mass tolerance of 10 ppm.\nFTMS fragment of 20 ppm.\nITMS fragment of 0.5 Da.\nSet XlinkX/PD Validator to FDR: 0.05.\nAt the consensus step set a peptide validator node with a target FDR for PSMs and peptides between 0.01 to 0.05 (Figure\u00a04[href=https://www.wicell.org#fig4]C).\nAdd a Peptide and Protein Filter with the next parameters:\nPeptide Confidence At Least: High\nMinimum Number of Peptide Sequences: 1\nAt the XlinkX/PD Consensus Validator set the cross-link spectrum match (CSM) and cross-link FDR threshold as 0.05.\nPerform manual curation of the identified cross-links:\nVerify the quality of the CSMs. The cross-linking, b and y ions should be visible and describe the amino acid sequence of the two peptides identified in the CSM. A clear example can be observed at Garcia-del Rio et\u00a0al.1[href=https://www.wicell.org#bib1]\nNote: As cross-linking technology has been evolving in the last 20 years, a community-wide effort has been done to development of methodological standards which are available for the reader18[href=https://www.wicell.org#bib18],19[href=https://www.wicell.org#bib19],20[href=https://www.wicell.org#bib20]\nEliminate the cross-link spectrum matches that involved N-terminal residues.Critical: Verify that the peptides identified at the CSMs correspond to the attributed proteins using the NextProt peptide uniqueness checker tool.\nModeling and prediction of interactions between AltProts and RefProts (Figure\u00a05[href=https://www.wicell.org#fig5]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2778-Fig5.jpg\nFigure\u00a05. Cross-link interaction modeling workflow and result of the interaction found between H3F3A and the AltProt IP_6276699\n(A) Workflow used to generate an interaction model. H3F3A is identified cross-linked to IP_627699 in MS analysis. H3F3A 3D model is obtained from Alphafold databased. IP_627699 FASTA sequence is obtained from OpenProt and is used to generate a 3D model at I-Tasser. Then the two models are docked in ClusPro. Finally, the best interaction model processed, the distances between the cross-linked residues are measured.\n(B) 3D ribbon model showing the interaction between H3F3A (blue) and IP_627699 (orange). The cross-linked residues are displayed in red and the distance of the interaction [20.82\u00a0\u00c5], confirmed the possible cross-link identification as its fits in the restricted distance of DSSO [5 to 30\u00a0\u00c5].\nRetrieve the AltProts sequences from OpenProt database.\nGenerate the 3D models at I-TASSER[href=https://seq2fun.dcmb.med.umich.edu/I-TASSER/about.html] (Iterative Threading ASSEmbly Refinement).21[href=https://www.wicell.org#bib21]\nNote: I-TASSER generates five models with the lowest free energy and highest confidence, and the first model usually has the highest score and better quality. However, lower-ranked models might have better quality. For more information, visit the I-TASSER server website.\nDownload the Alphafold[href=https://alphafold.ebi.ac.uk/]22[href=https://www.wicell.org#bib22] or PDB[href=https://www.rcsb.org/]23[href=https://www.wicell.org#bib23] 3D structures of the RefProts involved in the cross-links.\nFor RefProt-AltProt docking ClusPro[href=https://cluspro.bu.edu/login.php]24[href=https://www.wicell.org#bib24] tool is used, submit the RefProt as a receptor and the AltProt as a ligand at the ClusPro protein-protein docking server. Do not use any restraints in the docking.\nAfter the docking is finished, display all the balanced models, and download them. Additionally, download the coefficients for these models.Note: If you do not have any prior knowledge of what forces dominate in your complex, use the balanced coefficients models.\nOpen the complex in YASARA[href=http://www.yasara.org/]25[href=https://www.wicell.org#bib25] view and identify the number of the atoms involved in the cross-linked complex.\nTo verify the cross-linking distance, use the command:\n>DistanceAtomA,AtomB,bound=No\nIf the distance corresponds to constrains of DSSO, use the following command to label and join both cross-linked atoms (Figure\u00a05[href=https://www.wicell.org#fig5]B):\n>LabelDisAtomA,AtomB,Format=DIS,Height=0.7,Color=Black,X=0.0,Y=0.0,Z=0.0,bound=Yes\nNote: For DSSO, the distances described in the literature are from 5.3\u00a0\u00c526[href=https://www.wicell.org#bib26] to 30\u00a0\u00c5.27[href=https://www.wicell.org#bib27] Other molecular viewers such as Pymol[href=https://pymol.org/2/]28[href=https://www.wicell.org#bib28] or ChimeraX[href=https://www.cgl.ucsf.edu/chimerax/]29[href=https://www.wicell.org#bib29] can also be used.\nCross-linking network analysis\nTiming: 1\u00a0week\nFor the visualization and gene ontology (GO) enrichment of the network obtained by cross-link identification, we recommend the use of Cytoscape[href=https://cytoscape.org/].30[href=https://www.wicell.org#bib30] Additional apps have to be downloaded at Cytoscape App Store[href=https://apps.cytoscape.org/]: STRING[href=https://apps.cytoscape.org/apps/stringapp],31[href=https://www.wicell.org#bib31] ClueGO[href=https://apps.cytoscape.org/apps/cluego],32[href=https://www.wicell.org#bib32] CluePedia[href=https://apps.cytoscape.org/apps/cluepedia]33[href=https://www.wicell.org#bib33] and yFiles Layout Algorithms[href=https://apps.cytoscape.org/apps/yfileslayoutalgorithms].\nCritical: Before starting your network analysis, we recommend performing the Cytoscape and ClueGo tutorials found on their websites. This will provide you with a general panorama of the commands, formatting options, and different analyses that can be performed in the software.\nPPIs network treatment.\nExport the cross-links identified from Proteome Discoverer 2.5 as an Excel file (Figure\u00a06[href=https://www.wicell.org#fig6]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2778-Fig6.jpg\nFigure\u00a06. Cytoscape and ClueGo interface windows\n(A) Displays how to import a cross-linking network from a file of identification by Proteome Discoverer 2.5. Additionally, it shows how to label the columns as target and source nodes during this process.\n(B) Presents the control panel of the ClueGo app at Cytoscape. The loading marker square, where to load the query protein list, is highlighted in yellow.\nOpen the file and split the column description to obtain the gene symbols of the proteins identified in cross-link.\nWrite in two new columns (Gene A and Gene B) the gene annotations.Import this network from the file to Cytoscape.\nAssign the source node to the Gene A column and the target node to Gene B column.\nSelect the network (Figure\u00a07[href=https://www.wicell.org#fig7]A) and STRINGify it (STRING App).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2778-Fig7.jpg\nFigure\u00a07. Cross-linking network evolution during the processing steps\n(A) Shows a raw cross-linking network after import.\n(B) Stringified cross-linking network. Note that the AltProts are not in the STRING format, meaning they are not indexed at string database.\n(C) Displays only the interaction between the RefProts without cross-links. The enriched nodes are shown in gray STRING nodes.\n(D) After merging B and C, all the interactions are displayed. Formatting is done for the nodes and edges.\n(E) Presents a ClueGo enriched network. Only GO term nodes are displayed, the protein nodes are kept hidden.\n(F) Resulting network after merging D and E. Combined the information of enriched proteins, query RefProts and AltProt, connected to the GOterm. As well as the cross-link identified interactions, StringDB and other databased enriched interaction existing between the proteins of the network.\nNote: Verify that all RefProt nodes are now STRING nodes. If not (plain gray nodes at Figure\u00a07[href=https://www.wicell.org#fig7]B), verify that there are no spaces in the accession numbers or genes. For Cytoscape a space after the gene/accession creates a duplicate non-referenced node.\nOnce all the RefProts have a STRING node, copy the column of accession numbers, and input them in the STRING protein query. This step will identify the already described interactions between the RefProts (Figure\u00a07[href=https://www.wicell.org#fig7]C).\nNote: If there are RefProt nodes that do not have any interactors, select them one by one and add known interactors in the STRING menu (Figure\u00a07[href=https://www.wicell.org#fig7]C, bright gray STRING nodes).\nMerge the networks.Note: The resulting merged network (Figure\u00a07[href=https://www.wicell.org#fig7]D) will simplify the redundant cross-links. We recommend formatting this network to visualize the interactions of interest.\nCross-validate the RefProt interactions in other databases. We recommend BioGrid[href=https://thebiogrid.org/]34[href=https://www.wicell.org#bib34] and IntAct[href=https://www.ebi.ac.uk/intact/home].35[href=https://www.wicell.org#bib35]\nGO term enrichment (Figure\u00a06[href=https://www.wicell.org#fig6]B).\nOpen ClueGo App.\nSelect the functional analysis as analysis mode.\nInput the whole list of RefProt genes at the load marker list box.\nSelect the ontologies/pathways to use for the enrichment.\nSelect the specificity of the network.\nEnable the GO term fusion.\nRun the app.\nMerge the ClueGO network (Figure\u00a07[href=https://www.wicell.org#fig7]E) and the merged STRING network (Figure\u00a07[href=https://www.wicell.org#fig7]D).\nAt the resulting network (Figure\u00a07[href=https://www.wicell.org#fig7]F) select the preferred network layout and edit the node\u2019s properties. We recommend formatting the network in a way that you can obtain the information you need.", "Step-by-step method details\nStep-by-step method details\nThe package includes one initiation step, and seven major analysis steps. User can execute each analysis step with one exported function that calls a group of internal functions for data process, data analysis and plotting to achieve a set of analyses. This package organizes all functions related to each analysis step together as one R script under \u223c/eIF4F.analysis/R (file names in bold, Figure\u00a02[href=https://www.wicell.org#fig2]E). Although data process, analysis and result plotting functions are set as internal functions, users can access their source code from R scripts and modify the input parameter outside this package for their own usages. The following section explains the definition and organization of each function. The detailed documentation of all (exported and internal) functions within the package is available at the following weblink: https://a3609640.github.io/eIF4F.analysis/reference/index.html[href=https://a3609640.github.io/eIF4F.analysis/reference/index.html].\nAnalysis.R contains ten exported functions in the package to initialize package and to execute all analyses presented in Wu and Wagner (2021).1[href=https://www.wicell.org#bib1] Users can simply execute the following command in RStudio to get\u00a0all analyses performed and results stored under \u223c/eIF4F.analysis/eIF4F_output.\n> source(\"\u223c/eIF4F.analysis/Script/Analysis.R\")\nStep-1: Library initialization\nTiming: \u00a0<\u00a05\u00a0min\nInitialization processes rely on three exported functions to define subdirectories for output data, to define the graphic formats (font size and style), and to load data from downloaded data files. The definitions of three initialization functions were stored in \u223c/eIF4F.analysis/R/Load.R.\nRun the following command line to create the output directories to store the output files.\n> initialize_dir()\nNote: initialize_dir() creates sub-directories under \u223c/eIF4F.analysis/eIF4F_output (Figure\u00a02[href=https://www.wicell.org#fig2]F) to store the output results for each analysis step.\nRun the following command line to create variables that define font type, size, and orientation for plotting.\n> initialize_format()\nRun the following command line to acquire omics datasets from the download data files.\n> initialize_data()Note: initialize_data() is a wrapper of five data initialization functions that import, trim, clean up and merge datasets from the download data files. These data initialization functions have side effects that populate twelve global variables that process and merge omics and annotation data by sample IDs for the following analysis steps. The data contained by these global variables are available in the form of data frames, as input for our analysis functions, but do not show on the user\u2019s workspace. For users to access them, the data initialization step saves twelve global variables as csv files under \u223c/eIF4F.analysis/eIF4F_output/ProcessedData folder (Figure\u00a02[href=https://www.wicell.org#fig2]G).\nAlternatives: Five individual data initialization functions are also accessible to users for execution. Instead of running the wrapper function initialize_data() to generate all twelve global variables at once, users can generate global variables related to each analysis function with the following individual data initialization functions.\n> initialize_cnv_data()\n> initialize_RNAseq_data()\n> initialize_survival_data()\n> initialize_proteomics_data()\n> initialize_phosphoproteomics_data()\ninitialize_cnv_data() reads all CNV related datasets from TCGA, with a few internal functions. The implementation details of each operation are in \u223c/eIF4F.analysis/R/CNV.R. initialize_cnv_data() sets the values of three global variables TCGA_CNV_value, TCGA_CNV_sampletype and TCGA_CNVratio_sampletype for CNV analysis (step-2) and stores them as \u201cTCGA_CNV_value.csv\u201d, \u201cTCGA_CNV_sampletype.csv\u201d and \u201cTCGA_CNVratio_sampletype.csv\u201d under the ProcessedData folder.\nTCGA_CNV_value contains the unthresholded CNV value data of tumors from all TCGA cancer types combined, and comes from the dataset, \"Gistic2_CopyNumber_Gistic2_all_data_by_genes\".\nTCGA_CNV_sampletype contains the threshold CNV dataset of tumors from all combined TCGA cancer types. It comes from two datasets: \"Gistic2_CopyNumber_Gistic2_all_thresholded.by_genes\", and the annotation dataset \"TCGA_phenotype_denseDataOnlyDownload.tsv\".\nTCGA_CNVratio_sampletype contains the data of CNV ratios in tumors vs. adjacent normal tissues from individual TCGA cancer types. It comes from two datasets: \"broad.mit.edu_PANCAN_Genome_Wide_SNP_6_whitelisted.gene.xena\", and the annotation dataset, \"TCGA_phenotype_denseDataOnlyDownload.tsv\".initialize_RNAseq_data() reads the recomputed RNAseq data from both TCGA and GTEx. The implementation details of each operation are within the \u223c/eIF4F.analysis/R/DEG.R file. initialize_RNAseq_data() sets one global variable TCGA_GTEX_RNAseq_sampletype for the gene expression analysis (step-3), PCA (step-5) and correlating gene analysis (step-6), and stores TCGA_GTEX_RNAseq_sampletype as \u201cTCGA_GTEX_RNAseq_sampletype.csv\u201d in the ProcessedData folder.\nTCGA_GTEX_RNAseq_sampletype comes from the recomputed RNAseq dataset from both TCGA and GTEx, \"TcgaTargetGtex_RSEM_Hugo_norm_count\", and the annotation dataset \"TcgaTargetGTEX_phenotype.txt\".\ninitialize_survival_data() reads the RNAseq and patient survival data from TCGA.\u00a0The implementation details of this operation are within the \u223c/eIF4F.analysis/R/Survival.R file. This function sets the global variable TCGA_RNAseq_OS_sampletype for survival analysis (step-4), and store TCGA_RNAseq_OS_sampletype as \u201cTCGA_RNAseq_OS_sampletype.csv\u201d inside the ProcessedData folder.\nTCGA_RNAseq_OS_sampletype comes from three datasets: the RNAseq dataset \"EB++AdjustPANCAN_IlluminaHiSeq_RNASeqV2.geneExp.xena\", the survival dataset \"Survival_SupplementalTable_S1_20171025_xena_sp\" and the annotation dataset \"TCGA_phenotype_denseDataOnlyDownload.tsv\".\ninitialize_proteomics_data() reads the proteomics related data from CCLE and CPTAC LUAD, including: proteomics data, annotation data for cancer types, RNAseq data for the correlation analysis on protein RNA levels (step-7 and step-8). The implementation details of this operation are within the \u223c/eIF4F.analysis/R/RNAProCorr.R file.\nThis function sets three global variables for the CCLE data: (1) CCLE_RNAseq contains the\u00a0RNAseq data derived from \"CCLE_expression_full.csv\", (2) CCLE_Anno contains the\u00a0annotation data derived from \"sample_info.csv\", and (3) CCLE_Proteomics contains\u00a0the protein expression level data derived from \"protein_quant_current_normalized.csv\". This function stores CCLE_RNAseq, CCLE_Anno and CCLE_Proteomics as \u201cCCLE_RNAseq.csv\u201d, \u201cCCLE_Anno.csv\u201d, and \u201cCCLE_Proteomics.csv\u201d inside the ProcessedData folder.\nThis function sets two global variables as data frames for the CPTAC LUAD data published in Gillette et\u00a0al. (2020)7[href=https://www.wicell.org#bib7]: (1) CPTAC_LUAD_Proteomics contains proteomics data from \"Protein.xlsx\", and (2) CPTAC_LUAD_RNAseq contains RNAseq data from \"RNA.xlsx\". This function stores CPTAC_LUAD_Proteomics and CPTAC_LUAD_RNAseq as \u201cCPTAC_LUAD_Proteomics.csv\u201d and \u201cCPTAC_LUAD_RNAseq.csv\u201d files in the ProcessedData folder.initialize_phosphoproteomics_data() reads phospho-proteomics-related data from CPTAC LUAD, and sets two global variables with data frames for the protein expression\u00a0analysis (step-8). The implementation details of this operation are in the \u223c/eIF4F.analysis/R/Proteomics.R file. This function stores two global variables CPTAC_LUAD_Phos and CPTAC_LUAD_Clinic_Sampletype as \u201cCPTAC_LUAD_Phos.csv\u201d and \u201cCPTAC_LUAD_Clinic_Sampletype.csv\u201d in the ProcessedData folder.\nCPTAC_LUAD_Phos contains the phosphoproteomics data published in Gillette et\u00a0al. (2020)7[href=https://www.wicell.org#bib7] as \"Phos.xlsx\".\nCPTAC_LUAD_Clinic_Sampletype contains the annotation data and is derived from\u00a0\"S046_BI_CPTAC3_LUAD_Discovery_Cohort_Clinical_Data_r1_May2019.xlsx\" and \"S046_BI_CPTAC3_LUAD_Discovery_Cohort_Samples_r1_May2019.xlsx\".\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2259-Fig3.jpg\nFigure\u00a03. Illustration of internal code structure for analysis functions\n(A) The internal code structure for\u00a0initialize_data().\n(B) The internal code structure for\u00a0EIF4F_CNV_analysis().\n(C) The internal code structure for\u00a0EIF4F_DEG_analysis().\n(D) The internal code structure for\u00a0EIF4F_Survival_analysis().\nCritical: The first run of data initialization functions creates and stores the processed data from download files. Following runs of the initialization functions will check the existence of processed data files and read them, which take a much shorter time to complete than the first run. Figure\u00a03[href=https://www.wicell.org#fig3]A summarizes the internal code structure for five data initialization functions, and shows twelve global variables in dark gray boxes.\nStep-2: Analyze the copy number variation (CNV) status of EIF4F genes\nTiming: \u00a0<\u00a01\u00a0min\nThis step performs three types of analyses on CNV statuses of EIF4F genes across TCGA tumors and creates the analysis results both on screen and as pdf files stored in \u223c/eIF4F.analysis/eIF4F_output/CNV folder.\nRun the following command line in RStudio.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2259-Fig4.jpg\nFigure\u00a04. Illustrations of internal code structures for analysis functions\n(A) The internal code structure for\u00a0EIF4F_PCA().\n(B) The internal code structure for\u00a0EIF4F_Corrgene_analysis().\n(C) The internal code structure for\u00a0EIF4F_RNA_pro_correlation().\n(D) The internal code structure for\u00a0EIF4F_Proteomics_analysis().\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2259-Fig5.jpg\nFigure\u00a05. Example CNV analysis outputs from EIF4F_CNV_analysis()(A) The stacked bar plot shows the overall CNV statuses for EIF4F genes in all tumors combined from 33 TCGA cancer types. Percentage contributions of each group are labeled on the bars.\n(B) The stacked bar plot shows the CNV status for a single EIF4F gene in individual TCGA cancer types.\n(C) The matrix plot shows the co-occurrence of the EIF4F CNV statuses. Each cell is labeled with the Pearson correlation coefficient, and \u2018\u2018X\u2019\u2019 indicate statistical insignificance (p\u00a0>\u00a00.05, p values not shown).\n(D) The boxplots show, across 33 TCGA cancer types, ratios of EIF4G1 CNV values in malignant tumors to its average CNV value in normal adjacent tissues (NATs) of the same cancer type.\nAdapted from Wu and Wagner (2021).1[href=https://www.wicell.org#bib1]\n> EIF4F_CNV_analysis()\nNote: EIF4F_CNV_analysis() is a wrapper function of three internal composite functions that take input data frames and call internal functions for analysis. Figure\u00a03[href=https://www.wicell.org#fig3]B summarizes the internal code structure for this step. The detailed definitions of all internal functions are in \u223c/eIF4F.analysis/R/CNV.R.\nNote: .plot_bargraph_CNV_TCGA() takes the data frame TCGA_CNV_sampletype and calculates the frequency of each CNV status for EIF4F genes in all tumors combined from 33 TCGA cancer types. Its output is a stacked bar plot that ranks the EIF4F gene by the frequencies of copy number gain (Figure\u00a05[href=https://www.wicell.org#fig5]A). .plot_bargraph_CNV_TCGA() also calculates the frequency of CNV status from individual TCGA cancer types. Its output is a number of stacked bar plots, in which cancer types are listed in the alphabetical order (e.g., Figure\u00a05[href=https://www.wicell.org#fig5]B).Note: .plot_matrix_CNVcorr_TCGA() takes the data frame TCGA.CNV.value and generates a correlation matrix with the unthresholded CNV value data of tumors from all TCGA cancer type combined. This function calculates the correlation coefficients between all pairs of EIF4F\u00a0genes, and plots the correlation matrix (Figure\u00a05[href=https://www.wicell.org#fig5]C), using the cor.mtest() and corrplot() functions from \u201ccorrplot\u201d package.\nNote: .plot_boxgraph_CNVratio_TCGA() takes the data frame TCGA_CNVratio_sampletype and generates boxplots for CNV ratios in tumors vs. normal adjacent tissues (NATs) from individual TCGA cancer types. This function produces a CNV ratio boxplot for each gene in individual TCGA cancer types (e.g., Figure\u00a05[href=https://www.wicell.org#fig5]D).\nStep-3: Compare the gene expression and ratio of EIF4F genes\nTiming: \u00a0<\u00a01\u00a0min\nThis step compares the EIF4F RNA abundances and ratios across TCGA tumors, and saves the results in the \u223c/eIF4F.analysis/eIF4F_output/DEG folder. In this step, means of log-transformed gene expression values or RNA ratios are calculated in different samples, and compared by t-test.\nRun the following command line in RStudio.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2259-Fig6.jpg\nFigure\u00a06. Example expression analysis outputs from EIF4F_DEG_analysis()\n(A) The boxplot represents the comparison of RNAseq expressions across different cancer types.\n(B) The box and whisker plot shows corresponding mRNA expression of the same gene in tumor samples and normal adjacent tissues (NATs).\n(C) The violin plots show the indicated mRNA expression (transcripts per million), in all TCGA cancer patient samples combined. The two-tailed Student\u2019s t tests were performed. ns, not significant; \u2217P \u2264 0.05; \u2217\u2217p \u2264 0.01; \u2217\u2217\u2217p \u2264 0.001; \u2217\u2217\u2217\u2217p \u2264 0.0001.\nAdapted from Wu and Wagner (2021).1\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2259-Fig7.jpg\nFigure\u00a07. Example gene expression ratio analysis outputs from EIF4F_DEG_analysis()\n(A) The boxplots depict ratios of RNA counts between two genes, computed from each sample of a particular cancer type, for tumors and NATs considered separately.(B) The violin plots compare RNA ratios within metastatic tumors, primary tumors, or NATs in all 33 TCGA cancer study groups combined. Dashed lines mark the ratios of 1:1 and 4:1 in all panels. The two-tailed Student\u2019s t tests were performed. ns, not significant; \u2217p \u2264 0.05; \u2217\u2217p \u2264 0.01; \u2217\u2217\u2217p \u2264 0.001; \u2217\u2217\u2217\u2217p\u00a0\u2264 0.0001.\nAdapted from Wu and Wagner (2021).1[href=https://www.wicell.org#bib1]\n> EIF4F_DEG_analysis()\nNote: EIF4F_DEG_analysis() is a wrapper function of two internal composite functions that take input data frames and call internal functions for analysis. Figure\u00a03[href=https://www.wicell.org#fig3]C summarizes the internal code structure for this step. The detailed definitions of all internal functions are in \u223c/eIF4F.analysis/R/DEG.R.\nNote: .plot_boxgraph_RNAseq_TCGA() takes the data frame TCGA_GTEX_RNAseq_sampletype and performs three analyses on RNAseq data. It compares the RNA abundance of all EIF4F genes in tumors from 33 TCGA cancer types in a box plot (Figure\u00a06[href=https://www.wicell.org#fig6]A). Then it compares the expression of each EIF4F gene in tumors vs. NATs (e.g., Figure\u00a06[href=https://www.wicell.org#fig6]B). Finally, it compares the RNA expression in primary, metastatic tumors vs. NATs from all combined TCGA cancer types to produce violin plots (Figure\u00a06[href=https://www.wicell.org#fig6]C).\nNote: .plot_boxgraph_RNAratio_TCGA() takes the data frame TCGA_GTEX_RNAseq_sampletype to calculate the RNA ratios of input genes within each TCGA sample, and performs two analyses on the RNA ratios. It first compares RNA ratio in tumors vs. NATs from individual TCGA cancer types by boxplots (e.g., Figure\u00a07[href=https://www.wicell.org#fig7]A). It then compares RNA ratios in primary, metastatic tumors vs. NATs from all combined TCGA cancer types to produce violin plots (Figure\u00a07[href=https://www.wicell.org#fig7]B).\nStep-4: Correlate EIF4F gene expression to patient survival probability\nTiming: \u00a0<\u00a01\u00a0min\nThis step performs two types of survival analyses on EIF4F gene expression in TCGA tumors, and output results on screen and to the \u223c/eIF4F.analysis/eIF4F_output/Survival folder.\nRun the following command line in RStudio.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2259-Fig8.jpgFigure\u00a08. Example survival analysis outputs from EIF4F_Survival_analysis()\n(A) The KM plot of survival probabilities of TCGA patients with cancer according to mRNA expressions of EIF4G1 in their tumors. Two groups of patients with the top or bottom 20% of gene expression in their tumors were selected from all 10,235 TCGA patients with cancer. Statistical significance of differences in survival probabilities between the two groups was determined by p values yielded from log-rank tests. The shaded areas around each curve depict a 95% confidence region for that curve.\n(B) The KM plot of survival probabilities of TCGA patients with lung adenocarcinoma according to mRNA expressions of EIF4G1 in their tumors. Two groups of patients with the top or bottom 20% of gene expression in their tumors were selected from 508 TCGA patients with lung adenocarcinoma.\n(C) Univariable Cox proportional-hazards regression models for expression of translation initiation genes in all 10,235 patients with cancer from TCGA. The p value indicates the statistical significance of association between gene expression and survival (i.e., a significant fit).\nAdapted from Wu and Wagner (2021).1[href=https://www.wicell.org#bib1]\n> EIF4F_Survival_analysis()\nNote: EIF4F_Survival_analysis() is a wrapper function to call two internal composite functions that take input data frames and call internal functions for analysis. Figure\u00a03[href=https://www.wicell.org#fig3]D summarizes the internal code structure for this step. The detailed definitions of all internal functions are in \u223c/eIF4F.analysis/R/Survival.R.Note: .plot_KM_RNAseq_TCGA() takes the data frame TCGA_RNAseq_OS_sampletype and performs Kaplan-Meier (KM) analysis to associate survival probabilities with gene expression. This function takes arbitrary gene expression cutoff, 0.2 for 20% or 0.3 for 30%, to stratify the patient groups based on the top or bottom precents of gene expression within their tumors. This function performs KM analysis on all combined TCGA cancer types (e.g., Figure\u00a08[href=https://www.wicell.org#fig8]A) or individual cancer type such as \u201clung adenocarcinoma\u201d (e.g., Figure\u00a08[href=https://www.wicell.org#fig8]B). This function imports the survfit() and survdiff() functions from the \u201csurvival\u201d package to analyze the survival probabilities of patient groups, and produces the KM curve plots.\nNote: .plot_CoxPH_RNAseq_TCGA() takes the data frame TCGA_RNAseq_OS_sampletype, and quantitatively relates patient survival and gene expression in tumors by the Cox proportional-hazards (PH) regression method. This function can perform survival analyses on all combined TCGA cancer types or individual cancer type such as \u201clung adenocarcinoma\u201d as an argument. The function performs both univariable Cox-PH analysis using a single gene expression as the dependent variable (e.g., Figure\u00a08[href=https://www.wicell.org#fig8]C), and multivariable Cox-PH analysis to model patient survival and expressions of all initiation factors together. This composite function imports the analyse_multivariate() function from the \u201csurvivalAnalysis\u201d package for regression model. Proportional hazard assumptions of Cox Regression are tested by coxph() and cox.zph() functions from the \u201csurvival\u201d package. The resulting plots are produced with the forestplot() function in the \u201cforestplot\u201d package.\nStep-5: Principal component analysis on co-variation of EIF4F expression\nTiming: \u00a0<\u00a01\u00a0min\nThis step performs the principal component analyses (PCA) on EIF4F expression in GTEx healthy tissues, or/and TCGA tumors, and output results to the \u223c/eIF4F.analysis/eIF4F_output/PCA folder.\nRun the following command line in RStudio.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2259-Fig9.jpg\nFigure\u00a09. Example principal component analysis outputs from EIF4F_PCA()(A) PCA of RNA-seq-derived counts of EIF4G1, EIF4A1, EIF4E, EIF4EBP1, PABPC1, MKNK1, and MKNK2 from 7,388 tissue samples of various healthy tissue types in GTEx. Tissue types were observations, colored for visualization but not used to construct PCs.\n(B) The matrix plot shows the cos2 value for the contribution of each gene to each PC, from the PCA of 7,388 tissue samples of various healthy tissue types in (A).\n(C) PCA of normalized RNA-seq-derived counts of indicated genes from 9,162 primary and 392 metastatic tumors from TCGA and 7,388 healthy tissue samples from GTEx. Sample types were colored after analysis, for visualization.\n(D) The matrix plot shows the cos2 value for the contribution of each gene to each PC, from the PCA of TCGA tumor samples and GTEx normal tissue samples in (C).\n(E) Healthy tissue selection from (C). The tissue types were not used as variables to construct PCs, but were colored afterwards for visualization.\n(F) PCA of standardized RNA-seq-derived counts of indicated genes from 1,011 primary lung tumors (517 LUADs and 494 LUSCs) and 109 NATs from TCGA LUAD and LUSC study groups, and 287 healthy lung tissues from GTEx. Sample types were colored after analysis, for visualization.\nAdapted from Wu and Wagner (2021).1[href=https://www.wicell.org#bib1]\n> EIF4F_PCA()\nNote: EIF4F_PCA() is a wrapper function of two internal composite functions that take input data frames and call internal functions for analysis. Figure\u00a04[href=https://www.wicell.org#fig4]A summarizes the internal code\u00a0structure for this step. The detailed definitions of all internal functions are in \u223c/eIF4F.analysis/R/PCA.R.Note: .plot_PCA_TCGA_GTEX() takes the data frame TCGA_GTEX_RNAseq_sampletype and selects RNAseq data of three sample types: TCGA primary tumors, TCGA metastatic tumors, or GTEx healthy tissues. It performs separate PCA on EIF4F expression within the selected sample types and produces PCA results as biplots (e.g., Figure\u00a09[href=https://www.wicell.org#fig9]A), scree and matrix plots (e.g., Figure\u00a09[href=https://www.wicell.org#fig9]B). This function call also performs PCA on combined samples of all TCGA primary tumors, TCGA metastatic tumors and GTEx healthy tissues, which produces biplots (e.g., Figure\u00a09[href=https://www.wicell.org#fig9]C), scree and matrix plots (e.g., Figure\u00a09[href=https://www.wicell.org#fig9]D), as well as subset biplots that label only cancer types or healthy tissue types (e.g., Figure\u00a09[href=https://www.wicell.org#fig9]E). This function imports PCA() function from the \u201cFactoMineR\u201d package for PCA, and fviz_pca_biplot(), fviz_eig(), get_pca_var() functions from the \u201cfactoextra\u201d packages to produce biplots, scree and matrix plots.\nNote: .plot_PCA_TCGA_GTEX_tumor() takes the data frame TCGA_GTEX_RNAseq_sampletype and selects RNAseq data from one specific TCGA cancer type and its matched healthy tissue from GTEx. This function performs PCA on EIF4F expression within the selected sample types and generates biplots (e.g., Figure\u00a09[href=https://www.wicell.org#fig9]F), scree and matrix plots for PCA results.\nNote: .plot_PCA_CPTAC_LUAD() takes the data frame CPTAC_LUAD_Proteomics and selects proteomics data of input gene list. Due to the missing proteomics data for some inquired initiation proteins in the dataset, this function performs imputed PCA with the estim_ncpPCA() function from the \u201cmissMDA\u201d package.\nStep-6: Analyze the correlating genes (CORs) of EIF4F subunits\nTiming: \u00a0<\u00a030\u00a0min\nThis step analyzes the correlating genes of each EIF4F subunits from tumor or healthy samples, and outputs results to the \u223c/eIF4F.analysis/eIF4F_output/COR folder.\nRun the following command line in RStudio.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2259-Fig10.jpg\nFigure\u00a010. Example correlation gene analysis outputs from EIF4F_Corrgene_analysis()(A) Pearson\u2019s correlation coefficients between EIF4F (EIF4E, EIF4G1, EIF4A1, or EIF4EBP1) and 58,582 other genes were calculated separately across 1,122 lung tumors from LUSC and LUAD TCGA study groups, or across 287 healthy lung tissues from GTEx. Genes with significant positive (R\u00a0>\u00a00.3) or negative (R\u00a0<\u00a0-0.3) correlations were selected for analysis. The Venn diagrams show overlapping posCOR counts for EIF4F genes in healthy lungs.\n(B) The bar plots show the numbers of posCORs identified for each EIF4F gene in tumors or in healthy tissues.\n(C) The heatmap shows the correlation strengths of posCORs and negCORs for EIF4E, EIF4G1, EIF4A1, and EIF4EBP1 in healthy lungs and lung tumors.\n(D) The dot plot shows the enriched pathways in three clusters (K-means) of the heatmap (C)\u00a0yielded by REACTOME pathway analysis.\nAdapted from Wu and Wagner (2021).1[href=https://www.wicell.org#bib1]\n> EIF4F_Corrgene_analysis()\nNote: EIF4F_Corrgene_analysis() is a wrapper function to call one internal composite function that takes input data frames and calls internal functions for analysis. Figure\u00a04[href=https://www.wicell.org#fig4]B summarizes the internal code structure for this step. The detailed definitions of all internal functions are in \u223c/eIF4F.analysis/R/COR.R.\nNote: .plot_Corr_RNAseq_TCGA_GTEX() takes the data frameTCGA_GTEX_RNAseq_sampletype and selects RNAseq data from two sample types: TCGA tumors, or GTEx healthy tissues. This function separately identifies correlating genes (CORs) of EIF4E, EIF4A1, EIF4G1, and EIF4EBP1 from tumor samples or from healthy tissues. The significant CORs for each EIF4F subunit are identified and classified as positive or negative CORs (posCORs and negCORs). This function analyzes the overlap posCORs or negCORs of four EIF4F subunits from tumor samples or healthy tissues as Venn plots (e.g., Figure\u00a010[href=https://www.wicell.org#fig10]A). This function counts the numbers of posCORs and negCORs, and plots them in bar graphs for comparison (e.g., Figure\u00a010[href=https://www.wicell.org#fig10]B).Note: .plot_Corr_RNAseq_TCGA_GTEX() merges tumor and healthy correlation data, and clusters correlation strengths by similarity in heatmaps (e.g., Figure\u00a010[href=https://www.wicell.org#fig10]C), using the HeatmapAnnotation() and draw() functions from the \u201cComplexHeatmap\u201d package. Then this function extracts the gene lists identified in the clustering analysis with the row_order() function from \u201cComplexHeatmap\u201d. Finally, this function performs enriched pathway analysis on the extracted gene lists using the compareCluster() function from the \u201cclusterProfiler\u201d package, and produces dot plots (Figure\u00a010[href=https://www.wicell.org#fig10]D as an example).\nCritical: .plot_Corr_RNAseq_TCGA_GTEX() function only applies to EIF4F and related genes.\nStep-7: Analyze the correlation between RNA and protein expression\nTiming: \u00a0<\u00a01\u00a0min\nThis step examines the RNA and protein expression correlation in CCLE and CPTAC lung adenocarcinoma (LUAD) datasets, and outputs results to the \u223c/eIF4F.analysis/eIF4F_output/RNApro folder.\nRun the following command line in RStudio.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2259-Fig11.jpg\nFigure\u00a011. Example protein RNA correlation analysis outputs from EIF4F_RNA_pro_correlation() function, and example differential (phospho)protein expression analysis outputs from EIF4F_Proteomics_analysis()\n(A) The scatter plot shows the positive correlation between protein and mRNA expression levels of EIF4G1 across 109 LUADs from CPTAC. The significance of each correlation is indicated with a p value.\n(B) The scatter plot shows the correlation of eIF4G1 and eIF4A1 protein expression levels across 109 LUADs from CPTAC. The significance of each correlation is indicated with a p value.\n(C and D) The boxplots show, for eIF4G1, ratios to paired NATs (y axis) of mean peptide abundance. Depicted for NATs and LUADs at each tumor stage are: eIF4G1 whole protein (C)\u00a0and phosphorylation at serine 1,099 (D). Mean expression from NATs was normalized at 1. The dashed red line marks average abundance in all tumor stages combined, relative to NATs. The two-tailed Student\u2019s t tests were performed. ns, not significant; \u2217p \u2264 0.05; \u2217\u2217p\u00a0\u2264\u00a00.01; \u2217\u2217\u2217p \u2264 0.001; \u2217\u2217\u2217\u2217p \u2264 0.0001.Adapted from Wu and Wagner (2021).1[href=https://www.wicell.org#bib1]\n> EIF4F_RNA_pro_correlation()\nNote: EIF4F_RNA_pro_correlation() is a wrapper function to call two internal composite functions that take input data frames and call internal functions for analysis. Figure\u00a04[href=https://www.wicell.org#fig4]C summarizes the internal code structure for this step. The detailed definitions of all internal functions are in \u223c/eIF4F.analysis/R/RNAProCor.R.\nNote: .plot_scatter_RNApro_CCLE() takes the data frames, CCLE_RNAseq and CCLE_Proteomics, and selects RNAseq and proteomics data for EIF4F genes. It performs the Pearson correlation analysis between proteomics and RNAseq levels, and produces the results as scatter plots.\nNote: .plot_scatter_RNApro_LUAD() takes the data frames, CPTAC_LUAD_RNAseq and CPTAC_LUAD_Proteomics, and selects RNAseq and proteomics data for EIF4F genes. It performs the correlation analysis between the proteomics and RNAseq data for EIF4F genes and produces scatter plots (e.g., Figure\u00a011[href=https://www.wicell.org#fig11]A).\nStep-8: Analyze the protein co-expression and (phospho)protein expression across tumor stages\nTiming: \u00a0<\u00a05\u00a0min\nThis step examines the protein co-expression and compares (phospho)protein expression in different tumor stages, and outputs results to the \u223c/eIF4F.analysis/eIF4F_output/proteomics folder.\nRun the following command line in RStudio.\n> EIF4F_Proteomics_analysis()\nNote: EIF4F_Proteomics_analysis() is a wrapper function to call two internal composite functions that take input data frames and call internal functions for analysis. Figure\u00a04[href=https://www.wicell.org#fig4]D summarizes the internal code structure for this step. The detailed definitions of all internal functions are in \u223c/eIF4F.analysis/R/proteomics.R.\nNote: .plot_scatterplot_protein_LUAD() takes the data frame CPTAC_LUAD_Proteomics and selects the proteomics data of tumor samples. It analyzes the correlation between two input proteins across the LUAD tumor samples (e.g., Figure\u00a011[href=https://www.wicell.org#fig11]B).\nNote: .plot_boxgraph_protein_CPTAC() takes the data frames CPTAC_LUAD_Proteomics, CPTAC_LUAD_Phos and CPTAC_LUAD_Clinic_Sampletype, and selects the proteomics, phosphoproteomics and clinical stage data. It generates boxplots to compare the protein (e.g., Figure\u00a011[href=https://www.wicell.org#fig11]C) or phophosphorylation (e.g., Figure\u00a011[href=https://www.wicell.org#fig11]D) levels in tumors of different clinical stages.", "CellMap (http://cellmap.protein.properties[href=http://cellmap.protein.properties]) visualizes proteins by their subcellular localizations on images of cells\u2014optionally provided by the user\u2014users can submit images of cells on the web server and perform the remainder of steps described in this protocol using the image provided. On top, CellMap displays lines (edges) between proteins that interact according to the PPI database HIPPIE (Alanis-Lobato, Andrade-Navarro, & Schaefer, 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0001]). This protocol explains (1) where to navigate to open up CellMap; (2) what identifiers are needed to use the system; (3) how to add proteins to the viewer; and (4) how to make the visualization clearer (optional); (5) how to move proteins across localizations (optional); and (6) how to display lines connecting the proteins (optional). We also include an extra step (7) to change the cell image underlying the visualization.\nMaterials\nUsers will need a computer, tablet, or smartphone with an active internet connection. An up-to-date browser is required to operate CellMap. At least for first-time use, we suggest access on a desktop computer through Chrome/Chromium (v50+).\n1. Open the protein-protein interaction view in CellMap by navigating to https://cellmap.protein.properties/ppi[href=https://cellmap.protein.properties/ppi]. Once open, the viewer displays a full-screen image of a cell.\nAlternatively, navigate to the home page of CellMap (https://cellmap.protein.properties[href=https://cellmap.protein.properties]), select \u201cView the maps,\u201d hover over one of the maps, and select \u201cProtein-Protein Interactions.\u201d\n2. Identify proteins of interest: To begin using CellMap, you need an identifier (or accession number) from UniProt [RRID:SCR_002380, (The UniProt Consortium, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0017])] or a primary gene/protein name of one or several proteins of interest.For the following steps, we use an example introduced by others (Fujiki, Matsuzono, Matsuzaki, & Fransen, 2006[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0009]) who describe the role of the Human (NCBI:txid9606) PEX19 (peroxisomal biogenesis factor 19; HGNC:9713; UniProt identifier P40855). When located in the cytosol, PEX19 binds PEX3 (peroxisomal membrane bound peroxisomal biogenesis factor 3; HGNC:8858; UniProt identifier: P56589) (Fabregat et\u00a0al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0007]; May, 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0013]).\n3. Add PEX3 (UniProt P56589) to the PPI viewer by inserting the UniProt accession number or gene identifier into the \u201cSearch protein\u201d (gray font) input bar found at the center-top of the PPI viewer (results will populate as you type). Once the desired protein is listed in the drop-down list that appears, click on the item in the drop-down list to add the protein to the view. The protein will be displayed as a dot (disk drawn on cell, Figure 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0001] for PEX3).\nThe color of the dot represents the cellular compartment via a mapping function. The color legend can be found by scrolling down the page and expanding the section \u201cLegend\u201d; alternatively, it can be found at https://cellmap.protein.properties/legend[href=https://cellmap.protein.properties/legend] (Dallago, Goldberg, Andrade-Navarro, Alanis-Lobato, & Rost, 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0006]). The viewer uses the UniProt accession, even if queried with the gene/protein name.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/c458206d-c07b-4fc9-b3f5-b7354233b211/cpbi97-fig-0001-m.jpg</p>\nFigure 1\nSearching gene PEX3 in the protein-protein interaction viewer and adding its protein product to the view. Right panel: https://cellmap.protein.properties/ppi?p=P56589[href=https://cellmap.protein.properties/ppi?p=P56589]. The exact state (loaded protein in the mitochondrion) can be observed by opening the link provided.4. Optional: As the cell is in color, users can highlight protein \u201cdots\u201d to enhance visualization by decreasing the cell image opacity while maintaining the intensity of the protein dots (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0002]). To utilize this feature, click the half-filled star on the left side of the PPI viewer. There are three opacity levels to select from: click the button repeatedly until the desired opacity is found.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/c27ecdc8-dc7b-4f89-aa8c-dcd59fb5ecfe/cpbi97-fig-0002-m.jpg</p>\nFigure 2\nFading cell image: 100% visible, 50% visible, 10% visible.\n5. Move PEX3 from one localization to another in the viewer by clicking on the protein dot of P56589 in the mitochondrion. A tool tip with a drop-down menu \u201cNew localization\u201d will appear. Select \u201cperoxisome membrane\u201d from the drop-down menu. Once \u201cPeroxisome membrane\u201d is selected, the dot will move from the mitochondrion to the peroxisome membrane (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0003]). The color of the dot will have changed according to the color legend.\nWhen added to the viewer, proteins will be displayed in their first subcellular compartment [in alphabetical order according to the annotations of localization provided by selected sources (Goldberg et\u00a0al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0010]; Ramilowski et\u00a0al., 2015[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0014]; The UniProt Consortium, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0017]; Uhl\u00e9n et\u00a0al., 2015[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0019])]. PEX3 (P56589) will be displayed in the mitochondrion, since the possible subcellular localizations are mitochondrion, peroxisome, and peroxisome membrane (Ramilowski et\u00a0al., 2015[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0014]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/c53d4967-450c-4bad-98b2-cce0e9d95864/cpbi97-fig-0003-m.jpg</p>\nFigure 3\nMoving UniProt P56589 (Gene: PEX3) from mitochondrion to peroxisome membrane.\n6. Add an additional protein, PEX19 (UnitProt P40855), to the PPI viewer via the search bar as described in step 3.7. Display a line between P40855 (PEX19) and P56589 (PEX3) by clicking any of the protein dots in the PPI viewer. Hovering over the dots will display their UniProt identifiers. Clicking on a dot will display a line (edge) between the two proteins along with a floating-point value. The value represents the interaction score according to HIPPIE [the interaction score is the confidence of experimentally measured interactions: from low confidence (0) to high confidence (1) (Alanis-Lobato et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0001])]. Scrolling down the page, a table listing the proteins currently in the view, with their respective subcellular localizations, is presented (\u201cmapped\u201d meaning available on the graphic; \u201cunmapped\u201d meaning not available through the selected graphic; Fig. 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0004]).\nThe full-screen viewer offers several tweaks. Users may add more proteins to the visualization. On the top-right of the map viewer, by hovering over the \u201clayers\u201d icon (a stack of squares piled on one another), users can selectively hide proteins currently loaded onto the view. From the layers, users can also choose to display the annotations of the subcellular localizations by clicking the radio button next to the \u201clocalizations\u201d item in the list.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e5f38903-43d5-4fc1-9c8c-bfa3d6bad9ab/cpbi97-fig-0004-m.jpg</p>\nFigure 4\nTable of proteins loaded onto the viewer.\n8. Optional: Users may select a different cell map (cell image) by scrolling down the browser page and clicking on the gray button \u201cSelect a different map\u201d at the bottom (Fig. 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0004]). This will open a page displaying several images of cells or sections thereof. Users can select one of these maps by hovering over one image and clicking on the \u201cProtein-Protein Interactions\u201d menu item. This will open the new map and load the proteins which were being visualized prior.\n9. Optional: To discard the visualization, click the trashcan icon in the controls to the left of the PPI viewer.This protocol showcases the display of all known interaction partners of a protein. We will use PEX19 (peroxisomal biogenesis factor 19; HGNC:9713; UniProt identifier P40855) as starting point and visualize its interaction partners according to HIPPIE (Alanis-Lobato et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0001]).\nMaterials\nUsers will need a computer, tablet, or smartphone with an active internet connection. An up-to-date browser is required to operate CellMap. At least for the first-time use, we suggest access on a desktop computer through Chrome/Chromium (v50+).\n1. Navigate to the homepage of CellMap (https://cellmap.protein.properties[href=https://cellmap.protein.properties]) to open a protein-centric page. Utilize the search functionality on this page (second from the top; left panel of Fig. 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0005]) by entering the full or partial UniProt identifier (e.g., P408) or gene name (e.g., pex). The page will populate with squares containing the UniProt identifiers of possible hits. These squares will contain localization data as represented by smaller colored squares on the top left of a protein square, and the number of interaction partners at the bottom right of the protein square. For our example, we type \u201cpex19,\u201d which will display a single result \u201cP40855,\u201d i.e., the protein product of PEX19 in Human (NCBI:txid9606) according to UniProt, which interacts with 234 other proteins (number in lower right of box), and is located in red (cytoplasm) and green (peroxisome) (Fig. 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0005]). Clicking in the center of the square will open the protein-centric page.Throughout CellMap, each localization has a particular color associated to it via a function that maps a string (e.g., \u201cGolgi apparatus\u201d) to a color (in hexadecimal notation, e.g., #A378BF). On the home page, clicking one of the colored squares will filter the results only for proteins that are localized in the selected localization (e.g., clicking on a red square will only display proteins in the \u201ccytoplasm\u201d). For a legend of color-to-localization navigate to https://cellmap.protein.properties/legend[href=https://cellmap.protein.properties/legend].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/9f7c4db8-e5e3-45be-acf1-633fea97c64b/cpbi97-fig-0005-m.jpg</p>\nFigure 5\nFrom the search for \u201cpex19\u201d on the homepage (https://cellmap.protein.properties/?q=pex19[href=https://cellmap.protein.properties/?q=pex19]) to the protein-centric page of UniProt P40855 (https://cellmap.protein.properties/protein/P40855[href=https://cellmap.protein.properties/protein/P40855]).\n2. To navigate from the protein-centric page for PEX19 (P40855) to the visualization of its interaction partners, scroll to the section titled \u201cPartners\u201d on the protein-centric page. This section gives a graphical preview of PPI partners of the query protein (Fig. 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0006], middle panel, top half), as well as a list of squares of interaction partners (Fig. 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0006], middle panel, bottom half). Click on the \u201cExpand\u201d link between the preview and the list to open the PPI viewer, similar to what is described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-prot-0001]. Here, PEX19 (P40855) will be highlighted and all its interaction partners loaded (Fig. 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0006], right panel).Protein-centric pages contain: gene names, full protein name, and the origin of the data at the top of the page; a preview of the localizations of the protein on a map/cell image in the middle of the page; and, following a preview of PPI interaction partners of the protein on a map/cell image at the bottom of the page, the interaction partners (as squares; similar to the search page) of the protein according to HIPPIE (Alanis-Lobato et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0001]). In the last section, like on the search page, interaction partners can be filtered by localization by clicking on the smaller colored boxes inside the squares, and you can navigate to a new protein-centric page by clicking in the center of the square for any protein you see. Additionally, the protein squares contain a floating-point value at the bottom-left representing the confidence score for an interaction between that protein and the protein for which the protein-centric page is open [according to HIPPIE (Alanis-Lobato et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0001])].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/b3de30cb-41d8-40c4-abbe-8d433625b471/cpbi97-fig-0006-m.jpg</p>\nFigure 6\nFrom the protein-centric page of protein P40855 (https://cellmap.protein.properties/protein/P40855[href=https://cellmap.protein.properties/protein/P40855]), to the preview of interaction partners, to the protein-protein interaction visualization of all its partners (https://cellmap.protein.properties/ppi?p=P40855&partners[href=https://cellmap.protein.properties/ppi?p=P40855&partners]).\n3. The one-against-all/pairwise interactions display in the PPI viewer shows all known PPI partners of a query protein, selectable by clicking on any dot in the visualization (PEX19 in Fig. 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-fig-0007], left panel).\nLines (or edges) are colored light gray or black. Black lines represent PPIs that have experimentally been annotated to be binary interactions in the APID database (Alonso-L\u00f3pez et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.97#cpbi97-bib-0003]). Light gray lines represent PPIs for which no binary experimental annotation is available, and that might thus not directly interact, as may be the case for proteins in a complex.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/cfff45a8-97d0-45cc-986d-87baecef9d4b/cpbi97-fig-0007-m.jpg</p>\nFigure 7\nOne-against-all and all-against-all protein network visualizations (https://cellmap.protein.properties/ppi?p=P40855&partners[href=https://cellmap.protein.properties/ppi?p=P40855&partners]).4. To display the all-against-all PPI visualization/complete local PPI network, which displays all known interactions between all of the proteins shown in the map, click on the black filled circle on the left menu of the viewer (below the zoom-in/zoom-out, and above the trashcan icons).\nThis interaction modality may suggest a cascade of events (e.g., \u2018A interacts with B\u2019, \u2018B interacts with C\u2019, etc.). The complete local PPI network visualization requires some seconds to render for many (>100) interconnected proteins. As noted in the previous step: black lines represent binary PPIs, while light gray lines represent non-binary PPIs.\n5. To navigate the visualization, use the \u00b1 buttons on the left-hand controls in the PPI view to zoom in and out, and click-drag the cell image with the mouse to move the focus. This allows one to explore smaller neighborhoods in greater detail and to declutter, at least to some extent, the \u201chairballs\u201d of greatly connected networks.", "Pharos is the user interface to the Knowledge Management Center (KMC) for the IDG program, providing facile access to most data types collected by the KMC (Nguyen et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0041]; Sheils et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0050]). Given the complexity of the data surrounding any target, efficient and intuitive visualization has been a high priority for users to navigate and summarize search results and rapidly identify patterns. Underlying the interface is a GraphQL API that provides programmatic access to all KMC data, enabling the incorporation of IDG resources with other applications.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nSearch targets\n1. Navigate to Pharos (https://pharos.nih.gov[href=https://pharos.nih.gov]).\n2. To search for a target, click on the search box on the main page or in the top left corner of subsequent pages. Enter STAT3. Note that multiple search types are available in the drop-down menu. (Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0001])\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/1b57277d-3c90-4ff6-b523-f9ffe75afefb/cpz1355-fig-0001-m.jpg</p>\nFigure 1\nTypeahead search results for STAT3 scroll or arrow down to view more options.\n3. It is possible to search by pathway or view a list of diseases or ligands associated with a target. Additionally, pressing Enter or Return will allow a text-based search, which will return a list of results featuring \u2018STAT3\u2019 anywhere in the text.\n4. Press Enter or Return, or click the magnifying glass icon to search for the \u2018STAT3\u2019 text string.\n5. A list of 81 targets is returned, with \u2018STAT3\u2019 being at the top of the list. The rest of the targets will have the phrase \u2018STAT3\u2019 somewhere within the target details (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0002]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/5ddd1f5a-28e1-4b89-818b-bb4c309b3c58/cpz1355-fig-0002-m.jpg</p>\nFigure 2\nSearch Targets for STAT3 search results page.6. Click on the STAT3 card to view the target details.\nView target details\n7. Follow the steps above, or alternatively, click on the STAT3 (Target) option from the search box auto-complete. This will navigate directly to the STAT3 target details page.\n8. The target details page is divided into several sections that highlight an area of knowledge about the target.\n9. Scroll down to the \u201cProtein Summary\u201d section. A brief description of the target, as well as several identifiers, are available. In addition, the central radar plot charts the relative knowledge of a target compared to the rest of TCRD on a 0 to 1 scale. This data is sourced from the Harmonizome, which will be discussed further (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0003]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/b17686c0-f97d-4588-8179-4cb7afb187ab/cpz1355-fig-0003-m.jpg</p>\nFigure 3\nTarget details page for STAT3; the radar chart in the center depicts data from Harmonizome.\n10. Scroll down to the next section, \u201cIDG Development Level Summary.\u201d Displayed here is the current development level . Each level has the criteria listed, as well as links to the data for each property (Fig. 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0004]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/52d3cb79-73e6-4634-9b2f-e43420bba863/cpz1355-fig-0004-m.jpg</p>\nFigure 4\nIDG development level summary section that shows the current development level, and criteria met. Links provide the ability to view either the original source, or the relevant data in Pharos.\n11. On the left side panel, click on \u201cDisease Associations by Source.\u201d This will navigate within the page to a section displaying disease associations from a variety of sources.12. Scroll down to the \u201cDisease Novelty (Tin-x)\u201d section, just below Disease Associations. A scatterplot is visible that shows Tin-x data. This data is explained in Basic Protocol 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-prot-0006]. Briefly, it is natural language processed PubMed abstracts that chart a target's importance to a disease, as well as the novelty of that target to the disease. A dense chart indicates a large amount of knowledge about a target and its disease associations, whereas a sparser chart would indicate that target is not frequently studied and has fewer disease associations (Fig. 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0005]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e2e1504f-2a65-4869-8933-b8be0b84a7ad/cpz1355-fig-0005-m.jpg</p>\nFigure 5\nScatterplot depicting TIN-X data for STAT3. Hovering over a data point opens up a tooltip, providing novelty and importance data for the disease.\n13. Scroll down to the next section \u201cGWAS Traits.\u201d Here a table of GWAS traits is displayed. This list focuses on scoring and ranking protein-coding genes associated with traits from genome-wide association studies. This allows the discovery of traits most associated with a target, but also less emphasized traits (Fig. 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0006]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/54febf72-91fd-490f-b061-745b17d40e6f/cpz1355-fig-0006-m.jpg</p>\nFigure 6\nGWAS traits, and the associated TIGA scatterplot. For a more in-depth exploration of this data, click \u201cExplore on Target Illumination GWAS Analytics.\u201d\nFinding a list of under-studied targets that share disease associations with STAT3\n14. From the STAT3 target details page, click on \u201cDisease Associations by Source\u201d on the left panel.\n15. Click on the \u201cFind Similar Targets\u201d button, directly under the panel header (Fig. 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0007]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/688ce0eb-28ea-4055-8df1-3b2ec05ba043/cpz1355-fig-0007-m.jpg</p>\nFigure 7\nAdditional functions available within Pharos are shown within blue buttons. Users can click to browse filtered lists for targets similar to the current target, or associated diseases or ligands.\n16. The targets list page is now shown, with a target similarity filter applied, showing 17,876 targets (Fig. 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0008]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4873861a-0e42-44c1-8b12-aefdcbf2a4fe/cpz1355-fig-0008-m.jpg</p>\nFigure 8List of targets that share associated diseases with STAT3. The Jaccard index is a numerical value of the ratio of overlap between the associated diseases of the target in relation to the original target (STAT3). The Venn diagram is a visual representation of the ratio with the TDL level color coded.\n17. To refine this list for targets of interest to the IDG program (mentioned in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-prot-0001]), click on the \u201cRefined (2020)\u201d checkbox in the IDG Target Lists filter panel on the left side of the page. The list of targets shown is reduced to 290.\n18. To find only dark targets in this list, click the \u201cTdark\u201d value in the Target Development Level filter panel, returning 48 targets (Fig. 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0009]).\nDark targets are the most under-studied proteins from the three gene families with the most known druggable targets: GPCRs, ion channels, and kinases.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f5a69995-2dd4-487b-a430-4c4f1871cac8/cpz1355-fig-0009-m.jpg</p>\nFigure 9\nThe target list from Figure 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0008] filtered to display Target Development Level of Tdark, and on the Refined(2020) IDG target lists. Click on \u201cClick for details\u2026\u201d to view an expanded list of the overlapping values.\n19. Click on the \u201cclick for details\u2026\u201d text on the TMEM63A target card to view a list of associated diseases that this target shares with STAT3 (Fig. 10[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0010]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/464471e0-ec1f-4235-8264-0adc6a78cb11/cpz1355-fig-0010-m.jpg</p>\nFigure 10\nExpanded view of the Associated Disease Similarity section of the target card.\nDownload target list\n20. Click on the downward-facing arrow on the right side of the Targets header (Fig. 11[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0011]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a7dfd4cd-e5da-4388-9b56-51b32a07794f/cpz1355-fig-0011-m.jpg</p>\nFigure 11\nTarget toolbar illustrating the download button on the right side. To the left of the download button is the upload button, which allows for the uploading of custom lists, to explore in the Pharos interface.21. A window will pop open displaying a list of fields that can be selected (Fig. 12[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0012]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/3eda3936-0b0d-45d4-a45f-8117bc678fc7/cpz1355-fig-0012-m.jpg</p>\nFigure 12\nPopup window featuring the query builder which allows for the download of Pharos list data as a .csv file. Subsequent tabs display the raw SQL query used to generate the data, as well as a 10-line preview.\n22. Click on the Associated Diseases checkbox. Note that many fields are deactivated, to reduce the overall file size.\n23. Click on Name and Target Development Level under the Single Value Fields heading.\n24. Click the Run Download Query Button. A file download dialog will open. Depending on the complexity of the target list and the fields selected, this may take some time.\n25. After the file is downloaded, this list of targets can be used as a starting point for many of the protocols listed below.\nGraphQL queries\n26. Click on API on the main Pharos header.\n27. A code \u201csandbox\u201d is now visible, allowing testing of GraphQL queries to fetch complex data from Pharos. A distinct feature of GraphQL is the ability of the consumer to determine the exact fields returned from the query, as opposed to a SQL query, where the data returned is determined by the database developer.\n28. Click the \u201cEdit & Run\u201d button for one of the Sample Queries on the left panel, and then the \u201cPlay\u201d button in the top center. This will execute the query on the server and display the JSON results in the right panel.\n29. Click on the \u201cDocs\u201d tab on the right side of the page. A menu will open up that displays the queries available, the inputs required, and the responses and properties returned. Click on the \u201cDocs\u201d tab again to close the menu.30. Replace the text in the left column with this query:\n         \nquery PaginateData {\n\u2003\u2003batch(\n\u2003\u2003\u2003\u2003filter: {\n\u2003\u2003\u2003\u2003\u2003\u2003facets: [\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003{ facet: \"Target Development Level\", values: [\"Tdark\"] }\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003{ facet: \"IDG Target Lists\", values: [\"Refined (2020)\"] }\n\u2003\u2003\u2003\u2003\u2003\u2003]\n\u2003\u2003\u2003\u2003\u2003\u2003similarity: \"(P40763, Associated Disease)\"\n\u2003\u2003\u2003\u2003}\n\u2003\u2003) {\n\u2003\u2003\u2003\u2003results: targetResult {\n\u2003\u2003\u2003\u2003\u2003\u2003count\n\u2003\u2003\u2003\u2003\u2003\u2003targets(skip: 0, top: 100) {\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003name\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003gene: sym\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003accession: uniprot\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003idgTDL: tdl\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003similarityDetails: similarity {\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003commonOptions\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003}\n\u2003\u2003\u2003\u2003\u2003\u2003}\n\u2003\u2003\u2003\u2003}\n\u2003\u2003}\n}\n31. Press the play button. This query fetches all dark targets of interest to the IDG that share associated diseases with STAT3. Returned are the target name, gene symbol, Uniprot id, IDG TDL, and shared associated diseases (Fig. 13[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0013]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/96c5048f-b8dd-4d94-b19f-14648dd8a1b1/cpz1355-fig-0013-m.jpg</p>\nFigure 13\nGraphQL sandbox interface. Examples on the left side and documentation on the right allow for highly customizable data requests.\nEntire relational database download page\n32. Navigate to the TCRD website (http://juniper.health.unm.edu/tcrd/[href=http://juniper.health.unm.edu/tcrd/]).\n33. Click on the \u201cDownloads\u201d tab on the navigation bar at the top of the page to be redirected to a table of downloadable, e.g., MySQL dump of the full TCRD (latest.sql.gz).The Harmonizome resource contains processed datasets detailing functional associations between genes/proteins and their attributes extracted from 66 online resources. The information from the original datasets was distilled into attribute tables that define significant associations between genes and their attributes, where attributes could be other genes, proteins, pathways, cell lines, tissues, experimental perturbations, diseases, phenotypes, drugs, or other entities depending on the dataset. The Harmonizome web application can be accessed from https://maayanlab.cloud/Harmonizome/[href=https://maayanlab.cloud/Harmonizome/] (Rouillard et\u00a0al., 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0045]).\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nText editor or development environment of choice, such as Visual Studio (https://visualstudio.microsoft.com/vs/[href=https://visualstudio.microsoft.com/vs/]); most updated version of Python\n(https://www.python.org/downloads/[href=https://www.python.org/downloads/]); Python module for Harmonizome (https://maayanlab.cloud/Harmonizome/static/harmonizomeapi.py[href=https://maayanlab.cloud/Harmonizome/static/harmonizomeapi.py])\nMetadata search\n1. Navigate to the Harmonizome website (https://maayanlab.cloud/Harmonizome/[href=https://maayanlab.cloud/Harmonizome/]).\n2. The front page features a search bar where keywords of interest can be input. Click the filter button on the left of the search bar to narrow searches to \u201cgenes,\u201d \u201cgene sets,\u201d or \u201cdatasets\u201d (Fig. 14[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0014]). Type STAT3 into the search bar and click the submit button. The results page includes a single-gene landing page for STAT3 and 75 gene sets with STAT3 as an attribute (Fig. 15[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0015]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/779003d3-8d62-48eb-af15-dc05517b623e/cpz1355-fig-0014-m.jpg</p>\nFigure 14\nThe Harmonizome homepage. The filter drop-down menu on the left selects between searching for genes, gene sets, and datasets.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/cda7ab5a-908f-4c98-95b0-452115d10fa9/cpz1355-fig-0015-m.jpg</p>\nFigure 15\nSearch result page after querying \u201cSTAT3\u201d. One gene page and 75 gene set pages match the query term \u201cSTAT3\u201d.3. Click on the STAT3 \u201cgene\u201d result to be redirected to a single-gene landing page (Fig. 16[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0016]). The page includes identifying metadata for the gene, download links for accessing functional associations between STAT3 and other attributes, and links to other gene-related information from ARCHS4 (Lachmann et\u00a0al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0032]). Additionally, a list of functional associations for STAT3 from the various processed datasets included in Harmonizome is available (Fig. 17[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0017]). Click the \u201c+\u201d button to view associations for STAT3 for any of the datasets.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/0b6f91db-6205-4302-b368-b685b4021189/cpz1355-fig-0016-m.jpg</p>\nFigure 16\nSTAT3 single-gene landing page that includes identifying metadata for the gene, download links for retrieving functional association data, and gene-related information from ARCHS4.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/9dfd4b62-6cb0-4413-a88b-743727c37622/cpz1355-fig-0017-m.jpg</p>\nFigure 17\nExpandable lists of functional associations for STAT3 from each dataset.\n4. Click on any of the STAT3 \u201cgene set\u201d results. The gene set results page includes metadata for the STAT3 gene set; in this case the gene set includes all target genes of STAT3. All of the genes included in the gene set are found in the \u201cGenes\u201d section (Fig. 18[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0018]). Click on any of the gene symbols to be redirected to a single-gene landing page.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/9ebea5ff-0a4e-46bf-a6d3-12f12c642411/cpz1355-fig-0018-m.jpg</p>\nFigure 18\nSTAT3 gene set page from CHEA Transcription Factor Targets dataset.\nDownload page\n5. Click on the \u201cDownload\u201d section on the navigation bar at the top of the page to be redirected to a table of all the datasets included in Harmonizome (Fig. 19[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0019]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/66568828-88f8-40f2-8978-dbe9b54dbcc9/cpz1355-fig-0019-m.jpg</p>\nFigure 19\nDownload page for datasets included in Harmonizome.\n6. Click on \u201cAchilles\u201d in the resource column to be redirected to a page with identifying metadata for the resource and a list of all datasets derived from the resource (Fig. 20[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0020]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d969cd34-6ce1-4709-a31f-f855d3dcfcd6/cpz1355-fig-0020-m.jpg</p>\nFigure 20\nResource page for Achilles with identifying metadata for the Achilles resource.7. Click on \u201cCell Line Gene Essentiality Profiles\u201d in the dataset column to be redirected to a page with identifying metadata for the dataset and links to downloadables contained within this dataset (Fig. 21[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0021]). Further down the page are links to visualizations of the dataset contents and a table of gene sets (Fig. 22[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0022]). Click on any of the gene set names to be redirected to a gene set specific page.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/0cad1b0d-f519-4d93-8049-46128c25293e/cpz1355-fig-0021-m.jpg</p>\nFigure 21\nDataset page for \u201cAchilles Cell Line Gene Essentiality Profiles\u201d with identifying metadata for the dataset, in addition to download links for files included in this dataset.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/40b34728-3852-48b8-a43b-84779c5f1471/cpz1355-fig-0022-m.jpg</p>\nFigure 22\nLinks to visualizations of the dataset contents and a table of gene sets. Click any of the gene sets to be redirected to a gene set specific page.\nVisualize\n8. Click on the \u201cVisualize\u201d section on the navigation bar at the top of the page, and a drop-down menu will appear (Fig. 23[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0023]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/bf7c7c2c-9870-4943-9e99-e4659f8400b8/cpz1355-fig-0023-m.jpg</p>\nFigure 23\nDrop-down menu of visualization page options.\n9. Click on \u201cGlobal Heat Map\u201d within the drop-down menu to be redirected to an interactive clustergram that visualizes the appearance of each gene in Harmonizome. Select different gene classes with the buttons on the left. Switch the ordering of the clustergram between \u201ccluster\u201d and \u201crank\u201d by clicking the corresponding button (Fig. 24[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0024]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/cdaad6fc-6b47-491c-bcf5-b596582ca0a0/cpz1355-fig-0024-m.jpg</p>\nFigure 24\nGlobal Heat Map visualization organized by gene families and resources. Switch between gene families using the buttons on the left. Switch between \u201cCluster\u201d and \u201cRank\u201d using the toggle on the left. Query a gene of interest using the search bar at the bottom left.10. Click on \u201cDataset Heat Maps,\u201d \u201cGene Similarity Heat Maps,\u201d or \u201cAttribute Similarity Heat Maps\u201d within the drop-down menu to be redirected to a page with a drop-down menu of Harmonizome datasets. Open the drop-down menu and select any dataset to generate a hierarchically clustered heat map visualization of the dataset (Fig. 25[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0025]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e760be56-ba42-45f4-a9ee-c7dff3f1a3f8/cpz1355-fig-0025-m.jpg</p>\nFigure 25\nDataset Heat Maps page. Select a dataset from the drop-down menu and it will be visualized as a hierarchically clustered heat map.\n11. Click on \u201cDataset Pair Heat Maps\u201d within the drop-down menu to be redirected to a page with a drop-down menu of Harmonizome datasets. Open the drop-down menu and select a dataset. A second drop-down menu will appear for selecting a second dataset to compare. Click visualize to generate a hierarchically clustered heat map visualization of the two datasets (Fig. 26[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0026]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/1fb65319-c4ba-4878-8057-cda50185d5c7/cpz1355-fig-0026-m.jpg</p>\nFigure 26\nDataset Pair Heat Maps page. Select two datasets to compare from the drop-down menus and a hierarchically clustered heat map will be generated.\n12. Click on \u201cHeat Map with Input Genes\u201d within the drop-down menu to be redirected to a page with a drop-down menu of Harmonizome datasets and a gene list text box. Click the \u201cExample input\u201d button to populate the fields with an example dataset and gene set. Click \u201cSubmit\u201d to generate a hierarchically clustered heat map visualization of the associations between the uploaded genes and biological entities in the dataset (Fig. 27[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0027]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/df8efd7e-9533-4529-a959-3d33d82f95f2/cpz1355-fig-0027-m.jpg</p>\nFigure 27\nHeat Map with Input Genes page. Input a list of maximum 500 genes and select a dataset to build a hierarchically clustered heat map detailing associations between the input genes and biological entities in the dataset.\nPredict13. Click on the \u201cPredict\u201d section on the navigation bar at the top of the page and a drop-down menu will appear (Fig. 28[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0028]). Click \u201cIntro\u201d within the drop-down menu.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/66fee271-0eeb-4428-bba8-219852495704/cpz1355-fig-0028-m.jpg</p>\nFigure 28\nDrop-down menu of \u201cPredict\u201d options.\n14. The intro page contains information about how machine learning studies were devised using the Harmonizome datasets. A table with four separate case studies: \u201cIon Channel Predictions,\u201d \u201cMouse Phenotype Predictions,\u201d \u201cGPCR-Ligand Interaction Predictions,\u201d and \u201cKinase-Substrate Interaction Predictions\u201d contains links to view and download tables of predicted associations (Fig. 29[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0029]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f15eb384-7d61-48fa-80cb-1e63093093d0/cpz1355-fig-0029-m.jpg</p>\nFigure 29\nMachine learning case studies page with details about the case studies were performed. Click on the corresponding buttons to view the tables for each study or download the table of predicted associations.\nUsing the Harmonizome API\n15. These are the entity types supported by the Harmonizome API:\n         \nDATASET, GENE, GENE_SET, ATTRIBUTE, GENE_FAMILY, NAMING_AUTHORITY, PROTEIN, RESOURCE\nOpen a new or existing Python code file. Import the required Harmonizome API Python module at the top of the file:\n         \nfrom harmonizomeapi import Harmonizome, Entity\nThe Harmonizome object includes several methods to read, parse, and download data from the Harmonizome API. The Harmonizome object includes .get(), .next() and .download() methods. For example, to display the datasets available in Harmonizome, run the following code block:\n         \nentity_list = Harmonizome.get(Entity.DATASET)\nmore = Harmonizome.next(entity_list)\nIn order to minimize database queries and request times, the Harmonizome API uses a technique called \"cursoring\" to paginate large result sets. Therefore, the first line in the above code block returns the first 100 datasets, whereas the second line continues from where the previous entity list left off and retrieves the subsequent 14 datasets that are available in Harmonizome. The Harmonizome.get()and Harmonizome.next() methods can be used for all entity types supported by the Harmonizome API.16. To download datasets available in Harmonizome to a local directory, use the Harmonizome.download() generator function. Alternatively Harmonizome.download_df() can be used to download files and load them in directly as sparse (with an added sparse=True argument) or dense Pandas DataFrames (assumed). The function takes a list of datasets and downloadables as arguments. Leaving the datasets argument empty will download all datasets by default. Leaving the what argument empty will download all downloadables for each dataset by default. In the example code below, the gene_attribute_matrix.txt.gz downloadable from the \u201cCTD Gene-Chemical Interactions\u201d dataset is downloaded, decompressed, and saved to a local directory named after the dataset if it has not already been processed:\ndl, = Harmonizome.download(datasets=[\u02c8CTD Gene-Chemical Interactions\u02c8],\nwhat=[\u02c8gene_attribute_matrix.txt.gz\u02c8])\nMore information regarding the Harmonizome API is available at https://maayanlab.cloud/Harmonizome/documentation[href=https://maayanlab.cloud/Harmonizome/documentation].ARCHS4 (Lachmann et\u00a0al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0032]) is a web resource that provides access to published RNA-seq gene- and transcript-level data from human and mouse experiments. FASTQ files from RNA-seq experiments deposited in the Gene Expression Omnibus (GEO) were aligned using a cloud-based infrastructure. The ARCHS4 web interface facilitates the exploration of the processed data through querying tools, interactive visualizations, and single-gene landing pages that provide average expression of a specific gene across cell lines and tissues, top co-expressed genes, and predicted biological functions and protein\u2013protein interactions for each gene based on prior knowledge combined with co-expression.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nMost updated version of R (https://www.r-project.org/[href=https://www.r-project.org/]); R Studio (https://www.rstudio.com/[href=https://www.rstudio.com/]); rhdf5 library (https://www.bioconductor.org/packages/release/bioc/html/rhdf5.html[href=https://www.bioconductor.org/packages/release/bioc/html/rhdf5.html])\nMetadata search\n1. Navigate to the ARCHS4 web application (https://maayanlab.cloud/archs4/[href=https://maayanlab.cloud/archs4/]).\n2. Click the \u201cGet Started\u201d button on the homepage to proceed to the data search and visualization page (Fig. 30[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0030]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/93b1bf5b-28e3-484c-a0e5-0c4ff8b10ae3/cpz1355-fig-0030-m.jpg</p>\nFigure 30\nARCHS4 Homepage.\n3. The data search and visualization page by default shows an interactive 3D t-SNE scatter plot of all the human gene expression samples found in ARCHS4 (Fig. 31[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0031]). The metadata search field on the left enables querying of specific terms that will be highlighted in the 3D scatter plot. Searching for the term \u201cPancreatic Islet\u201d and then clicking on the search button results in the highlighting of the relevant samples. The samples that are related to the search term cluster in the scatter plot because the samples contain similar expression profiles (Fig. 32[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0032]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/2fbc8a1a-3187-423a-af0d-ca83acd3b404/cpz1355-fig-0031-m.jpg</p>\nFigure 31\nData visualization and search page that includes a 3D interactable scatter plot of gene expression data.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/083d6a49-5ff6-40e0-8fb6-a34875e484af/cpz1355-fig-0032-m.jpg</p>\nFigure 323D scatter plot of human gene expression data that includes the term \u201cPancreatic islet.\u201d\n4. Any submitted search term will be found in its corresponding section within the \u201cSearch Result\u201d table below the interactive t-SNE scatter plot visualization. The table contains metadata regarding the organism, number of samples, and number of series, as well as a button to download an R script that can be used to retrieve the identified sample files. An X button is also available to delete the query (Fig. 33[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0033]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/858025bd-c1cf-4353-8393-f43f92647619/cpz1355-fig-0033-m.jpg</p>\nFigure 33\nSearch results table with Pancreatic islet samples listed in their respective section with metadata and options to download an R script to process the samples or delete the query.\nSignature search\n5. Switching to the signature search functionality can be done by clicking on the corresponding tab within the \u201cSearch\u201d field on the left (Fig. 34[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0034]). The signature search uses a set of highly and lowly expressed genes from each sample to identify matching samples to the given input.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/1e2eb4d1-4137-4289-911a-88a449a5d9ae/cpz1355-fig-0034-m.jpg</p>\nFigure 34\nSignature search field that allows for querying of up- and down-regulated genes to identify samples that match the input.\n6. Query the example up and down gene sets by clicking \u201cTry an example.\u201d The corresponding samples are highlighted within the scatter plot and are added to the \u201cSearch Result\u201d table (Fig. 35[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0035]). Note that the previous query of \u201cPancreatic Islet\u201d is still visualized within the scatter plot and listed in the \u201cSearch Result\u201d table.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f203f042-20c7-429e-8be3-91b61d50419f/cpz1355-fig-0035-m.jpg</p>\nFigure 35\nExample query from the signature search visualized in the 3D scatter plot. The identified samples are added to the \u201cSearch Result\u201d table.\nEnrichment analysis7. Switch to the enrichment search by clicking on the corresponding tab within the \u201cSearch\u201d field on the left (Fig. 36[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0036]). The enrichment search highlights samples that are enriched in gene sets from eight gene set libraries. Select the gene set library, gene set of interest within the selected library, and a signature direction.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/8a54d15e-67f5-4467-8396-437868064eeb/cpz1355-fig-0036-m.jpg</p>\nFigure 36\nEnrichment search field that allows for selection gene set library, gene set within the library, and choice of up-regulated or down-regulated signatures.\n8. Query the example by clicking \u201cSearch enriched samples.\u201d The corresponding samples are highlighted within the scatter plot and added to the \u201cSearch Result\u201d table along with the previous queries (Fig. 37[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0037]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6fee6941-2f4e-4404-be65-78669d0b4f6a/cpz1355-fig-0037-m.jpg</p>\nFigure 37\nExample query from the enrichment search visualized in the 3D scatter plot. The identified samples are added to the \u201cSearch Result\u201d table.\nGene-centric visualization\n9. Switch to gene-centric searches by clicking on the orange button under the \u201cSpecies\u201d field in the upper left. Use this field to also switch between human and mouse samples by clicking the corresponding teal button (Fig. 38[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0038]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/46539dd0-854a-476b-8c11-9ec50281f78b/cpz1355-fig-0038-m.jpg</p>\nFigure 38\nSelection buttons for switching between human and mouse samples, as well as buttons for switching between sample queries and single-gene queries.\n10. The page will now contain an interactive t-SNE scatter plot where each point represents a gene instead of a sample (Fig. 39[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0039]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/ba65fe03-fcd0-4c31-ba88-56e39f301d28/cpz1355-fig-0039-m.jpg</p>\nFigure 39\nScatter plot of single genes instead of samples where the distance between genes quantifies similarity of their expression profiles across all samples in ARCHS4.\n11. Choose a gene set library and a gene set within the \u201cSearch\u201d field on the left (Fig. 40[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0040]). Query the default options by clicking \u201cSearch genes.\u201d\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/36b5d423-923e-46c4-8de2-375fa1bcacfe/cpz1355-fig-0040-m.jpg</p>\nFigure 40\u201cSearch genes by gene set\u201d field where a gene set library and gene set within the library are selected to be queried.\n12. The corresponding samples are highlighted within the scatter plot and added to the \u201cSearch Result\u201d table under the \u201cGenes\u201d section (Fig. 41[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0041]). The table includes the number of genes included in the queried gene set which can be clicked to view the gene symbols in the gene set (Fig. 42[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0042]). Additionally, the gene set can be submitted to Enrichr (Kuleshov et\u00a0al., 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0028]) for gene set enrichment analysis by clicking on the Enrichr icon within the table (Fig. 43[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0043]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/7b177ad3-4d2f-40c7-bd05-91f73356cd83/cpz1355-fig-0041-m.jpg</p>\nFigure 41\nGenes from the selected gene set library and gene set are displayed on the scatter plot. The genes are added to their respective section in the \u201cSearch Result\u201d table.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/b645c339-a6dd-4c4b-b0a7-174a3d0adba3/cpz1355-fig-0042-m.jpg</p>\nFigure 42\nClicking on the number of genes in the \u201cSearch Result\u201d table displays the genes included in the queried gene set.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/af742546-d19e-4cc0-a0de-b4d845151484/cpz1355-fig-0043-m.jpg</p>\nFigure 43\nClicking on the Enrichr icon in the \u201cSearch Results\u201d table displays gene set enrichment analysis results for the genes from the queried gene set.\nGene search\n13. Single genes can be queried using the autocomplete field within the \u201cSearch\u201d field on the left. Input a gene of interest, for example SOX2, and click the search button (Fig. 44[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0044]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a39e83d9-642c-4445-9bf8-9f3d726d3952/cpz1355-fig-0044-m.jpg</p>\nFigure 44\n\u201cSearch genes\u201d field populated with the gene symbol \u201cSOX2\u201d.\n14. A single-gene page is generated for SOX2 (Fig. 45[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0045]). The top of the page includes a description of the gene and links to other resources with identifying metadata for the gene. The \u201cFunctional Annotation Prediction\u201d section contains ROC curves and tables of gene sets from six distinct gene set libraries SOX2 is predicted to be a member of based on co-expression. Known associations are marked in teal.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/5bce2a38-d300-49b7-bb04-27424d5bc718/cpz1355-fig-0045-m.jpg</p>Figure 45\nSingle-gene page for SOX2 with identifying metadata at the top of the page. Additionally, tables of predicted functions from various gene set libraries are depicted along with ROC curves to quantify the ability to predict gene sets that SOX2 is a known member of from co-expression data.\n15. The \u201cMost similar genes based on co-expression\u201d section contains a table of the top 100 genes that are most similar to SOX2 based on the Pearson correlation of their expression across all ARCHS4 samples (Fig. 46[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0046]). The most correlated genes from the table can be submitted to Enrichr by clicking the corresponding link in the top right.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a5379a4f-ae45-4fe7-952c-221612cb2d63/cpz1355-fig-0046-m.jpg</p>\nFigure 46\nTable of the top 100 genes most similar to SOX2 based on co-expression. The genes can be submitted to Enrichr by clicking the \u201cUpload to Enrichr\u201d button.\n16. The \u201cTissue Expression\u201d section contains a dendrogram of tissue types divided into organs and cell types. The average expression of SOX2 within a specific tissue or a cell type context is visualized as a collection of box plots (Fig. 47[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0047]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/3871b4b2-4d02-4d96-9243-0af3605671b9/cpz1355-fig-0047-m.jpg</p>\nFigure 47\nTissue expression atlas for SOX2 that quantifies the expression of SOX2 in various tissue types.\n17. The \u201cCell Line Expression\u201d section contains a dendrogram of various cell lines organized by the tissue of origin. The plot visualizes the average expression of SOX2 across the cell lines based on data from ARCHS4 (Fig. 48[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0048]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/877a2c59-053e-4d01-9f82-fdd5243e4b19/cpz1355-fig-0048-m.jpg</p>\nFigure 48\nCell line expression atlas for SOX2 that quantifies the expression of SOX2 in various cell lines.\nDownloading gene expression data from ARCHS418. As described in previous steps, after submitting a search within the data search and visualization page, the \u201cSearch Results\u201d table includes a download link to an R script that can be used to retrieve the selected samples. Click the download icon to download the script.\n19. Open R Studio and copy and paste the R script from the downloaded R file into R Studio.\n20. Ensure that the \u201crhdf5\u201d library is installed. Open the console in R Studio and input the following:\n         \nif (!requireNamespace(\"BiocManager\", quietly = TRUE))\n\u2003install.packages(\"BiocManager\")\nBiocManager::install(\"rhdf5\")\n21. Now run the R script downloaded from ARCHS4 to produce an expression matrix for the selected samples that were returned from the search. The expression matrix can be used for further analysis; for example, it can be used to compute the average expression of a gene in a specific disease, cell line, or tissue contexts.PrismEXP is an Appyter (Clarke et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0012]; Lachmann et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0030]) that employs machine learning to predict gene function using gene-gene mRNA co-expression correlations from mRNA-sequencing (RNA-seq) data sourced from ARCHS4, a database composed of human and mouse RNA-seq sample gene counts from GEO (Lachmann et\u00a0al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0032]). The difference between gene function predictions made by PrismExp and the gene function prediction available from the ARCHS4 website is that the ARCHS4 data is divided first into clusters, and then gene-gene correlations are computed for each cluster. 51 correlation matrices are precomputed and stored in the cloud. At runtime, the correlation data is extracted from the cloud storage and a pretrained Random Forest model is applied on the correlation features to rank the level of association of a single gene to all gene sets from a user-specified gene set library.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nNavigating the input form\n1. Navigate to the PrismEXP Appyter (https://appyters.maayanlab.cloud/PrismEXP/[href=https://appyters.maayanlab.cloud/PrismEXP/]).\n2. The Appyter input form includes a \u201cGene Selection\u201d section with a field for inputting a gene symbol of interest for which novel functions will be predicted. Additionally, the \u201cGMT Selection\u201d section includes a field for selecting a GMT file from which predictions will be made (Fig. 49[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0049]). Click the \u201cUpload\u201d button within the \u201cGMT Selection\u201d section to upload a custom GMT file (Fig. 50[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0050]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e73a9efb-ebc6-490d-8a96-56bdb73ccc90/cpz1355-fig-0049-m.jpg</p>\nFigure 49\nPrismEXP Appyter input form where the user is prompted to input a gene symbol of interest and specify a gene set library (in GMT format) to make predictions from.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/0b175edf-5aca-4b8d-b388-018604be29d3/cpz1355-fig-0050-m.jpg</p>\nFigure 50\nAlternative input form option for uploading a custom GMT file.3. Click submit on the Appyter input form, and a Jupyter Notebook with the input parameters will be launched in the cloud.\nGene function predictions\n4. A Jupyter Notebook will begin executing in the cloud once the input form is submitted. The notebook includes an option to download the notebook, toggle display of the code, and run the notebook locally. Additionally, a table of contents exists with clickable elements that link to specific sections within the notebook (Fig. 51[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0051]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/fd57371a-ecc7-4182-a30f-1f79c203747f/cpz1355-fig-0051-m.jpg</p>\nFigure 51\nThe launched Appyter notebook with options to download the notebook, toggle the code, and instructions for running the Appyter locally. Additionally, a table of contents on the left allows for easy traversal between sections of the notebook.\n5. Scroll down to the \u201cLoad Gene Correlation\u201d section. The Dataframe displays genes that correlate with your query gene in 51 pre-computed correlation matrices from ARCHS4 (Fig. 52[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0052]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/fb2c4e3e-9fae-474b-9184-a2d8455b1637/cpz1355-fig-0052-m.jpg</p>\nFigure 52\nDataframe of 51 correlation matrices, each displaying correlation values between the query gene and other mouse genes.\n6. Scroll down to the \u201cAvg Correlation Scores\u201d section. This Dataframe displays computed correlation scores to each of the gene set terms from the GMT file based on co-expression values between the query gene and each of the genes included in the gene set (Fig. 53[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0053]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/9d937ba7-132e-452f-adbc-69ae921bc98d/cpz1355-fig-0053-m.jpg</p>\nFigure 53\nDataframe of average correlations between each gene set from the specified gene set library and the query gene from the previous 51 correlation matrices.\n7. The average correlation score matrices are used as the input features for the PrismEXP model. Scroll down to the \u201cPrediction Validation\u201d section. The ROC curve displayed in this section characterizes how well the known annotations for this gene were recovered by the PrismEXP model (Fig. 54[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0054]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/dfe2a1d4-0ed7-475d-ac4c-56d30514f75c/cpz1355-fig-0054-m.jpg</p>\nFigure 54ROC curve that quantifies the ability of the PrismEXP model to retrieve previously known associations between gene set annotations and the query gene.\n8. Scroll down to the \u201cTop Predictions\u201d section. The Dataframe displays the top 20 gene set terms that the query gene is predicted to be associated with. The table displays the prediction score from the model, z-score, p-value, and Bonferroni corrected p-value (Fig. 55[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0055]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/aecf5053-f960-4d68-a212-42c2f490463a/cpz1355-fig-0055-m.jpg</p>\nFigure 55\nTable of top predicted associations for the query gene.\n9. Scroll down to the \u201cDownload Files\u201d section. Click on the appropriate link to download the prediction table or ROC curve in .pdf or .png format (Fig. 56[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0056]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/80e0cc12-78df-4c1a-8451-fa237a201452/cpz1355-fig-0056-m.jpg</p>\nFigure 56\nDownload links to prediction table and ROC curve image.Geneshot is a search engine for querying biomedical terms to retrieve lists of genes most associated with the term from PubMed ID (PMID) co-mentions (Lachmann et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0031]). To convert search terms to genes, Geneshot uses one of two resources: GeneRIF and AutoRIF. Both GeneRIF and AutoRIF are text files documenting gene-PubMed ID associations. These associations are used to rank genes for a query term based on the number of co-mentions. Geneshot further prioritizes other related genes based on co-occurrence and co-expression matrices with the genes associated with the term from the literature. Additionally, Geneshot includes a gene function prediction feature that prioritizes novel gene set membership for a query gene based on co-occurrence or co-expression.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nText editor or development environment of choice, such as Visual Studio (https://visualstudio.microsoft.com/vs/[href=https://visualstudio.microsoft.com/vs/]); most updated version of Python (https://www.python.org/downloads/[href=https://www.python.org/downloads/]); and Python requests library (https://requests.readthedocs.io/en/master/user/install/[href=https://requests.readthedocs.io/en/master/user/install/])\nPubMed query\n1. Navigate to the Geneshot homepage (https://maayanlab.cloud/geneshot/[href=https://maayanlab.cloud/geneshot/]).\n2. The PubMed Query page includes an input form for submitting search terms (Fig. 57[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0057]). The top search bar is for terms that the search should include, whereas the lower search bar is for terms that should be omitted from the search. Toggle the size of the gene set that will be used to make further predictions with the \u201cTop Associated Genes to Make Predictions\u201d filter. Use the toggle bar to switch between AutoRIF and GeneRIF (Maglott, Ostell, Pruitt, & Tatusova, 2011[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0037]) as the underlying databases for gene-PMID associations. Click \u201cWound Healing\u201d in the example section of the input form to launch a search (Fig. 58[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0058]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e6e8ca67-bee2-4e3b-ba5a-3250a81d18a9/cpz1355-fig-0057-m.jpg</p>\nFigure 57Geneshot homepage. The search bars allow for querying terms to be included and omitted from the search. Additional options exist for toggling between GeneRIF and AutoRIF and adjusting the gene set size for making predictions.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/20c02b96-b1b3-4f63-b52c-42f5eeea9012/cpz1355-fig-0058-m.jpg</p>\nFigure 58\nSubmitted search form populated with the term \u201cWound healing.\u201d\n3. The first output from the search is a scatter plot of all genes associated with \u201cwound healing\u201d (Fig. 59[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0059]). The x-axis of the scatter plot displays the counts of Publications with Search Term, and the y-axis shows the fraction of Publications with Search Term/Total Publications. Hover over any point on this plot to display the gene name and its corresponding x and y values.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/223189f8-32ff-4513-8561-2fc294dde907/cpz1355-fig-0059-m.jpg</p>\nFigure 59\nScatter plot of all genes associated with \u201cwound healing.\u201d Each point represents a gene and interacting with any point reveals the gene name, x-axis value, and y-axis value.\n4. Clicking on any of the points in the scatter plot generates a histogram displaying the association of the gene with the search terms based on literature co-mentions over time (Fig. 60[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0060]). The number of publications for the selected gene that do not match the search term is displayed as pink bars, while the number of publications matching the search term and the gene is displayed as blue bars.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f845d256-286c-4813-8a5a-a38cd5c871f8/cpz1355-fig-0060-m.jpg</p>\nFigure 60\nClicking on any of the points in the scatter plot generates a histogram of associations between the gene and \u201cwound healing\u201d over time. The blue bars represent publications mentioning the gene and search term, whereas purple bars represent publications mentioning just the gene.5. Scroll down to view the tables of associated genes and predicted genes (Fig. 61[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0061]). The left table includes the top genes associated with \u201cwound healing\u201d ranked by number of PubMed ID co-mentions. The right table shows the top 200 genes predicted to be associated with \u201cwound healing\u201d based on co-expression with the top 20 genes from the associated table. Each of the tables include a row of buttons that, when clicked, filter the genes from each table into a specific gene family. Additionally, the genes from each table can be submitted to Enrichr for gene set enrichment analysis, and each table itself can be downloaded.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/8dad3ea8-e62f-4859-87c6-f57527f613f4/cpz1355-fig-0061-m.jpg</p>\nFigure 61\nTable of top genes associated with \u201cwound healing\u201d ranked by number of publications that mention the gene and search term (left). Table of genes predicted to be associated with \u201cwound healing\u201d based on co-expression with the literature-derived genes (right). Both tables can be downloaded and the genes from both tables can be submitted to Enrichr for gene set enrichment analysis.\n6. To recalculate the predictions, use the drop-down menu above the associated table to select a new gene-gene similarity matrix and increase or decrease the associated gene set size using the scroll bar. Click the \u201cRecalculate Predictions\u201d button to update the prediction table (Fig. 62[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0062]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/ac970c0e-6993-41d6-a980-cccd371d9b27/cpz1355-fig-0062-m.jpg</p>\nFigure 62\nThe predicted gene table from the \u201cwound healing\u201d search can be recalculated by selecting a different gene-gene similarity matrix for predictions and changing the gene set size derived from the associated gene table.\nGene function predictions7. Navigate to the Gene Function Prediction page by clicking the corresponding link within the navigation bar at the top of the page. This page includes an input form for selecting a gene of interest, Enrichr gene set library from which gene functions will be sourced from, and a gene-gene similarity matrix from which predictions will be calculated (Fig. 63[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0063]). By using functional prediction by association, the input gene can be predicted to be a member of gene sets. Click the example to launch a query.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/7b9f9022-86c3-4352-8d7a-72d0b863bab8/cpz1355-fig-0063-m.jpg</p>\nFigure 63\nGene function prediction page. The input form allows for the selection of a query gene, a gene set library from which gene sets with functional association terms will be retrieved, and a gene-gene similarity matrix from which predictions will be made.\n8. A table of the top predicted functions and ROC curve of prediction performance are generated (Fig. 64[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0064]). Known associations within the table are highlighted in blue, whereas previously unknown associations are not highlighted. The table is available for download.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/175e7c04-6845-40c3-ad93-35561ab1d576/cpz1355-fig-0064-m.jpg</p>\nFigure 64\nTable of top predicted associations for TNF from the KEGG Pathways gene set library. Known functions are highlighted in blue. The ROC curve quantifies the ability of the prediction method to retrieve functions that TNF is known to be associated with.\nGene set augmentation\n9. Navigate to the Gene Set Augmentation page by clicking the corresponding link within the navigation bar at the top of the page. The input form on this page includes a text box for pasting a gene set for augmentation, a drop-down menu of gene-gene similarity matrices from which predictions will be calculated, and a toggle bar for switching between GeneRIF and AutoRIF for retrieving publication counts for each gene (Fig. 65[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0065]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/45f5cb0e-ba8c-4222-8f06-6c3fdb1bcd0a/cpz1355-fig-0065-m.jpg</p>\nFigure 65Gene set augmentation page. The text box accepts a list of gene symbols that will be used as an unweighted gene set to predict related genes based on the selected gene-gene similarity matrix. The source of gene publication data can be changed with a toggle bar between GeneRIF and AutoRIF.\n10. Click on the \u201cmixed genes\u201d example to submit a query. The input genes are first sorted into quantiles based on their novelty in the literature (Fig. 66[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0066]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/edbf8017-932e-4a81-83fc-c8df45fee8bc/cpz1355-fig-0066-m.jpg</p>\nFigure 66\nThe \u201cmixed genes\u201d example query with the quantile counts for each of the queried genes.\n11. Scroll to the bottom of the page where there is a table with the submitted genes on the left, and a table of genes predicted to be associated with the input genes based on the selected gene-gene similarity matrix, in this case ARCHS4 co-expression, on the right (Fig. 67[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0067]). The \u201cuser upload\u201d table ranks the genes by the amount of PubMed abstracts they are mentioned in, along with their novelty. The predicted genes table ranks genes by their similarity score with the input gene set. Genes from both tables can be submitted to Enrichr for gene set enrichment analysis, and each table can be downloaded.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/ad2b116c-1fb5-4d96-a24e-99cfd64ce328/cpz1355-fig-0067-m.jpg</p>\nFigure 67\nTable of queried genes, their publication counts, and novelty (left). Table of top 200 genes predicted to be associated with the query gene set, gene publication counts, and similarity score with the query gene set (right). Each table can be downloaded and the genes from each table can be sent to Enrichr for gene set enrichment analysis.\nGeneshot API example\n12. Open a new or existing Python code file. Import the JSON and requests libraries at the top of the file as follows.\n         \nimport json\nimport requests13. Call the requests.post method to send a POST request to the URL. The payload variable contains the parameters that are sent to the API endpoint specified in GENESHOT_URL. In this case the endpoint is /search and the parameters are rif, which specifies whether AutoRIF or GeneRIF is used as the association file, and term, which specifies the query term for the search.\n         \nGENESHOT_URL = \u02c8https://maayanlab.cloud/geneshot/api/search\u02c8\npayload = {\"rif\": \"generif\", \"term\": \"hair loss\"}\nresponse = requests.post(GENESHOT_URL, json=payload)\ndata = json.loads(response.text)\nprint(data)\n14. Use the json.loads method to view the response as a JSON object containing all genes related to the query term.\n         \n{\n\u2003\"PubMedID_count\": 34412,\n\u2003\"gene_count\": {\n\u2003\u2003\"ABCC6P2\": [\n\u2003\u2003\u2003\u20031,\n\u2003\u2003\u2003\u20030.25\n\u2003\u2003],\n\u2003\u2003\"ABI3\": [\n\u2003\u2003\u2003\u20032,\n\u2003\u2003\u2003\u20030.125\n\u2003\u2003],\n\u2003\u2003...\n\u2003\u2003\u2003},\n\u2003\u2003\u2003\"query_time\": 1.121943712234497,\n\u2003\u2003\u2003\"return_size\": 298,\n\u2003\u2003\u2003\"search_term\": \"hair loss\"\n}\nFor more information on using the various Geneshot API endpoints, please refer to the API documentation (https://maayanlab.cloud/geneshot/api.html[href=https://maayanlab.cloud/geneshot/api.html]).TIN-X (Target Importance and Novelty eXplorer; Cannon et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0011]) is an informatics workflow, REST API, and web application used to identify, visualize, and explore protein-disease associations. TIN-X is based on text mining data processed from scientific literature. TIN-X visualizations plot information for protein-disease associations along two axes, specifically \u201cnovelty\u201d and \u201cimportance.\u201d Briefly, Novelty is used to estimate the scarcity of publications about a protein target, whereas Importance estimates the strength of the association between that protein and a specific disease.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nBrowse diseases\n1. Navigate to the TIN-X web app (https://www.newdrugtargets.org/[href=https://www.newdrugtargets.org/]).\n2. The default TIN-X mode, \u201cBrowse Diseases,\u201d (upper-left) starts with the Disease Ontology (DO; Schriml et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0048]; see Internet Resources). The DO hierarchy can then be navigated using the left panel (Fig. 68[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0068]). Given this hierarchical nature, a larger number of target-disease associations can be text-mined from biomedical literature for higher-level terms (e.g., N = 13405 for \u201cnervous system disease\u201d), as opposed to child terms (e.g., N = 9733 for \u201cneurodegenerative disease,\u201d N = 4587 for \u201cSynucleinopathy,\u201d N = 4587 for \u201cParkinson's Disease\u201d) or leaf terms (e.g., N = 227 for \u201cEarly Onset Parkinson's Disease\u201d).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/90da5617-44f3-4712-bcc8-26dbd1dfa6e2/cpz1355-fig-0068-m.jpg</p>\nFigure 68\nThe TIN-X \u201cBrowse Disease\u201d view (left side) with Parkinson's Disease selected. Targets associated with Parkinson's Disease (right side) are plotted on a log scale of Importance versus Novelty, with each data point colored according to its Target Development Level (TDL).3. Searching by disease name is also supported. Targets with stronger associations (higher Importance) are in the upper part of the plot, while targets with a higher number of publications (lower Novelty) are located on the left side of the plot. Points situated in the upper-right area of the plot (if any) are most likely to be of interest, as they are located at the Pareto frontier, i.e., targets for which a large number of published papers mentioning that target also mention the selected disease.\n4. Targets are colored by Target Development Levels, and can be filtered as such (Tclin/Tchem/Tbio/Tdark). They can also be filtered by protein superfamily (e.g., kinases). Upon selecting a protein, links to both Pharos and DrugCentral are provided for that protein (Fig. 69[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0069]); selecting the titles allows the user to navigate through abstracts or to examine the document of interest in PubMed (additional clicks are required).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/def6b893-e517-4251-8237-f242d3972cb0/cpz1355-fig-0069-m.jpg</p>\nFigure 69\nClicking a target point within the Parkinson's Disease example, \u201cSynaptogyrin-3\u201d (SYNGR3) displays details including the full name and family of the target, Target Development Level (TDL), links to Pharos and DrugCentral, and, importantly, links to the associated two research articles (bottom).\n5. Once the desired level of granularity for diseases is reached, the user can examine target-disease associations, which are plotted along the Novelty-Importance axes in log-log format. To reach \u201cParkinson's Disease,\u201d one must click Disease of anatomical entity \u2192 Nervous System Disease \u2192 Neurodegenerative disease \u2192 Synucleinopathy \u2192 Parkinson's Disease.6. A highly-ranked gene associated with Parkinson's Disease is \u201cSynaptogyrin-3\u201d (SYNGR3) and is classified as Tdark (Fig. 69[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0069]). While the exact function of SYNGR3 is unknown, there is recently published evidence that SYNGR3 encodes for a synaptic vesicle protein that interacts with a dopamine transporter (Ega\u00f1a et\u00a0al., 2009[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0017]). The most novel association (lowest Importance) is for \u201cTripartite motif-containing protein 10\u201d (TRIM10), which is supported by one genome-wide association study (Witoelar et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0063]) focused on the overlap between Parkinson's Disease and autoimmune diseases.\n7. Both the \u201cBrowse Diseases\u201d and the \u201cBrowse Targets\u201d exploratory modes support an interactive way to manipulate the number of points displayed on the scatter plot. To change the number of plotted points, simply go to the top right side of the panel, where a vertical bar is placed between a \u201c+\u201d and a \u201c-\u201d sign. Sliding this bar up or down increases or decreases the number of visible points within the plot. By default, 300 or fewer points are plotted. Thresholds are defined by non-dominated solution (NDS) ranking, a.k.a. Pareto frontier, meaning that all hidden points are inferior to those visible in one or both variables.\nBrowse targets\n8. From the upper left menu, \u201cBrowse Targets\u201d can be selected. The Drug Target Ontology (Lin et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0034]) hierarchy becomes visible, and can be navigated from the left panel (Fig. 70[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0070]). For each protein, Diseases are plotted with log\u2013log Importance\u2013Novelty axes and color-coded according to the top hierarchical Disease Ontology term (e.g., diseases of anatomical entity, diseases of metabolism, etc.).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/58f5c6e9-340e-4928-bd1c-9d7b6d27539d/cpz1355-fig-0070-m.jpg</p>\nFigure 70\nStarting with the superfamily Kinase, the user can further refine the selection to Protein kinase \u2192 CAMK group \u2192 TRIO family \u2192 Kalirin by using the left navigation pane within Browse Targets.9. Searching by target name is supported. Diseases with stronger associations (higher Importance) are in the upper part of the plot, while diseases with a higher number of publications (lower Novelty) are on the left side of the plot. Diseases that are likely of most interest are plotted in the upper-right area of the plot (Fig. 71[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0071]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/213848b8-82c7-46c4-87df-6903f6a389eb/cpz1355-fig-0071-m.jpg</p>\nFigure 71\nWithin \u201cBrowse Targets,\u201d diseases associated with Kalirin (KALRN) are plotted with log\u2013log Importance\u2013Novelty axes, and are colored according to the top hierarchical Disease Ontology term.\n10. The plot, however, remains target-centric. Upon clicking on a point, the disease name and protein name are displayed, with appropriate links to Pharos and DrugCentral (Fig. 72[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0072]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/eb77d038-cf86-4267-ba92-4c406a5bdb05/cpz1355-fig-0072-m.jpg</p>\nFigure 72\nFor the example target Kalirin (KALRN), the most novel association (lowest Importance) is for \u201cX-linked nonsyndromic deafness.\u201d This detailed view includes the full name and family of the target, links to Pharos and DrugCentral, and in this case, the one article responsible for this association between KALRN and X-linked nonsyndromic deafness.\n11. When selecting a target family (e.g., kinase), the user can drill down to the desired level of granularity before examining disease associations for a specific protein. Starting from Kinase, for example, the user must click Protein kinase \u2192 CAMK group \u2192 TRIO family \u2192 Kalirin, before diseases associated with Kalirin (KALRN) are displayed (Fig. 70[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0070]).12. The top disease (highest Importance, lowest Novelty) associated with KALRN is \u201cdisease by infectious agent,\u201d followed by \u201cpsychotic disorder.\u201d We recommend repeated scrolling before identifying a leaf term corresponding to the Disease Ontology (see Internet Resources). For example, next to \u201cpsychotic disorder\u201d is \u201cschizophrenia\u201d (a child term); this association is supported by 26 publications, including Miller et\u00a0al. (2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0039]). The most novel association (lowest Importance) is for \u201cX-linked nonsyndromic deafness\u201d (Fig. 72[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0072]), supported by Cai et\u00a0al. (2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0010]). This association is genuine, as the gene name (KALRN) is mentioned in the abstract, in relation to the rs333332 SNP.\nSharing and downloading data\n13. Whether in \u201cBrowse Diseases\u201d or \u201cBrowse Targets\u201d mode, the user can share data in two ways. First, for any given plot, the specific URL (universal resource locator) for that visualization can be copied and shared with third-party users. This can be done by clicking on the \u201cShare\u201d button. Second, the data can be exported (in comma-separated value format), and thus archived or post-processed with third-party software. Exported data includes Novelty and Importance scores, in addition to Disease names and identifiers in the \u201cBrowse Targets\u201d mode, as well as Target names and identifiers in the \u201cBrowse Diseases\u201d mode, respectively.DrugCentral is an online compendium (Ursu et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0057]) centered on \u201cactive pharmaceutical ingredients\u201d and their link to \u201cpharmaceutical products.\u201d DrugCentral distills relevant information from \u201cpharmaceutical product\u201d (or formulation) package inserts; while these are frequently referred to as \u201cdrugs\u201d by patients and medical practitioners, herein we reserve the term \u201cdrugs\u201d for \u201cactive pharmaceutical ingredients.\u201d All data, including downloads, related to DrugCentral can be accessed at its designated web portal (https://drugcentral.org/[href=https://drugcentral.org/]). DrugCentral provides information on active ingredients, chemical entities, pharmaceutical products, drug mode of action, medical uses (indications, contra-indications and off-label uses), and pharmacologic action, as well as adverse events (Ursu et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0056]). As of 2021, DrugCentral (Avram et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0003]) separately stores adverse events for women and men, and provides regulatory information extracted from the FDA Orange Book (see Internet Resources). DrugCentral is current (as of the date of the release) with regulatory approvals from the United States (US FDA), the European Union (EMA), Japan (PDMA) and, more recently, some drugs approved in China and Russia. Limited information on drugs that have been discontinued or withdrawn is available, particularly for drugs approved outside the U.S. when package inserts and relevant information are not in English.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a 100 Mbps or higher (fast) Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nQueries supported by DrugCentral\n1. Navigate to the DrugCentral portal (https://drugcentral.org/[href=https://drugcentral.org/]).\n2. The main DrugCentral search bar supports three types of queries: drug, target, and disease. Each of these will filter and prioritize results according to a four-level ranking system ordered from highest to lowest, as follows:\n         \nQuery term matching drug name (or synonyms) mechanism of action target, or drug indication (see below).Query term matching disease term in drug contraindications or off-label uses, targets listed in drug bioactivity profiles (not MoA targets), or pharmacologic action descriptions.\nQuery term matching the short drug description text.\nQuery term matching full text in the FDA drug labels processed from DailyMed (Fig. 73[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0073]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/50b941d8-f5be-4315-99fa-9ff5ce217bf8/cpz1355-fig-0073-m.jpg</p>\nFigure 73\nDrugCentral homepage. DrugCentral search bar supports three types of queries: drug, target, and disease.\n3. For example, drug query results are sorted to display active ingredients first (e.g., omeprazole), followed by related ingredients (e.g., esomeprazole) and by other active ingredients that are co-formulated with the queried substance into pharmaceutical products. A query by brand name (e.g., Prilosec) includes other antacids such as sodium bicarbonate, antibiotics such as amoxicillin and clarithromycin (co-prescribed with omeprazole to treat stomach ulcers caused by Helicobacter pylori), as well as acetyl-salicylic acid, which is combined with omeprazole for the prevention of stroke. (Fig. 74[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0074])\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e071e781-de44-434f-9216-bb6561d361d3/cpz1355-fig-0074-m.jpg</p>\nFigure 74\nDrugCentral search results for \u201cOmeprazole\u201d first lists drugs indicated for \u201cOmeprazole\u201d (e.g., sodium bicarbonate) followed by drugs indicated in complications.\n4. Disease names are mappable to multiple terminologies such as Disease Ontology, MeSH, SNOMED-CT, and MedDRA. Disease term queries first retrieve indications, followed by off-label and contraindications, then other sections (e.g., side effects) that contain medical/disease terms. For example, the query \u201cParkinson's disease\u201d (PD) first lists drugs indicated for PD (e.g., ropinirole), followed by drugs indicated in complications of PD (e.g., fludrocortisone is indicated for the PD-associated orthostatic hypotension), then by drugs that list PD as side-effect (e.g., dimenhydrinate) (Fig. 75[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0075]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/45b8e384-2337-4fbc-93c7-1ed2abb93cfb/cpz1355-fig-0075-m.jpg</p>\nFigure 75\nDrugcentral query result for \u201cParkinson's disease\u201d (PD) first lists drugs indicated for PD (e.g., ropinirole), followed by drugs indicated in complications of PD (e.g., fludrocortisone is indicated for the PD-associated orthostatic hypotension), then by drugs that list PD as side-effect (e.g., dimenhydrinate).5. Target name queries support input as text (e.g., \u201cmuscarinic m1\u201d), gene symbol (CHRM1), or UniProt (P11229) and SwissProt (ACM1_HUMAN) identifiers. It is recommended to use the exact target names adopted by UniProt, though gene/protein identifiers are preferred.\nQueries supported by DrugCentral: REDIAL\n6. Given its basic science focus, the machine-learning-based REDIAL-2020 platform (KC et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0024]), which is also part of DrugCentral, supports queries by drug name (e.g., omeprazole), by PubChem compound identifier (e.g., 4594), or by chemical structure in the SMILES (Weininger, 1988[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0061]) format (e.g., COc1ccc2nc(S(=O)Cc3ncc(C)c(OC)c3C)[nH]c2c1). Regardless of format, all input queries for REDIAL-2020 are converted to SMILES format in order to predict anti-viral properties (Fig. 76[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0076]).\nAlso see Basic Protocol 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-prot-0008].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e77e6bc0-2069-4b6b-973a-72ef6f863bc8/cpz1355-fig-0076-m.jpg</p>\nFigure 76\nDrugCentral REDIAL query result for Omeprazole. All input queries for REDIAL-2020 are converted to SMILES format in order to predict anti-viral properties.\nQueries supported by DrugCentral: L1000\n7. The other search interface available in DrugCentral, implemented in R-Shiny (https://shiny.rstudio.com/[href=https://shiny.rstudio.com/]), supports browsing and searching for drug names for which gene perturbation profiles were recorded across one more of the 81 cell lines collected during the LINCS (Library of Integrated Cellular Signatures) project. Based on the L1000 perturbation profiles for 1613 drugs, the L1000 DrugCentral app allows users to query (via drug names) which drugs have the most similar gene perturbation profiles, ranked by cell lines (Fig. 77[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0077]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/7766546c-7c41-4f0f-898c-f06f9f939591/cpz1355-fig-0077-m.jpg</p>\nFigure 77\nThe L1000 search input home page. The L1000 DrugCentral app allows users to query (via drug names) which drugs have the most similar gene perturbation profiles, ranked by cell lines.\nDrugCentral Drugcards: A step-by-step content guide8. At its core, DrugCentral is a drug-centric resource. Thus, all queries are likely to provide information that is displayed in the form of \u201cdrug cards.\u201d Data elements identified when searching a drug by name would be thus retrieved in a similar manner when searching by target or by disease, as both queries result in lists of drug cards.\n9. Each drug card can be directly accessed (linked out) by observing the following (specific) format:\nhttps://drugcentral.org/drugcard/<DrugcentralStruct.ID>\nwhere \u201cDrugcentralStruct.ID\u201d is the DrugCentral structure ID number. For example, DrugcentralStruct.ID=824 resolves to dexamethasone. This manner of mining drug cards is not intended for casual users. Rather, this format is intended for programmatic access to DrugCentral content (Fig. 78[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0078]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/8154b846-deb4-4a1b-962a-ba3a09c7d642/cpz1355-fig-0078-m.jpg</p>\nFigure 78\nDrugCentral Accession \u201cDrugcentralStruct.ID\u201d for cross referencing DrugCentral drug cards.\n10. What follows is a \u201csection by section\u201d guide to drug card content, shown by section title. These are not intended as comprehensive explanations, but rather as brief illustrations of the diverse content available through DrugCentral.\n11. \u201cStem definition\u201d displays International Nonproprietary Names (INN), which are associated with \u201cpharmacologically related groups\u201d; that section also displays Chemical Abstract Services (CAS) registry numbers, in addition to DrugCentral IDs.\n12. \u201cDescription\u201d depicts the two-dimensional chemical structure (as well as three separate chemical structure file formats), a number of synonyms, and computed chemical descriptors such as Lipinski's \u201crule of 5\u201d (Lipinski et\u00a0al., 2001[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0035]). The intellectual property/regulatory status of the drug (if available) is also shown under \u201cStatus,\u201d with one of three options\u2014OFP: off patent; OFM: off market; and ONP; on patent\u2014respectively (Avram, Curpan, Halip, Bora, & Oprea, 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0004]).\n13. \u201cDrug dosage\u201d provides a sample (typically, the \u201cmaximum dose strength\u201d) of the dosages available for oral/non-oral formulations of the drug.14. \u201cADMET Properties\u201d (Absorption, Distribution, Metabolism, Excretion, and Toxicity) provides experimental ADMET values, when available. These properties are half-life, systemic clearance, volume of distribution at steady state and fraction unbound, all intravenous pharmacokinetic parameters (Lombardo, Berellini, & Obach, 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0036]), the fraction excreted unchanged in urine (extent of metabolism), water solubility, and their composite parameter BDDCS (Biopharmaceutical Drug Disposition Classification System), as discussed elsewhere (Benet, Broccatelli, & Oprea, 2011[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0006]), and MRTD, Maximum Recommended Therapeutic Daily Dose (Contrera, Matthews, Kruhlak, & Benz, 2004[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0013]).\n15. \u201cApprovals\u201d shows the date of approval by regulatory agencies (if available).\n16. \u201cFDA adverse event reporting system (Female),\u201d followed by \u201cFDA Adverse Event Reporting System (Male)\u201d lists adverse events, separated by sex, in the decreasing order of the likelihood ratio (Huang, Zalkikar, & Tiwari, 2011[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0020]).\n17. \u201cPharmacologic action\u201d highlights the drug annotations corresponding to (sometimes multiple) ATC (Anatomical, Therapeutic, and Chemical) classification system codes available at WHOCC (see Internet Resources); chemical ontology information from ChEBI (EBI Web Team; see Internet Resources); and MeSH (Medical Subject Headings; see Internet Resources) terms from the MeSH Browser.\n18. \u201cDrug use\u201d lists indications, off-label use, and contra-indications, mapped to SNOMED-CT (Bhattacharyya, 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0008]) and DOID (Disease Ontology: Institute for Genome Sciences at the University of Maryland), where available. Drug indications and contra-indications are mined from package inserts (drug labels), whereas off-label uses are from literature.\n19. \u201cAcid dissociation constants calculated using MoKa v3.0.0\u201d shows calculated acid/base dissociation constants, as calculated with the MoKa software (Milletti et\u00a0al., 2010[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0040]).\n20. \u201cOrange Book patent data (new drug applications)\u201d and \u201cOrange Book exclusivity data (new drug applications)\u201d complement DrugCentral information on marketed pharmaceutical formulations by adding FDA Orange Book (Orange Book: Approved drug products with therapeutic equivalence evaluations; see Internet Resources) for patents, as well as exclusivity data, for new drug applications.21. \u201cBioactivity Summary\u201d distills information from multiple bioactivity databases, e.g., ChEMBL (Mendez et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0038]) and the IUPHAR Guide to Pharmacology (Armstrong et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0002]), in addition to scientific literature and information from drug labels. Numeric information is converted to the negative log molar of the effective drug concentration at measurement. Mechanism-of-action drug targets (Santos et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0047]) are marked separately.\n22. The \u201cExternal reference\u201d section contains drug identifiers used by other on-line resources. This section includes identifiers used in medical practice, such as the Veterans Health Administration (e.g., VHA unique identifier, VUID), the National Drug File reference terminology (NDFRT; see Internet Resources) and RxNorm (see Internet Resources), as well as identifiers used by PubChem, ChEBI, DrugBank, etc.\n23. Last but not least, the \u201cPharmaceutical products\u201d section provides direct links to DailyMed (https://dailymed.nlm.nih.gov/dailymed/[href=https://dailymed.nlm.nih.gov/dailymed/]), while incorporating simple meta-data descriptors such as \u201ccategory\u201d (e.g., prescription vs. over-the-counter), number of ingredients, administration route, etc. This section also includes a clickable container that captures the full text (no images) of the FDA approved package insert.\nDrugCentral Target Cards: A step-by-step content guide\n24. In Addition to DrugCentral's Drugcards, a set of Target Cards can be directly accessed by observing the following (URL) syntax:\n         \nhttps://drugcentral.org/target/\u2264UniprotAccession.ID>\n25. For example, https://drugcentral.org/target/P23975/[href=https://drugcentral.org/target/P23975/] resolves to Sodium-dependent noradrenaline transporter. This method of mining Target Cards is not intended for casual users. Rather, this format is intended for programmatic access to machine readable Target metadata (Fig. 79[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0079]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e8a10993-d25d-4f7e-be74-c71b9e0f91b3/cpz1355-fig-0079-m.jpg</p>\nFigure 79\nDrugCentral's Target Card. Target card depicts Accession, Swissprot, Organism, and Gene & Target class followed by Drug relations where the Drugs Bioactivity mechanism-of-actions are marked.\n26. What follows is a \u201csection by section\u201d guide to Target card content and target metadata.27. \u201cDescription\u201d depicts the Accession ,Swissprot, Organism, and Gene & Target class, followed by Drug relations where the Drugs Bioactivity mechanism-of-actions are identified and marked.\n28. To retrieve all cross-referenced Drug Central Targetcards mapped to Uniprot Accession Ids, use the following (machine readable) URL syntax (Fig. 80[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0080]):\nhttps://drugcentral.org/static/Drugcentral_uniprot_Mapping.txt[href=https://drugcentral.org/static/Drugcentral_uniprot_Mapping.txt]\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/85461967-0780-4264-a5ca-649f270d8ace/cpz1355-fig-0080-m.jpg</p>\nFigure 80\nUniprot Accession IDs used for cross-referencing and machine querying DrugCentral Targetcards (https://drugcentral.org/static/Drugcentral_uniprot_Mapping.txt[href=https://drugcentral.org/static/Drugcentral_uniprot_Mapping.txt]).\nAdditional information\n29. The \u201cDownload Database dump 9/18/2020 (Postgres v10.12)\u201d option contains all the information stored in DrugCentral. It requires a new or existing Postgres database setup. Users are directed to consult the Postgresql documentation on how to install, configure, and load database contents. This is also available via public instance at drugcentral:unmtid-dbs.net: 5433, username=\"drugman\", password=\"dosage\", with responsiveness depending on user load.\n30. Example queries to extract subsets of data from DrugCentral: These require a local instance of DrugCentral loaded into a PostgreSQL database. To load the DrugCentral database dump assuming PostgreSQL is up and running and the user has admin privileges, run the following in PostgreSQL:\n         \n#create database drugcentral and then run using the OS shell\n$gunzip -c drugcentral.dump.06212018.sql.gz | psql drugcentral#Example 1: Select Off-patent drugs that bind to \u201cMast/stem cell growth factor #receptor Kit\u201d as mode-of-action target\u201d in DrugCentral's Postgres Db.\n-select\n\u2003distinct(structures.name) as drug_name\n\u2003\u2003from\n\u2003structures\n\u2003\u2003join act_table_full on structures.id = act_table_full.struct_id\n\u2003\u2003Where\n\u2003\u2003structures.status =\u02c8OFP\u02c8 and\n\u2003act_table_full.moa = 1 and\n\u2003act_table_full.target_name = \u02c8Mast/stem cell growth factor receptor Kit\u02c8\n#Example 2: Select drugs indicated for seasonal allergic rhinitis that have #the lowest LLR for somnolence in males.\n\u2003-select\n\u00a0\n\u2003distinct(structures.name) as drug_name,\n\u2003\u2003faers_male.*\n\u2003\u2003from\n\u2003\u2003\u2003structures\n\u2003\u2003\u2003join struct2atc on structures.id = struct2atc.struct_id\n\u2003\u2003\u2003join atc on struct2atc.atc_code = atc.code\n\u2003\u2003\u2003join faers_male on structures.id=faers_male.struct_id\n\u2003\u2003Where\n\u2003\u2003\u2003\u2003atc.l2_name = 'ANTIHISTAMINES FOR SYSTEMIC USE' and\n\u2003\u2003\u2003\u2003faers_male.meddra_name = \u02c8Somnolence\u02c8 and\n\u2003\u2003\u2003\u2003faers_male.llr <= 2*faers_male.llr_threshold\n\u2003\u2003\u2003\u2003order by\n\u2003\u2003\u2003\u2003faers_male.llr asc31. To download additional example SQL queries for extracting subsets of data from DrugCentral, use the following URL: https://unmtid-shinyapps.net/download/example_query.sql[href=https://unmtid-shinyapps.net/download/example_query.sql].There is currently an urgent need to find effective drugs for treating coronavirus disease 2019 (COVID-19). DrugCentral REDIAL-2020 (KC et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0025]) is a suite of machine learning models that forecast activities for live viral infectivity, viral entry, and viral replication specifically for severe acute respiratory syndrome coronavirus 2 (SARS-CoV-2), in vitro infectivity, and human cell toxicity. This application serves the scientific community when prioritizing compounds for in vitro screening and may ultimately accelerate identifying novel drug candidates for COVID-19 treatment. REDIAL-2020 consists of eleven independently trained machine learning models using high-throughput screening data from the NCATS COVID19 portal (https://opendata.ncats.nih.gov/covid19/index.html[href=https://opendata.ncats.nih.gov/covid19/index.html]) and includes a similarity search module that queries the underlying experimental dataset for similar compounds. These models were developed using experimental data generated by the following assays: the SARS-CoV-2 cytopathic effect (CPE) assay and its host cell cytotoxicity counterscreen; the Spike\u2013ACE2 protein\u2013protein interaction (AlphaLISA) assay and its TruHit counterscreen, the angiotensin-converting enzyme 2 (ACE2) enzymatic activity assay; the 3C-like (3CL) proteinase enzymatic activity assay; the SARS-CoV pseudotyped particle entry (CoV-PPE) assay and its counterscreen (CoV-PPE_cs); the Middle East respiratory syndrome coronavirus (MERS-CoV) pseudotyped particle entry assay (MERS-PPE) and its counterscreen (MERS-PPE_cs); and the human fibroblast toxicity (hCYTOX) assay (Fig. 81[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0081]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/002b575c-3421-45c5-87f9-54f41b60d7fe/cpz1355-fig-0081-m.jpg</p>\nFigure 81\nREDIAL Home page with Search SMILES, drug names, and PubChem CIDs enabled.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a 100 Mbps or higher (fast) Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nREDIAL: A step-by-step content guide\n1. By accessing REDIAL-2020 (http://drugcentral.org/Redial[href=http://drugcentral.org/Redial]) from any web browser, including mobile devices, the submission page is displayed.2. The web server accepts SMILES, drug names or PubChem CIDs as input. Regardless of input, the protocol converts drug names (from DrugCentral) or PubChem CIDs into SMILES.\n3. The user interface provides a summary of the models, such as model type, which descriptor categories were used for training, and the evaluation scores.\n4. The user interface depicts the processes of cleaning the chemical structures (encoded as SMILES) before training the machine learning models (Fig. 82[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0082]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6d2dc644-c481-4b7b-af3a-87a0046cdb43/cpz1355-fig-0082-m.jpg</p>\nFigure 82\nREDIAL interface provides a summary of the models, such as model type, which descriptor categories were used for training, and the evaluation scores. The user interface further depicts the processes of cleaning the chemical structures (encoded as SMILES) before training the machine learning models.\n5. As an example, amodiaquine has been shown to have promising anti-SARS-CoV-2 behavior in several papers (Bocci et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0009]; Si et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0051]), but its mechanism of action has not been well established yet. When given as an input to REDIAL, the webapp opens a new window with the predicted activities.\n6. The prediction results table shows that amodiaquine is predicted to be active in cytopathic effect experiments, while there are no clues on its mechanism (inactive in AlphaLISA, ACE2, 3CL assays) (Fig. 83[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0083]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/c5dc8103-4840-4ddd-b776-7a3265366416/cpz1355-fig-0083-m.jpg</p>\nFigure 83\nREDIAL prediction results table with example search term \u201camodiaquine.\u201d Amodiaquine is predicted to be active in cytopathic effect experiments while there are no clues on its mechanism (inactive in AlphaLISA, ACE2, and 3CL assays).\n7. REDIAL-2020 links directly to DrugCentral for approved drugs and to PubChem for chemicals (where available), enabling easy access to further information on the query molecule (Fig. 84[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0084]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/748bafee-b578-4fc3-96c3-f27addde1b33/cpz1355-fig-0084-m.jpg</p>\nFigure 84REDIAL links directly to DrugCentral for approved drugs and to PubChem for chemicals (where available), enabling easy access to further information on the query molecule.\n8. Using REDIAL-2020 estimates, promising anti-SARS-CoV-2 compounds would ideally be active in the CPE assay while inactive in cytotox and in hCYTOX.\nQueries supported by REDIAL\n9. Input queries such as drug name and PubChem CID are converted to SMILES before processing. Each SMILES string input is subject to four different steps, namely, converting the SMILES into canonical SMILES, removing salts (if present), neutralizing formal charges (except permanent ones), and standardizing tautomers. REDIAL-2020 predicts input compound activity across all eleven assays: CPE, cytotox, AlphaLISA, TruHit, ACE2, 3CL, CoV-PPE, CoV-PPE_cs, MERS-PPE, MERS-PPE_cs, and hCYTOX (Fig. 85[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0085]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a34a711d-680e-4e68-90e1-6905dbc48bce/cpz1355-fig-0085-m.jpg</p>\nFigure 85\nREDIAL-2020 results page predicting compound activity across all eleven assays: CPE, cytotox, AlphaLISA, TruHit, ACE2, 3CL, CoV-PPE, CoV-PPE_cs, MERS-PPE, MERS-PPE_cs, and hCYTOX.\nAdditional information\n10. All of the codes and the trained models are available from: https://doi.org/10.5281/zenodo.4606720[href=https://doi.org/10.5281/zenodo.4606720].\n11. The source code and specific models are available through Github at: https://github.com/sirimullalab/redial-2020[href=https://github.com/sirimullalab/redial-2020], or via Docker Hub (https://hub.docker.com/r/sirimullalab/redial-2020[href=https://hub.docker.com/r/sirimullalab/redial-2020]) for users preferring a containerized version. All the pre-ML processing and \u201cdata cleaning\u201d scripts are here: https://github.com/sirimullalab/redial-2020/tree/master/data-cleaning[href=https://github.com/sirimullalab/redial-2020/tree/master/data-cleaning]\n12. All workflows and procedures were performed using the KNIME platform 10. The NCATS data associated with the aforementioned assays were downloaded from the COVID-19 portal. https://opendata.ncats.nih.gov/covid19/assays[href=https://opendata.ncats.nih.gov/covid19/assays].Drugmonizome (Kropiwnicki et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0027]) serves processed data extracted from drug and small molecule databases available from a variety of online repositories and data portals. The processed data is provided in the form of drug set libraries which serve as the underlying database for drug set enrichment analysis. Drugmonizome enables users to submit lists of drugs and small molecules as the input query. These drug sets are compared against various drug set libraries that contain known associations between drugs and their attributes, for example, side effects, indications, targets, pathways, induced gene expression signatures, and other attributes. Additionally, Drugmonizome provides options for querying metadata associated with drug sets to find relevant drugs, small molecules, and drug sets for a given free-text query.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nMetadata search\n1. Navigate to the Drugmonizome homepage (https://maayanlab.cloud/drugmonizome/[href=https://maayanlab.cloud/drugmonizome/]). The metadata search is displayed by default. Using the search bar, users can submit query terms of interest to identify resources, drug set libraries, drug sets, and small molecules contained in Drugmonizome. Example terms are suggested for each type of metadata search (Fig. 86[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0086]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/0804f5fc-0019-4a8b-b422-b89739966c52/cpz1355-fig-0086-m.jpg</p>\nFigure 86\nDrugmonizome metadata search page with drug set search enabled.\n2. Alternate between resource, drug set library, drug set, and small molecule metadata searches by clicking the corresponding tab. When performing metadata searches for drug sets, use the filter table to query terms within specific resources, drug set libraries, and association types.\n3. Upon submitting a term of interest using the search bar, a list of results that match the term is displayed (Fig. 87[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0087]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/2937b83f-4bf3-4da2-8be7-4e89cea0c350/cpz1355-fig-0087-m.jpg</p>\nFigure 87Drugmonizome metadata search page with example term \u201cHeadache\u201d queried using the search bar.\n4. Clicking on any term displays a page with identifying metadata for the resource, drug set library, drug set, or small molecule. When perusing drug set metadata, a search bar exists for querying specific small molecules of interest within the set (Fig. 88[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0088]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/eb539aac-22a3-4ddc-b545-f6141f7295ce/cpz1355-fig-0088-m.jpg</p>\nFigure 88\nDrug set page that includes identifying metadata for the drug set and the small molecules included in the drug set. The search bar can be used to query specific drugs or small molecules of interest.\nDrug set enrichment\n5. Navigate to the drug set enrichment page by clicking the corresponding tab on the website header. The drug set enrichment page includes a search box where a list of drugs and small molecules can be pasted. The page also includes several example drug sets that are pasted into the box when clicked (Fig. 89[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0089]). As an example, click the \u201c69 in vitro COVID-19 hits from a drug screen by Ellinger et\u00a0al.\u201d link to populate the search box with a small molecule set.\nNOTE: Drug and small molecule entities can be queried by name, DrugBank IDs, Broad Institute Accession Numbers (BRD-IDs), SMILES strings, and InChIKeys.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6a637340-7728-4da5-92fc-3d7028084778/cpz1355-fig-0089-m.jpg</p>\nFigure 89\nDrug set enrichment page with the \u201cEllinger et\u00a0al.\u201d example drug set pasted into the search box.\n6. Click the \u201cPerform Drug Set Enrichment Analysis\u201d button and a results page of all resources with enriched terms is returned. Each of the resources with enriched drug set libraries are represented as an icon with the number of enriched terms for each resource (Fig. 90[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0090]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/ee6ba075-6d8a-4699-81dd-a82620bbbfff/cpz1355-fig-0090-m.jpg</p>\nFigure 90Enrichment results page after submitting the \u201cEllinger et\u00a0al.\u201d example drug set. Each resource is represented by an icon and the number of enriched drug sets from each resource is displayed above the icon.\n7. Click on any of the resource icons to be redirected to a page with the top enrichment results for each drug set library represented by a toggleable bar graph or scatter plot. The drug set library enrichment results can be expanded by clicking the corresponding button (Fig. 91[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0091]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/9ad5b403-a784-4b7e-b5bc-36e4a34604d3/cpz1355-fig-0091-m.jpg</p>\nFigure 91\nAfter clicking on the SIDER resource, the top enriched terms from both drug set libraries from SIDER are displayed side by side. Bar charts and scatter plots visualize the top enriched terms. The view for a particular library can be expanded by clicking the \u201cexpand\u201d button.\n8. The expanded page includes the scatter plot, bar graph, and table view of the top enriched terms. The table representation displays the top enriched terms and their p-values, odds ratio, and corrected q-values. Terms of interest can be queried using the search bar above the table. The table is also available for download as a .TSV file (Fig. 92[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0092]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a3701dc7-f6b5-4525-a5e4-a0f90740e2e3/cpz1355-fig-0092-m.jpg</p>\nFigure 92\nExpanded view for the SIDER Side Effects drug set library. This view includes the bar chart of top enriched terms, scatter plot of top enriched terms, and table of top enriched terms with each of their p-values, odds ratios, overlap sizes, and corrected q-values.\nResources pages\n9. Navigate to the resources page by clicking the corresponding tab on the website navigation bar (Fig. 93[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0093]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/b84e8517-7388-41c4-bd08-a2af1fd2782e/cpz1355-fig-0093-m.jpg</p>\nFigure 93\nThe resource page listing all drug data resources included in Drugmonizome.10. Each of the drug data resources used to create drug set libraries is cataloged on this page. Click on the DrugBank resource card to view metadata specific to DrugBank, as well as drug set libraries curated from DrugBank (Fig. 94[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0094]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/20d5fb06-ab6c-4ac4-8c85-8cbe9c4e7395/cpz1355-fig-0094-m.jpg</p>\nFigure 94\nExpanded view of the DrugBank resource with identifying metadata and drug set libraries curated from DrugBank.\n11. Click on the \u201cDrugBank Small Molecule Targets\u201d library to be redirected to a page with identifying metadata for the drug set library. The metadata for the drug set library includes download links for the .DMT files in drug name or InChIKey format (Fig. 95[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0095]). Additionally, each of the drug sets included in this library are listed below. Clicking on any drug set name redirects to a page with metadata specific to the drug set, as well as the set of associated small molecules.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/135b8f90-d131-4577-8cc8-b32b7ee8f07f/cpz1355-fig-0095-m.jpg</p>\nFigure 95\nExpanded view of the DrugBank Small Molecule Targets drug set library with metadata that include download links for the DMT file in drug name and InChIKey formats. All drug sets included in the library are listed below and each drug set can be expanded to view drug set\u2013specific metadata and the list of small molecules included in the drug set.A wealth of data from a multitude of sources is readily available for thousands of bioactive small molecules in Drugmonizome (Kropiwnicki et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0027]). The information in Drugmonizome can be harnessed to develop machine learning models that utilize such data to predict the properties of small molecules that are poorly annotated. The Drugmonizome database draws upon a variety of publicly available resources to label each small molecule by its associations with pathways, protein targets, induced gene expression profiles, chemical features, and other attributes. Drugmonizome-ML is an Appyter (Clarke et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0012]) that executes a machine learning pipeline as a Jupyter notebook using the data curated for creating Drugmonizome. Drugmonizome-ML can be used to make predictions for indications and other attributes such as drug targets or side effects for poorly annotated pre-clinical bioactive small molecules.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nInput dataset selection\n1. Navigate to the Drugmonizome-ML Appyter (https://appyters.maayanlab.cloud/Drugmonizome_ML/[href=https://appyters.maayanlab.cloud/Drugmonizome_ML/]). The input form is divided into three sections: input dataset selection, target label selection, and machine learning pipeline.\n2. Select datasets from Drugmonizome and SEP-L1000 (Kropiwnicki et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0027]; Wang, Clark, & Ma'ayan, 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0060]) to populate the feature matrix that will be used for learning and classification. Each of the datasets\u2019 contents are described using tooltips (Fig. 96[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0096]). For the demonstration, select the \u201cLINCS Gene Expression Signatures\u201d from the \u201cTranscriptomic and Imaging Datasets\u201d subfield and \u201cMorgan Fingerprints\u201d from the \u201cChemical Fingerprints Generated for Compounds from SEP-L1000\u201d subfield.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6bf8a115-3720-426f-b6f1-88fdc84ec0d7/cpz1355-fig-0096-m.jpg</p>\nFigure 96\nInput dataset selection section of the Drugmonizome-ML Appyter. Each input dataset is annotated with tooltips.3. Additional options for pre-processing the feature matrix are available. If selecting features from various data sources, it is likely that not all compounds will be included across all feature sets; therefore, a toggleable option decides whether drugs with missing data are retained or dropped from the feature matrix. Additionally, because some of the available feature sets are binary association matrices, there is the option to apply TF-IDF normalization to account for frequency of common and rare features among the small molecules (Fig. 97[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0097]). In general, the default settings for these options are recommended.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/148822d4-d800-47e5-9166-ea1b56c03873/cpz1355-fig-0097-m.jpg</p>\nFigure 97\nToggleable options for deciding whether to retain or drop drugs with missing data and TF-IDF normalization.\nTarget label selection\n4. In this section, select the positive class label for a binary classification problem. There is the option to select an attribute from any of the Drugmonizome drug set libraries in an autocomplete field where relevant drug-set labels from Drugmonizome are offered as potential class labels (Fig. 98[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0098]). Type any characters into the autocomplete field and matching drug-set labels will be displayed. For the demonstration, type \u201cneuropathy peripheral (from SIDER Side Effects)\u201d into the autocomplete field.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/b26f2b43-551d-422a-a3b1-ef2d0e81837a/cpz1355-fig-0098-m.jpg</p>\nFigure 98\nTarget label selection with \u201cAttribute\u201d selected. The autocomplete field can be populated with search terms that match to drug-set labels in Drugmonizome which will be used as the positive class to predict.\n5. Alternatively, upload a newline-separated .txt file of compounds to be used as positive examples of a class to predict by selecting the \u201cList\u201d option in the \u201cTarget Label Selection\u201d section. Example .txt files are available for download to understand the structure of the file (Fig. 99[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0099]). Choose the drug identifier format (drug name or InChI key in which small molecules within the text file are described. InChI Keys are the recommended format.<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d2770c40-4ad4-4b31-8243-9174f4b4ee08/cpz1355-fig-0099-m.jpg</p>\nFigure 99\nTarget label selection with \u201cList\u201d selected. Newline-separated .txt files can be uploaded with small molecules that are part of a positive class to predict. The drug identifier format drop-down menu allows specification of how small molecules are cataloged within the uploaded file (names or InChI key).\n6. The \u201cInclude stereoisomers\u201d option decides whether to match compounds from the feature matrix to the target vector using the first 14 characters of the InChIKey (which encodes chemical connectivity), thus including stereoisomers of a particular small molecule, or whether to consider only one form of a molecule and match by the whole InChIKey.\nMachine learning pipeline\n7. In this section, select data visualization options, machine learning classifiers, machine learning hyperparameters, and methods to evaluate the classifier (Fig. 100[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0100]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/2b6f3de0-ca0f-4cd1-84ec-9ac50f8c9388/cpz1355-fig-0100-m.jpg</p>\nFigure 100\nMachine learning pipeline section with methods for data visualization, machine learning classifier selection, hyperparameter settings, and metrics to evaluate the classifier.\n8. Select your preferred data visualization method from the drop-down menu under the \u201cData Visualization Method\u201d field. The default and recommended method is UMAP.\n9. If applicable, select a dimensionality-reduction algorithm from the drop-down menu under the \u201cDimensionality Reduction Algorithm\u201d field.\n10. If applicable, select a feature-selection method from the drop-down menu under the \u201cMachine Learning Feature Selection\u201d field.\n11. The \u201cMachine Learning Algorithm\u201d section includes 9 distinct classifiers that can be chosen by clicking on the corresponding classifier name. Furthermore, each classifier has hyperparameter fields that can be modified. For example, select the \u201cExtra Trees classifier\u201d. Input \u201c1250\u201d in the \u201cn_estimators\u201d field. Select \u201centropy\u201d in the \u201ccriterion\u201d drop-down menu. Select \u201clog2\u201d in the \u201cmax_features\u201d drop-down menu. All other hyperparameters can be kept as default.12. Select whether to calibrate algorithm predictions by selecting the appropriate choice in the \u201cCalibrate algorithm predictions\u201d field. This setting will calibrate the predictions output by the chosen model, eliminating model-imparted bias. It is recommended to keep this setting as default.\n13. Select a cross-validation method from the drop-down menu under the \u201cCross-Validation Algorithm\u201d field. The recommended option is Repeated Stratified Group K-Fold because this cross-validation method will maintain class ratios across train and validation splits. Furthermore, choose the number of cross-validation folds and cross-validation repetitions in the subsequent fields. For the demonstration, input \u201c10\u201d into the \u201cNumber of Cross-Validation Folds\u201d field and \u201c3\u201d into the \u201cNumber of Cross-Validated Repetitions\u201d field.\n14. Choose the primary evaluation metric for assessing the performance of the model from the drop-down menu under the \u201cPrimary Evaluation Metric\u201d field. The default and recommended metric is \u201croc_auc\u201d.\n15. Choose any additional evaluation metrics from the drop-down menu under the \u201cEvaluation Metrics\u201d field, and these metrics will also be reported for the trained model.\n16. Click \u201cSubmit\u201d at the bottom of the input form.\nNavigating the Drugmonizome-ML Appyter Notebook\n17. A Jupyter Notebook will begin executing in the cloud once the input form is submitted. The notebook includes an option to download the notebook, toggle displaying the code, and run the notebook locally. Additionally, a table of contents exists with clickable elements that link to specific sections within the notebook (Fig. 101[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0101]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a85d8682-3d92-4bd0-937a-f24a7b683a4c/cpz1355-fig-0101-m.jpg</p>\nFigure 101\n(1) To learn more about Appyters, click any of the header tabs to navigate to information pages. (2) Clickable options to download the Jupyter Notebook and toggle code when viewing the notebook, as well as the option to run the notebook locally. (3) Table of contents with clickable elements that link to a specific section within the notebook.18. Scroll down to the \u201cSelect Input Datasets and Target Classes\u201d section or click on the corresponding section from the table of contents. The feature matrix that was generated based on the selected features from the input form is displayed. The feature matrix is composed of 19,898 compounds and 3026 features from LINCS Gene Expression Profiles and TF-IDF normalized Morgan Fingerprints (Fig. 102[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0102]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/024eafdb-f0ac-4bc3-b50b-b4cf8eeb776c/cpz1355-fig-0102-m.jpg</p>\nFigure 102\nInput dataset visualized in Dataframe format. The number of matched compounds in the target vector is displayed, along with a downloadable .txt file of unmatched compounds.\n19. Additionally, information is displayed about how the target array is constructed, how many compounds from the target array are included in the feature matrix, and how many compounds were discarded because they were not included in the feature matrix. Unmatched compounds are available for download.\n20. Navigate to the \u201cDimensionality Reduction and Visualization\u201d section to view the input feature space using the dimensionality reduction and visualization methods that were selected in the input form. Positive class labels are labeled within the visualization to demonstrate how the class of interest is clustered in the feature space (Fig. 103[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0103]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d35e3fe8-d5be-4ddd-854d-d6797e35819c/cpz1355-fig-0103-m.jpg</p>\nFigure 103\nDimensionality Reduction and Visualization Section with input feature space visualized using UMAP.\n21. Navigate to the \u201cMachine Learning\u201d section to view the trained classifier and evaluations of the classifier's performance. The receiver operating characteristic curve (Fig. 104[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0104]), precision-recall curve (Fig. 105[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0105]), and confusion matrix (Fig. 106[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0106]) are displayed. Click the hyperlinks in the figure headers to download the figures.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/0a564553-751f-47a7-81ac-0ade5580b774/cpz1355-fig-0104-m.jpg</p>\nFigure 104\nReceiver Operating Characteristic (ROC) curves of classifier performance after cross-validation splits.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/77955fc6-a023-43a2-b7f9-f8bf97f13e15/cpz1355-fig-0105-m.jpg</p>\nFigure 105\nPrecision-recall (PR) curves of classifier performance after cross-validation splits.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4be7a2eb-6b52-4cdd-8710-877e05e13297/cpz1355-fig-0106-m.jpg</p>\nFigure 106\nConfusion matrix for cross-validation predictions from the trained classifier.22. Navigate to the \u201cExamine Predictions\u201d section to view the predictions made by the model in addition to the distributions of mean probability estimates and t-statistics. Figures displaying the distribution of mean cross-validation predictions (Fig. 107[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0107]), distribution of t-statistics (Fig. 108[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0108]), a UMAP visualization of the feature space with overlaid predictions (Fig. 109[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0109]), and a filterable table of the top predicted compounds (Fig. 110[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0110]) are displayed. Click the hyperlinks in the figure and table headers to download the corresponding figure or table.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/adcd459c-d068-46d4-8948-54e4f54354d4/cpz1355-fig-0107-m.jpg</p>\nFigure 107\nMean probability distribution for classifier predictions including compounds with known positive labels, unknown class labels, and a simulated null distribution.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a8e38b8a-7b01-464c-a60f-c7014119bbf1/cpz1355-fig-0108-m.jpg</p>\nFigure 108\nT-statistic distribution for classifier predictions including compounds with known positive labels, unknown class labels, and a simulated null distribution.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/2bdcb984-54f3-4fd9-896a-d44a5dd9963b/cpz1355-fig-0109-m.jpg</p>\nFigure 109\nUMAP dimensionality reduction of the input feature space with predicted compounds overlayed. The color of each point corresponds to the mean predicted probability, whereas the size of the point corresponds to the significance of the probability.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/787c1380-9719-4a2c-a9c4-4bdcd5093bfe/cpz1355-fig-0110-m.jpg</p>\nFigure 110\nTable of the top predicted compounds ranked by prediction probability.\n23. Navigate to the \u201cFeature Importance\u201d section to view the most important features from the input feature matrix that were used to make predictions. A table of the most important features used by the model to make predictions (Fig. 111[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0111]), as well as a figure depicting the distributions of average and cumulative sum of feature importance (Fig. 112[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0112]), are displayed. Click the hyperlinks in the figure and table headers to download the corresponding figure or table.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6152ae5e-9a3b-4337-9864-109635951db2/cpz1355-fig-0111-m.jpg</p>\nFigure 111\nFeature importance table.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/7c09ff52-33dd-463f-a70d-87858a009ffd/cpz1355-fig-0112-m.jpg</p>\nFigure 112\nFeature importance graphs with distribution scores for each feature and a cumulative distribution score across all features.Harmonizome (Rouillard et\u00a0al., 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0045]) is a collection of processed datasets that abstract knowledge about genes and proteins. Using the processed data from Harmonizome, Harmonizome-ML enables interactive imputation of knowledge about the function and other properties of genes and proteins using machine learning. Combined with the user-friendly interface of an Appyter (Clarke et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0012])\u2013a web-based software application enabling users to execute bioinformatics workflows without coding\u2013the Harmonizome-ML Appyter can be used to build and evaluate machine learning pipelines with Harmonizome data in an accessible way. The Harmonizome-ML Appyter asks users to select or upload attributes for learning as well as specify a target vector to predict. Users also need to select from various machine learning algorithms and performance evaluation methods. Once these options are selected, the workflow is executed, and the results are presented as a Jupyter Notebook that is shareable and downloadable.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nNavigating the input page\n1. Navigate to the Harmonizome-ML Appyter (https://appyters.maayanlab.cloud/#/harmonizome_ml[href=https://appyters.maayanlab.cloud/#/harmonizome_ml]). The input form is divided into two sections: \u201cattribute and prediction class dataset selection\u201d and \u201csettings.\u201d\n2. In the \u201cAttribute and Prediction Class Selection\u201d section, select attributes by clicking on the check box to the left of an attribute of choice; a blue check mark indicates that an attribute has been selected. Users may opt to upload a custom attribute dataset using the \u201cBrowse\u201d button as well. Target selection can be from Harmonizome or customized; click on the text for the target selection desired and customize the class in the text box below (Fig. 113[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0113]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/16993b37-3b49-4dff-b279-b1524bda777c/cpz1355-fig-0113-m.jpg</p>\nFigure 113\u201cAttribute and Prediction Class Dataset Selection\u201d section of the input form. Two datasets are selected to be used as features in the classifier algorithm. Hovering over tool tips displays information about each dataset. There is also an option to upload custom attribute datasets. The Target Selection subsection allows for selection of a class for the classifier to predict.\n3. The \u201cSettings\u201d section includes settings for various algorithms (dimensionality reduction, manifold projection, ML feature selection, cross validation, ML algorithm, hyperparameter search type, evaluation metrics) that can be customized. Simply click on the drop-down menu below an algorithm to view and update the options. For example, clicking on the drop-down menu for \u201cDimensionality Reduction Algorithm\u201d displays the following options: PCA, truncated SVD, incremental PCA, ICA, and Sparse PCA. Click on the desired algorithm to use it for dimensionality reduction (Fig. 114[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0114]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/1d9b3eaf-5265-4f74-86eb-f903a03b8225/cpz1355-fig-0114-m.jpg</p>\nFigure 114\nSettings section including a variety of scikit-learn options for building the classifier as well as options for visualizing and evaluating classifier performance and predictions.\n4. Once all selections have been made, click on the \u201cSubmit\u201d button at the bottom of the page to run the analyses and generate the notebook.\nNavigating the notebook\n5. Each notebook generated by the Harmonizome-ML Appyter includes explanations followed by code, data, and figures (both static and interactive). To download the notebook, toggle notebook code, or run the notebook locally, select the appropriate button at the top of the page. The notebook is divided into three sections (which can be accessed through the table of contents on the left side of the page): Inputs, Dimensionality Reduction, and Machine Learning (Fig. 115[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0115]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/54f6c907-c7c6-4617-b2fb-f93574cc5b3e/cpz1355-fig-0115-m.jpg</p>\nFigure 115Options to download the Appyter notebook, toggle the code, and run the notebook locally. A table of contents on the left allows for navigating the various sections of the notebook.\n6. Navigate to the \u201cInputs\u201d section to view the feature matrix Dataframe generated from the datasets selected in the input form (Fig. 116[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0116]). Note that some Dataframes contain additional columns that can be explored by scrolling left to right. The first two Dataframes are individual datasets, whereas the final Dataframe displays the concatenated feature matrix that will be used for classification.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/10495795-1cb1-4d39-97a8-65e6d99331f9/cpz1355-fig-0116-m.jpg</p>\nFigure 116\nThe input feature datasets visualized as Dataframes. The first and second Dataframes describe the \u201cCCLE Cell Lines Gene Expression Profiles\u201d and \u201cENCODE Transcription Factors Targets\u201d datasets, respectively. The final Dataframe represents the concatenated feature matrix composed of the previous two datasets.\n7. Scroll down to view the target array created from the dataset containing the class label to be predicted. Genes that are known to be associated with the class label are annotated with a 1, whereas genes not known to be associated with the class label are annotated with a 0 (Fig. 117[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0117]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/13c3ae35-6b4d-4bfc-859f-48cd6f943af4/cpz1355-fig-0117-m.jpg</p>\nFigure 117\nTarget array created from the \u201cDISEASES Text-mining Gene-Disease Association Evidence Scores\u201d dataset which contains the class label \u201ccancer DOID:162\u201d. Genes in the target array associated with the class label are marked with a 1, whereas genes that are not known to be associated with the class label are marked with a 0.\n8. Navigate to the \u201cDimensionality Reduction\u201d section. The process of dimensionality reduction involves transforming data from high-dimensional spaces to low-dimensional spaces without losing too much information. The input features are reduced using PCA and visualized in a 3D scatter plot (Fig. 118[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0118]). The reduced features are also projected onto a manifold with T-SNE (Fig. 119[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0119]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6cf4a670-f4ab-400a-9165-c6a9354e30fa/cpz1355-fig-0118-m.jpg</p>Figure 118\n3D scatter plot of PCA reduced input features with genes associated with the target label are colored yellow.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a4f6e6fd-f377-4978-92ce-81ce4d7c9abd/cpz1355-fig-0119-m.jpg</p>\nFigure 119\nT-SNE visualization of the PCA reduced features.\n9. Navigate to the \u201cMachine Learning\u201d section which features the machine learning pipeline assembled from the input form submission. A model is generated and trained via the customized pipeline and then used to predict genes that are strongly correlated with the target attribute. General explanations for the model's performance are provided with ROC curves and a prediction matrix (Fig. 120[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0120]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/338ef202-e32a-41e3-acf9-dc85446e329d/cpz1355-fig-0120-m.jpg</p>\nFigure 120\nReceiver operating characteristic (ROC) curves and prediction matrix displaying model performance across cross-validation splits.\n10. The prediction results are provided at the end of the pipeline and can be downloaded as a tab-separated (.tsv) file by clicking on results.tsv at the end of the notebook (Fig. 121[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0121]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/3e792367-681f-470b-ba41-223002b067b8/cpz1355-fig-0121-m.jpg</p>\nFigure 121\nTable of top genes predicted to be associated with the class label. The results table is available for download by clicking the \u201cresults.tsv\u201d link.Target Illumination GWAS Analytics (TIGA; Yang et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0064]) is a web application that facilitates drug target illumination by scoring and ranking protein-coding genes associated with traits from genome-wide association studies (GWAS). Similarly, TIGA can score and rank traits with the same gene-trait association metrics. Rather than a comprehensive analysis of GWAS for all biological implications and insights, this focused application provides a rational method by which GWAS findings can be aggregated and filtered for applicable, actionable intelligence, with evidence usable by drug discovery scientists to enrich prioritization of target hypotheses. TIGA derives its GWAS summary and metadata solely from the NHGRI-EBI GWAS Catalog and study-associated publications. Thus, TIGA traits are identified by Experimental Factor Ontology (EFO) terms.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nNavigating the input page\n1. Navigate to the TIGA web app (https://unmtid-shinyapps.net/shiny/tiga/[href=https://unmtid-shinyapps.net/shiny/tiga/]).\nTrait to gene search\n2. A trait query may be specified by browsing and selecting from the Traits (ALL) tab, or via the Trait query field.\n3. To find genes associated with the EFO term \"worry measurement\" (EFO_0009589), begin typing \"worry\" in the Trait query field, and autosuggest will assist in selecting the trait, (Fig. 122[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0122]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a9eb77c7-74fb-4f78-95c3-0090e29d36d0/cpz1355-fig-0122-m.jpg</p>\nFigure 122\nTIGA gene plot for trait \"worry measurement\" (EFO_0009589).\n4. TIGA results will be displayed via the HitsTable tab and HitsPlot tab (Fig. 123[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0123]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/3888dd71-f1e9-4222-b3c6-944139c3b7f6/cpz1355-fig-0123-m.jpg</p>\nFigure 123\nTIGA gene hitlist for trait \"worry measurement\" (EFO_0009589).\n5. The HitsTable is ranked by meanRankScore as a measure of the strength and confidence of the inferred gene-trait association.6. The HitsPlot displays hits with meanRankScore on the horizontal axis, and Effect on the vertical axis, either measured by odds ratio (OR) or N_beta (count of beta values).\n7. Hits are annotated, either in the table as columns or as hover-tooltips, with several identifiers, measures, and variables, derived from the aggregated GWAS, or annotated from IDG. Target Development Levels (TDLs) are also color coded for ease of use, facilitating identification of well-known targets (Tclin) and under-studied targets (Tdark).\n8. From the HitsTable, for a specific gene, the magnifying-glass icon links to the TIGA provenance for the corresponding gene-trait association. The provenance displays studies and publications supporting the association, with GWAS Catalog and PubMed link-outs, respectively, (Fig. 124[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0124]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d2a89097-7c7c-43a8-beee-416c5f9bbc9f/cpz1355-fig-0124-m.jpg</p>\nFigure 124\nTIGA provenance for trait \"worry measurement\" (EFO_0009589) associated gene Musculoskeletal embryonic nuclear protein 1 (MUSTN1), with two studies and associated publications, with GWAS Catalog and PubMed link-outs, respectively.\nGene to trait search\n9. In Gene query mode, TIGA behaves much the same as in Trait query mode, but with traits as hits. Data that pertain to gene-trait associations will be the same, such as provenance, regardless of query mode.\n10. TIGA genes are, as in the Catalog, identified by Ensembl Gene IDs. The Gene query field will autosuggest based on gene symbols. Thus, by typing \"RAS\", autosuggest will assist in selecting \"RASA2\", \"Ras GTPase-activating protein 2.\"\n11. As in Gene query mode, results will be via HitsTable and HitsPlot tabs.Kinase Enrichment Analysis 3 (KEA3) (Kuleshov et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0029]) is a web-based server application that infers overrepresented upstream kinases whose putative substrates are present in a user-inputted list of differentially-phosphorylated proteins. To infer upstream kinases, KEA3 uses a collection of kinase-substrate libraries created from processing data from several online databases. Kinase enrichment analysis results are provided for each kinase-substrate library, as well as two integrated approaches to integrate all libraries: MeanRank and TopRank. The gene sets from the kinase-substrate libraries are compared to the user-inputted protein list, and Fisher's Exact Test is used to compute the significance of the overlap to prioritize kinases. The resulting ranked lists of kinases, as well as visualizations of the significant kinases as networks, are returned to the users as interactive and downloadable figures.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nNOTE: There is a tutorial on navigating KEA3 results (https://maayanlab.cloud/kea3/templates/tutorial.jsp[href=https://maayanlab.cloud/kea3/templates/tutorial.jsp]) from which some of the steps in this protocol have been paraphrased.\nSubmitting a gene set to KEA3\n1. Navigate to the KEA3 homepage (https://maayanlab.cloud/kea3/[href=https://maayanlab.cloud/kea3/]).\n2. Gene/protein sets may be submitted to KEA3 in two ways: by uploading the set as a plain text file or by pasting a list, one gene/protein name per line, into a text box. When submitting genes/proteins using the text box, a checklist below the text box denotes duplicates and confirms valid gene symbols in the input. Once uploaded or inputted, click on the \u201cSubmit\u201d button to begin the analysis (Fig. 125[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0125]).\nNote that only HGNC-approved gene symbols will be accepted.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/1d212170-9ab1-41da-b2b0-ecc2ec3afd54/cpz1355-fig-0125-m.jpg</p>\nFigure 125KEA3 homepage with gene input box. HGNC gene symbols can be pasted into the text box or a newline-separated .txt file containing the input gene list can be uploaded.\nNavigating KEA3 results\n3. Scroll down to view the \u201cIntegrated results\u201d tab, which includes bar charts, tables, subnetwork visualizations, and a clustergrammer visualization of integrated results across all KEA3 libraries using the MeanRank and TopRank methods (Fig. 126[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0126]). The MeanRank method calculates the average rank, whereas the TopRank method calculates the best scaled rank of each kinase across all libraries containing the kinase. The tables can be downloaded in TSV format and visualizations can be downloaded in SVG and PNG format. Use the slider above each visualization to change the number of top results that are displayed.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/624aca82-b9cf-4b3b-a63a-9edbbddaead3/cpz1355-fig-0126-m.jpg</p>\nFigure 126\nSnippet of the integrated results tab showing the top enriched kinases using the MeanRank and TopRank methods through a variety of tables and visualizations.\n4. The Tables tab displays interactive tables of ranked kinases for each individual KEA3 library (Fig. 127[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0127]). The tables are organized into kinase-kinase substrate interaction libraries, protein-protein interaction libraries, and libraries with all associations. Each table displays the top 10 ranked kinases using the Fisher's Exact Test p-value. Click on any of the table headers to re-sort the table. Clicking on any of the kinase names will redirect you to a single-gene landing page in Harmonizome. Access the complete list of kinases by downloading any table in TSV format using the download icon.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e91a7b20-056e-43a6-8894-9d4a3e33fd99/cpz1355-fig-0127-m.jpg</p>\nFigure 127\nTables tab showing the top enriched kinase results from the kinase-substrate interaction libraries. Each table can be re-sorted by clicking the table headers for each table. Specific terms of interest can be queried in any of the search bars within each table.5. The Networks tab displays global kinase co-regulatory networks generated by applying Weighted Gene Co-expression Network Analysis (WGCNA; Langfelder & Horvath, 2008[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0033]) to ARCHS4 (Lachmann et\u00a0al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0032]), GTEx (Aguet et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0001]), and TCGA (Tomczak, Czerwi\u0144ska, & Wiznerowicz, 2015[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0055]) data in order to visualize the top-ranked kinases in the context of the larger human phosphorylation network; the top-ranked kinases are highlighted in the network (Fig. 128[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0128]). To choose the top-ranked kinases from a specific library, navigate to the \u201cSelect a library\u201d drop-down menu and click on the desired library. Download each network as an SVG or PNG file by selecting the corresponding download button.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a8032b90-54b8-4e38-bd5b-b322fe68dc60/cpz1355-fig-0128-m.jpg</p>\nFigure 128\nNetworks tab displaying human kinome regulatory networks that were produced by applying Weighted Gene Co-expression Network Analysis (WGCNA) to ARCHS4, GTEx, and TCGA datasets. Kinases are colored by tissue type based on the highest correlation between the kinase and parent WGCNA module.\n6. The Subnetworks tab displays kinase co-regulatory network visualizations which have been dynamically generated from the top-ranked kinases in each library (Fig. 129[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0129]). An edge between two kinases indicates an interaction supported by library evidence from either a kinase-substrate interaction library (directed edge) or protein-protein interaction library (undirected edge). Hover over an edge to display the library evidence supporting the interaction. Download each network as an SVG or PNG by clicking the desired file type in the bottom left corner of the graph.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/615db24d-4a83-4e31-9a43-cf5c1527850f/cpz1355-fig-0129-m.jpg</p>\nFigure 129\nSubnetworks tab displaying the kinase-kinase co-regulatory networks showing the top-ranked kinases from enrichment results for kinase-substrate interaction libraries.7. The Bar Charts tab provides bar charts showing the \u2013log(p-value) of the top-ranked kinases for each individual library (Fig. 130[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0130]). The bar charts are organized into kinase-kinase substrate interaction libraries, protein-protein interaction libraries, and libraries with all associations. Use the slider above each figure to change the number of top kinases within the figure. Download any given chart as an SVG or PNG by selecting the desired file type in the bottom left-hand corner of the chart.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/cfed2a8e-de30-4a58-a13e-98c85bc85e72/cpz1355-fig-0130-m.jpg</p>\nFigure 130\nBar charts tab displaying the -log(p-value) of top-ranked kinases from the kinase-substrate interaction libraries.\n8. The Clustergrammer tab uses the Clustergrammer (Fernandez et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0018]) application to provide an interactive clustergram of overlapping substrate targets between the input and the top library results (Fig. 131[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0131]). Share, take a snapshot, download, or crop the clustergram matrix using the icons in the menu bar on the left side of the clustergram. Customize row order and column order by selecting one of the options (alphabetically, cluster, rank by sum, rank by variance) under \u201cRow Order\u201d and \u201cColumn Order,\u201d respectively. Search for rows using the text search box. Adjust the dendrogram groups, which show clusters at different hierarchical levels and are represented by gray triangles and trapezoids along the bottom and right axes, using the gray triangular sliders on the right and bottom-left sides of the clustergram.\nNOTE: A tour of Clustergrammer that explains its features in more depth can be found at http://maayanlab.github.io/clustergrammer/scrolling_tour[href=http://maayanlab.github.io/clustergrammer/scrolling_tour]. More details on interacting with the clustergram can be found in the Clustergrammer documentation at https://clustergrammer.readthedocs.io/interacting_with_viz.html[href=https://clustergrammer.readthedocs.io/interacting_with_viz.html].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/06e638c6-1153-45b4-8bd1-05679e799872/cpz1355-fig-0131-m.jpg</p>\nFigure 131This interactive visualization highlights the relationships between the most common kinase-substrate associations detected as overlapping with the input. Each column represents a protein set from a KEA3 library, while the rows are putative substrates from the input list which overlap with proteins within each of the KEA3 library sets. Rows and columns can be sorted by sum to observe the KEA3 sets with the most substrates.\n9. Open a new or existing Python code file. Import the JSON and requests libraries at the top of the file.\n         \nimport json\nimport requests\n10. Call the requests.post method to send a POST request to the URL. The payload variable contains the parameters that are sent to the API endpoint specified in KEA3_URL. In this case the endpoint is /enrich and the parameters are query_name, which specifies the name of the query, and gene_set, which specifies the query gene list to be enriched.\n         \nKEA3_URL = \u02c8https://maayanlab.cloud/kea3/api/enrich/\u02c8\npayload = {\"query_name\":\"myQuery\", \"gene_set\":[\"FOXM1\",\"SMAD9\",\"MYC\",\"SMAD3\",\"STAT1\",\"STAT3\"]}\n\u00a0\n         \nresponse = requests.post(KEA3_URL, json=payload)\n\u00a0\ndata = json.loads(response.text)\nprint(data)\n11. Use the json.loads method to view the response as a JSON object containing the top enrichment results from various libraries.\n         \n{\n\u02c8Integrated--meanRank\u02c8:\n\u00a0\n         \n[{\u02c8Query Name\u02c8: \u02c8myQuery\u02c8,\n\u2003\u2003\u02c8Rank\u02c8: \u02c81\u02c8,\n\u2003\u2003\u02c8TF\u02c8: \u02c8CDK4\u02c8,\n\u2003\u2003\u02c8Score\u02c8: \u02c837.73\u02c8,\n\u2003\u2003\u02c8Library\u02c8: \u02c8STRING.bind,20;ChengPPI,2;PhosDAll,39;BioGRID,4;HIPPIE,13;ChengKSIN,29;STRING,107;MINT,59;mentha,2;prePPI,137;PTMsigDB,3\u02c8,\n\u2003\u2003\u02c8Overlapping_Genes\u02c8: \u02c8SMAD3,STAT1,MYC,STAT3,SMAD9,FOXM1\u02c8},\n\u00a0\n         \n\u2003\u2003{\u02c8Query Name\u02c8: \u02c8myQuery\u02c8,\n\u2003\u2003\u2003\u02c8Rank\u02c8: \u02c82\u02c8,\n\u2003\u2003\u2003\u02c8TF\u02c8: \u02c8PDGFRA\u02c8,\n\u2003\u2003\u2003\u02c8Score\u02c8: \u02c848.38\u02c8,\n\u2003\u2003\u2003\u02c8Library\u02c8: \u02c8STRING.bind,11;ChengPPI,7;PhosDAll,59;BioGRID,110;HIPPIE,2;STRING,61;mentha,8;prePPI,129\u02c8,\n\u2003\u2003\u2003\u02c8Overlapping_Genes\u02c8: \u02c8SMAD3,STAT1,MYC,STAT3,SMAD9,FOXM1\u02c8},\n}\nNOTE: More detailed instructions, as well as examples from the command line and in R, can be found at https://maayanlab.cloud/kea3/templates/api.jsp[href=https://maayanlab.cloud/kea3/templates/api.jsp].PubMed contains millions of publications that co-mention drugs with other biomedical terms such as genes or diseases. DrugShot is an Appyter (Clarke et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-bib-0012]) that enables users to enter any biomedical search term into an input form to receive ranked lists of drugs and small molecules based on their relevance to the search term. DrugShot then deploys a Jupyter Notebook in the cloud to display ranked lists of drugs. To achieve this, DrugShot cross-references returned PubMed IDs with DrugRIF, a curated resource of drug-PMID associations, to produce an associated compound list where each compound is ranked according to the total co-mentions with the search term from shared PubMed IDs. Additionally, lists of compounds predicted to be associated with the search term are generated based on drug-drug co-occurrence in the literature, and drug-drug co-expression correlations computed from L1000 drug-induced gene expression profiles. Through its search functionality and abstraction of drug sets from different sources, DrugShot facilitates hypothesis generation by suggesting small molecules related to any searched biomedical term.\nNecessary Resources\nHardware\nDesktop or a laptop computer, or a mobile device, with a fast Internet connection\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]), Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/]), Apple Safari (https://www.apple.com/safari/[href=https://www.apple.com/safari/]), or Microsoft Edge (https://www.microsoft.com/en-us/edge[href=https://www.microsoft.com/en-us/edge])\nQuery biomedical term\n1. Navigate to the DrugShot Appyter (https://appyters.maayanlab.cloud/DrugShot/[href=https://appyters.maayanlab.cloud/DrugShot/]). The Appyter input form includes options to query a biomedical term to retrieve a prioritized list of small molecules that is augmented using drug-drug similarity matrices, or to submit a list of small molecules to be augmented using drug-drug similarity matrices.2. Input a biomedical term into the \u201cBiomedical Term\u201d field. The default string used for this demonstration is \u201cLung Cancer.\u201d Input an integer ranging from 20 to 200 in the \u201cAssociated Drug Set Size\u201d field; this value is used to determine the size of the unweighted drug set that is used to predict related compounds. The larger the value selected, the broader the resulting predictions will be (Fig. 132[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0132]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/60c1c7c0-a6f9-4cae-acb4-ea9b15837569/cpz1355-fig-0132-m.jpg</p>\nFigure 132\nBiomedical Term input form with \u201cLung Cancer\u201d input in the Biomedical Term field. The associated drug set size is 50; therefore, the unweighted drug set will include 50 small molecules.\n3. Click submit on the Appyter input form and a Jupyter Notebook with the input parameters will be launched in the cloud.\n4. The first output element of the notebook is a table of \u201cTop Associated Compounds\u201d (Fig. 133[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0133]). This table provides the top-ranked drug and compound names associated with the query term (Index Column), the count of PubMed publications associating each drug with the search term (Column 1), and the fraction of the publications associating the drug and search term divided by the total number of publications related to the drug regardless of search term (Column 2). Click on the hyperlinked filename below the table title to download a .csv file listing all the associated compounds. This file also includes a Score column containing values that are the product of the first two columns.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6cff2a89-28b8-4e76-a3d8-655fcb421b4d/cpz1355-fig-0133-m.jpg</p>\nFigure 133\nTable of Top 20 Associated Compounds. This table provides the top-ranked drug and compound names associated with the query term (Column 1); the count of PubMed publications associating each drug with the search term (Column 2); and the fraction of the count from Column 2, divided by the total number of publications related to that drug (Column 3).5. The second output component of this notebook is a scatter plot (Fig. 134[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0134]) of the values from the table of \u201cTop Associated Compounds.\u201d The x axis displays the integer counts of Publications with Search Term, and the y axis shows the fraction of Publications with Search Term/Total Publications. Hover over any point on this plot to display the compound's name and its corresponding x and y values.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/5a5b31c1-cccc-4edc-b2f3-a63ac345ee10/cpz1355-fig-0134-m.jpg</p>\nFigure 134\nScatter Plot of Drug Frequency in Literature. The x-axis displays the integer counts of Publications with Search Term, and the y-axis shows the fraction of Publications with Search Term/Total Publications. Hovering over any point on this plot displays the compound's name and its corresponding x and y values.\n6. An unweighted drug set is created through ranking small molecules from the association table by the product of the total associated publications and their normalized fraction.\nQuerying a list of small molecules\n7. Alternatively, submit a newline-separated .txt file of small molecule names using the input form, thereby omitting steps 2-6. The submitted small molecules will be used as the unweighted drug set that will be used in subsequent steps (Fig. 135[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0135]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6e8b8599-f9ef-4153-9d4b-95edfa249235/cpz1355-fig-0135-m.jpg</p>\nFigure 135\nList input form where newline-separated .txt files of small molecule names are uploaded for drug set augmentation.\nLiterature co-mentions predictions8. A receiver operating characteristic (ROC) curve that describes the ranking of associated compounds in the DrugRIF literature co-mentions matrix is output (Fig. 136[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0136]). This plot shows the True Positive Rate on the y-axis and the False Positive Rate on the x-axis. The predicted compounds are computed using average co-mention counts of PubMed IDs between the unweighted drug set, and other drugs and small molecules within DrugRIF. The area under the curve (AUC) is shown to the right of the plot, and hovering over any point on the curve displays the associated x and y values.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/ae57d331-c154-41a0-bb50-1d9b08e4dfcf/cpz1355-fig-0136-m.jpg</p>\nFigure 136\nReceiver operating characteristic curve for rankings of unweighted drug set in co-occurrence matrix. The area under the curve (AUC) is shown to the right of the plot, and hovering over any point on the curve displays the associated x and y values.\n9. The literature co-mentions prediction matrix is seeded with the unweighted drug set, and the top predicted compounds are ranked by their average co-mentions with the small molecules in the unweighted drug set. The \u201caverage co-mentions\u201d values are provided in a table that displays the top 20 predicted compounds (Fig. 137[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0137]). Click on the hyperlinked filename below the Table 2 header to download the table as a .csv file.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/68c20319-d148-4bea-8a57-ac49743a6144/cpz1355-fig-0137-m.jpg</p>\nFigure 137\nTable of top 20 predicted compounds predicted from DrugRIF co-occurrence. Click on the hyperlinked filename below the table header to download a .csv file listing the complete ranked set of predicted compounds and their associated similarity scores.10. The top 50 co-occurrence-predicted compounds are queried using the DrugEnrichr API for drug set enrichment analysis. The top 10 enriched terms from the down-regulated and up-regulated GO Biological Processes drug set libraries and the SIDER drug set library are displayed as bar plots (Fig. 138[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0138]). Click the link below the bar plots to be directed to the DrugEnrichr enrichment results page (Fig. 139[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0139]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f7241c06-0eef-4d63-aa85-ba5d8cef2293/cpz1355-fig-0138-m.jpg</p>\nFigure 138\nBar plots of top 10 enriched terms across three separate drug set libraries after drug set enrichment analysis of the top 50 co-occurrence predicted drugs using the DrugEnrichr API. Colored bars correspond to terms with significant p-values (<0.05). An asterisk (*) next to a p-value indicates the term also has a significant adjusted p-value (<0.05).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4f58f398-70b5-4f69-be01-4a6a2575c315/cpz1355-fig-0139-m.jpg</p>\nFigure 139\nDrugEnrichr link to drug enrichment analysis results from querying the top 50 co-occurrence predicted compounds.\nSignature similarity predictions\n11. A receiver operating characteristic (ROC) curve that describes the ranking of associated compounds in the L1000 signature similarity matrix is output (Fig. 140[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0140]). This plot shows the True Positive Rate on the y-axis and the False Positive Rate on the x-axis. The predicted compounds are computed using average cosine similarity of drug-induced gene expression signatures between the unweighted drug set and other drugs and small molecules within the co-expression prediction matrix. The area under the curve (AUC) is shown to the right of the plot, and hovering over any point on the curve displays the associated x and y values.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/224ab31b-d94c-47b1-a1cb-4f89c0460a9b/cpz1355-fig-0140-m.jpg</p>\nFigure 140\nReceiver operating characteristic curve for rankings of unweighted drug set in co-expression matrix. The area under the curve (AUC) is shown to the right of the plot, and hovering over any point on the curve displays the associated x and y values.12. The signature similarity prediction matrix is seeded with the unweighted drug set, and the top predicted compounds are ranked by their average cosine similarity to the small molecules in the unweighted drug set. The \u201caverage cosine similarity\u201d values are provided in a table that displays the top 20 predicted compounds (Fig. 141[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0141]). Click on the hyperlinked filename below the table header to download the table as a .csv file.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e8453760-c9ad-4454-871a-3ef240ed198a/cpz1355-fig-0141-m.jpg</p>\nFigure 141\nTable of top 20 predicted compounds predicted from L1000 co-expression. Click on the hyperlinked filename below the table header to download a .csv file listing the complete ranked set of predicted compounds and their associated similarity scores.\n13. The top 50 signature-similarity predicted compounds are queried using the DrugEnrichr API for drug set enrichment analysis. The top 10 enriched terms from the down-regulated and up-regulated GO Biological Processes drug set libraries and the SIDER drug set library are displayed as bar plots (Fig. 142[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0142]). Click the link to be directed to the DrugEnrichr enrichment results page (Fig. 143[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.355#cpz1355-fig-0143]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/60095b34-1d90-4bba-9052-149c8cca4fc5/cpz1355-fig-0142-m.jpg</p>\nFigure 142\nBar plots of top 10 enriched terms across three separate drug set libraries after drug set enrichment analysis of the top 50 co-expression predicted drugs using the DrugEnrichr API. Colored bars correspond to terms with significant p-values (<0.05). An asterisk (*) next to a p-value indicates the term also has a significant adjusted p-value (<0.05).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/b2621e53-bc4f-40b0-8730-4c310263596c/cpz1355-fig-0143-m.jpg</p>\nFigure 143\nDrugEnrichr link to drug enrichment analysis results from querying the top 50 co-expression predicted compounds.", "Step-by-step method details\nStep-by-step method details\nPre-surgical analgesia\nTiming: 40\u00a0min\nIn this step, SSTCre/+ P1-P2 mouse pups are removed from their home cage and injected with analgesic treatment.\nNote: Pups can be genotyped for SST-Cre as described here: https://www.jax.org/Protocol?stockNumber=013044&protocolID=38270[href=https://www.jax.org/Protocol?stockNumber=013044&protocolID=38270]. However, when breeding a SSTCre/Cre mouse to a C57BL/6J wild-type mouse, all pups in the litter will be SSTCre/+ and no genotyping is needed. If other mouse lines are used, the respective genotyping protocol should be followed.\nNote: Analgesic treatments should be adjusted according to the relevant governmental regulations on animal experimentation.\nRemove the parents from the home cage and temporally place them in a separate clean cage.\nMove the P1-P2 pups to a small box with some of the bedding from the home cage.\nPlace the parents back in the home cage and bring the pups to a separate surgery room.\nPlace half of the box on the heating pad at 37\u00b0C to allow the pups to move away from the warmth.\nWeigh the animal and document the weight.\n30\u00a0min before the surgery, inject Buprenorphine (0.1\u00a0mg/kg) (as analgesic) via subcutaneous (s.c.) injection in the neck with an insulin syringe (30G\u00a0\u00d7\u00a01/2\u2033) by pulling up the skin to create space for the inserted needle.\nNeonatal pial-surface electroporation\nTiming: 15\u201320\u00a0min per pup\nIn this step, P1-P2 pups are anesthetized using isoflurane and then neonatal pial-surface electroporation is performed. The electroporation protocol is adapted from previously published protocols.2[href=https://www.wicell.org#bib2],3[href=https://www.wicell.org#bib3],4[href=https://www.wicell.org#bib4],5[href=https://www.wicell.org#bib5]\nLoad the micro-syringe with plasmid DNA solution.\nNote: To easily load the micro-syringe, put a drop of plasmid DNA solution on a piece of parafilm and tap it with the back of the micro-syringe; DNA solution will slowly enter by capillarity. Then, insert the plunger.\nAnesthetize and maintain the anesthesia as described below:Turn on the oxygen extractor and wait for the flow to stabilize (1\u20132\u00a0min).\nSet the oxygen flow to 1 L/min.\nPlace the pup into the induction chamber (oxygen 1 L/min, isoflurane 4%).\nAfter 3.5\u00a0min, test anesthesia with a gentle foot pinch. If the animal is not anesthetized (i.e., it retracts its paw), wait another 30\u00a0s and check it again. Wait for loss of motor response.\nOnce anesthetized, transfer the pup to the heating pad and place the snout into a 3D-printed whole-body mouse mold (we use a mold from a previously published protocol7[href=https://www.wicell.org#bib7]). Keep the mouse under anesthesia (oxygen 1 L/min, isoflurane 4%).\nNote: A detailed description of the protocol for isoflurane-induced anesthesia in pups has been previously published by Ho and colleagues.7[href=https://www.wicell.org#bib7] Alternative anesthesia protocols, such as anesthesia by hypothermia4[href=https://www.wicell.org#bib4] and analgesic treatments can be used according to the relevant governmental regulations.\nInject Meloxicam (5\u00a0mg/kg) (as additional analgesic) subcutaneously in the neck with an insulin syringe (30G\u00a0\u00d7\u00a01/2\u2033).\nIf necessary, mark the pup by cutting toes or by other less invasive means, such as toe tattoo.\nWipe the skin over the skull with Chlorhexidine soap solution and apply Emla\u00ae cream 5% (Lidocaine/Prilocaine) generously.\nPut a drop of neurogel on both the positive and the negative electrodes (we used platinum tweezertrodes electrodes of 5\u00a0mm diameter from BTX, Cat#45-0489; the positive electrode is the one with the blue screw, as depicted in Figure\u00a01[href=https://www.wicell.org#fig1]C). Use a 1\u20132\u00a0mm thick layer of neurogel covering the entire electrode surface.\nTwo minutes after the Emla\u00ae cream 5% application, use spring scissors and cut a small window (1.5\u00a0\u00d7\u00a01.5\u00a0mm) into the skin covering the skull. Cut only three sides of the window, and use the forceps to flip the skin and expose the skull (Figure\u00a01[href=https://www.wicell.org#fig1]B).Deliver the plasmid DNA above the pia surface by using a previously prepared sharpened glass micropipette.\nHold the capillary between the thumb and the middle finger, keep it tilted at 30\u00b0\u201345\u00b0 and puncture the skull without penetrating the brain.\nNote: The skull is very soft, in particular at the coronal suture. Since the dura covers the inner surface of the skull, this meningeal layer is easily penetrated when puncturing the skull.\nInject around 0.5\u00a0\u03bcL (the capillaries are graduated and have a tick every 1\u00a0\u03bcL) by pressing the plunger with your index finger.\nNote: Since the pia is tightly connected to the brain surface, if you do not puncture the brain you will be injecting right above the pia surface. When injecting above the cortical somatosensory area, you should be able to see the blue plasmid DNA solution spread evenly over a circular surface of around 2.5\u20133\u00a0mm in diameter. If you don\u2019t see this, you are probably injecting too deep inside the brain and not in the meningeal space (Figure\u00a01[href=https://www.wicell.org#fig1]B).\nOptional: Since the skin is quite thin and transparent, it is possible to directly puncture the skin and the skull together, without cutting a window first. We prefer this strategy as it is less invasive and reduces the stress on the parents. However, it is advisable to begin making a window to better understand the anatomy and get experienced injecting in the meningeal space.\nPlace the negative 5\u00a0mm electrode over the injection window (the positive electrode is on the opposite side, under the jaw) and apply two trains of ten 50 millisecond-long 99\u00a0V pulses, with a 950 millisecond-long interval using a standard electroporator (ECM399, BTX); wait 3\u00a0s between the two trains (Figure\u00a01[href=https://www.wicell.org#fig1]C).Note: Before performing neonatal pial-surface electroporation in the first pup, check once that the electrodes are working. To do so, make a bridge of neurogel between the electrodes and deliver a train of pulses; if you see bubbles forming, the electrodes are working correctly.\nCritical: Be sure to have a gentle, but firm, contact between the head, the neurogel and the electrodes. When delivering a train of pulses, you should be able to see some small bubbles popping inside the neurogel under the negative electrode.\nWipe the residues of neurogel off the head.\nFlip the skin back in position and seal with a small drop of tissue glue (e.g., Vetbond glue).\nPut the animal in a warm recovery cage (a small box with home cage bedding to ensure the pups smell like the home cage and are accepted back by their parents) which is half placed on the heating pad at 37\u00b0C (to allow the animals to move away from the heated side).\nRepeat with a new pup until the entire litter has been electroporated; then, return the pups to the parents.\nPlace the pups back in their home cage and observe if the parents correctly nest them.\nIf the mother does not take care of the pups after surgery, check the section troubleshooting 1[href=https://www.wicell.org#sec7.1].\nFollow up with analgesic treatment according to your local animal experimentation guidelines.\nBrain slicing and imaging\nTiming: 2\u00a0days\nIn this step, pups are perfusion-fixed. Then, the brain is extracted and sliced with a vibratome. Finally, the sections are imaged.\nPerfuse-fix the pups at the required age; in our protocol we tested both P10 and P15 (for a detailed protocol on adult animals, please see Wu et\u00a0al., 20218[href=https://www.wicell.org#bib8]):\nTransfer the pups to a dedicated experimental room with a hood for fixative perfusion.Deeply anesthetize one pup by injecting a solution of Ketamine (65\u00a0mg/kg) and Xylazine (13\u00a0mg/kg) intraperitoneally using an insulin syringe (30G\u00a0\u00d7\u00a01/2\u2033).\nTest anesthesia with a gentle foot pinch and wait for the loss of motor response.\nOpen the thorax and the diaphragm by using fine scissors.\nExpose the heart, place a small needle (26G) connected to a peristaltic pump (or equivalent pumping system) into the left ventricle and cut a small hole in the right atrium.\nInject 5\u00a0mL of PBS 1\u00d7 to remove the circulating blood followed by 10\u00a0mL of Paraformaldehyde 4% in PBS 1\u00d7 (PFA 4%).\nDissect the brain and postfix it in PFA 4% at 4\u00b0C overnight (12\u201316 h).\nMove the brain to PBS 1\u00d7 and store at 4\u00b0C until slicing (within a few weeks).\nNote: While storing at 4\u00b0C, protect the brains from light to prevent fluorescence loss.\nSlice the brains with a vibratome (Leica). Check the vibratome manual for more details on best practices:\nTo cut coronal sections, trim the cerebellum with a sharp blade and glue the cut posterior side of the brain on a vibratome metal block using superglue, with the olfactory bulbs facing up.\nSubmerge the block and the brain in PBS 1\u00d7.\nCut 80\u00a0\u03bcm-thick coronal sections (speed\u00a0= 0.3\u00a0mm/s, amplitude\u00a0= 1\u00a0mm) and transfer them on microscope slides with a tape frame on the borders working as a spacer (if you electroporated the somatosensory area, collect sections matching images from position 153 to position 297 of the Allen Brain reference Atlas https://mouse.brain-map.org/experiment/thumbnails/100048576?image_type=atlas[href=https://mouse.brain-map.org/experiment/thumbnails/100048576?image_type=atlas]).\nAdd a few drops of Fluoromount-GTM mounting medium with DAPI, cover with a coverslip, and let the slides dry flat overnight (12\u201316 h) at 4\u00b0C.On the following day, start imaging fluorescent cells using a confocal microscope. We used a Leica SP8 with an HC PL APO CS2 40\u00d7 water immersion objective, Z-stacks of 0.5\u00a0\u03bcm, with a resolution of 1600\u00a0\u00d7\u00a01600 pixels. Both synaptophysin::EGFP and tdTomato channels have to be imaged. An example of a labeled cell is reported in Figure\u00a01[href=https://www.wicell.org#fig1]D. If there appear to be no cells labeled, check the section troubleshooting 2[href=https://www.wicell.org#sec7.3].\nNote: It is advisable to immobilize the coverslip by sealing it with some nail polish on the sides. If imaging takes several days it is advisable to store slides vertically at 4\u00b0C to avoid the coverslip from slowly compressing the tissue.\nImage processing, manual tracing and presynaptic boutons counting\nTiming: 1.5\u20132\u00a0h per cell\nIn this step, tdTomato axons are traced and synaptophysin::EGFP boutons are counted through a custom-made MATLAB script. This script consists of 4 MATLAB files that have to be run sequentially (Figure\u00a02[href=https://www.wicell.org#fig2]):\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2144-Fig2.jpg\nFigure\u00a02. Pipeline of the computational analysis\nextractConfocalData.m: this script uses the Bio-formats library provided by the Open Microscopy Environment consortium to read the confocal picture and converts it into a .mat file. You can check\u00a0the list of supported formats here: https://docs.openmicroscopy.org/bio-formats/6.10.0/supported-formats.html[href=https://docs.openmicroscopy.org/bio-formats/6.10.0/supported-formats.html]. The output is a MATLAB format picture.\nfilterAndSegment.m: this script filters the synaptophysin::EGFP channel using a Wiener filter followed by a bilateral filter.9[href=https://www.wicell.org#bib9],10[href=https://www.wicell.org#bib10] Then, both synaptophysin::EGFP and tdTomato channels are thresholded using Otsu\u2019s method.11[href=https://www.wicell.org#bib11] Finally, synaptophysin::EGFP puncta are segmented using a watershed algorithm.12[href=https://www.wicell.org#bib12] The output is a MATLAB file that can be used for axon tracing.manualTracing.m: this script allows the user to trace cell axons by clicking some anchor points on the process of interest. The script uses these points to find the shortest path that goes through the tdTomato+ axon using the fast marching method.13[href=https://www.wicell.org#bib13],14[href=https://www.wicell.org#bib14] The path is computed both from source to sink and from sink to source to improve the centerline tracing. Then, the script calculates synaptophysin::EGFP puncta sizes by finding puncta profiles falling onto 3D centerline profiles (Figure\u00a03[href=https://www.wicell.org#fig3]). Finally, the script computes the path distance. The output is a MATLAB table containing the length of the traced axons and the sizes of all detected boutons.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2144-Fig3.jpg\nFigure\u00a03. Presynaptic bouton quantification\nThe picture shows the sequential steps of the custom-made MATLAB script to quantify presynaptic boutons.\n(A) An example of a cell with some axons traced in red (scale bar\u00a0= 100\u00a0\u03bcm).\n(A\u2032) Magnification of the blue box in A, with an example of a traced axon (scale bar\u00a0= 10\u00a0\u03bcm).\n(A\u2033) Magnification of the orange box in (A\u2032). Puncta along the traced axon are mapped and segmented (scale bar\u00a0= 5\u00a0\u03bcm).\n(B) Fluorescent intensity profile of green puncta over axonal length. Puncta size is measured at half of the intensity.\nconvertMATtoTXT.m: this script converts the MATLAB tables into .txt tables ready to be analyzed through RStudio (see \u201cquantification and statistical analysis[href=https://www.wicell.org#quantification-and-statistical-analysis]\u201d).\nNote: Computing time depends on the size of the confocal picture and the computational power of your machine. In the following protocol, we report waiting times when processing pictures of 1.5 GB on a machine equipped with Intel\u00ae Xeon\u00ae Silver 4114 CPU @ 2.20 GHz 2.20 GHz (2 processors) and 512 GB of RAM.\nOpen a web browser and go to the website https://github.com/argunsah/punctaDensity[href=https://github.com/argunsah/punctaDensity] (or to https://doi.org/10.5281/zenodo.6980507[href=https://doi.org/10.5281/zenodo.6980507]).Download the code (press \u201cCode\u201d and select \u201cDownload ZIP\u201d), and extract the punctaDensity-main.zip folder.\nOpen MATLAB. Go to the Home tab\u00a0>\u00a0Preferences\u00a0>\u00a0General\u00a0>\u00a0MAT-Files and make sure that \u201cMATLAB Version 7.3 or later (save -v7.3)\u201d is selected.\nChange the current MATLAB folder to the punctaDensity-main folder; from now on, all newly generated files will be saved here.\nRun extractConfocalData.m file (Figure\u00a04[href=https://www.wicell.org#fig4]A):\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2144-Fig4.jpg\nFigure\u00a04. User interface (UI) of the MATLAB script\n(A) UI of extractConfocalData.m.\n(B\u2013D) UI of manualTracing.m. Each step is described in details in the section \u201cimage processing, manual tracing and presynaptic boutons counting[href=https://www.wicell.org#sec3.4].\u201d\nSelect the folder containing your confocal images.\nSelect the confocal images you want to analyze.\nPress \u201cAdd\u201d.\nYour selected images will appear in the window on the right.\nPress \u201cDone\u201d.\nWait until all selected confocal files are converted into MATLAB format images (these files have the same names as the confocal picture, but .mat extension).\nNote: This could take up to 5\u00a0min per confocal picture.\nRun filterAndSegment.m file:\nSelect MATLAB format images from step 27f similarly to steps 27a\u2013e.\nWait until all files are processed and new MATLAB files are created (these files end with \u201c_info.mat\u201d).\nNote: This could take up to 15\u201320\u00a0min per confocal image.\nNote: In case the script gives an error, check the section troubleshooting 3[href=https://www.wicell.org#sec7.5].\nRun manualTracing.m file:\nSelect files ending with \u201c_info.mat\u201d from step 28b similarly to steps 27a\u2013e.\nWhen an image appears (it takes up to 2\u00a0min), adjust the image contrast using the \u201cAdjust Contrast\u201d window. Do not press \u201cAdjust Data\u201d when you are done. Just simply close the window (Figure\u00a04[href=https://www.wicell.org#fig4]B).Note: The image is displayed as a grayscale max intensity projection of both channels merged; this is to facilitate the manual tracing. Nevertheless, only the green channel will be considered for presynaptic bouton counting.\nTo trace the axons, use two keyboard buttons (\u201cz\u201d for zooming in, \u201cx\u201d for zooming out) and two mouse buttons (\u201cleft click\u201d to locate a point, \u201cright click\u201d to conclude the tracing).\nChoose an axon and left-click at the tip of a process. Keep clicking more points along the process. When you are done, right-click and wait for the red tracing to be shown on the image (Figure\u00a04[href=https://www.wicell.org#fig4]C).\nNote: The more left-clicked points, the longer it will take to trace the process.\nNote: If there are multiple tangled processes it is necessary to left-click several points along the axon you want to trace.\nIn the \u201cTracing Quality\u201d window press \u201cYes\u201d if the tracing of the axon looks fine. Press \u201cNo\u201d to discard it and start again the tracing (Figure\u00a04[href=https://www.wicell.org#fig4]C).\nPress \u201cDone\u201d when you have completed tracing all the processes you want to trace. A MATLAB table containing the length of the axons and the sizes of all detected boutons will be created (this file ends with \u201c_infoNums_#date#.mat\u201d). #data# suffix is added to help the user track when the analysis has been performed. A final image with all traced axons labeled in red will be saved for reference (this file ends with \u201c_figure.png\u201d) (Figure\u00a04[href=https://www.wicell.org#fig4]D).\nWait for the next image to be loaded and repeat the tracing procedure.\nRun convertMATtoTXT.m file:\nSelect files ending with \u201c_infoNums_#date#.mat\u201d from step 29f similarly to steps 27a\u2013e.\nA .txt file will be created for further analysis using R language (these files end with \u201c_infoNums_#date#_newFormat.txt\u201d) (see Table\u00a01[href=https://www.wicell.org#tbl1]).\ntable:files/protocols_protocol_2144_3.csvThe value in bold in the first row is the length of the process (\u03bcm), while the other values in the same column are the size (\u03bcm) of every detected puncta along that process. The matrix has as many rows as the largest number of puncta detected for a process, therefore, empty rows are filled with \u201cNaN\u201d.\nNote: In case the script gives an error, check the section troubleshooting 4[href=https://www.wicell.org#sec7.7].", "Step-by-step method details\nStep-by-step method details\nData preprocessing\nTiming: A few minutes (with the computer\n      used in Diomedi et\u00a0al., 2020[href=https://www.wicell.org#bib4], see \u2018materials and equipment[href=https://www.wicell.org#materials-and-equipment]\u2019)\n    \n      Generalized linear models (GLMs) are a flexible generalization of ordinary\n      linear regression used for dependent variables that have a distribution\n      other than Gaussian. Indeed, Poisson distribution is the most used for\n      modelling the number of spikes that a neuron generate in a brief time\n      interval (bin) (Triplett and Goodhill, 2019[href=https://www.wicell.org#bib18];\n      Pillow et\u00a0al., 2008[href=https://www.wicell.org#bib15];\n      Truccolo et\u00a0al., 2005[href=https://www.wicell.org#bib20];\n      Paninski, 2004b[href=https://www.wicell.org#bib12];\n      Dayan and Abbott, 2001[href=https://www.wicell.org#bib3]). Generally speaking, fitting a\n      Poisson GLM requires a dependent variable of which variations can be\n      explained by the variations of a set of independent variables (hereafter\n      called \u2018regressors\u2019). Thus, in our application, before fitting the models,\n      data must be pre-processed to create the vector of spike counts Y (the\n      dependent variable, vector of size [total N\u00b0 of bins \u00d7 1]) and the\n      regressors matrix X (matrix of size [total N\u00b0 of bins \u00d7 N\u00b0 of\n      regressors]). X and Y must have the same number of rows to associate the\n      spike count in each bin to the corresponding values of the regressors.\n    \n      Depending on the aims of the study and the task, the independent variables\n      can be both dummy (i.e., with 0 or 1 values only) or continuous (for more\n      details, see \u2018Note\u2019 of this paragraph; Figure\u00a01[href=https://www.wicell.org#fig1],\n      central part). A huge variety of different features can be included in the\n      GLM, based on the inputs processed by the brain area of interest. Indeed,\n      this method has been applied to the visual domain (e.g.,\n      Pillow et\u00a0al., 2008[href=https://www.wicell.org#bib15]), to the motor domain (e.g.,\n      Goodman et\u00a0al., 2019[href=https://www.wicell.org#bib6];\n      Takahashi et\u00a0al., 2017[href=https://www.wicell.org#bib17];\n      Paniski et\u00a0al., 2004a[href=https://www.wicell.org#bib11], 2004b[href=https://www.wicell.org#bib12])\n      using as regressors kinematics, joint apertures and forces and also to thedecision making (e.g., Park et\u00a0al., 2014[href=https://www.wicell.org#bib13]). Each\n      application involves different independent variables and consequently a\n      different construction of the X matrix. We suggest to carefully review the\n      literature for each domain since it is not possible to provide here all\n      the details. We will spend just a few words on filtering the variables.\n      Whereas it is fundamental to pass the visual stimuli (i.e., images)\n      through filters (the most used are Gabor-like that simulate the receptive\n      fields of the neurons in the early visual cortices) and fit the neural\n      activity with the result of this convolution (Liu et\u00a0al., 2016[href=https://www.wicell.org#bib10]), this step is not necessary for models that involve kinematics or motor\n      parameters in general. GLMs with both filtered and non-filtered (Goodman et\u00a0al., 2019[href=https://www.wicell.org#bib6]; Takahashi et\u00a0al., 2017[href=https://www.wicell.org#bib17]) features have been used.\n      When filters are applied, the most common are raised cosine functions (Pillow et\u00a0al., 2008[href=https://www.wicell.org#bib15]; Truccolo et\u00a0al., 2010[href=https://www.wicell.org#bib19]), but also simple sine and\n      cosine functions (Truccolo et\u00a0al., 2005[href=https://www.wicell.org#bib20]).\n    \n      It is recommendable to include in the model also the cell previous spike\n      activity that can account for internal computations not directly linked\n      with external, measured variables. The spike history is usually included\n      at different time lags, both filtered with raised cosine functions (Pillow et\u00a0al., 2008[href=https://www.wicell.org#bib15]) or not (Diomedi et\u00a0al., 2020[href=https://www.wicell.org#bib4]).\n    \n        Choose the time window of interest (or the entire trial). In the case\n        that neurons have not been recorded simultaneously, align each cell\n        neural activity on the timing of an event of reference.\n      \nNote: the analysis can focus on a fixed\n      time window around the event of alignment or in variable period on a trial\n      basis between two behavioral events. In the latter case, since the time\n      duration could be variable between repetitions, each trial will result ina different number of bins. For the sake of simplicity, in\n      Diomedi et\u00a0al. (2020)[href=https://www.wicell.org#bib4], we chose a fixed time interval\n      (from 3000\u00a0ms before movement onset to 1720\u00a0ms after it, to get an integer\n      number of bins, see below).\n    \n        Choose an appropriate bin width depending on the dynamics of the\n        processes of interest. The choice of the best bin width depends on the\n        focus of the study. To capture fine temporal dynamics such as cell\n        refractory period and correlation between neurons, bin width in the\n        order of a few ms should be used (as small as 1\u20132\u00a0ms,\n        Pillow et\u00a0al., 2008[href=https://www.wicell.org#bib15]). For \u2018slower\u2019 processes (or\n        with an uncertain temporal variability), such as kinematics encoding,\n        wider bin widths have been used (40\u00a0ms,\n        Diomedi et\u00a0al., 2020[href=https://www.wicell.org#bib4]; 20\u00a0ms,\n        Goodman et\u00a0al., 2019[href=https://www.wicell.org#bib6]; 50\u00a0ms,\n        Hatsopoulos et\u00a0al., 2007[href=https://www.wicell.org#bib8]).\n      \n        For each trial:\n        \n            Bin the spike trains within the chosen time frame with the chosen\n            bin width to obtain the spike count Y vector (recommended:\n            histcounts MATLAB function).\n          \n            Average each independent variable (e.g., the eye tracks, kinematics\n            data \u2026) within the same bin edges that the Y vector (recommended:\n            histcounts MATLAB function).\n            \nNote: the spike count of the cell\n              (or even of other units, if recorded in parallel,\n              Truccolo et\u00a0al., 2010[href=https://www.wicell.org#bib19]) with different time\n              lags can be included among the independent variables, thus adding\n              information about spike history.\n            \n            If needed, build the vectors for the dummy independent variables:\n            for each bin of the Y vector, assign 1 when a particular condition\n            is met, otherwise 0. See the note below for an example. In\n            Diomedi et\u00a0al. (2020)[href=https://www.wicell.org#bib4], we built a number of\n            dummy variables that contained information about the different task\n            phases (behavioral events): planning, movement, holding phases,\n            etc., toward specific targets.Concatenate tip-to-tail the spike counts of all trials to obtain a\n        unique Y vector (size: [total N\u00b0 of bins \u00d7 1]) that contains the cell\n        activity. Concatenate also the independent variables vectors and pool\n        them together in a unique X matrix [total N\u00b0 of bins \u00d7 N\u00b0 of\n        regressors].\n      \nNote: To avoid the inclusion in the model\n      of certain continuous variables, it is possible to discretize them in\n      fixed intervals and transform them into a set of dummy variables. This\n      procedure is useful especially in such contexts in which the proper\n      encoding of some variables is unknown. For example, let\u2019s consider a\n      variable A to be included in the model because it is thought to modulate\n      neural activity. It can be a regressor as it is (A), but also, for\n      example, squared (A2) or filtered with a set of Gaussian\n      kernels (varying mean and sigma) producing profoundly different effects on\n      GLM fitting. When understanding the precise type of encoding of the\n      variable is not the focus of the work, the discretization in a bunch of\n      dummy variables can be a solution to avoid annoying, complex data\n      elaboration. This approach has some drawbacks: first, as already\n      mentioned, it does not provide information about the type of encoding;\n      second, it can critically enlarge the dimensionality of the model. For\n      example, if the variable A (range [0 10]) is discretized in dummy\n      variables using fixed intervals with unitary width (i.e., the first will\n      take 1 when A is in the range [0 1), the second will take 1 when A is\n      between [1 2) and so on), we will end up with a set of 10 dummy variables\n      that represent A. To give an experimental example, in\n      Diomedi et\u00a0al. (2020)[href=https://www.wicell.org#bib4], we discretized the gazeposition version, elevation and vergence in a 3D-grid associating to each\n      little spatial volume a dummy variable that took the value 1 in every bin\n      in which the animal fixates in it (otherwise, 0). In conclusion,\n      discretization of continuous variables can help in such situations in\n      which the type of encoding is unknown and out of the scope of the work,\n      but it should be applied after a careful evaluation of pros and contra.\n    \nCritical:\n        Once the X matrix has been built, it is highly recommendable to\n        standardize (calculating the z-score, i.e., first subtracting the mean\n        and then dividing for the standard deviation;\n        Bring, 1994[href=https://www.wicell.org#bib1]) the continuous variables to get beta\n        coefficients directly comparable. This step is fundamental when one\n        wants to further study the beta coefficients estimated during the\n        fitting, especially when the regressors have different scales and/or are\n        expressed with different units of measurement. Note that dummy variables\n        will be 0 or 1 by definition and unitless, so they can be introduced in\n        the model without further standardization. It would not make much sense\n        to get a beta coefficient that refers to \u2018standard deviation\u2019 increments\n        (or decrements) of a dummy variable.\n      \n        Be careful in adding new independent variables since if the sample size\n        (i.e., the number of bins) is too small respect to the number of\n        regressors, the fitting can lead to a poor estimation and unreliable\n        beta coefficients. Although there is not a rule that always applies, you\n        can use the \u2018one in ten rule\u2019, a rule of thumb which states that the\n        maximal number of regressors in a model is equal to the number of\n        observations (bins) / 10 (Steyeberg and Harrel, 2004[href=https://www.wicell.org#bib16]; Peduzzi et\u00a0al., 1996[href=https://www.wicell.org#bib14];\n        Harrel et\u00a0al., 1996[href=https://www.wicell.org#bib7]).\n      \nFitting procedure\nTiming: A few days (with the computerused in Diomedi et\u00a0al., 2020[href=https://www.wicell.org#bib4], see \u2018materials and equipment[href=https://www.wicell.org#materials-and-equipment]\u2019; highly depending on the number of cells and complexity of the model,\n      see the note in \u2018materials and equipment[href=https://www.wicell.org#materials-and-equipment]\u2019 section)\n    \n      The direct comparisons between beta coefficients in non-linear models are\n      usually not straight-forward and often discouraged since many calculations\n      on them are not statistically correct (for example, when they are input of\n      an exp function, as in this case; see the equation in the paragraph\n      below). For this reason, we suggest grouping the regressors with similar\n      \u2018meaning\u2019 (e.g., the variables that indicate the 3D position of gaze in\n      terms of version, elevation and vergence angles or x, y and z; the\n      variables that encode kinematics; the variables that contain information\n      about the decision\u2026) in \u2018blocks\u2019. After the fitting of the complete model,\n      each block of regressors will be removed in turn to evaluate its influence\n      in terms of goodness-of-fit (see below; Figure\u00a01[href=https://www.wicell.org#fig1],\n      right).\n    \n      In this protocol, the fitting procedure consists of two stages: 1)\n      selection of most influential regressors via LASSO optimization; 2) GLM\n      fitting of the complete and nested models considering only the regressors\n      previously selected. During this latter stage, the models are not LASSO\n      regularized to get a value of the goodness of fit that is not penalized by\n      LASSO additional term. Moreover, the protocol requires that blocks of\n      regressors are removed \u2018manually\u2019 starting from the complete model to\n      build the nested models, whereas if LASSO was used in this step, there\n      would be the possibility that other regressors would be removed\n      \u2018automatically\u2019 (by LASSO) besides those removed \u2018manually\u2019 in an\n      uncontrollable way.\n    \nNote: The LASSO optimization introduces a\n      penalization term during the fitting procedure that shrinks the beta\n      estimate values and sets the less influent to 0. The weight of this newterm is represented by the hyper-parameter \u03bb and it can be adjusted to\n      avoid overfitting and/or to handle a lower number of selected variables\n      (see Figure\u00a02[href=https://www.wicell.org#fig2]). This optimization is commonly used\n      when a model includes many independent variables and their correlation\n      with the dependent variable is not known a priori.\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/549-Fig2.jpg\n          Figure\u00a02. Example of feature selection using LASSO optimization method\n          on randomly generated data\n        \n          Left: Cross-validated deviance of LASSO fit models as a function of\n          the shrinking parameter lambda (\u03bb). The green dashed line corresponds\n          to the lambda that produce the minimum deviance. Lower lambdas (on the\n          right of the green line) produce a model with too many parameters that\n          suffer of overfitting. Higher lambdas (on the left of the green line)\n          tend to shrink too much the model removing important regressors.\n          Right: Beta coefficients fitted by LASSO as a function of \u03bb. Each\n          coloured line represents the value of the coefficient corresponding to\n          a regressor in the model. The green dashed line corresponds to the\n          lambda that produce the minimum deviance, thus the optimal beta\n          coefficients. Lower lambdas (on the right of the green line) produce a\n          model that retains more features (fewer beta coefficients have a 0\n          estimate). With higher lambdas (on the left of the green line)\n          important features are removed from the model, greatly worsening the\n          fit.\n        \nThe fitting procedure includes the following steps:\n        Fit a cross-validated (10-fold) LASSO GLM with Poisson link function to\n        explain the spike count in Y with the regressors in X (in MATLAB:\n        betas\u00a0= lassoglm (X, Y, 'poisson', 'CV', 10)). Calling the MATLAB\n        lassoglm function with these inputs, it will automatically vary\n        the shrinking parameter \u03bb to individuate the value that returns the\n        minimal cross-validated deviance of the model (see\n        Figure\u00a02[href=https://www.wicell.org#fig2]).Remove from the X matrix the independent variables (columns) that\n        correspond to the zero beta coefficients assigned during LASSO fitting\n        (take the betas of the model with the \u03bb that minimizes the deviance).\n      \n        Fit the complete GLM with Poisson canonical link function (no LASSO\n        regularized; fitglm MATLAB function) to explain the spike count\n        in Y with all the remaining regressors in X after the LASSO selection.\n        For each block of variables, fit a nested model (fitglm MATLAB\n        function) with all the remaining regressors in X (non-zero LASSO betas)\n        except those belonging to that block. To avoid overfitting, an\n        additional cross-validation (k-fold or leave-one-out) can be performed\n        during this step, training on a part of the dataset and testing on the\n        other part (see the Note below).\n      \n        Fit also the \u2018null\u2019 model that includes no regressors at all. The\n        goodness-of-fit of this last model will be used as reference value (fitglm\n        with (real) Y and a vector of 1s of the same length, as an X\n        placeholder). If the cross-validation was performed during step 7, it\n        must be used here as well to fit the \u2018null\u2019 model on the same parts of\n        the dataset.\n      \n      As use case, in Diomedi et\u00a0al. (2020)[href=https://www.wicell.org#bib4], we grouped the\n      regressors in \u2018extrinsic\u2019 blocks (namely EYE POSITION, EYE SPEED/DIR,\n      POSTSACC, DELAY, PREP, PREMOV, MOV, HOLD, PREMOV2, MOV2) that carried\n      information about task phases (and so, likely the ongoing corresponding\n      neural processes); and the \u2018intrinsic\u2019 SPIKE HISTORY block that carried\n      information about the previous cell spiking activity. We thus obtained i)\n      1 complete model that included all the regressors blocks, ii) 10 nested\n      models removing a different extrinsic block for each run, iii) 1 \u2018only\n      extrinsic\u2019 model using all the extrinsic blocks but not the SPIKE HISTORY,iv) 1 \u2018only intrinsic\u2019 model removing all the extrinsic blocks (i.e., X\n      matrix consisted only of the variables in SPIKE HISTORY) and v) the \u2018null\u2019\n      model.\n    \n      Prior to further evaluation (see next section) of the fitting, it is\n      possible to assess the appropriateness of the model at glance by plotting\n      an estimate of the firing rate (predict MATLAB function) vs the\n      real spike rate. Alternatively, the firing rate at time t predicted by the\n      model can be calculated as the exponential of the linear combination of\n      the regressors:\n    \n(Equation\u00a01)\n \u03bc t  =\nexp\n(  \u03b2 0 \n+  \u03b2 1 \nX\n 1 , t \n+ ... +\n \u03b2 K \nX\n K , t \n)\n      where\n      \n K \n      is the number of regressors,\n      \n{  \u03b2 k  }\nk = 1 , ...\nK\n      are the beta coefficients,\n      \n\u00a0X\n k , t \n      is the value of kth variable at time t and\n      \n \u03b2 0 \n      is the intercept of the model. Figure\u00a03[href=https://www.wicell.org#fig3] (left) shows\n      an example of recorded (black line) vs estimated (red line) firing rate of\n      a parietal neuron during a reaching task. The predicted activity closely\n      matches the observed, peaking just after movement onset (time: 0 s) and\n      resulting slightly inhibited respect to baseline during the hold phase.\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/549-Fig3.jpg\n          Figure\u00a03. Neural data recorded from a parietal neuron during a\n          fix-to-reach task in darkness\n        \n          Left: Observed (black line) and estimated (red line) firing rates\n          (non-overlapping 40\u00a0ms bins). In the plot, data are cross-validated\n          (for the estimation data never seen by the model are used) and\n          averaged across 10 trials. Right: Bars show the weights (w-values) of\n          the 10 blocks of regressors (the \u2018functional fingerprint\u2019 of the\n          neuron) on the neural activity. Asterisks indicate most important\n          w-values for each cell. Adapted from\n          Diomedi et\u00a0al., 2020[href=https://www.wicell.org#bib4].Note: Apply this procedure separately for\n      each cell data.\n    \nNote: For non-LASSO regularized models,\n      the cross-validation strategy is not necessary. When dealing with highly\n      variable data such as neural recordings, the approach can be useful to get\n      more robust estimates. In this protocol, we suggest averaging\n      cross-validated models that revealed to be the simplest, but valid\n      solution to finally handle unique values (Zhang and Zou, 2020[href=https://www.wicell.org#bib22]; Jung and Hu, 2015[href=https://www.wicell.org#bib9]).\n    \nStatistical analysis\nTiming: A few minutes (with the computer\n      used in Diomedi et\u00a0al., 2020[href=https://www.wicell.org#bib4], see above)\n    \n      After the fitting procedure during which we estimated the beta\n      coefficients for all the models detailed above, it is possible to extract\n      information in different ways from the models depending on the aims of the\n      study. Our statistical analyses focused on the goodness-of-fit (GOF) of\n      the complete vs the nested models to get insights about the underlying\n      neural modulations. We will also give some advices on how to treat the\n      beta coefficients of the complete model to get more complementary\n      information.\n    \n        Goodness-of-fit: get the w-values. In the context of non-linear models,\n        the goodness-of-fit is measured as likelihood (or its logarithm). The\n        log-likelihood\n        \n \u2113  , easier to compute, is used by the algorithms to estimate the\n        parameters (betas) through a procedure called Maximum Likelihood\n        Estimation (MLE). In our case, the likelihood is the probability, given\n        a model, to observe a given spike train thus it ranges from 0 to 1,\n        while its logarithm (the log-likelihood) ranges from -\u221e to 0.\n        \n            Assess the GOF of all the models. It can be done in two ways:\n            \n                the\n                \n \u2113 \n                value is directly provided by the fitting function (in MATLAB;\n                from the GeneralizedLinearModel object in MATLAB obtained with\n                fitglm function.\n              \n                by calculation with the following formula, remembering that the\n                firing rate depends on the beta coefficients\n                \n \u03b2(see Equation\u00a01[href=https://www.wicell.org#fd1]):\n                \n(Equation 2)\n\u2113\n( y ,\n\u03b2 )\n=\n\u2211\nt = 1\nT\n y t \nlog\n(\n \u03bc t \n)\n +\n\u2211\nt = 1\nT\n y t \nlog\n ( \u0394 ) \n \u2212 \n\u2211\nt = 1\nT\nlog\n(\n y t \n! )\n\u2212 \u0394\n\u2211\nt = 1\nT\n \u03bc t \n                where y is the spike count, \u03bc is the firing rate predicted by\n                the model (see above), \u0394 is the bin width, t\u00a0is the bin number\n                and T the total number of bins.\n              \n            For higher interpretability, calculate McFadden\u2019s pseudo-R2\n            (Cameron and Windmeijer, 1997[href=https://www.wicell.org#bib2]):\n            \n(Equation\u00a03)\nR pseudo 2\n= 1 \u2212\n \u2113 complete \n \u2113 null \n            Starting from the log-likelihood of the complete (\n \u2113 complete \n ) and null (\n \u2113 null \n ) models.\n \u2113 null \n            represented the fitting of the simplest possible model and, by\n            definition, it is independent from every regressor (the model\n            includes only a constant term). McFadden\u2019s pseudo-R2 can\n            be interpreted as the more common R2 in ordinary linear\n            regression, ranging from 0 (extremely poor fit) to 1 (perfect fit),\n            but it tends to be remarkably lower (values of 0.2 to 0.4 are\n            considered excellent fit).\n          \n            Select only the units with a\n            \n R pseudo 2 \n> t h r e\ns h o l d\n            to discard noisier cells for which the model failed to capture\n            neural modulations. In Diomedi et\u00a0al. (2020)[href=https://www.wicell.org#bib4], we\n            set the threshold at 0.05, as in previous works (Goodman et\u00a0al., 2019[href=https://www.wicell.org#bib6]; Paninski et\u00a0al., 2004a[href=https://www.wicell.org#bib11]).\n          \n            For each nested model, compute a relative pseudo-R2 as:\n            \n(Equation\u00a04)\nR relativepseudo 2\n=\n \u2113 nested \n\u2212\n \u2113 null \n\u2113\nc o m p\nl e t e\n\u2212\n \u2113 null \n            where\n            \n \u2113 nested \n            is the log-likelihood of the nested model. This value compares thelog-likelihood (i.e., the goodness-of-fit) of each nested model with\n            the complete model (and the null model).\n          \n            Convert the relative pseudo-R2 in a weight for each\n            nested model:\n            \n(Equation\u00a05)\nw \u2212 v a l\nu e = \n1  \u2212\nR\nr e l a\nt i v e\np s e u\nd o\n2\n      This score is directly associated with the importance of the group (block)\n      of variables removed to build the nested model with respect to the\n      complete model. Whether a block of regressors contained important\n      information for the model, its removal causes a great worsening of the\n      fit, the relative pseudo-R2 will decrease (towards 0) and the\n      w-value will increase (towards 1). Vice versa, whether a regressors\u2019 block\n      had little influence on the complete model, its removal will cause a\n      little worsening of the fit, an increase (towards 1) in the relative\n      pseudo-R2 resulting finally in a low w-value (towards 0). The\n      Figure\u00a03[href=https://www.wicell.org#fig3] (right) shows an example \u2018functional\n      fingerprint\u2019 of a parietal neuron composed by 10 different w-values. The\n      w-values marked with the asterisks are important to describe the neural\n      modulations (computed as described in the next section, point B.). For\n      more details about the 10 w-values meaning, please see\n      Diomedi et\u00a0al., 2020[href=https://www.wicell.org#bib4].\n    \nCritical: If the complete, nested and\n      null models have been cross-validated during the fitting (steps 7 and 8 in\n      \u2018fitting procedure[href=https://www.wicell.org#sec3.2]\u2019 section), average the\n      log-likelihoods across the different testing partitions of the data to\n      compute all the scores in this section.\n    \n        Goodness-of-fit and analysis of the w-values, a few suggestions. The set\n        of w-values describes a \u2018functional fingerprint\u2019 characteristic for each\n        cell and summarizes the neural modulations elicited by the entire blocks\n        of regressors. The \u2018functional fingerprints\u2019 can be further analyzed toinvestigate the relative weights of the groups of variables on the\n        population, the presence of specialized subpopulations of cells, the\n        dynamics of the encoding\u2026 Here we provide a few suggestions following\n        the analyses in Diomedi et\u00a0al. (2020)[href=https://www.wicell.org#bib4], but the\n        w-values can be hypothetically treated with many other approaches.\n        \n            It is possible to compare directly the distributions of the w-values\n            across the neural population to investigate the relevance of each\n            block of variables on the spiking activity. We suggest the use of\n            the median values and non-parametric tests such as Wilcoxon\u2019s to\n            evaluate the significance of the observed differences (see\n            Figure\u00a04[href=https://www.wicell.org#fig4]A for an application).\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/549-Fig4.jpg\n                  Figure\u00a04. Figure\u00a0and data adapted from\n                  Diomedi et\u00a0al. (2020)[href=https://www.wicell.org#bib4] showing the w-values\n                  across the population\n                \n                  (A) Box plot of w-values for each block of regressors across\n                  the population (2 animals separated).\n                \n                  (B) The w-values are plotted in ascending order for each block\n                  of regressors.\n                \n                  (C) The neural population (a dot for each cell) is projected\n                  onto the 3 first principal components (PCs) of the principal\n                  component analysis (PCA) performed on the 10 w-values.\n                \n                  (D) Histogram showing the minimum number of regressors\u2019 blocks\n                  (w-values) necessary for each cell to reach at least the 85%\n                  of its extrinsic w-values total sum (i.e., the blocks of\n                  regressors with a significant effect on cell modulations).\n                \n            It is also possible to assess which regressor blocks significantly\n            influence each cell activity.\n            \nFor each cell, sum all its w-values.\n                Iteratively, add together the w-values in descending order up to\n                reach the 85% of the total sum. The blocks needed to achieve\n                this value are considered significant in modulating cell\n                activity (see Figure\u00a04[href=https://www.wicell.org#fig4]D).\n              \n            The \u2018functional fingerprints\u2019 describe single cell activity\n            patterns. However, it might be interesting to move towards apopulation analysis starting from the compact view provided by the\n            \u2018functional fingerprints\u2019 and to seek functionally specialized\n            sub-populations. This can be done in a few steps:\n            \n                Visualize the N-dimensional data points (N\u00a0= number of w-values)\n                in a 2D or 3D space performing a Principal Component (PC)\n                Analysis on the matrix [N\u00b0 of units \u00d7 N] and plotting the\n                projections of the neurons on the first PCs (2 or 3).\n              \n                It is possible to apply standard clustering algorithms (e.g.,\n                K-means or hierarchical clustering) on the functional\n                fingerprints and identify clusters of units with shared activity\n                patterns.\n              \nFigure\u00a04[href=https://www.wicell.org#fig4]C shows the functional structure of our neural\n      population (Diomedi et\u00a0al., 2020[href=https://www.wicell.org#bib4]) that was\n      characterized by the lack of units clustered according to their functional\n      fingerprints.\n    \n        Beta coefficients analysis: a few suggestions. Beta coefficients carry\n        information about the effect of each single regressor included in the\n        model on neural activity. However, since the non-linearity of the\n        Poisson GLMs (an additive change in the predictors has a multiplicative\n        effect on the response, see Equation\u00a01[href=https://www.wicell.org#fd1]) and the\n        normalization needed to compare the betas, usually it is not recommended\n        to interpret directly these regression coefficients. Anyway, we here\n        report a couple of indirect analyses that can be performed on beta\n        coefficients of the complete GLM to extract additional information (see\n        Diomedi et\u00a0al., 2020[href=https://www.wicell.org#bib4]).\n        \n            Correlation analysis to investigate the encoding of variables in the\n            population:\n            \n                For each variable of interest, build a beta vector that\n                represents the population response to that variable extracting\n                from the complete model of each unit the beta value\n                corresponding to the variable. Mathematically, the beta vector\n                for the kth variable will be\n                \n{\n\u03b2\n c , k \n}\nc = 1 ,\n ... M\n                where M is the number of the units in the population.Compute the correlation coefficient r between beta vectors (Zhang et\u00a0al., 2017[href=https://www.wicell.org#bib21]). We recommend using Spearman's rank correlation that is best\n                suited to deal with the non-linearity of the modulations rather\n                than standard Pearson coefficient.\n                \nNote: high correlation\n                  coefficients mean high similarity in the population response\n                  to the two tested variables.\n                \n            Clustering: similarly to what suggested for the functional\n            fingerprints, it is possible to run standard clustering algorithms\n            on the beta coefficients in order to eventually identify\n            subpopulations that process information in different ways. For\n            example, in Diomedi et\u00a0al. (2020)[href=https://www.wicell.org#bib4], we found two\n            separate clusters within our neural population that were differently\n            influenced by their own previous spiking activity.", "Step-by-step method details\nStep-by-step method details\nHere we describe step-by-step how to train the XGBoost pan-cancer survival prediction model, infer a pan-cancer survival network by performing network propagation on the important genes identified during model training, and find significantly enriched biological pathways based on the network propagation results. To illustrate these steps, we show as an example the results for 25 different TCGA cohorts and 100 replications of model training from (Thedinga and Herwig, 2022[href=https://www.wicell.org#bib24]).\nSurvival prediction with XGBoost\nTiming: 1\u20132\u00a0weeks (\u223c7\u00a0h per replication)\nTCGA patients from 25 different cancer cohorts are randomly split into 80% training and 20% test patients and a survival prediction model is trained on the gene expression data corresponding to the training patients. Model training includes a feature selection step, where the number of genes used for survival prediction is reduced to 500 in each replication, and a hyperparameter optimization step, where model hyperparameters such as tree depth are tuned. We refer the reader to (Thedinga and Herwig, 2022[href=https://www.wicell.org#bib24]) for a more detailed description. After training is completed, the test data is then used to evaluate the trained model. This procedure is repeated 100 times for different splits of the patients into training and test data. Users might also run a smaller number of replications (e.g., 10) to reduce runtime of this step. However, results of the network propagation and over-representation analysis following the survival prediction step can vary depending on the number of replications.\nNavigate to the directory you have downloaded the XGBoost Survival Network repository into.\nRun the model replications of XGBoost model training as follows:\n>python run_xgb_survival_replications.py\n\u00a0\u00a0\u00a0\u00a0--result results/\n\u00a0\u00a0\u00a0\u00a0--features features/\n\u00a0\u00a0\u00a0\u00a0--replication_start 1\n\u00a0\u00a0\u00a0\u00a0--replication_end 100\n\u00a0\u00a0\u00a0\u00a0--threads 64to run 100 replications of model training. If you want to run a single model replication only or distribute model training (e.g., to multiple servers), you can also execute each model replication separately by setting the -s and -e flags to the respective model replication. E.g., for running model replication 3 only, type: troubleshooting 2[href=https://www.wicell.org#sec5.3], 3[href=https://www.wicell.org#sec5.5], and 4[href=https://www.wicell.org#sec5.7].\n>python run_xgb_survival_replications.py\n\u00a0\u00a0\u00a0\u00a0--result results/\n\u00a0\u00a0\u00a0\u00a0--features features/\n\u00a0\u00a0\u00a0\u00a0--replication_start 3\n\u00a0\u00a0\u00a0\u00a0--replication_end 3\n\u00a0\u00a0\u00a0\u00a0--threads 64\nCritical: The code for running the model replications of XGBoost training will use 64 threads. You should change the threads argument to the number of threads you want XGBoost to use according to the machine you are using.\nNote: The random seed used for splitting the data into training and test sets is computed based on the model replication (i.e., seed\u2217num_replication). If you want to reproduce the results from (Thedinga and Herwig, 2022[href=https://www.wicell.org#bib24]), you should use the default seed of 135, otherwise you can change the seed via the --seed argument in the program call.\nNote: The machine learning step is the most time-consuming step of the protocol and is heavily dependent on the hardware that is used. Our time estimations rely on the use of a Supermicro 2023US-TR4 Linux server with dual AMD EPYC 7601 CPU and 64 cores.\nNote: Although the XGBoost framework generally offers GPU support, the objective function and metric (survival:cox and cox-nloglik, respectively) used in this protocol for survival prediction are currently not supported on GPU.\nNote: The number of replications determines both the runtime and the accuracy of the results. We strongly recommend using as much as 100 replications since this allows the XGBoost method to sufficiently exhaust the large amount of features. However, runtime could be reduced by reducing the number of replications.Optional: Visualize model performances of the survival prediction models trained in the different model replications by plotting each cohort against the C-Indices obtained in the different model replications using boxplots. You can create such a visualization by running:\n>Rscript plotPredictionPerformance.R\n\u00a0\u00a0\u00a0\u00a0--output_file \u201cmodel_performance_xgb_pancancer.pdf\u201d\n\u00a0\u00a0\u00a0\u00a0--result_path results/\n\u00a0\u00a0\u00a0\u00a0--num_replications 100\nwhere num_replications should be set to the number of model training replications you have performed in the previous step. An example visualizing the survival prediction performances for the 100 replications of model training from (Thedinga and Herwig, 2022[href=https://www.wicell.org#bib24]) is shown in Figure\u00a01[href=https://www.wicell.org#fig1].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1611-Fig1.jpg\nFigure\u00a01. Survival prediction performances\nThe pan-cancer XGBoost survival prediction performance (depicted as C-Index boxplots) from (Thedinga and Herwig, 2022[href=https://www.wicell.org#bib24]) for 100 replications of model training on 25 TCGA cancer cohorts.\nNetwork propagation with NetCore\nTiming: 1\u00a0day\nGene weights are derived from the feature importance scores (measured as gain, see https://xgboost.readthedocs.io/en/latest/python/python_api.html[href=https://xgboost.readthedocs.io/en/latest/python/python_api.html]) that were computed by the XGBoost algorithm in each replication of model training. To compute the weight of a gene, the sum of feature importance scores corresponding to this gene over all XGBoost model replications is calculated. All gene weights are then fed into NetCore as initial weights for network propagation. NetCore (Barel and Herwig, 2020[href=https://www.wicell.org#bib1]) is a network propagation method based on node coreness and also implements a module identification step. The module identification step returns subnetworks connecting the most highly weighted input genes to genes that received a significant weight in the network propagation step.\nNavigate to the directory you have downloaded the XGBoost Survival Network repository into.\nPrepare XGBoost pan-cancer survival prediction results for network propagation.\nCompute gene weights from the feature importance scores calculated during the different replications of pan-cancer XGBoost training. To compute the gene weights for network propagation from the survival prediction results, type:\n>python prepare_XGBoost_results_for_NetCore.py\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--result_path results/\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--num_replications 100\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--output_path survival_network/where num_replications should be set to the number of replications you have performed for XGBoost pan-cancer model training.\nNote: Gene weights are calculated as the sum of feature importance scores for each gene over all model replications. Additionally, gene identifiers are converted from Ensembl IDs as used in XGBoost model training to Hugo Symbols to be compatible with the protein-protein interaction (PPI) network used in network propagation with NetCore. Genes that do not map to a Hugo Symbol are discarded as they cannot be used in network propagation.\nPerform network propagation with NetCore as follows:\n>python <path_to_netcore>/netcore/netcore.py\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-e <path_to_netcore>/data/CPDB_high_confidence.txt\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-w survival_network/pancancer_gene_weights.txt\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-pd <path_to_netcore>/data/CPDB_high_confidence_edge_permutations/\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-o survival_network/\nwhere <path_to_netcore> should be set the path where NetCore has been downloaded to. Troubleshooting 2[href=https://www.wicell.org#sec5.3] and 5[href=https://www.wicell.org#sec5.9]. As an example, Figure\u00a02[href=https://www.wicell.org#fig2] shows the largest network module identified by NetCore based on the gene weights from (Thedinga and Herwig, 2022[href=https://www.wicell.org#bib24]), which were computed from 100 replications of XGBoost pan-cancer training.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1611-Fig2.jpg\nFigure 2. Pan-cancer survival network module\nLargest network module identified by NetCore (Barel and Herwig, 2020[href=https://www.wicell.org#bib1]) network propagation and module identification based on pan-cancer important features identified in (Thedinga and Herwig, 2022[href=https://www.wicell.org#bib24]) from 100 replications of XGBoost model training. Orange nodes correspond to seed genes, while genes that were inferred during network propagation are colored in gray. Figure reprinted with permission from Thedinga and Herwig (2022)[href=https://www.wicell.org#bib24].\nOverrepresentation analysis of the survival sub-network\nTiming: 30\u00a0minGenes contained in the network modules identified by NetCore are further analyzed by over-representation analysis (ORA) to find significantly enriched biological pathways. In (Thedinga and Herwig, 2022[href=https://www.wicell.org#bib24]) ORA is performed with QIAGEN Ingenuity Pathway Analysis (IPA) (Kr\u00e4mer et\u00a0al., 2014[href=https://www.wicell.org#bib17]). However, since QIAGEN IPA is a commercial application and thus not freely available, we demonstrate here how to perform ORA with the ORA application implemented in ConsensusPathDB (Herwig et\u00a0al., 2016[href=https://www.wicell.org#bib10]; Kamburov and Herwig, 2022[href=https://www.wicell.org#bib15]).\nExtract genes from the network modules identified by NetCore. The following script reads the file \u201ccore_norm_subnetworks.txt\u201d, which is generated by NetCore during the module identification step and extracts all genes that appear in any of the identified network modules.\n>python extract_network_module_genes.py\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--result_path survival_network/\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--output_file survival_network/network_module_genes.txt\nExtract genes contained in the high-confidence ConsensusPathDB protein-protein interaction network used in network propagation for use as a background list of genes in the over-representation analysis. To extract the genes from the high-confidence protein-protein interaction network, run:\n>python extract_ppi_network_genes.py\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--ppi_path <path_to_netcore>/data/CPDB_high_confidence.txt\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--output_file survival_network/CPDB_ppi_network_genes.txt\nwhere <path_to_netcore> should be set the path where NetCore has been downloaded to.\nNote: When performing over-representation analysis, the background list of genes is important because it influences the resulting p-value computations. As default, the ConsensusPathDB uses all annotated genes as background, but this can be modified by the\u00a0user. A reasonable choice of background genes for analyzing functional information of the network modules could be, for example, the set of genes that are covered by the underlying protein-protein interaction network.\nPerform over-representation analysis (ORA) on the extracted network module genes (Figure\u00a03[href=https://www.wicell.org#fig3]). Troubleshooting 6[href=https://www.wicell.org#sec5.11].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1611-Fig3.jpg\nFigure\u00a03. Over-representation analysis with ConsensusPathDB\nRed numbers (1\u20138) illustrate the steps necessary to perform an over-representation analysis of the module genes identified during network propagation using the ConsensusPathDB (Herwig et\u00a0al., 2016[href=https://www.wicell.org#bib10]) ORA implementation.Open a browser window and go to http://cpdb.molgen.mpg.de/[href=http://cpdb.molgen.mpg.de/].\nOn the ConsensusPathDB website, select the \u201cover-representation analysis\u201d tab in the \u201cgene set analysis\u201d category.\nUpload gene data.\nUpload the file \u201cnetwork_module_genes.txt\u201d from the folder \u201csurvival_network\u201d as file containing gene identifiers.\nUpload the file \u201cCPDB_ppi_network_genes.txt\u201d from the folder \u201csurvival_network\u201d as background list of genes.\nSelect \u201cgene symbol (HGNC symbol)\u201d as gene/protein identifier type.\nClick \u201cProceed\u201d.\nSelect functional sets for ORA.\nIn \u201cPathway-based sets\u201d, select \u201cpathways as defined by pathway databases\u201d.\nClick \u201cFind enriched sets\u201d at the bottom of the page.\nDownload ORA results in tab-delimited format.\nNote: In (Thedinga and Herwig, 2022[href=https://www.wicell.org#bib24]), QIAGEN IPA (Kr\u00e4mer et\u00a0al., 2014[href=https://www.wicell.org#bib17]) was used for over-representation analysis instead of ConsensusPathDB (Herwig et\u00a0al., 2016[href=https://www.wicell.org#bib10]; Kamburov and Herwig, 2022[href=https://www.wicell.org#bib15]). For this reason results can deviate from the results shown in the paper.\nNote: Over-representation analysis is typically dependent on annotation of gene sets, for example pathways, protein complexes, transcription factor target sets etc. Thus, when performing a Fisher test with these gene sets and the user\u2019s gene list, different pathways can be identified when using different pathway databases. If you do not have access to QIAGEN IPA, we suggest using ConsensusPathDB because it has collected such pathway-based gene sets from different source databases and over-representation analysis is done with all gene sets in parallel in order to gain a more comprehensive result.\nAlternatives: It is also possible to use other publicly available tools such as PANTHER (Mi et\u00a0al., 2021[href=https://www.wicell.org#bib20]), Enrichr (Kuleshov et\u00a0al., 2016[href=https://www.wicell.org#bib18]), or DAVID (Huang et\u00a0al., 2009a[href=https://www.wicell.org#bib11]; 2009b[href=https://www.wicell.org#bib12]) for analyzing over-representation of the gene list obtained after network propagation and module identification.", "Step-by-step method details\nStep-by-step method details\nRNA ribodepletion and sample concentration\nTiming: 2 h\nIn this section, we remove the ribosomal RNA in the total RNA sample.\nFor cell line and fresh-frozen tissue samples, purify total RNA with Trizol or spin-column based extraction kits. Use tissue homogenizer when necessary.\nTo detect mRNA m6A modification, extract mRNA with Dynabeads mRNA DIRECT Kit. To detect whole-transcriptome wide m6A modification, deplete ribosomal RNA with RiboMinus Eukaryote System v2. Then purify the longer (>200 nt) RNA using RNA Clean & Concentrator-5. The concentration of RNA was measured by Qubit\u2122 RNA HS Assay Kit.\nLibrary preparation\nTiming: 2\u00a0days total\nTiming: 5-6 h for step 3\nTiming: 7-8 h for step 4\nTiming: 15-16 h for step 5\nIn this section, we convert m6A into a6m6A, perform the cyclization, reverse transcription, and construct the library.\nRemove poly A and ligate adapters. The library construction strategy was modified from m1A-MAP approach (Li et\u00a0al., 2017[href=https://www.wicell.org#bib10]).\nAnneal 30\u2013100\u00a0ng poly A+ RNA or ribo- RNA (300\u00a0ng to 1\u00a0\u03bcg total RNA) with oligo-dT, digest the hybrid with RNase H (NEB), followed by DNase I (NEB) to remove oligo-dT and purified by RNA Clean & Concentrator (RCC) Kits (Zymo Research).\nPurify the RNA and fragment the RNA by sonication using Bioruptor\u00ae Plus sonication device (Diagenode) or NEBNext\u00ae Magnesium RNA Fragmentation Module, and the program is 30\u00a0s on/off, 30 cycles to \u223c 150 nt. End-repair the RNA with PNK enzyme (NEB) at 37\u00b0C for 30\u00a0min to expose the 3\u2032 hydroxyl group.\nAdd 0.6% calibration spike-in mix in the reaction. Ligate the RNA fragment with 3\u2032 adapter (key resources table[href=https://www.wicell.org#key-resources-table]) using T4 RNA ligase2, truncated KQ (NEB) at 25\u00b0C for 2 h.Digest the excessive RNA adaptor by adding 1\u00a0\u03bcL of 5\u2032 Deadenylase (NEB) into the ligation mix followed by incubation at 30\u00b0C for 1 h. Then add 1\u00a0\u03bcL of RecJf (NEB), incubating at 37\u00b0C for another 1 h. Add 1\u00a0\u03bcL of RT primer (50\u00a0\u03bcM), anneal at 75\u00b0C for 5\u00a0min, 37\u00b0C for 15\u00a0min, and 25\u00b0C for 15\u00a0min.\nPause point: The ligated RNA could be stored at \u221280\u00b0C for at least one month.\nCritical: Poly A tail acts as competitor of m6A sites for MjDim1 Methyltransferase and Allyl-SAM cofactor. We add poly A elimination step with RNase H.\nLabel m6A sites and perform reverse transcription. Troubleshooting 3[href=https://www.wicell.org#sec6.5].\nAdd 15\u00a0\u03bcL of dynabeads C1 (Thermo Fisher Scientific) in the reaction to purify the 3\u2032 adapter-ligated RNA. Wash the beads, resuspend the beads in 6\u00a0\u03bcL of H2O, and denature the RNA at 70\u00b0C for 30\u00a0s and cooled in ice to eliminate secondary structure.\nPerform the m6A enzymatic labeling on beads. Add 2\u00a0\u03bcL of 10\u00a0\u00d7\u00a0MjDim1 reaction buffer (400\u00a0mM HEPES, pH 8.0, 400\u00a0mM NH4Cl, 40\u00a0mM MgCl2,), 2\u00a0\u03bcL of SUPERase In RNase Inhibitor (Thermo Fisher Scientific), 6\u00a0\u03bcL of Allylic SAM, and 4\u00a0\u03bcL of MjDim1 enzyme (1.6\u00a0mM) in the reaction and incubate at 50\u00b0C for 1 h.\nRemove the supernatant, then add 4\u00a0\u03bcL of H2O, 1\u00a0\u03bcL of 10\u00a0\u00d7\u00a0MjDim1 reaction buffer, 1\u00a0\u03bcL of RNase inhibitor, 2\u00a0\u03bcL of allylic SAM, and 2\u00a0\u03bcL of MjDim1 enzyme in the reaction and incubate at 50\u00b0C for 20\u00a0min.\nRepeat step c for 6 times to thoroughly label the most m6A sites.Wash the beads and resuspend the beads in 25\u00a0\u03bcL of H2O. Add 1\u00a0\u03bcL of 125\u00a0mM I2 in the reaction and mix thoroughly. Keep the reaction in dark at 25\u00b0C for 1 h, then add 1\u00a0\u03bcL of 40\u00a0mM Na2S2SO3 to quench I2.\nCritical: I2 should be kept in darkness. The dynabeads C1 would reduce some of the I2, do not reduce the concentration of I2.\nWash the beads and resuspend the beads in 9\u00a0\u03bcL of H2O. Add 10\u00a0\u00d7\u00a0RT buffer (SuperScript\u2122 III First-Strand Synthesis SuperMix from Thermo Fisher Scientific) 2\u00a0\u03bcL, 10\u00a0mM dNTP 2\u00a0\u03bcL, 25\u00a0mM MgCl2 2\u00a0\u03bcL, 0.1\u00a0M DTT 1.25\u00a0\u03bcL, RNaseOUT 2\u00a0\u03bcL, and HIV-RT enzyme (Worthington Biochemical Corporation) 2\u00a0\u03bcL in the tube to perform reverse transcription (RT) at 37\u00b0C for 3 h. (For Input RT 1\u00a0h with 1\u00a0\u03bcL of enzyme is sufficient). Then wash the beads and resuspend the beads with 8\u00a0\u03bcL of H2O.\nCritical: Do not replace HIV-RT enzyme (Worthington Biochemical Corporation) with other reverse transcriptase, as they might stop at the N1, N6-ethanoadenine and N1, N6-propanoadenine (the derivates of a6m6A after treatment with I2) sites.\nLigate cDNA 3\u2032 adapter, construct library.\nAdd 1\u00a0\u03bcL of 10\u00a0\u00d7\u00a0RNase H buffer, 1\u00a0\u03bcL of RNase H into the resuspended RT product, put the reaction into thermocycler (Bio-Rad) at 37\u00b0C for 30\u00a0min. Wash the beads and resuspend the beads in 50\u00a0\u03bcL of H2O. Boil the beads at 95\u00b0C for 10\u00a0min to elute the cDNA.\nPurify the cDNA by DNA Clean & Concentrator-5 (Zymo Research) to remove short adapters and elute the RNA with 10\u00a0\u03bcL H2O.\nPause point: The purified cDNA could be stored at \u221220\u00b0C for at least one month.Add 2\u00a0\u03bcL of 10\u00a0\u00d7\u00a0T4 RNA ligase buffer, 2\u00a0\u03bcL of 10\u00a0mM ATP, 10\u00a0\u03bcL of 50% PEG8000, 1\u00a0\u03bcL of cDNA 3\u2032adapter (50\u00a0\u03bcM) (key resources table[href=https://www.wicell.org#key-resources-table]) and 1\u00a0\u03bcL of T4 RNA ligase 1 was added into the eluted cDNA and the ligation was performed at 25\u00b0C for 12 h. The reaction was purified by DNA Clean & Concentrator-5 (Zymo Research) and eluted with 21\u00a0\u03bcL of H2O.\nPause point: The ligated cDNA could be stored at \u221220\u00b0C for at least one month.\nUse 1\u00a0\u03bcL of supernatant for qPCR test and the remaining 15\u00a0\u03bcL for library construction. Use NEBNext\u00ae Ultra\u2122 II Q5\u00ae Master Mix and NEBNext adaptors for the library amplification. Set up the qPCR reaction and parameters as follows:\ntable:files/protocols_protocol_1997_6.csv\nNote: Set the PCR program as follows:\ntable:files/protocols_protocol_1997_7.csv\nCalculate the \u0394Ct value of amplification and determine the optimal cycle number for library construction (Choose the cycle that is in the middle of the s-shape curve and before the exponential amplification curve reaching the plateau).\nConstruct libraries. Set up the PCR reaction and parameters as follows:\ntable:files/protocols_protocol_1997_8.csv\nNote: Set the PCR program as follows:\ntable:files/protocols_protocol_1997_9.csv\nPurify the amplified libraries with 0.8\u00a0\u00d7\u00a0Ampure beads.\nEquilibrate the AMPure XP beads to 25\u00b0C for 30\u00a0min. Add 50\u00a0\u03bcL nuclease free H2O into the PCR tube to make a final volume of 100\u00a0\u03bcL. Add 80\u00a0\u03bcL (0.8\u00a0\u00d7) AMPure XP beads to each PCR mixture. Mix well by pipetting up and down 10 times with the pipette set at 90\u00a0\u03bcL. Incubate at 25\u00b0C for 5\u00a0min.\nPlace the beads on a magnetic rack for 5\u00a0min. Decant the beads.Add 200\u00a0\u03bcL freshly prepared 80% EtOH to the beads without mixing them. Incubate at 25\u00b0C for at least 30 s. Decant the beads. Repeat the wash for another time.\nCritical: Use freshly prepared 80% EtOH. Do not disturb the beads.\nAspirate the remaining EtOH and air dry for 2\u20135\u00a0min.\nCritical: Do not over-dry the beads. If the beads crack, the recovery rate would be much lower.\nAdd 23\u00a0\u03bcL of RNase-free H2O. Mix well by pipetting up and down 10 times and incubate at 25\u00b0C for 5\u00a0min. Place the beads on a magnetic rack for 5\u00a0min.\nTransfer 21\u201322\u00a0\u03bcL of the eluate (there might be 1\u20132\u00a0\u03bcL dead volume) to a new 1.5\u00a0mL tube. Use 1\u00a0\u03bcL of supernatant for concentration measurement.\nMeasure the concentration of the libraries by Qubit Fluorometer and Qubit dsDNA HS kit.\nCheck the quality of the purified libraries by 2100 Bioanalyzer Instrument.\nPause point: The purified libraries could be stored at \u221220\u00b0C for at least one month.\nSend the libraries for NGS deep sequencing. Sequence the libraries on NovaSeq 6000 System Ten with paired-end 2\u00a0\u00d7\u00a0150\u00a0bp read length.\nBioinformatic analysis of the sequencing data\nTiming: 11\u201314\u00a0days total\nTiming: 10\u201315 h for step 6\nTiming: 1\u20132\u00a0days for step 7\nTiming: 3\u20134\u00a0days for step 8\nTiming: 4\u20135\u00a0days for step 9\nTiming: 10\u201320 h for step 10\nIn this section, we analyze the data and identify bona fide RNA m6A sites.\nPreparing and quality control of raw sequence data.Firstly, we suggest the available memory of computer is more than 128 GB, and number of threads is more than 16. Download our perl, R, and Bash Shell scripts used below from GitHub https://github.com/CTLife/m6A-SAC-seq[href=https://github.com/CTLife/m6A-SAC-seq]. Download the Human hg38 reference genome and transcriptome files from the UCSC Genome Browser (https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/[href=https://hgdownload.soe.ucsc.edu/goldenPath/hg38/bigZips/]) and the ncbiRefSeq GTF file from https://genome.ucsc.edu/cgi-bin/hgTables[href=https://genome.ucsc.edu/cgi-bin/hgTables]. Use these files to build index for the aligner STAR:\n> STAR --runMode genomeGenerate --genomeFastaFils hg38.genome.fa --genomeDir hg38 --sjdbGTFfile hg38.ncbiRefSeq.gtf\nThe scripts recognize paired-end FASTQ files by their suffixes automatically. Therefore, the two FASTQ files of each paired-end sequencing sample must be tagged by \u201cR1\u2033 and \u201cR2\u2033 respectively. Put all the raw FASTQ files into a same folder \u201c1-rawFASTQ\u201d and rename them, let their suffixes be \u201c. R1.fastq.gz\u201d or \u201c. R2.fastq.gz\u201d. For more details, please see \u201cperl m6A-SAC-seq_1.pl -help\u201d.\nBefore preprocessing, we should check adapter content, Troubleshooting 4[href=https://www.wicell.org#sec6.7] duplication level, Troubleshooting 5[href=https://www.wicell.org#sec6.9] and genome contamination Troubleshooting 6[href=https://www.wicell.org#sec6.11] to decide whether it is necessary to generate more reads or redo the experiment. Then check the quality of the raw FASTQ files by FASTQC, fastp, and FASTQ_screen, all of them were integrated as a script:\n> perl m6A-SAC-seq_1.pl -in 1-rawFASTQ\nCritical: \u2018--sjdbGTFfile hg38. ncbiRefSeq.gtf\u2019 should be indicated for better mapping reads with lots of mutation.\nLinear regression for m6A fraction and mutation rate.\nTo correlate observed mutation rates and m6A fractions of the spike-in sequences in each motif, extract the spike-ins from raw FASTQ files:\n> perl spikeins.pl -in 1-rawFASTQ -out Spikeins\n> perl spikeins_RC.pl -in 1-rawFASTQ -out Spikeins_RC\nThen, we can pool the results from same group together or detect mutation rates of spike-ins for each sample. Mutation rates and linear correlations of adenines (A)\u00a0for each motif can be generated by:\n> Rscript linear_correlations.shLinear regression models were used: y=ax\u00a0+ b. Where y is the observed mutation rate and x is the m6A fraction. In the file \u201cspikeins.fit.txt\u201d from results, the first column is one of 256 5-mer motifs, the second column is the value of a, and the third column is the value of b.\nCritical: Pool the spike-ins from same group together to increase the reliability of linear regression.\nPreprocessing and mapping.\nRemove adaptors and bases with low quality by using Trimmomatic. This step with quality statistics was implemented by one script:\n> perl m6A-SAC-seq_2.pl -in 1-rawFASTQ -out 2-removedAdapters\nThen adapter-free reads with barcodes will be removed PCR duplicates by using the clumpify.sh in BBMap:\n> clumpify.sh in=end1 in2=end2 out=output_end1 out2=output_end2 -Xmx60g reorder=f dedupes=t subs=0\nCritical: \u2018-Xmx60g\u2019 indicates that this step is memory consumption.\nCritical: \u2018dedupes=t subs=0\u2019 parameters will keep only one of the duplicates with same barcode and sequence.\nThis step with quality statistics was implemented by one script:\n> perl m6A-SAC-seq_3.pl -in 2-removedAdapters -out 3-removedDups\nFor using the variant detection tool VarScan, we need to separate stranded reads into plus-strand and minus-strand reads in further analysis. Therefore, we convert paired-end reads into single-end reads by reformat.sh in BBMap before mapping. If we use other tools to call mutation, this step is not required. One script with quality statistics was implemented (Optional):\n> perl m6A-SAC-seq_4.pl -in 3-removedDups -out 4-finalFASTQ\nRemove barcodes and map reads to the reference genome using STAR:\n> STAR --runMode alignReads --clip3pNbases 6 --alignSJDBoverhangMin 1 --outSAMmultNmax 1 --outMultimapperOrder Random --outFilterMismatchNoverReadLmax 0.1 --outFilterMismatchNmax 999 --outFilterMultimapNmax 20 \u2013outFileNamePrefix ouput --genomeDir STAR_index --readFilesIn input_end1 input_end2\nThis step with quality statistics, and detecting gene expression with Salmon or Kallisto were implemented by one script:\n> perl m6A-SAC-seq_5.pl -genome hg38 -mis 0.1 -in 4-finalFASTQ\n-out 5-rawBAMCritical: \u2018--outFilterMismatchNoverReadLmax 0.1\u2019 allows more mismatches of the reads with mutation.\nRemove reads with low MAPQ, or on unplaced and unlocalized contigs (Optional):\n> perl m6A-SAC-seq_6.pl -genome hg38 -in 5-rawBAM -out 6-finalBAM\nMutation calling and m6A sites identification.\nSplit each BAM file into two files, plus strand and minus strand (Optional):\n> perl split_strand.pl -in 6-rawBAM -out callVariants\nCall mutations for each BAM file by SAMtools and VarScan:\n> samtools mpileup --fasta-ref hg38.genome. fa --output name.pileup name.bam\n> java -jar VarScan.jar somatic SAC-seq.pileup backfround.pileup outDir\nIdentify m6A sites from the outputs from VarScan. Most m6A sites were found in conserved motif DRACH (D\u00a0= G/A/U, R\u00a0= G/A, H\u00a0= A/U/C), only a few sites located at non-DRACH motifs. So, we classified all the m6A sites into two categories: DRACH and nonDRACH. These several steps can be done by:\n> perl identify.pl -in inputDir -backgound 0.05 -p 0.05 -diff 0.1 -cov 10\nCritical: \u2018-backgound 0.05 -p 0.05 -diff 0.1 -cov 10\u2019 indicates the thresholds of maximum background mutation rate, p-value, minimum mutation rate (background was removed), and coverage. These values should be changed based on your sequencing depth and species of samples.\nQuality control of the identified m6A sites.\nSome figures can be used to assess the quality, reliability, and features of the identified m6A sites:\n> perl figures.pl -in inputDir -out outDir\nCritical: The genomic distribution and metagene profile of DRACH sites must be in line with expectations. Otherwise, the m6A sites could not be used to further analysis.", "Step-by-step method details\nStep-by-step method details\nInstall STNMF\nTiming: 10\u00a0min\nDownload the code package.\nDownload the code package from https://github.com/jiankliu/STNMF-SNN[href=https://github.com/jiankliu/STNMF-SNN] (GitHub: https://doi.org/10.5281/zenodo.10049958[href=https://doi.org/10.5281/zenodo.10049958]) or directly clone the GitHub repository, and then add the extracted folder to the MATLAB search path.\nData preprocess\nTiming: 1\u00a0min\nAfter obtaining the stimulus array and spike trains of the neurons, the data need to be processed into a form suitable for STNMF. The following steps use simulated V1 simple cell data as input.\nAccording to the CB and the spike sequence to generate the effective spike-triggered stimulus images (STE).\nBased on the spikes of the recorded neuron to find the response time (tsp) of each spike in turn.\nFind the stimulus sequence within each time period [tsp-nt+1, tsp] in turn. For the i-th spike, the corresponding 20-frame stimulus sequence is [   s  i  \u2212 19     , \u2026   s  i  \u2212 1    ,  s i   ].\nApply Singular Value Decomposition on STA of neurons to obtain spatial receptive fields (spSVD) and temporal filters (tmSVD).\nBased on tmSVD, average the stimulus sequence [   s  i  \u2212 19     , \u2026   s  i  \u2212 1    ,  s i   ] corresponding to the i-th spike, and reshape it into a row to obtain     s  ( \u03c4 )   i   . Form the     s  ( \u03c4 )   i    of all spikes into an N\u00d7P matrix STE.\n>load('Data.mat');\n>pretreatment_snn(CB,spike,nt,nx,ny);\n>load('Datapre.mat');\n>STE=getSTE_snn(SS,spklist,tmSVD,nt,nx,ny);\nNote: The nt is generally set to 20, representing 20 stimulus frames. STA is the spike-triggered average method,6[href=https://www.wicell.org#bib6] which calculates the receptive field of neurons based on stimuli and spikes. The generated simulation data or user data is stored in Data.mat. The available data format for STNMF is obtained by pretreatment_snn. The effective spike-triggered stimulus images are then generated using get_STE_snn.\nOptional: MATLAB runtime version, Data_Preprocessing.exe.Decompose STE based on STNMF to obtain W and M\nTiming: 5\u00a0min\nIn this step, the STNMF is used to infer the spatial receptive field of the subunits.\nPerform the STNMF on the STE to obtain the weight matrix W and the module matrix M.\nPreset the number of subunits k.\nRun the algorithm to get a N\u00d7K weight matrix W and a K\u00d7P module matrix M.\nReshape each row in the module matrix M to the size of the stimulus frame.\n> load('STE.mat');\n> k=8;\n> STNMFanalysis_snn('STE.mat','subunit.mat',k,20,1);\nNote: Remove the noise-like frames in the background, and all preceding frames have clear receptive fields, thus obtaining the spatial receptive field of the subunit. Run STNMFanalysis_SNN obtains spatial receptive fields for inferred subunits.\nOptional: MATLAB runtime version, STNMF_WM.exe.\nAnalyses of the M and W for more computational components\nTiming: 15\u00a0min (for steps 5 and 6)\nTiming: 25\u00a0s (for steps 7\u20139)\nIn this step, the temporal filter, nonlinearity, and synaptic connection weights of the subunits are inferred by analyzing the module matrix.\nObtain the temporal filter for the subunit.\nCalculate the nonlinearity based on the obtained spatial filter and temporal filter for each subunit.\nFirstly, convolve the spatial and temporal filters of the stimulus with the subunits to obtain the generator signal.\nThen divide it into 40 bins of equal size, ensuring that each bin contains an equal number of data points.\nFinally, visualize the nonlinearity by plotting the mean generator signal against the mean spike rate in each bin as a histogram mean.\n> calculatTF_subSP_snn('Datapre.mat', 'subunit.mat', 'subTemporalKernel.mat', k,k ,nt,nx,ny);\n> CalculateOutputGainandDrawNL_snn('Data.mat', 'Datapre.mat', 'subunit.mat', 'subTemporalKernel.mat', 'gainNL.mat');Note: By utilizing the spatial receptive field of this subunit to filter all stimulus sequences and subsequently averaging them, we can obtain the temporal filter for that specific subunit. Run calculatTF_subSP_snn to obtain the time filter for the inferred subunits. Run CalculateOutputGainandDrawNL_snn to obtain gain and nonlinearity.\nOptional: MATLAB runtime version, temporalfilter.exe, GainandNL.exe.\nIn this part, by analyzing the weight matrix to infer the synaptic connection weights and spike trains of the subunits.\nAverage each column of the weight matrix to get the connection weight    W j    for each subunit.\nSum each column of the weight matrix W to obtain the ON-OFF attribute of the.\nIf the subunit is of type OFF, take the minimum of each row in the weight matrix W. Then classify the spike into the module corresponding to the minimum.\n>get_module_weight_snn('STE.mat','subunit.mat','module_weight.mat');\n>load('module_weight.mat','W_matrix');\n>load('subunit.mat','unit','Ngood');\n>spklist_sub=get_subSTA_min_snn(Ngood,spklist,W_matrix,STE,nx,ny);\n>subunit_sp=get_subunit_spike_train_snn(Ngood,spike,spklist_sub);\n>save('subunit_sp.mat','subunit_sp');\nNote: If the sum of the weights of the subunits is greater than 0, it is an ON property, and if the sum is negative, it is an OFF property. Run get_module_weight_snn, get_subunit_spike_train_snn to obtain the synaptic weights and spike subsets.\nOptional: MATLAB runtime version, module_weight.exe, subunit_spike_train.exe.", "Step-by-step method details\nStep-by-step method details\nIn a single command, you can visualize protein-DNA binding state of a region of your interest from raw sequencing data:\nsnakemake --snakefile cooperative_binding_analysis.smk\nplots/single_binding/suppressed_merged_demo_S2_to_example_spanning_lf_15_r\nf_15_extended_left_150_right_150_roi_peak_229.fp.pdf\nplots/single_binding/suppressed_merged_demo_S2_to_example_spanning_lf_15_r\nf_15_extended_left_150_right_150_roi_peak_229.methylation.pdf --configfile\nconfigs/config.yaml\nBelow we will discuss the steps executed by the pipeline to generate the final figures.\nCAUTION: This command will run relatively quickly because demo dataset has been used. If using full raw sequencing data, it will take around \u223c10\u201315\u00a0h for the first run to prepare close to final datasets required for visualization. After that, for any regions of your interest it will take less than 5\u00a0min for visualization.\nAdapter trimming and alignment\nTiming: \u223c5\u201310 h\nIn this step, we perform standard preprocessing of NGS data and alignment to the reference genome of interest. We use \u201cTrim Galore\u201d (see rule \u201ctrim_galore_pe\u201d in snakemakes/trim_galore_pe.smk) for adapter trimming and \u201cBismark\u201d (see rule \u201cBismark_align_pe\u201d in snakemakes/Bismark_align_pe.smk) for bisulfite sequence alignment.\nSuppressing cytosine methylation in contexts other than CpG or GpC dinucleotides\nTiming: \u223c1 h\nIn this step, we refine the methylation calls by Bismark based on the methyl-transferases used in the dSMF experiment.\nNote: We should only be considering methylation in CpG and GpC context based on the enzymes used in dSMF. To ensure this, we suppress methylation calls that are in other contexts (HCH; H: A,C,T) and (DGD; D: A,T,G).\nExtracting adjacent or overlapping bisulfite reads\nTiming: \u223c1 h\nIn this step, we extract properly aligned paired-end reads based on their SAM flags (<read, mate>:\u00a0<99, 147> or <83, 163>) from the merged alignment file (see step 1.3), and generate longer reads by concatenating mate to read in case of pair <99, 147> and vice-versa in the case of <83, 163> (for further details about the meaning of SAM flags, see https://broadinstitute.github.io/picard/explain-flags.html[href=https://broadinstitute.github.io/picard/explain-flags.html]).Note: In overlapping regions, information on the leftmost read is kept. We then extract all longer reads that are overlapping or adjacent in nature. These reads form \u201cDNA molecules\u201d on which we will map binding states. Standard Illumina paired-end sequencing with read lengths 150\u00a0bp leads to typically long enough (median length of 269\u00a0bp) \u201cDNA molecules\u201d (Rao et\u00a0al., 2021[href=https://www.wicell.org#bib10]) to map transcription factor (TF) and nucleosomal binding.\nDefining footprints\nTiming: \u223c1 h\nIn this step, we define footprints on individual DNA molecules.\nNote: It can be assumed that exogenous CpG and GpC methyltransferases reach saturation by methylating all accessible CpG and GpC dinucleotides. A footprint is called when one or more unmethylated cytosines are found between two methylated cytosines. We consider footprints of at least 10\u00a0bp long. If two footprints are separated by just one bp, we merge them to define a longer footprint. A DNA molecule with all unmethylated cytosines in CpG or GpC contexts, which results in no footprint is given special consideration because it could arise due to complete occlusion by nucleosomes, thus a footprint size of the whole DNA molecule length is assigned in this case.\nCritical: With Drosophila S2 cells dSMF data, in open enhancers, one should expect about 40%\u201350% of naked-DNA, about 15%\u201320% of TF-bound and about 30%\u201340% of nucleosomal DNA (Rao et\u00a0al., 2021[href=https://www.wicell.org#bib10]). We imagine a similar distribution in other organisms based on the conservation of chromatin structure across eukaryotes.\nAssigning states on DNA molecules at single binding sites\nTiming: \u223c1 h\nIn this step, we assign binding states on individual DNA molecules mapped to ROI listed in a bed file.Note: An example file is available at the GitHub repository \u201cinput_bed/example.bed\u201d. The ROI coordinates are midpoints of the broader regions one is interested in. For TFs, we consider 15\u00a0bp upstream and downstream of the midpoint to be sufficient. We then apply conditions to assign a footprint as naked-DNA, TF, or nucleosomal: No footprint in ROI+-15 or footprint of length less than 10\u00a0bp is assigned as naked-DNA, footprints of length between 10 and 50\u00a0bp in ROI+-15 are assigned as TFs, and others are assigned as nucleosomal. We discard\u00a0<\u00a050\u00a0bp sized footprints on the edge of DNA molecules because we only know the starting point of the footprint in these cases and hence the real length of these footprints is unknown.\nAssigning binding states at a pair of binding sites\nTiming: \u223c5\u00a0min\nIn this step, we assign co-binding states on individual DNA molecules that span a pair of TF binding sites. Prepare a bedpe formatted file (see .bedpe format description here[href=https://bedtools.readthedocs.io/en/latest/content/general-usage.html]: https://bedtools.readthedocs.io/en/latest/content/general-usage.html#bedpe-format[href=https://bedtools.readthedocs.io/en/latest/content/general-usage.html#bedpe-format]) to provide the centers of the two binding sites. For an example, see \u201cinput_bed/example_cobinding.bedpe\u201d. This step can be executed by running the following command:\nsnakemake -np --snakefile cooperative_binding_analysis.smk\nplots/cobinding_bedpe/suppressed_merged_S2_to_example_cobinding_lf_15_rf_\n15_extended_left_300_right_300_roi_peak_110_4_and_peak_110_6.fp.pdf --\nconfigfile configs/config.yaml\nNote: Each TF binding site can have three states: naked or unbound, TF-bound, and nucleosome-bound. Thus, a pair of TF binding sites will have a total of nine states. States at individual sites are assigned as described in step 6. Additionally, a footprint spanning both sites that is\u00a0<100\u00a0bp in length is labeled as co-bound.\nCritical: The bedpe file can only have the filename extension \u201c.bedpe\u201d.", "Step-by-step method details\nStep-by-step method details\nHerein we describe step-by-step analytical procedures starting from raw data processing all the way through to integrated data analysis. The raw data undergo serial processing steps covering: quality control, read mapping, data filtering, normalization, and statistical calling. Subsequently, the processed data are first subjected to integrated analysis on dynamic chromatin states, to reveal differential clusters of cis-regulatory elements (CREs) that demonstrate similar dynamic chromatin modifications. Afterwards, the specific clusters of CREs with characteristic gain or loss of enhancer signatures are annotated to target genes, using either linear or spatial proximity information. Differential mRNA expression is further analyzed for these genes along with their associated functional network. The relevant biological information of the data used and their functional interpretation are discussed in great detail in (Yun et\u00a0al., 2021[href=https://www.wicell.org#bib16]).\nData processing\nTiming: 2\u20133\u00a0days\nIn this section, the raw data from different genomic approaches are processed in a stepwise manner and are transformed into a format compatible with the subsequent integrative analysis. In brief, a QC step is applied to check the ChIP-seq and ATAC-seq data quality prior to reads mapping to mouse genome, followed by the removal of duplicated reads. Subsequently, genotype-specific open chromatin states are identified by calling significant peaks on ATAC-seq in each cellular condition. Next, transcriptome data profiled by RNA-seq are processed in a similar fashion but with different tools. In addition, the RNA-seq read counts are extracted for all annotated genes and differential expression of protein-coding genes between single or double mutant cells and wildtype cells is analyzed. Finally, chromatin interaction data stored in raw .FASTQ files of pCHiC are converted into readable promoter-associated DNA interaction files. The data processing steps are described in great detail below.\nProcess raw reads in ATAC-seq and ChIP-seq data for each genotype.Perform QC and reads mapping by running custom scripts (\u201cget_data.sh\u201d) on the input .FASTQ files.\n> get_data.sh -g [GENOTYPE] -m [OUTPUT_FOLDER] -i [INPUT_FASTQ] -x mm10\nNote: QC analysis is carried out with FastQC package, and raw reads are mapped to Mus musculus (house mouse) genome assembly GRCm38 (mm10) using Bowtie2, with parameters allowing to keep reads for at most 2 alignment and 1 mismatch in the seed (20\u00a0bp default).\nFilter the mapped reads by removing duplicate reads with custom scripts (\u201cprocess_aligned_reads.sh\u201d) as below.\n> process_aligned_reads.sh -g [GENOTYPE] -m [OUTPUT_FOLDER] -x mm10\nNote: This process utilizes Picard tools with the \u201cMarkDuplicates\u201d function for data filtering, and generates sorted .BAM files.\nIdentify significant ATAC-seq peaks by running MACS2 callpeak on filtered .BAM files with a pre-defined p value at 1e-20.\n> macs2 callpeak -t [INPUT_BAM] -g mm -f BAM -n [OUTPUT_FILE_NAME] -p 1e-20 --nomodel --nolambda --bdg\nNote: The parameter --nomodel here is specified for single-read ATAC-seq data (the exemplar data), without modeling the fragment size and by default extends the reads for 200\u00a0bp. This may not accurately reflect the actual length of nucleosome-free regions.\nProcess RNA-seq raw data and analyze differential expression of protein-coding genes between mutant and wildtype samples.\nProcess RNA-seq data by running custom scripts (\u201crunRNA_STAR_paired.pl\u201d) on paired .FASTQ files (r_1 and r_2) for each genotype.\nNote: This process covers QC analysis using FastQC, then reads mapping and uniquely mappable\u00a0reads extraction using STAR package which allows at most 2 mismatches, and subsequently\u00a0read counts computation for all annotated genes using a python package HTSeq.\n> runRNA_STAR_paired.pl [INPUT_r_1_FASTQ] [INPUT_r_2_FASTQ] [GENOTYPE] mm10 STAR-GENOMES-mm10.gencode.vM7.comprehensive gencode.vM7.comprehensive.annotation.gtf [exons y/n]\nAnalyze pairwise differential gene expression between any mutant condition (Npm1c, Flt3-ITD, or DM) and WT counterpart by running custom scripts (\u201cRNAseq_differential_analysis.R\u201d) on .HTSEQ.COUNTS files generated in step 2a.\n> Rscript RNAseq_differential_analysis.RNote: Bioconductor package DESeq2 is the core analytical tool utilized in this step. The output files are in .CSV format (e.g., \u201cWT.DM.PC.diffExp.csv\u201d).\nProcess the promoter-associated chromatin interaction data profiled by pCHiC assays in each cellular condition.\nProcess pCHiC raw data (paired reads, r_1 and r_2) using HiCUP pipeline to map and filter the data and eventually output valid HiC fragments (termed di-tags) stored in .BAM files.\n> hicup_digester --genome Mouse_GRCm38 --re1 A\u02c6AGCTT,HindIII [mm10_GENOME.fa]\n> hicup --bowtie2 [BOWTIE2_PATH] --index [mm10_REFERENCE_GENOME_PATH] --digest [mm10_HINDIII_DIGESTION_FILE] --format Sanger --longest 800 --shortest 150 [INPUT_r_1_FASTQ] [INPUT_r_2_FASTQ]\nNote: The format of all input files is described in the HiCUP pipeline documentation (https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html[href=https://www.bioinformatics.babraham.ac.uk/projects/hicup/read_the_docs/html/index.html]). To execute HiCUP, the input HindIII_digestion_file needs to be generated using hicup_digester (included in the hicup software) using the first code above.\nTransform valid HiC di-tags into statistically significant chromatin interactions associated with all mouse promoters using Bioconductor package CHiCAGO.\nConvert filtered read pairs in .BAM files generated by HiCUP into the CHiCAGO input data format, .CHINPUT.\n> bam2chicago.sh [INPUT_BAM] CHiC.mm10.baitmap Digest.mm10.rmap [OUTPUT_FILE] nodelete\nNote: The availability of the shell script, as well as the description and preparation of input files can be referred to CHiCAGO online instruction (https://bitbucket.org/chicagoTeam/chicago/src/master/chicagoTools/[href=https://bitbucket.org/chicagoTeam/chicago/src/master/chicagoTools/]). The rmap file (.RMAP) and baitmap file (.BAITMAP) are tab-separated files describing the restriction digestion fragments and the coordinates of the baited/captured restriction fragments, respectively, all with numeric IDs. Both files can be generated by a CHiCAGO script (\u201ccreate_baitmap_rmap.pl\u201d) which is accessible via clicking the link above.\nFurther statistical analysis is performed on .CHINPUT files from genotype replicates to generate a list of significant promoter-associated DNA interactions.\n> Rscript runChicago.R --design-dir [DESIGN_FILES_PATH] [CHINPUT_FILE_1, CHINPUT_FILE_2,\u2026] [OUTPUT_FILE] WT.CHiC.R1.chinput,WT.CHiC.R2.chinput WT.CHiC.R1-2\nNote: Significant interactions are called when CHiCAGO scores are \u22655. The format of CHiCAGO input files is described in the CHiCAGO pipeline documentation (https://bitbucket.org/chicagoTeam/chicago/src/master/chicagoTools/[href=https://bitbucket.org/chicagoTeam/chicago/src/master/chicagoTools/]).Critical: Data processing by HiCUP and CHiCAGO are heavy computation tasks which favor usage of multiple CPU cores and large memory. The running time can be reduced to a reasonable duration in a computational environment with at least 24 threads and 48 GB RAM.\nData integration (i)\u2014Dynamic chromatin states\nTiming: 4\u20136 h\nWe first apply a multilayered approach (Ma et\u00a0al., 2020[href=https://www.wicell.org#bib9]) to integrate the multiomics chromatin analysis at all cis-regulatory elements (CREs) in wildtype and mutant HSPCs. As CREs are usually rendered accessible by chromatin binding factors such as transcription factors (TFs), their presence can be implied by open chromatin sites, which are profiled by ATAC-seq. We therefore identify all open chromatin sites across four cellular conditions by creating a compendium of ATAC-seq consensus peak sets. Afterwards, the read counts for each chromatin condition (H3K4me1, H3K4me3, H3K27ac and ATAC-seq) of each genotype (WT, Npm1c, Flt3-ITD and DM) at these potential CREs are computed to build a data matrix for further clustering analysis. Subsequently, the data matrix is processed in a similar way as for single-cell RNA-sequencing with the Seurat package, treating all CREs (as columns, equivalent to cells in a typical Seurat workflow) as separate data points across all 16 assay conditions (as rows, 4 chromatin profiles\u00a0\u00d7\u00a04 phenotypes, equivalent to genes in Seurat). This allows dimensionality reduction to classify and visualize clusters of CREs with similar patterns across wildtype and mutant cells. Meanwhile, specific clusters of chromatin regions showing leukemia-specific alterations of chromatin activation marks are identified for downstream gene network analysis.\nCreate a catalog of ATAC-seq consensus peak sets across four cellular states and convert it into a data table listing 2-kilo base (kb) bins at these consensus peaks (\u00b11\u2009kb from peak summit) in a format of .SAF required for read counts extraction using featureCounts.Make a sample list (\u201csamplesheet_ATAC.csv\u201d) indicating which ATAC-seq samples to be processed and the path to the storage of filtered reads (in .BAM files) and peak files (created by MACS2), using the layout below (row 3\u20136 are examples).\ntable:files/protocols_protocol_2081_1.csv\nBy running custom scripts (\u201cATAC_consensus_peakmax.R\u201d) on the sample list (\u201csamplesheet_ATAC.csv\u201d) generated in step 4a, a list of consensus peak sets (\u201cATAC_consensus_peaks.bed\u201d) is computed on ATAC-seq peaks from all genotypes including all their replicates. Then supplement this list with the information of which sample has maximal ATAC-seq signal at each peak (\u201cATAC_consensus_peakmax.bed\u201d).\n> Rscript ATAC_consensus_peakmax.R\nNote: This step is performed by running DiffBind within our custom scripts.\nIdentify the peak summit of each consensus peak sets (summit of the sample with maximal ATAC-seq signal identified in step 4b) and convert this information to featureCounts input file (\u201cATAC_consensus_summit2kb_adj.saf\u201d) by creating genome coordinates of 2-kb bins surrounding ATAC-seq consensus peak summits (\u00b11\u2009kb from peak summit), with the help of running custom scripts (\u201cATAC_peaksummit_to_saf.R\u201d).\n> awk '{print $1\"\\t\"$2\"\\t\"$3\"\\t\"$5\"\\t\"\"[SAMPLE]\"}' [PEAK_FILE]\u00a0>\u00a0[SAMPLE_PEAK_SUMMIT_BED]\n> cat [ALL_PEAK_SUMMIT_BED] | sort -k1,1 -k2,2n\u00a0>\u00a0ATAC_all_summit.bed\n> bedtools intersect -a ATAC_consensus_peakmax.bed -b ATAC_all_summit.bed -wa -wb\u00a0>\u00a0ATAC_consensus_peakmax_intersect_summit.bed\n> sort -k4,4 -k9,9rn ATAC_consensus_peakmax_intersect_summit.bed | sort -uk4,4 | awk '{print $6\"\\t\"$7\"\\t\"$8\"\\t\"$4}' | sort -k1,1 -k2,2n\u00a0>\u00a0ATAC_consensus_peak_summit.bed\n# \u201cATAC_consensus_peak_summit.bed\u201d is the input file for subsequent conversion to .SAF file\n> Rscript ATAC_peaksummit_to_saf.R\nExtract the read counts for each genomic approach (H3K4me1, H3K4me3, H3K27ac, and ATAC-seq) in each cellular condition at the 2-kb bins of ATAC-seq consensus peaks from the corresponding .BAM files (with replicates merged and normalized as count per million total read counts) using featureCounts.\n> featureCounts -a ATAC_consensus_summit2kb_adj.saf -F SAF -t exon -g GeneID --largestOverlap -o ATAC_consensus_summit2kb_adj_counts.txt [ALL_BAM_FILES]Perform integrative analysis on multilayered chromatin profiling data of all four genotype samples by running custom scripts (\u201cMultiomics_Seurat_analysis_v2022.R\u201d) to identify clusters of CREs (accessible chromatin regions) with similar dynamic chromatin states across WT and mutant conditions.\n> Rscript Multiomics_Seurat_analysis_v2022.R\n# input files \u201cATAC_consensus_summit2kb_adj_counts.txt\u201d and \u201cATAC_consensus_summit2kb_adj_counts.txt.summary\u201d were generated in step 5 by featureCounts\nNote: A prerequisite to dimensionality reduction analysis is a data matrix containing CREs as column (equivalent to cells in a typical Seurat workflow) and samples as rows (equivalent to genes in Seurat), filling with normalized read counts (CPM) on merged replicates of each condition,\u00a0in a layout format as listed below. An exemplar data matrix (\u201cATAC_consensus_summit2kb_adj_cpm_merge_transpose.txt\u201d) is provided in the key resources table[href=https://www.wicell.org#key-resources-table].\ntable:files/protocols_protocol_2081_2.csv\nNote: By analyzing our exemplar data, this step generates three plots as shown in Figure\u00a01[href=https://www.wicell.org#fig1]. Using a heuristic method (ElbowPlot() function in Seurat package), we observe an \u2018elbow\u2019 around PC7-8 (Figure\u00a01[href=https://www.wicell.org#fig1]A), suggesting that the majority of true signal is captured in the first 8 PCs. Subsequent analysis using FindClusters() function outputs 10 communities, followed by computation of 10 clusters by non-linear dimensionality reduction algorithms: UMAP or tSNE (Figure\u00a01[href=https://www.wicell.org#fig1]B). And we found individual clusters were more well separated in tSNE plot than in UMAP. Therefore, the 10 tSNE-clusters are further subjected to heatmap plotting, to demonstrate individually dynamic patterns across WT and mutant conditions (Figure\u00a01[href=https://www.wicell.org#fig1]C). Furthermore, we extract the genomic coordinates of Cluster-6 CREs as exemplar data to analyze their associated gene network. This creates a bed file (\u201cMultiomics_Cluster-6_summit200bp.bed\u201d) which contains genome coordinates of a 200\u00a0bp region surrounding ATAC-peak summit of Cluster-6 for downstream annotation analysis.Note: To link a set of CREs (tSNE clusters) with mutation condition, by qualitatively analyzing the dynamic pattern of chromatin profiles associated with mutation alone or in combination in the heatmap (Figure\u00a01[href=https://www.wicell.org#fig1]C), we identified several clusters which demonstrate synergistic impact of mutations on chromatin modulation. For example, we identified CREs showing gains of enhancer marks and accessibility by mutations (e.g., Flt3-ITD and DM), which were separated by marked gain of accessibility (Cluster-5) and H3K27ac (Cluster-6). In comparison, Cluster-8 and Cluster-1 demonstrate mutation-associated loss of enhancer signatures, characterized by concurrent loss of H3K4me1 and accessibility, with or without evident loss of H3K27ac, respectively. More molecular information on these specific clusters can be referred to (Yun et\u00a0al., 2021[href=https://www.wicell.org#bib16]) where the exemplar data were generated.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2081-Fig1.jpg\nFigure\u00a01. A multilayered approach to analyze dynamic chromatin marks upon mutational synergy induced leukemia\n(A) The elbow plot determines number of PCs to capture the variation in the data.\n(B) Non-linear dimensionality reduction by UMAP or tSNE clustering.\n(C) Heatmap shows individual clusters of CREs with dynamic patterns of chromatin modifications and accessibility across WT and mutant conditions.\nData integration (ii)\u2014Differential gene network\nTiming: 6 hIn this step, we will link the CREs which demonstrate mutation-specific alteration of chromatin states to their associated genes with linear or spatial proximity. From step 6, Cluster-6 is selected as an exemplar group of CREs showing increased chromatin activity induced by mutations. The two mutations (Npm1c and Flt3-ITD) exert a strong synergy to induce a marked gain of H3K27ac, elevated levels of H3K4me1 and ATAC-seq, indicating the acquisition of enhancer signals by leukemia induction. Using the promoter-associated chromatin interaction data from pCHiC assays, Cluster-6 CREs are assigned to target genes when the CREs overlap with bait promoters or interaction fragments revealed by the pCHiC data. These target genes are then examined for differential expression analysis between mutant and WT samples, by checking them in the global analysis from step 2b. Since Cluster-6 CREs represent leukemia-specific gain of chromatin activity, the mutation-induced up-regulated genes linked to Cluster-6 CREs are further selected for gene ontology analysis, leading to the identification of leukemia-specific gene network related to chromatin alteration at 3D level.\nPrepare the CRE annotation file using chromatin interaction information indicated by pCHiC data.\nMake a sample list (\u201cmakematrixsample.txt\u201d) indicating the genotypes and the correspondent .RDS files (created by \u201crunChicago.R\u201d in step 3b) which contain promoter-associated DNA interactions generated by CHiCAGO, using the format below.\ntable:files/protocols_protocol_2081_3.csv\nNote: For each genotype, interaction data of two replicates are merged by running runChicago.R as input samples.\nGenerate a consensus matrix of significant chromatin interactions (CHiCAGO score \u22655) detected in at least one genotype by running makePeakMatrix.R in CHiCAGO package (output file: \u201cpCHiC_matrix.txt\u201d).\n> Rscript makePeakMatrix.R --twopass ./makematrixsample.txt pCHiC_matrix\u00a0>\u00a0pCHiC_matrix.log\nIdentify specific target genes associated with Cluster-6 CREs which were identified in step 6 by utilizing chromatin interaction information.\n> Rscript Cluster_CREs_genes_diffexp.R# Annotation input files \u201cDigest.mm10.rmap\u201d and \u201cpCHiC_fragID_Gene.txt\u201d are provided in the KRT, while \u201cpCHiC_matrix.txt\u201d was generated in step 7b. Differential gene expression input file \u201cWT.DM.PC.diffExp.csv\u201d was generated in step 2b.\nNote: Annotation is achieved by the exploration of pCHiC data (\u201cpCHiC_matrix.txt\u201d from\u00a0step 7b), which include genomic coordinates of gene promoters (as \u201cbait\u201d fragment) and their interacting regions (as other end \u201coe\u201d fragment). Next, by intersecting CREs with\u00a0either \u201cbait\u201d or \u201coe\u201d fragments, the target genes associated with specific CREs can\u00a0be identified. These genes are further analyzed for altered expression by combined mutations (DM leukemia) in comparison to WT (Figure\u00a02[href=https://www.wicell.org#fig2]A, and the output file \u201cCluster-6_genes_DMvsWT_diffexp.txt\u201d). Up- or down-regulation is defined as fold-change \u22651.5 and adjP <0.05. This step can be achieved by running custom scripts (\u201cCluster_CREs_genes_diffexp.R\u201d).\nSelect the upregulated genes from previous step (the file \u201cCluster-6_DMvsWT_upgenes.txt\u201d from step 8) to load into web server ShinyGO v0.76 (http://bioinformatics.sdstate.edu/go/[href=http://bioinformatics.sdstate.edu/go/]) for gene network or pathway analysis.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2081-Fig2.jpg\nFigure\u00a02. Altered expression of genes and gene network linked to DM-specific chromatin alteration\n(A) Differential expression of genes associated with Cluster-6 CREs showing leukemia-specific gain of chromatin activity.\n(B) Top 10 enriched network pathways of DM leukemia upregulated genes associated with Cluster-6 CREs.\nNote: The search species is set for \u201cMouse\u201d and \u201cGO Biological Process\u201d is selected as target pathway database, with default parameters (FDR cut-off at 0.05) and setting 10 pathways to show. The output plot (Figure\u00a02[href=https://www.wicell.org#fig2]B) is generated under the tab Network.", "Step-by-step method details\nStep-by-step method details\nStable cell line generation\nTiming: 2\u00a0weeks\nNote: All steps involving lentivirus vectors and experiments should be performed in accordance with institute policies and obtained approval from the Institutional Biosafety Committee.\nPackage shRNA-encoding lentiviruses in 293T cells\nPurchase lentiviral vectors containing the appropriate gene-targeting or control shRNAs, or clone the shRNA sequences into the lentiviral vector PLKO.1 or lentiCRISPR-v2.\nProduce lentivirus in 293T cells.\nThe day before transfection, resuspend 5\u00d7105 293T cells in DMEM medium supplemented with 10% fetal bovine serum and 50\u00a0U/mL penicillin-streptomycin and seed in a 6-well plate at a density sufficient to reach \u223c80% confluence \u223c24\u00a0h later.\nMix the shRNA plasmids with packaging plasmids (as indicated in the table below) in 100\u00a0\u03bcL Opti-MEM medium and then add 5\u00a0\u03bcL P3000 reagent and mix well.\ntable:files/protocols_protocol_1334_6.csv\nDilute 6.5\u00a0\u03bcL Lipofectamine 3000 in 100\u00a0\u03bcL Opti-MEM, mix with the plasmid/P3000 mixture, incubate for 10\u00a0min at 25\u00b0C, and then gently add dropwise to the plated cells in 2\u00a0mL medium.\nAlternatives: Lipofectamine 2000 or polyethylenimine (PEI) reagent can also be used as transfection reagents.\nIncubate the plate at 37\u00b0C. After 12 h, exchange the medium for fresh DMEM medium and continue the incubation at 37\u00b0C.\nAt 48\u00a0h and 72\u00a0h incubation, collect and pool the supernatants and centrifuge at 500\u00d7g for 10\u00a0min to pellet residual 239T cells.Remove the virus-containing supernatants and either transduce cells freshly or freeze at \u221280\u00b0C until use. Physical and functional titers can be tested before freezing the virus. For the physical titer, it can be easily determined by Lenti-X qRT-PCR Titration Kit or other RT-qPCR based method to detect lentiviral backbone components. For functional titer, if the vector contains fluorescence markers, use fluorescence-activated single cell sorting (FACS) to count the infected cells number after being infected with serial dilution of the viral supernatant. If the vector contains antibiotic resistance genes (e.g., Puromycin), colony-forming units can be quantified after the target cells were transduced with serial dilution of the viral stock and selected with the antibiotic. The Multiplicity of Infection (MOI) should be >1 for transduction.\nPrepare Caco-2 cells and transduce them with shControl or shMETTL3 lentiviruses.\nThe day before transduction, seed 5\u00a0\u00d7 105 Caco-2 cells in high-glucose MEM supplemented with 20% fetal bovine serum, MEM NEAA and 50\u00a0U/mL penicillin-streptomycin and seed in a 6-well plate at a density sufficient to reach \u223c50% confluence \u223c24\u00a0h later.\nRemove the supernatant from the seeded Caco-2 cells, add 800\u00a0\u03bcL shControl or shMETTL3\u00a0lentivirus in 1.2\u00a0mL medium, and then add Polybrene to a final concentration of 8\u00a0\u03bcg/mL.\nNote: The precise volume for added virus will be based on the lentiviral titer (see troubleshooting problem 1[href=https://www.wicell.org#sec7.1]).\nSpin the plates at 485\u00d7g for 2\u00a0h at 25\u00b0C.\nIncubate the plates at 37\u00b0C for 12 h, exchange the transduction medium for fresh MEM medium, incubate at 37\u00b0C for 48 h, and then start selection of stably transduced cell lines by changing to medium containing puromycin at 2\u00a0\u03bcg/mL. The exchange media must be treated with 10% bleach in the hood for at least 30\u00a0min before disposal.Passage and expand the cells in puromycin (2\u00a0\u03bcg/mL-containing MEM medium for \u223c7\u00a0days, and then measure METTL3 mRNA and protein expression to determine silencing efficiency (Figure\u00a01[href=https://www.wicell.org#fig1]). If necessary, continue the selection until high knockdown efficiency is achieved. Usually, the high knockdown efficiency indicates that the gene expression in the shRNA transduced cells is below 20% compared to control cells by RT-qPCR or Western blotting (see troubleshooting problem 1[href=https://www.wicell.org#sec7.1]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1334-Fig1.jpg\nFigure\u00a01. Expression of METTL3 in control and METTL3-knockdown Caco-2 cells\n(A) RT-qPCR analysis of METTL3 mRNA in Caco-2 cells expressing control or two METTL3-targeting shRNAs. Expression levels were normalized to GAPDH mRNA. Data are expressed as the mean\u00a0\u00b1 SEM (n\u00a0= 3). \u2217p\u00a0< 0.05 by Student\u2019s t test.\n(B) Western blot analysis of METTL3 protein in Caco-2 cells expressing control or METTL3-targeting shRNA. Glyceraldehyde 3-phosphate dehydrogenase (GAPDH) was probed as a loading control.\nThese data are from the original Figures 1[href=https://www.wicell.org#fig1]D and 4F in Li et\u00a0al. (2021)[href=https://www.wicell.org#bib8].\nInfect cells with SARS-CoV-2\nTiming: 1\u00a0week\nNote: All steps involving SARS-CoV-2 live virus should be performed in a BSL3 laboratory.\nPrepare SARS-CoV-2 for infection using standard procedures for virus propagation and quantification of infectious units by plaque assay with Vero E6 cells (Harcourt et\u00a0al., 2020[href=https://www.wicell.org#bib4]). We obtained SARS-CoV-2 isolate USA-WA1/2020 from BEI Resources.\nResuspend the control and METTL3-knockdown Caco-2 cells in complete MEM medium and seed at \u223c3\u00a0\u00d7 106 cells in one 10-cm dish or one full 6-well plate. Prepare at least triplicate dishes/wells per experimental condition.Infect Caco-2 cells with SARS-CoV-2 at a multiplicity of infection (MOI) of 2 diluted in serum-free MEM medium, one hour after infection, the virus-containing medium is disposed, and infected cells are washed by DPBS once, add fresh complete MEM medium and incubate the cells at 37\u00b0C for 24 h. Collect the supernatants and then harvest the cells by addition of TRIzol.\nObtain the viral copy number in the supernatant by RT-qPCR analysis by using iTaq Universal SYBR Green One-Step Kit and comparison with a standard curve generated from in vitro-transcribed viral nucleocapsid (N) region (Figure\u00a02[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1334-Fig2.jpg\nFigure\u00a02. Standard curve for calculation of viral copy number by RT-qPCR\nThe equation and R2 are shown. N_Sarbeco primer sets were used for qPCR (Corman et\u00a0al., 2020[href=https://www.wicell.org#bib2]). Data are presented as the mean\u00a0\u00b1 SEM (n=3).\nProceed to the following step-by-step method details[href=https://www.wicell.org#step-by-step-method-details] section for extraction and quantification of viral RNA and cellular mRNA.\nExtract and purify viral RNA from cell supernatants and mRNA from infected cells\nTiming: 1\u00a0day\nThis section describes the steps for RNA extraction from supernatants and infected Caco-2 cells collected in the preceding step.\nNote: All steps with SARS-CoV-2 live virus-infected cells and supernatants should be performed in a BSL3 laboratory.\nExtract viral RNA from Caco-2 culture supernatants.\nAdd 30\u00a0mL TRIzol LS per 10\u00a0mL supernatant, mix by pipetting, and incubate for 5\u00a0min at 25\u00b0C.\nNote: Samples can be transferred from the BSL3 laboratory after completion of this step.\nPause point: The supernatant-TRIzol LS mixture can be frozen at \u221280\u00b0C for \u22647\u00a0days before moving on to RNA extraction.\nAliquot the culture supernatant/TRIzol LS mixture into 50\u00a0mL tubes at 20\u00a0mL per tube. Add 4\u00a0mL of chloroform to each tube and shake for 20 s.Incubate the tubes for 10\u00a0min at 25\u00b0C.\nCentrifuge the tubes at 12,000\u00d7g for 15\u00a0min at 4\u00b0C.\nTransfer the \u223c7\u00a0mL upper phase to a fresh 50\u00a0mL tube, add 2\u00a0\u03bcL GlycoBlue, and mix.\nAdd 10\u00a0mL isopropanol per tube, mix, and incubate for 10\u00a0min at 25\u00b0C .\nCritical: The isopropanol step precipitates the RNA.\nCentrifuge the 50\u00a0mL tubes at 12,000\u00d7g for 15\u00a0min at 4\u00b0C.\nRemove and discard the supernatant and gently wash the RNA pellet by adding 1\u00a0mL of 75% ethanol.\nAspirate the ethanol, air dry the RNA pellet for 5\u201310\u00a0min, and dissolve in 20\u00a0\u03bcL nuclease-free water.\nPerform in-column DNase I treatment at 25\u00b0C for 15\u00a0min and concentrate it using an RNA Clean & Concentrator-5 kit according to the manufacturer\u2019s instructions https://files.zymoresearch.com/protocols/_r1013_r1014_r1015_r1016_rna_clean_concentrator-5.pdf[href=https://files.zymoresearch.com/protocols/_r1013_r1014_r1015_r1016_rna_clean_concentrator-5.pdf]\nAliquot the RNA and store at \u221280\u00b0C until use.\nAlternatives: RNA in the supernatants can also be extracted using a Direct-zol RNA Purification kit.\nExtract cellular mRNA from infected Caco-2 cells.\nAdd TRIzol directly to the adherent Caco-2 cells at a 10cm dish or 6-well plate, aspirate the lysate, and mix by pipetting five times.\nExtract total RNA from the sample and perform on-column DNase I treatment using a Direct-zol Miniprep Plus RNA kit according to the manufacturer's instructions https://files.zymoresearch.com/protocols/_r2070t_r2070_r2071_r2072_r2073_direct-zol_rna_miniprep_plus_kit.pdf[href=https://files.zymoresearch.com/protocols/_r2070t_r2070_r2071_r2072_r2073_direct-zol_rna_miniprep_plus_kit.pdf]\nNote: If fewer than 5\u00a0\u00d7 106 cells are extracted, use the Direct-zol RNA Microprep/Miniprep kit.\nIsolate mRNA from the 100\u00a0\u03bcg purified total RNA sample using a Magnetic mRNA Isolation\u00a0kit\u00a0according to the manufacturer\u2019s instructions https://www.neb.com/-/media/nebus/files/manuals/manuals1550.pdf?rev=bf3077d4994b492fb81dc09702571e84&hash=EC035147BC5AE92258976BFD95143341[href=https://www.neb.com/-/media/nebus/files/manuals/manuals1550.pdf?rev=bf3077d4994b492fb81dc09702571e84&hash=EC035147BC5AE92258976BFD95143341]. Perform two rounds of polyA selection to ensure the mRNA is of high purity.\nCheck the mRNA quality (e.g., Agilent TapeStation) to be sure that ribosomal RNA (rRNA) is depleted (Figure\u00a03[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1334-Fig3.jpg\nFigure\u00a03. Examination of Caco-2 mRNA purity using Agilent TapeStation(A and B) Gel image (left) and electropherogram (right) of two samples of total cellular RNA (A) and RNA after depletion of ribosomal RNA (rRNA) (B). Samples were analyzed using an Agilent RNA Screen TapeStation.\nAlternatives: rRNA depletion can also be performed using an rRNA depletion kit such as the RiboMinus Eukaryote System v2.\nFragment the purified RNA and perform MeRIP\nTiming: 2\u00a0days\nThis section describes the steps for performing immunoprecipitation of m6A (MeRIP) from the isolated viral RNA and cellular mRNA. The steps are identical for both RNA types.\nNote: During optimization experiments, we determined that this fragmentation step can be performed on as little as 300\u00a0ng of cellular mRNA (see troubleshooting problems 2[href=https://www.wicell.org#sec7.3]and 3[href=https://www.wicell.org#sec7.5]).\nRNA fragmentation\nDilute equal amounts of supernatant RNA or cellular mRNA in 9\u00a0\u03bcL nuclease-free water.\nAdd 1\u00a0\u03bcL of 10X Fragmentation Buffer (Ambion RNA Fragmentation Reagent) to the RNA samples, mix well by pipetting, and incubate at 70\u00b0C for 15\u00a0min.\nAdd 1\u00a0\u03bcL of Stop Solution (Ambion RNA Fragmentation Reagent) per sample and place on ice.\nCritical: Incubate for exactly 15\u00a0min after adding the fragmentation buffer and place immediately on ice after adding the stop solution. These steps are important to ensure the optimal fragment size (100\u2013200\u00a0bp).\nAdd a 350\u00a0\u03bcL TE buffer to the fragmented RNA.\nAdd 40\u00a0\u03bcL 3\u00a0M sodium acetate, 1.5\u00a0\u03bcL GlycoBlue, and 1\u00a0mL 100% ethanol to the solution, mix gently, and incubate at \u221280\u00b0C for \u223c14 h.\nPlace the mixture on ice and then centrifuge at 12,000\u00d7g at 4\u00b0C for 30\u00a0min. Aspirate the supernatant without touching the RNA pellet.\nWash the RNA pellet by adding 1\u00a0mL of 75% ethanol and centrifuging at 12,000\u00d7g at 4\u00b0C for 5\u00a0min. Aspirate the supernatant without touching the RNA pellet.Air dry the RNA pellet for 5\u00a0min and then dissolve in 20\u00a0\u03bcL nuclease-free water.\nPause point: The RNA can be stored in \u221280\u00b0C for \u22647\u00a0days.\nMeasure the concentration of fragmented RNA using a Nanodrop or Qubit.\nRemove 10% of the fragmented RNA solution (2\u00a0\u03bcL) and place at \u221280\u00b0C for use as the \u201cinput RNA\u201d sample. The remaining \u223c18\u00a0\u03bcL of fragmented RNA is used for MeRIP.\nNote: The recovery rate of fragmented RNA should be >90%.\nMeRIP of fragmented RNA\nAdd 500\u00a0\u03bcL MeRIP binding buffer to the \u223c18\u00a0\u03bcL fragmented RNA solution and mix gently.\nAdd 5\u00a0\u03bcL of anti-m6A antibody and 2.5\u00a0\u03bcL of RNase inhibitor to the sample.\nRotate the sample at 4\u00b0C for 2\u00a0h to allow binding of the antibody to m6A-modified RNA fragments.\nFor each immunoprecipitation (IP) sample, prepare 50\u00a0\u03bcL of protein A/G magnetic beads by washing twice with a MeRIP binding buffer.\nAdd the IP sample solution to the pre-washed magnetic beads and rotate at 4\u00b0C for 2 h.\nCollect the beads using a magnetic stand and discard the supernatant.\nAdd 500\u00a0\u03bcL of MeRIP binding buffer to the magnetic beads, mix well, collect the beads using the magnetic stand, and discard the supernatant.\nAdd 500\u00a0\u03bcL of MeRIP wash buffer to the tube, mix well, and collect the beads with a magnetic stand, discard the supernatant. Repeat the wash twice.\nAdd 50\u00a0\u03bcL of freshly prepared MeRIP elution buffer to the beads and incubate in a ThermoMixer at 25\u00b0C for 20\u00a0min. Collect the eluates.\nRepeat step i for a second elution of the beads and combine the two eluates.\nPurify RNA from the eluates using an RNA Clean & Concentrator kit according to the manufacturer\u2019s instructions https://files.zymoresearch.com/protocols/_r1013_r1014_r1015_r1016_rna_clean_concentrator-5.pdf[href=https://files.zymoresearch.com/protocols/_r1013_r1014_r1015_r1016_rna_clean_concentrator-5.pdf]. Eluate the MeRIP RNA in 6\u00a0\u03bcL RNase-free water.Critical: The MeRIP elution buffer must be prepared fresh for each experiment to ensure efficient elution (see troubleshooting problems 2[href=https://www.wicell.org#sec7.3] and 3[href=https://www.wicell.org#sec7.5]).\nLibrary preparation for next-generation sequencing (NGS) and bioinformatics analysis of the sequencing results\nTiming: 3\u20137\u00a0days\nThis section describes the steps to prepare the library for NGS from the input RNA and MeRIP RNA samples and the bioinformatics analysis of the sequencing results.\nPrepare input and MeRIP sample libraries for NGS.\nTake out the purified input and MeRIP RNA and equalize the volume to 6 uL by adding 4 uL RNase-free water to 2 uL input RNA.\nPrepare the libraries using an Illumina TruSeq Stranded mRNA kit with the following modifications.\nNote: Locate and thaw the reagents before starting the preparation.\nInstead of the kit instructions, bring the volume of the RNA solutions up to 18\u00a0\u03bcL with FPF mix, which contains the random hexamer primers.\nInstead of repeating the fragmentation, heat the samples at 65\u00b0C for 5\u00a0min, which disrupts\u00a0RNA structure and facilitates priming, and then proceed to the first-strand synthesis\u00a0step. Thereafter, follow the manufacturer\u2019s instructions. https://support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/samplepreps_truseq/truseq-stranded-mrna-workflow/truseq-stranded-mrna-workflow-checklist-1000000040600-00.pdf[href=https://support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/samplepreps_truseq/truseq-stranded-mrna-workflow/truseq-stranded-mrna-workflow-checklist-1000000040600-00.pdf]\nCheck the library quality, normalize, and pool the libraries for NGS. The sequencing is performed in the Illumina NovaSeq 6000 instrument. The read length is 101\u00a0bp pair end and the reading depth is \u223c50 M/sample.\nAlternatives: The library can also be prepared using an Illumina TruSeq Stranded Total RNA kit (or similar kit) with the following modifications. Instead of the kit instructions, add 8.5\u00a0\u03bcL EPH buffer to 8.5\u00a0\u03bcL of RNA samples. Instead of repeating the fragmentation, heat the samples at 65\u00b0C for 5\u00a0min to disrupt the RNA structure and facilitate priming, and then proceed to first-strand synthesis. Thereafter, follow the manufacturer\u2019s instructions https://support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/illumina_prep/RNA/illumina-stranded-total-rna-checklist-1000000124515-01.pdf[href=https://support.illumina.com/content/dam/illumina-support/documents/documentation/chemistry_documentation/illumina_prep/RNA/illumina-stranded-total-rna-checklist-1000000124515-01.pdf] (see troubleshooting problem 3[href=https://www.wicell.org#sec7.5]).Bioinformatic analysis of sequencing data (please also refer to Li et\u00a0al. (2021)[href=https://www.wicell.org#bib8])\nRemove sequencing adaptors using Cutadapt (parameters: cutadapt -a ADAPTER_FWD -A ADAPTER_REV -o out.1.fastq -p out.2.fastq reads.1.fastq reads.2.fastq; where assuming the sequencing file is in reads.1.fastq and reads.2.fastq). The target reads for each sample is \u223c50 M. Align the sequencing reads to the SARS-CoV-2 genome using Bowtie2 or STAR (parameters for Bowtie2: bowtie2 -x SARS_CoV_2 genome -1 1.fastq.gz -2 2.fastq.gz -S 1_map_and_unmap.sam; parameters for STAR: star --runThreadN 4 --genomeDir SARS_CoV_2 --readFilesIn reads.1.fastq, reads.2.fastq --outFileNamePrefix 1/1_ --outSAMtype BAM SortedByCoordinate; where assuming the sequencing file is in reads.1.fastq and reads.2.fastq)\nUse Samtools to generate and sort bam files for aligned reads (parameters: samtools sort 1_map.bam-o 1_map.sorted.bam; samtools index 1_map.sorted.bam; where assuming the 1 is the bam file generated from Bowtie2 or STAR)\nCall m6A peaks with m6A viewer or MACS2 based on the paired m6A-RIP/input data from the aligned reads (parameters: macs2 callpeak -t MeRIP_map.sorted.bam -c input_map.sorted.bam -f BAMPE -g 2.9903e+4 -n MeRIP-CoV2 -p 0.05 --out 3 --nomodel --extsize 200\u00a0--call-summits).\nFor the m6A viewer, we used the following parameters: expected peak length 200, enrichment fold >3, false discovery rate <0.01, minimum peak height 10,000. For MACS2, we used the following parameters: p value\u00a0< 0.05, call-summit no model, BAMPE, maximum duplicate fragments\u00a0= 1 or keep all duplicates.\nUpload the BedGraph file and the bed file to view the called peaks in Integrative Genomics Viewer (Figure\u00a04[href=https://www.wicell.org#fig4]) (see troubleshooting problem 4[href=https://www.wicell.org#sec7.7]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1334-Fig4.jpg\nFigure\u00a04. Genome browser tracks for input total RNA and MeRIP-Seq RNA from SARS-CoV-2SARS-CoV-2 reads were aligned with STAR and peaks were called by MACS2 without removing duplicates. Blue, red, and green colors indicate input viral RNA, and viral RNA extracted from supernatants of control and METTL3-knockdown cells. Bed files of the so-called peak regions are shown under the MeRIP track of each group. The scale of the peak density is the same for all groups. The enlarged view shows m6A signals in the 3\u2032 end of the SARS-CoV-2 genome. These data are from the original Figure\u00a01[href=https://www.wicell.org#fig1]F in Li et\u00a0al. (2021)[href=https://www.wicell.org#bib8].\nValidate the Me-RIP-Seq findings by performing MeRIP-RT-qPCR.\nDesign 4 sets of qPCR primers for the m6A-enriched nucleocapsid (N) region in SARS-CoV-2 (primers for N1\u2013N4 are listed here: MeRIP-N1: nCoV_N1-F: 5\u2032-GACCCCAAAATCAGCGAAAT-3\u2032, nCoV_N1-R : 5\u2032-TCTGGTTACTGCCAGTTGAATCTG-3'; MeRIP-N2: nCoV_N2-F: CACATTGGCACCCGCAATC, nCoV_N2-R: GAGGAACGAGAAGAGGCTTG; MeRIP-N3: nCoV_N3-F: GGAACTAATCAGACAAGGAAC, nCoV_N3-R: GAAATTTGGATCTTTGTCATC; MeRIP-N4: nCoV_N4-F: ACATTCCCACCAACAGAGCC, nCoV_N4-R: CAGCACTGCTCATGGATTG).\nFor negative controls, design primers to amplify SARS-CoV-2 envelope (E) gene and human glyceraldehyde 3-phosphate dehydrogenase (GAPDH), which represent unmethylated viral and Caco-2 genes, respectively. For the present analysis, the primers were designed based on the location of the m6A peaks in the SARS-CoV-2 viral genome (Figure\u00a05[href=https://www.wicell.org#fig5]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1334-Fig5.jpg\nFigure\u00a05. Validation of MeRIP-Seq data by MeRIP-RT-qPCR\n(A) Expanded view of the m6A signals in the SARS-CoV-2 genome (as in Figure\u00a04[href=https://www.wicell.org#fig4]) with the qPCR primer design of the\u00a0N\u00a0gene (N1\u20134) and E gene shown below.(B) MeRIP-RT-qPCR of viral RNA extracted from SARS-CoV-2-infected Caco-2 cells expressing shControl or shMETTL3. Equal amounts of viral RNA were immunoprecipitated with control IgG antibody or anti-m6A antibody. Amplification of E and GAPDH RNA served as negative viral and Caco-2 controls, respectively. Data are presented as the mean\u00a0\u00b1 SEM (n\u00a0= 3). \u2217p\u00a0< 0.05, \u2217\u2217p\u00a0< 0.01, and \u2217\u2217\u2217p\u00a0< 0.001 by Student\u2019s t test. These data are adapted from the original Figures 1[href=https://www.wicell.org#fig1]F and 1G in Li et\u00a0al. (2021)[href=https://www.wicell.org#bib8].\nPerform MeRIP on the RNA samples as described in steps 10\u201311 above. In the IP step, include samples incubated with IgG control antibodies equal to the amount of anti-m6A antibody (5\u00a0\u03bcg). Equalize the final volume of purified 10% input and MeRIP RNA to 6 uL in RNase-free water.\nTake 1\u00a0\u03bcL of the final eluates from the MeRIP samples and 1\u00a0\u03bcL of the reserved input RNA and perform reverse transcription with an iScript cDNA synthesis kit using the following reaction conditions:\ntable:files/protocols_protocol_1334_7.csv\nDilute the cDNA 10-fold in Nuclease-free water, and prepare 10\u00a0\u03bcL PCR reaction samples as indicated below and run the samples on a LightCycler 480.\ntable:files/protocols_protocol_1334_8.csv\nCalculate the enrichment of m6A-modified RNA as a percentage of input RNA for each independent replicate using the following equation.\nEnrichment (as % of input)\u00a0= 2Ct (Input) \u2212 Ct (MeRIP) \u00d7 10\nAverage the values for the three biological replicates per group (Figure\u00a05[href=https://www.wicell.org#fig5]B) (See troubleshooting problem 5[href=https://www.wicell.org#sec7.9]).Note: Multiplication by 10 corrects for the percentage of input (10%). The absolute input % value of the positive region should be >0.5 and the negative control should be <0.1. The relative value of the positive m6A modified region is compared with IgG. The value of fold enrichment from the positive region should be more than 5 over IgG, and the value of the negative region should be about 1.\nCritical: At least three biological replicates should be included for each group.", "Step-by-step method details\nStep-by-step method details\nIsolation and fixation of nuclei from the human or mouse brain\nTiming: 2 h\nIn this step, cell nuclei are isolated from the human or mouse brain through mechanical homogenization (Dounce homogenizer) in the presence of a detergent (0.1% Triton X-100), and preserved with a fixative (2% PFA).\nIsolation of nuclei was adapted from (Krishnaswami et\u00a0al., 2016[href=https://www.wicell.org#bib5]; Lacar et\u00a0al., 2016[href=https://www.wicell.org#bib6]) with minor modifications. In particular, Tris buffer was replaced with an equal molarity of HEPES buffer to avoid interference with PFA fixation.\nNote that nuclei isolation may alter native 3D genome structure, because the cytoskeleton and gene transcription may be disrupted. Nuclei should be isolated as fast as possible and kept at 4\u00b0C until fixation to minimize changes to the 3D genome. We have not tested fixation before nuclei isolation, because homogenization is generally more challenging for fixed tissues.\nNuclei isolation\nOptional: If using cells rather than nuclei, skip this section and proceed directly to Fixation (Step 14).\nChill a 2\u00a0mL Dounce homogenizer (Sigma D8938) on ice for up to 200\u00a0mg of tissue. For larger tissues, use a homogenizer of a larger size (D9063 for 7\u00a0mL, D9938 for 15\u00a0mL, D9188 for 40\u00a0mL, D0189 for 100\u00a0mL) and scale up the reaction accordingly.\nFreshly prepare 1\u00a0mM DTT. Each reaction consumes 6\u00a0\u03bcL. The following recipe (1\u00a0mL) is sufficient for 150 reactions:\ntable:files/protocols_protocol_744_5.csv\nVortex to mix.\nFreshly prepare Nuclei Isolation Buffer without Triton: The following recipe (6\u00a0mL) is for 1 reaction:\ntable:files/protocols_protocol_744_6.csv\nVortex to mix. Chill on ice.\nFreshly prepare Nuclei Isolation Buffer with Triton: The following recipe (2\u00a0mL) is for 1 reaction:\ntable:files/protocols_protocol_744_7.csv\nVortex to mix. Chill on ice.\nAdd 2\u00a0mL ice-cold Nuclei Isolation Buffer with Triton to the homogenizer.Add up to 200\u00a0mg tissue to the homogenizer.\nDounce the tissue with 5 strokes of the loose pestle (A), and 15 strokes of the tight pestle (B).\nTransfer the homogenate to a conical tube.\nCentrifuge at 100 g for 8\u00a0min at 4\u00b0C.\nCarefully remove supernatant without disrupting the soft pellet. Resuspend in 2\u00a0mL Nuclei Isolation Buffer without Triton.\nCentrifuge at 100 g for 8\u00a0min at 4\u00b0C.\nCarefully remove supernatant without disrupting the soft pellet. Resuspend in 2\u00a0mL Nuclei Isolation Buffer without Triton.\nFilter by a 40-um cell strainer (Corning 352340) or other sizes (352235 for 35\u00a0\u03bcm, 352360 for 100\u00a0\u03bcm).\nNote: The nuclei suspension may be cloudy because of debris (e.g., myelin). Debris does not affect downstream procedures, and will be partially solubilized during SDS treatment (Step 26 and Step 27) at the Chromatin Conformation Capture step.\nFixation\nOptional: If using cells rather than nuclei, start from here.\nFreshly prepare 1% BSA in PBS: Dissolve 0.1\u00a0g BSA (Gemini 700-106P) in 10\u00a0mL PBS (ThermoFisher 10010023). Each reaction consumes 1.2\u00a0mL. Chill on ice.\nAdd 133.3\u00a0\u03bcL 32% PFA (EMS 15714; store at 4\u00b0C for up to a month after opening) to each 2\u00a0mL cells or nuclei (final concentration: 2%).\nCritical: PFA is hazardous. Perform the above and following steps (until Step 19: resuspension of the pellet in 1% BSA in PBS) in a fume hood and properly dispose of waste.\nNote: We have not tested other types of formaldehyde (e.g., methanol-containing), other fixatives, or unfixed cells or nuclei.\nRotate at 18\u00b0C\u201327\u00b0C for 10\u00a0min.\nAdd 200\u00a0\u03bcL ice-cold 1% BSA in PBS. Invert to mix.Note: BSA, rather than the more widely used glycine, is used to react with excess PFA because in our hands, reaction between glycine and PFA acidifies the solution (yellow when phenol red is present, indicating pH\u00a0< 6) and would dissolve all cells or nuclei if left for too long (> 1 hour on ice).\nNote: The above step is not aimed to fully quench PFA. Addition of BSA greatly reduces loss of cells or nuclei by preventing cells or nuclei from sticking to the side of the tube, and from aggregating when spun down and resuspended in 1% BSA in PBS.\nCentrifuge at 1000 g for 5\u00a0min at 4\u00b0C.\nOptional: If using cells rather than nuclei, centrifuge at 600 g instead.\nRemove supernatant. Resuspend in 1\u00a0mL ice-cold 1% BSA in PBS.\nNote: The above step fully quenches PFA, and was adapted from Thomsen et al. (2016)[href=https://www.wicell.org#bib14].\nMeasure cell or nuclei density with a disposable hemocytometer (INCYTO DHC-N01; manufacturer\u2019s protocol: http://www.incyto.com/shop/item.php?it_id=1482380591[href=http://www.incyto.com/shop/item.php?it_id=1482380591]) and optionally Trypan Blue (ThermoFisher 15250061) if debris is abundant.\nAliquot up to 500 k\u20131\u00a0m cells or nuclei per tube. There is no lower bound in principle; see before you begin[href=https://www.wicell.org#before-you-begin] for details. Each adult mouse brain approximately corresponds to 8 tubes for the cortex and 2 tubes for the hippocampus (2 sides combined). Too many cells (> a few million) may lead to insufficient digestion/ligation and aggregation of cells or nuclei.\nCentrifuge at 1000 g for 5\u00a0min at 4\u00b0C.\nOptional: If using cells rather than nuclei, centrifuge at 600 g instead.\nRemove supernatant. Store indefinitely at \u221280\u00b0C.\nNote: The pellet may be large because of debris. Debris does not affect downstream procedures.\nPause Point: Fixed cells or nuclei can be stored indefinitely at \u221280\u00b0C.\nChromatin conformation capture (3C/Hi-C)Timing: 2\u00a0days (or shorter, if using commercially available kits)\nIn this step, after detergent (0.5% SDS) treatment, chromatin in fixed nuclei is digested with restriction enzyme(s) (e.g., MboI, DpnII, and/or NlaIII), and re-ligated with a DNA ligase to form artificial linkages (i.e., \u201cchromatin contacts\u201d) between genomic loci that are far away along the linear sequence but nearby in the 3D space. Success of digestion and ligation is assessed by extracting DNA from a small portion (5%) of the reaction, and measuring its length distribution.\nThis step was adapted from (Nagano et\u00a0al., 2017[href=https://www.wicell.org#bib7]; Rao et\u00a0al., 2014[href=https://www.wicell.org#bib8]), and can be replaced with other 3C/Hi-C protocol or commercially available 3C/Hi-C kits such as the Arima-SC kit.\nDigestion\nThaw 500 k\u20131\u00a0m fixed cells or nuclei on ice.\nOptional: If starting from fixed cells rather than nuclei, perform the following additional steps:\nPrepare Hi-C Lysis Buffer. Each reaction consumes 1\u00a0mL:\ntable:files/protocols_protocol_744_8.csv\nVortex to mix. Chill on ice.\nFreshly prepare Hi-C Lysis Buffer with Inhibitor. Each reaction consumes 600\u00a0\u03bcL:\ntable:files/protocols_protocol_744_9.csv\nVortex to mix. Chill on ice.\nResuspend cells in 600\u00a0\u03bcL ice-cold Hi-C Lysis Buffer with Inhibitor.\nIncubate on ice for 15\u00a0min, occasionally inverting the tube.\nCentrifuge at 2500 g for 5\u00a0min at 4\u00b0C.\nRemove supernatant. Resuspend in 500\u00a0\u03bcL ice-cold Hi-C Lysis Buffer.\nCentrifuge at 2500 g for 5\u00a0min at 4\u00b0C.\nPrepare 0.5% SDS. Each reaction consumes 50\u00a0\u03bcL. The following recipe (100\u00a0\u03bcL) is sufficient for 1 reaction:\ntable:files/protocols_protocol_744_10.csv\nVortex to mix.\nResuspend cells or nuclei in 50\u00a0\u03bcL 0.5% SDS.\nNote: SDS treatment is necessary to obtain a large number of contacts per cell. Without SDS treatment, the number of contacts per cell may decrease by 2 orders of magnitudes.\nIncubate at 62\u00b0C for 10\u00a0min.Add 145\u00a0\u03bcL water and 25\u00a0\u03bcL 10% Triton X-100 (Sigma 93443) (final concentration: 1.14%). Pipette to mix.\nRotate at 37\u00b0C for 15\u00a0min.\nAdd restriction enzyme(s) and buffer: 25\u00a0\u03bcL 10 X NEBuffer 2 (NEB B7002S) and 20\u00a0\u03bcL 25\u00a0U/\u03bcL MboI (NEB R0147M). Alternatives include: 25\u00a0\u03bcL 10 X CutSmart Buffer and 20\u00a0\u03bcL 10\u00a0U/\u03bcL NlaIII (NEB R0125L), 25\u00a0\u03bcL 10 X NEBuffer DpnII and 10\u00a0\u03bcL 50\u00a0U/\u03bcL DpnII (NEB R0543M), or a combination of multiple enzymes.\nRotate at 37\u00b0C for 1\u201324 h.\nTake 5% (13\u00a0\u03bcL out of the total 265\u00a0\u03bcL) and store at 4\u00b0C as a Digestion Control.\nLigation\nCentrifuge at 1000 g for 5\u00a0min at 4\u00b0C.\nFreshly prepare Ligation Buffer. Each reaction consumes 2 tubes. The following recipe (1 tube) is sufficient for 0.5 reactions:\ntable:files/protocols_protocol_744_11.csv\nVortex to mix.\nRemove supernatant leaving \u223c50\u00a0\u03bcL. Resuspend in 1 tube of Ligation Buffer.\nCentrifuge at 1000 g for 5\u00a0min at 4\u00b0C.\nRemove supernatant leaving \u223c50\u00a0\u03bcL. Resuspend in 1 tube of Ligation Buffer.\nAdd 10\u00a0\u03bcL 1\u00a0U/\u03bcL T4 DNA ligase (ThermoFisher 15224-025). Invert to mix.\nIncubate at 16\u00b0C for 4 h, occasionally inverting to tube.\nTake 5% (50\u00a0\u03bcL out of the total 1\u00a0mL) and store at 4\u00b0C as a Ligation Control.\nCentrifuge at 1000 g for 5\u00a0min at 4\u00b0C.\nRemove supernatant. Store indefinitely at \u221280\u00b0C.\nPause Point: Ligated cells or nuclei, as well as the Digestion Control and Ligation Control, can be stored indefinitely at \u221280\u00b0C.\nQuality control\nCentrifuge the Digestion Control and Ligation Control at 1000 g for 5\u00a0min at 4\u00b0C.\nRemove supernatant from each control. Add 95\u00a0\u03bcL PBS (ThermoFisher 10010023) and 5\u00a0\u03bcL 0.8\u00a0U/\u03bcL Proteinase K (NEB P8107S) (final concentration: 0.04\u00a0U/\u03bcL) per control. Vortex to mix.Lyse the controls by running the following PCR program:\ntable:files/protocols_protocol_744_12.csv\nPurify the controls with PCR purification columns (Zymo D4013; manufacturer\u2019s protocol: https://files.zymoresearch.com/protocols/_d4003t_d4003_d4004_d4013_d4014_dna_clean_concentrator_-5.pdf[href=https://files.zymoresearch.com/protocols/_d4003t_d4003_d4004_d4013_d4014_dna_clean_concentrator_-5.pdf]) using a 1:5 ratio between lysate and DNA Binding Buffer. Elute into 6\u00a0\u03bcL TE (ThermoFisher AM9849) per control.\nNote: For each control (100\u00a0\u03bcL), add 500\u00a0\u03bcL DNA Binding Buffer. 6\u00a0\u03bcL is the minimum elution volume of the column.\nPause Point: Purified controls can be stored indefinitely at \u221220\u00b0C.\nMeasure DNA concentration with a Qubit 1\u00d7 dsDNA HS Assay (ThermoFisher Q33230; manufacturer\u2019s protocol: https://assets.thermofisher.com/TFS-Assets/LSG/manuals/MAN0017455_Qubit_1X_dsDNA_HS_Assay_Kit_UG.pdf[href=https://assets.thermofisher.com/TFS-Assets/LSG/manuals/MAN0017455_Qubit_1X_dsDNA_HS_Assay_Kit_UG.pdf]). Measure DNA lengths with a Bioanalyzer High Sensitivity DNA kit (manufacturer\u2019s protocol: https://www.agilent.com/cs/library/usermanuals/public/HighSensitivity_DNA_KG.pdf.pdf[href=https://www.agilent.com/cs/library/usermanuals/public/HighSensitivity_DNA_KG.pdf.pdf]; or Fragment Analyzer). To evaluate the results, please refer to Expected Outcomes[href=https://www.wicell.org#expected-outcomes] for details, and Figure\u00a01[href=https://www.wicell.org#fig1] for representative Bioanalyzer traces.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/744-Fig1.jpg\nFigure\u00a01. Representative Bioanalyzer traces for quality control of the chromatin conformation capture (3C/Hi-C) step, using a combination of NlaIII and MboI restriction enzymes on mouse cells\n(A) Digestion Control.\n(B) Ligation Control. Both were run on a Bioanalyzer High Sensitivity DNA kit.\nWhole-genome amplification (WGA) by tagmentation\nTiming: 1\u00a0day\nIn this step, single cell or nuclei are sorted into multi-well plates, lysed, and amplified with transposition (Tn5) and PCR.\nThe procedure below describes amplification with our implementation of the Illumina Nextera chemistry. If higher sensitivity is required\u2014for example, when the number of contacts obtained is insufficient for distinguishing desired cell types or for 3D modeling with desired spatial resolution, please follow the procedure of our multiplex end-tagging amplification (META) method (Tan et\u00a0al., 2018[href=https://www.wicell.org#bib12]). META can detect 2 times as many contacts as Nextera, but involves custom Tn5 transposomes and 2 additional PCR steps.\nFor first-time users, we recommend starting with 1 96-well plate. Experienced users may amplify 4\u20138 plates at a time, depending on the number of available PCR machines.Nextera Index Primers listed in the key resources table[href=https://www.wicell.org#key-resources-table] allow the pooling of up to 384 cells or nuclei to be sequenced on the same lane. Other index designs may allow more cells or nuclei to be pooled (e.g., 10-bp dual Nextera indices from IDT allows 3,840).\nFlow sorting and lysis\nThaw a tube of ligated cells or nuclei on ice.\nFreshly prepare 300\u00a0\u03bcM DAPI. Each reaction consumes 1\u00a0\u03bcL. The following recipe is sufficient for 100 reactions:\ntable:files/protocols_protocol_744_13.csv\nVortex to mix.\nResuspend cells or nuclei in 1\u00a0mL PBS (ThermoFisher 10010023). Chill on ice.\nAdd 1\u00a0\u03bcL 300\u00a0\u03bcM DAPI (final concentration: 300\u00a0nM). Pipette to mix. Transfer to a flow sorting tube and chill on ice.\nFreshly prepare Dip-C Lysis Buffer. Each cell consumes 2\u00a0\u03bcL. The following recipe (1\u00a0mL) is sufficient for 4 96-well plates:\ntable:files/protocols_protocol_744_14.csv\nVortex to mix. Aliquot to 80\u00a0\u03bcL in 12-strip tubes.\nNote: Addition of Carrier ssDNA reduces loss of input DNA materials by preventing genomic DNA from sticking to the side of the tube, especially in PCR tubes that are not low-retention.\nNote: Volume (0.25\u00a0\u03bcL) of 60\u00a0mg/mL Qiagen Protease does not need to be exact. If desired, however, pipetting accuracy can be increased by freshly diluting 60\u00a0mg/mL Qiagen Protease prior to addition (e.g., 1:100 dilution followed by the addition of 25\u00a0\u03bcL instead of 0.25\u00a0\u03bcL).\nOptional: Before the addition of 60\u00a0mg/mL Qiagen Protease, Dip-C Lysis Buffer can be stored indefinitely at \u221220\u00b0C.\nAdd 2\u00a0\u03bcL Dip-C Lysis Buffer per well to a DNA low-bind 96-well plate (semi-skirted: Eppendorf 0030129504; or skirted: Eppendorf 0030129512, depending on the FACS and PCR machines).\nNote: To maximize speed, use a 12-channel pipette to add solution to each well in the above and all subsequent steps.Seal with film (Bio-Rad MSB1001) and a roller (Bio-Rad MSR0001).\nPause Point: Dip-C Lysis Buffer can be stored on ice for a few hours before sorting.\nSort a single diploid cell or nucleus per well, based on DAPI signal (linear scale). Seal tightly to avoid evaporation. Please refer to Expected Outcomes[href=https://www.wicell.org#expected-outcomes] for details, and Figure\u00a02[href=https://www.wicell.org#fig2] for representative flow cytometer diagrams and gates.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/744-Fig2.jpg\nFigure\u00a02. Representative flow cytometry diagrams with 2 roughly equivalent gating strategies\nThe minor fraction of particles with double, triple, or even higher DAPI signals (\u201cV450-A\u201d) were aggregates from the Chromatin Conformation Capture step. Both were run on a BD FACSAria flow sorter. The 2 gating strategies arose from personal preferences of different flow cytometer operators, and do not affect the results. Note that we primarily study cells in the G0/G1 phase of the cell cycle; the corresponding gate (e.g., \u201cG1\u201d in (B)) should be adjusted when studying other phases of the cell cycle.\nCritical: Once cells or nuclei are sorted into Dip-C Lysis Buffer, avoid cross contamination of liquid between wells.\nCentrifuge at 1000 g for 1\u00a0min.\nPause Point: Sorted cells or nuclei in Dip-C Lysis Buffer can be stored on ice for a few hours before lysis.\nLyse the cells by running the following PCR program:\ntable:files/protocols_protocol_744_15.csv\nStore at \u221280\u00b0C.\nPause Point: Lysed cells or nuclei can be stored for a few months at \u221280\u00b0C.\nNote: The film (Bio-Rad MSB1001) may peel over time at \u221280\u00b0C. This can be avoided by changing to cold-resistant film (Bio-Rad MSF1001) after lysis.\nOptional: For longer-term storage, cells or nuclei can be sorted into empty 96-well plates rather than Dip-C Lysis Buffer, and stored indefinitely at \u221280\u00b0C.\nTransposition\nPrepare Transposition Buffer. Each well consumes 8\u00a0\u03bcL. The following recipe (1\u00a0mL) is sufficient for 1 96-well plate:table:files/protocols_protocol_744_16.csv\nVortex to mix.\nOptional: Transposition Buffer can be stored indefinitely at \u221220\u00b0C.\nFreshly prepare Transposition Mix. Each well consumes 8\u00a0\u03bcL. The following recipe is sufficient for 1 96-well plate (with 10% overhead):\ntable:files/protocols_protocol_744_17.csv\nPipette to mix. Aliquot to 69\u00a0\u03bcL in 12-strip tubes.\nCritical: The amount of Tn5 transposome per well (\u223c0.015\u00a0\u03bcL above) determines the length of the final sequencing library. It should be titrated in a pilot experiment with a concentration gradient of Tn5 transposome to obtain an average length of \u223c500\u00a0bp. Please refer to troubleshooting 2[href=https://www.wicell.org#troubleshooting] for details, and Figure\u00a03[href=https://www.wicell.org#fig3] for representative Bioanalyzer traces.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/744-Fig3.jpg\nFigure\u00a03. Representative Bioanalyzer traces for titration of Tn5 transposome concentration during the whole-genome amplification (WGA) by tagmentation step\n(A) Coarse titration of Illumina TDE1 on purified HeLa gDNA. Range for further titration is indicated by a dashed green box. Note that gDNA only gives approximate results because transposition is slightly different between gDNA and lysate.\n(B) Fine titration of Tn5 transposome from a different vendor (TTE Mix V50 from Vazyme TD501) on nuclei lysate (see troubleshooting 2[href=https://www.wicell.org#troubleshooting] for details). Range suitable for sequencing is indicated by a dashed green box (\"acceptable\u201d), and the optimal concentration shown by a solid green box (\u201cbest\u201d). All were run on a Bioanalyzer High Sensitivity DNA kit.\nNote: Nextera Tn5 transposome is also available from Vazyme (TTE Mix V50 of TD501) and from Diagenode (C01070012; not tested). We primarily use Vazyme.\nOptional: For first-time users, a Positive Control well can be set up as 2\u00a0\u03bcL of a 5 pg/\u03bcL dilution of any genomic DNA (e.g., diluting 100\u00a0ng/\u03bcL HeLa gDNA (NEB N4006S) 1:20,000 in water). A Negative Control well can be set up as 2\u00a0\u03bcL water. Please refer to troubleshooting 1[href=https://www.wicell.org#troubleshooting] for details.Add 8\u00a0\u03bcL Transposition Mix per well (total volume: 10\u00a0\u03bcL), avoiding touching the liquid (i.e., pipette onto the side, rather than the bottom, of the well). Vortex and spin down.\nNote: Before PCR amplification, we typically avoid touching the liquid with pipette tips to minimize loss of input DNA materials. In particular, if pipette tips touch the liquid, genomic DNA may stick to the tips and get lost when tips are withdrawn from the liquid. However, the efficacy of this precaution has not been tested systematically; touching the liquid may be acceptable if the resulting data is satisfactory.\nTranspose the genome by running the following PCR program:\ntable:files/protocols_protocol_744_18.csv\nStopping\nFreshly prepare Stop Mix. Each well consumes 2\u00a0\u03bcL. The following recipe (1\u00a0mL) is sufficient for 4 96-well plates:\ntable:files/protocols_protocol_744_19.csv\nVortex to mix. Aliquot to 80\u00a0\u03bcL in 12-strip tubes.\nOptional: Before the addition of 60\u00a0mg/mL Qiagen Protease, Stop Mix can be stored indefinitely at \u221220\u00b0C.\nNote: Addition of 10% Triton X-100 is for ease of pipetting.\nAdd 2\u00a0\u03bcL Stop Mix per well (total volume: 12\u00a0\u03bcL per well), avoiding touching the liquid (i.e., pipette onto the side, rather than the bottom, of the well). Vortex and spin down.\nStop transposition by running the following PCR program:\ntable:files/protocols_protocol_744_20.csv\nPause Point: Stopped reactions can be stored on ice for a few hours before amplification.\nAmplification\nFreshly prepare PCR Mix. Each well consumes 11\u00a0\u03bcL. The following recipe (1.178\u00a0mL) is sufficient for 1 96-well plate (with 10% overhead):\ntable:files/protocols_protocol_744_21.csv\nVortex to mix. Aliquot to 97\u00a0\u03bcL in 12-strip tubes.Add 1\u00a0\u03bcL 12.5\u00a0\u03bcM Nextera i5 Primer and 1\u00a0\u03bcL 12.5\u00a0\u03bcM Nextera i7 Primer per well (total volume: 14\u00a0\u03bcL per well; final concentration during PCR: 500\u00a0nM each), avoiding touching the liquid (i.e.,\u00a0pipette onto the side, rather than the bottom, of the well). Arrange the indices so no cells share the same index on each sequencing run; see Figure\u00a04[href=https://www.wicell.org#fig4] for an example arrangement.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/744-Fig4.jpg\nFigure\u00a04. Example arrangement of Nextera i7 and i5 indices on a 96-well plate\nOptional: For simpler pipetting, 12.5\u00a0\u03bcM Nextera i5 Primer and 12.5\u00a0\u03bcM Nextera i7 Primer can be 1:1 pre-mixed into a 96-well plate (6.25\u00a0\u03bcM each), and stored indefinitely at \u221220\u00b0C.\nAdd 11\u00a0\u03bcL PCR Mix per well (total volume: 25\u00a0\u03bcL per well), avoiding touching the liquid (i.e., pipette onto the side, rather than the bottom, of the well). Vortex and spin down.\nAmplify the genome by running the following PCR program:\ntable:files/protocols_protocol_744_22.csv\nNote: The above PCR program consists of 14 cycles, which is suitable for the input DNA amount of human and mouse samples (\u223c6 pg per cell or nucleus, given a diploid genome size of \u223c6 Gb). The number of cycles may need adjustment if an organism has a very different genome size.\nPause Point: PCR reactions can be stored on ice for a few hours, or indefinitely at \u221220\u00b0C.\nPurification and size selection\nPool all wells from a 96-well plate.\nPurify with PCR purification columns (Zymo D4013) using a 1:5 ratio between PCR reaction and DNA Binding Buffer (Zymo D4004-1-L to order extra). Elute into 400\u00a0\u03bcL TE (ThermoFisher AM9849) per plate.\nCritical: Avoid cross contamination of liquid between plates that use overlapping indices.Note: Each 96-well plate (total volume: 2.4\u00a0mL) can be pooled directly into 12\u00a0mL DNA Binding Buffer and vortexed (total volume: 14.4\u00a0mL). Because each PCR purification column can only load 800\u00a0\u03bcL at a time, we typically use 6 columns per plate to save time; each column only needs to be loaded 3 times. After loading and washing, elute each column into 66.7\u00a0\u03bcL TE (ThermoFisher AM9849) and pool (total volume: 400\u00a0\u03bcL).\nOptional: For the Positive Control and Negative Control, each well (25\u00a0\u03bcL) is mixed with 125\u00a0\u03bcL DNA Binding Buffer. Elute each into 6\u00a0\u03bcL TE (ThermoFisher AM9849).\nPause Point: Purified libraries can be stored indefinitely at \u221220\u00b0C.\nMeasure DNA concentration with a Qubit 1\u00d7 dsDNA HS Assay. Measure DNA lengths with a Bioanalyzer High Sensitivity DNA kit (or Fragment Analyzer). To evaluate the results, please refer to Expected Outcomes[href=https://www.wicell.org#expected-outcomes] for details, and Figure\u00a03[href=https://www.wicell.org#fig3] and 5[href=https://www.wicell.org#fig5]A for representative Bioanalyzer traces.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/744-Fig5.jpg\nFigure\u00a05. Representative Bioanalyzer traces before and after size selection of a sequencing library\n(A) Before size selection.\n(B) After size selection with 0.7 X SPRISelect beads.\n(C) Similar to (B) but with 0.6 X beads. All were run on a Bioanalyzer High Sensitivity DNA kit.\nRemove short fragments from half of the library (200\u00a0\u03bcL) with 0.7 X (140\u00a0\u03bcL) or 0.6 X (120\u00a0\u03bcL) SPRIselect beads (Beckman Coulter B23317; manufacturer\u2019s protocol: https://www.beckman.com/techdocs/B24965AA/wsr-128718[href=https://www.beckman.com/techdocs/B24965AA/wsr-128718]). Elute into 50\u00a0\u03bcL TE (ThermoFisher AM9849).Note: The remaining half (\u223c200\u00a0\u03bcL) serves as a back-up in case size selection or sequencing fails. Depending on sample submission requirements (i.e., minimum DNA amount) of the sequencing provider, the above purification and size selection steps can be scaled down (e.g., to half of the volume), with any extra liquid stored indefinitely at \u221220\u00b0C in the form of a mixture of PCR reactions and DNA Binding Buffer.\nPause Point: Final libraries can be stored indefinitely at \u221220\u00b0C.\nMeasure DNA concentration with a Qubit 1\u00d7 dsDNA HS Assay. Measure DNA lengths with a Bioanalyzer High Sensitivity DNA kit (or Fragment Analyzer). To evaluate the results, please refer to Expected Outcomes[href=https://www.wicell.org#expected-outcomes] for details and Figure\u00a05[href=https://www.wicell.org#fig5] for representative Bioanalyzer traces.\nSequencing and data analysis\nSequence on an Illumina sequencer (e.g., HiSeq or NovaSeq) with paired-end 150-bp reads and dual 8-bp indices.\nNote: To saturate the sequencing library, we sequence each cell with 3\u20136\u00a0m read pairs.\nNote: If 3D reconstruction of diploid genome structures (by reading heterozygous SNPs) is not required, shorter read lengths (e.g., paired-end 75\u00a0bp) can be used.\nOptional: Before deep sequencing, the presence and prevalence of chromatin contacts can be tested at almost no cost with as few as 1,000 reads per plate (e.g., on a MiSeq), which allows the calculation of the \u201ccontact rate\u201d CR and the \u201ccontact density\u201d CD; see quantification and statistical analysis[href=https://www.wicell.org#quantification-and-statistical-analysis] for details.\nAnalyze data with the dip-c (https://github.com/tanlongzhi/dip-c[href=https://github.com/tanlongzhi/dip-c]) package.", "This protocol provides guidance on using the VESPER program to align two experimentally determined cryo-EM maps or to fit an atomic structure either determined by an experimental method or modeled using a computational tool into a cryo-EM map. The input required for this protocol is the reference experimental cryo-EM map and either a simulated cryo-EM map of an atomic structure or an experimental EM map. Transformations of the top 10 or user-specified number of superimpositions along with transformed models are outputted.\nNecessary Resources\nHardware\nAny computer with Linux or macOS operating system, at least i5 processor, and 16 GB RAM\nSoftware\nPython version 3.8.10 or higher (https://www.python.org/downloads/[href=https://www.python.org/downloads/])\nNumpy version 1.21.0 or higher (https://numpy.org/install/[href=https://numpy.org/install/])\nSciPy version 1.7.0 or higher (https://scipy.org/install/[href=https://scipy.org/install/])\nFFTW version 3.3.10 (http://www.fftw.org/download.html[href=http://www.fftw.org/download.html])\nPyMOL version 2.4.1 or higher (optional), which is used for visualization, (https://pymol.org/2/[href=https://pymol.org/2/])\nGCC compiler version 9.4.0 or higher (https://gcc.gnu.org/[href=https://gcc.gnu.org/])\nFiles\nTwo cryo-EM maps in the format of MRC or CCP4\nInstall VESPER program\n1. Download the VESPER code from the VESPER GitHub page by opening the command line window and type:\n         \ngit clone https://github.com/kiharalab/VESPER\n2. In the command line window, change the working directory to the directory containing VESPER code as follows:\n         \ncd /your_path_to_VESPER/VESPER_code/\n3. Compile VESPER source code to generate an executable version of the code called VESPER, then move it to VESPER main directory:\n         \nmake\ncp VESPER ../\nRun VESPER program\n4. Prepare input files by downloading a cryo-EM map from EMDB (https://www.emdataresource.org/[href=https://www.emdataresource.org/]) and a protein structure from PDB (https://www.rcsb.org/[href=https://www.rcsb.org/]).As an example, we used the structure of the Hsp90/Cdc37/Cdk4 complex, which has EMD-3342 and a fitted PDB entry, 5FWM. EMD-3342 is determined at resolution 8 \u00c5\u00a0and has the author-recommended contour level of 0.015. To show the ability of VESPER to find the best fitting for PDB entry 5FWM in EMD-3342, we first randomly rotated and shifted the atomic structure, 5FWM.\n5. Generate a simulated cryo-EM map from the atomic structure using the molmap function in Chimera (https://www.cgl.ucsf.edu/chimera/[href=https://www.cgl.ucsf.edu/chimera/]) as shown below, or using any other software.\nOpen Chimera and run the following commands on the Chimera command line:\n         \nopen path_to_PDB_file/file_name.pdb (open PDB structure)\nopen path_to_EM_map_file/map_file.mrc (open the EM map to which the PDB structure is fitted into)\nmolmap #0 [map_resolution] onGrid #1 (generate a simulated map using the experimental map properties)\nvolume #2 save path_to_save_map/file_name.mrc (save the generated EM map)\n6. Open the command-line window to run the VESPER command and specify the different parameters as follows:\nVESPER -a [MAP1.mrc] -b [MAP2.mrc] (other parameters) > [VESPER_output_filename]\n-a: Path to the reference cryo-EM map\n-b: Path to the target cryo-EM map\n-t: Density threshold of the reference map, default = 0.00\n-T: Density threshold of the target map, default = 0.00\n-s: Sampling grid space for resampling the EM maps, default = 7.0 \u00c5\n-A: Sampling angle interval for defining a set of rotations, i.e., 360 \u00f7 angle interval, default = 30\u00b0\n-c: Number of CPU cores used for running VESPER in parallel, default = 2\n-g: Bandwidth of the gaussian filter, default = 16.0, and sigma = 0.5 \u00d7 (float number)\n-N: Refine top [int] models, default = 10\n-S: Show top models in PDB format, default = false\n-V: Vector product mode, default = true\n-L: Overlap mode, default = false\n-C: Cross-correlation coefficient mode, default = false-P: Pearson's correlation coefficient mode, default = false\n-F: Laplacian filtering mode, default = false\n-E: Evaluation mode of the current position, default = false\nVESPER output is written to a file named VESPER_output_filename, which includes the top 10 or user-specified number of transformations applied on the target EM map to align it with the reference EM map, along with several scores evaluating the alignments. Also, the output file has the vector information of the top models. Each vector is represented by two atoms, C\u03b1 for the start position and C\u03b2 for the end position.\nFor our example, we used 3 \u00c5 and 10\u00b0 for voxel spacing and angle interval, respectively. Regarding density contour level, we used the author-recommended contour level for experimental map EMD-3342 and 0.2 for the simulated EM map of transformed 5FWM. Also, we used 20 CPU cores, which took about 20 min to complete the computation.\n         \ntable:\n\ufeff0\nVESPER -a emd_3342.map -b molmap_5fwm_transformed.mrc -t 0.015 -T 0.2 -s 3 -A 10 -c 20 -S true > vesper_result_3_10.txt\n7. To transform a target density map according to the rotation and translation of each of the top alignments in VESPER output, run the following command:\npython transform_em_map.py [parameters]\n-i1 or --input1: Name of the reference EM map file\n-i2 or --input2: Name of the target EM map file\n-t: Name of the result file from VESPER\n-odir (optional): Directory to store the generated transformed target EM map files. If not specified, the transformed target maps would be written to the current directory.\ntable:\n\ufeff0\npython transform_em_map.py -i1 emd_3342.map -i2 molmap_5fwm_transformed.mrc -t vesper_result_3_10.txt\nThe names of output files would have the following format: target_transform_model_#.mrc, in which # specifies the model number starting from 1.\nCalculating normalized Z-score for top models in VESPER output file8. To calculate the normalized Z-score of the top models in the VESPER output file, run the cluster_score.py script as follows:\npython cluster_score.py -i [VESPER_output_filename] -c [Clustering cutoff] -o [Output_filename]\n-i: Name of the input file\n-c: Clustering cutoff for Z-scores which is used as follows: [Clustering cutoff \u00d7 (Maximum DOT score \u2013 Minimum DOT score)] and it ranges from 0 to 1, default = 0.2\n-o: Output filename (optional): Name of the output file to store the normalized Z-scores of the models. If not specified, the output file would have the same name as the input filename followed by .normzscore\ntable:\n\ufeff0\npython cluster_score.py -i vesper_result_3_10.txt -o normalized_Z_score_emd3342.txt\nThe output file will contain the Z-score of each model in the input file, one line for each model as shown below:\nVisualizing VESPER results\n\u00a0\n         \ntable:\n\ufeff0,1\nNormalized z-score for top 10 models:,Normalized z-score for top 10 models:\n#0,16.768505095477142\n#1,16.7211460976927\n#2,16.677474206555694\n#3,16.49780033833288\n#4,15.465432251303666\n#5,15.44975116092615\n#6,15.382853401143898\n#7,15.26661155761643\n#8,15.138023713287225\n#9,15.070875549608642\n9. To visualize the reference EM map (Fig. 3A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0003]), open PyMOL and run the following commands on the PyMOL command line:\n         \nbg_color white (this command changes the background color from black to white)\nset normalize_ccp4_maps, 0\nload xxxx.mrc (replace xxxx with the EM map file name)\nisosurface xxxx_isosurface, xxxx, reference_contour_level (replace reference_contour_level with the author-recommended contour level of the reference map in EMDB)\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/dc17148c-8007-4d0e-981d-568b039838ca/cpz1494-fig-0003-m.jpg</p>\nFigure 3Results of the VESPER program on EMD-3342 and simulated map of PDB entry 5FWM. (A) Experimental map EMD-3342, (B) EMD-3342 in gray and the vector representation of the top model by VESPER of protein complex PDB: 5FWM. The top model is shown as a set of spheres of different colors, where blue means that the matched vectors of the two EM maps are aligned well, green means no alignment, and red represents the alignment in opposite directions. (C) EMD-3342 in gray and the top model of VESPER in cyan, which has an RMSD of 3.44 \u00c5.\n10. To visualize the PDB file containing vector representation of top models by VESPER (Fig. 3B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0003]), open PyMOL and run the following commands on the PyMOL command line:\n         \nload xxxx.pdb, discrete =1 (replace xxxx by the file name you chose earlier in step 3)\nset transparency, 0.4\nhide cartoon, xxxx\nshow spheres, xxxx\nspectrum b, rainbow_rev, xxxx\nThe PDB structures of top models will be shown as a set of spheres representing vectors, and spheres are colored based on their DOT score.\n11. To visualize any of the MRC files of the top superimposed models generated in step 4 (Fig. 3C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0003]), open PyMOL and run the following commands in the PyMOL command line:\n         \nload target_transform_model_#.mrc (Replace # with the model number you want to visualize)\nisosurface model#_isosurface, target_transform_model_#, target_contour_level (replace target_contour_level by the contour level used for the target map)This protocol provides guidance on using the VESPER web server for fitting an atomic model in a cryo-EM map or aligning two cryo-EM maps in a fast manner. Users can align two EM maps in a few steps without the need to download or install any files. Also, users can specify only a few parameter values while the rest of the parameters would have their default values. The Input required for this protocol is two cryo-EM maps or their EMDB IDs. An e-mail of the result files will be sent to the user after the computation is completed.\nNecessary Resources\nHardware\nAny up-to-date computer with internet access\nSoftware\nAn up-to-date web browser such as Google Chrome (https://www.google.com/chrome/[href=https://www.google.com/chrome/]) or Mozilla Firefox (https://www.mozilla.org/en-US/firefox/[href=https://www.mozilla.org/en-US/firefox/])\nPyMOL (optional), which is used for visualization (https://pymol.org/2/[href=https://pymol.org/2/])\nFiles\nBesides using the search boxes to enter specific cryo-EM map IDs from EMDB, users can upload their cryo-EM maps. The density maps to be uploaded are in the format of MRC or CCP4.\nSubmit a job to the VESPER web server\n1. Open the web browser and type the URL https://kiharalab.org/em-surfer/vesper.php[href=https://kiharalab.org/em-surfer/vesper.php]. Figure 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0004] shows the VESPER web page, which contains a brief description of VESPER and how it works, three main boxes for specifying parameters, and the submit and reset buttons.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/00231d16-af1a-4e1a-83f6-43df592d4c6c/cpz1494-fig-0004-m.jpg</p>\nFigure 4\nScreenshot of the main page of the VESPER web server.\n2. In the first box, Step 1 (Search parameters), specify voxel spacing in Angstroms to be applied on density maps. Three voxel spacing options are available, which are 5, 7, and 10 \u00c5. The default voxel spacing is 7 \u00c5. The second parameter to specify is the angular search degree. Choose one of the four angular search intervals, which are 20\u00b0, 30\u00b0, 60\u00b0, and 90\u00b0 degrees.For the example in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-prot-0001], the Hsp90/Cdc37/Cdk4 complex which has EMD-3342 and a fitted 5FWM, we used 5 \u00c5 and 20\u00b0 for voxel spacing and angle interval, respectively.\n3. The second box, Step 2 (Query maps), is for specifying cryo-EM maps and their density contour levels. You can either enter the EMDB ID of both reference and target maps or upload your density maps. For each map, specify the contour level to be used for that map.\nHere, we uploaded emd-3342 and the simulated map of transformed 5FWM, as specified in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-prot-0001]. For contour level, we used 0.015 and 0.2 for emd-3342 and the simulated map of randomly transformed 5FWM, respectively.\n4. In the last box, Step 3 (Email), enter a valid e-mail address, to which you want to receive VESPER results.\n5. Once all parameters are entered, click on the submit button, which will show the following message: \u201cYour request has been submitted! Once the result is ready, the result files will be sent to the e-mail specified.\u201d After the computation is completed, you will receive an e-mail from [email\u00a0protected][href=https://currentprotocols.onlinelibrary.wiley.com/cdn-cgi/l/email-protection] titled \u201cVESPER Calculation Result\u201d with the results attached. The size of density maps along with voxel and angle intervals affect the amount of computation time needed. Small density maps will take a couple of minutes to be processed by VESPER.6. The VESPER result e-mail will have a link to download archived result files, which include the following: an EM map of the reference structure, one PDB file that contains vectors in the target map, 10 MRC files for each of the top 10 models of the target EM map named target_transform_model_#.mrc, one text file contains normalized Z-scores of the top 10 alignments, and the VESPER_README.pdf file containing descriptions of result files and directions on how to visualize them.\nVisualizing VESPER results\n7. To visualize VESPER output files, follow steps 9, 10, and 11 of Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-prot-0001]. The only difference in the commands is in the file names. Use Reference.mrc for the reference map file in step 9 and customMapResult.pdb for the PDB file containing vector representation of top models in step 10.This protocol provides guidance on using MAINMAST for building a protein main chain directly from a cryo-EM map of resolution \u223c4\u20135 \u00c5 or better. The required input files for this protocol are the reference cryo-EM map, protein chain sequence, and predicted secondary structure from protein sequence. The output is a C\u03b1 model of the protein main chain, from which a full-atom model could be constructed then refined.\nNecessary Resources\nHardware\nAny computer with Linux or macOS operating system, at least an i5 processor, and 16 GB RAM\nSoftware\nFortran version 9.4.0 or higher (https://fortran-lang.org/[href=https://fortran-lang.org/])\nMap2map procedure from SITUS package version 3.0 or higher (http://situs.biomachina.org/[href=http://situs.biomachina.org/])\nSPIDER2 (https://github.com/yuedongyang/SPIDER2[href=https://github.com/yuedongyang/SPIDER2])\nPULCHRA version 3.04 or higher (https://www.pirx.com/pulchra/[href=https://www.pirx.com/pulchra/])\nPyMOL version 2.4.1 or higher (optional) for visualization: (https://pymol.org/2/[href=https://pymol.org/2/])\nFiles\nCryo-EM map of up to the size 150\u00d7150\u00d7150 (in the current setting, can be changed in the code)\nFASTA file which contains protein sequence\nSPD3 file which contains the secondary structure predicted by SPIDER2 (Fig. 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0005])\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/aae22dc2-bcfe-4824-9a9a-50b98a69a80d/cpz1494-fig-0005-m.jpg</p>\nFigure 5\nScreenshot of the SPD3 file generated by SPIDER2 for Chain A of PDB: 4CI0.\nInstall MAINMAST program\n1. Download MAINMAST source code from https://kiharalab.org/mainmast/Downloads.html[href=https://kiharalab.org/mainmast/Downloads.html].\n2. Open the command-line window and change the working directory to the directory containing the archived file. Then, unarchive MAINMAST.tgz by typing:\ntar zxvf MAINMAST.tgz\nA new directory named MAINMAST/ will be generated.\n3. Change the directory to MAINMAST by typing:\n         \ncd MAINMAST\n4. Compile the source code of the two main programs of MAINMAST by typing the following commands in the command-line command:\n         \ngfortran MAINMAST.f -O3 -fbounds-check -o MAINMAST -mcmodel = medium\ngfortran ThreadCA.f -O3 -fbounds-check -o ThreadCA -mcmodel = medium\nThese commands will generate two executable programs, named MAINMAST and ThreadCA.\nRun MAINMAST program\n5. Prepare the following input files:a.If the density map is in MRC format, convert it to SITUS format by running the map2map procedure from SITUS packages as follows:\n               \necho 2|map2map density_map_name.mrc density_map_name.situs\nb.Predict protein secondary structures from protein amino acid sequence using SPIDER2 by running the following command:\n               \nrun_local.sh protein_seq_filename.seq\nAs an example, we used chain A of F420-reducing [NiFe] hydrogenase Frh, which has an EMD-2513 of resolution 3.36 \u00c5 and a fitted PDB structure with ID 4CI0. Chain A was manually segmented from the density map using Chimera's \u201czone tool.\u201d\n6. Run the first part of the MAINMAST program, which is called MAINMAST, to trace the protein main chain from the density map. The MAINMAST command identifies local dense points (LDPs) in the density map, which then are connected by a Minimum Spanning Tree (MST). After that, the MST is refined by a tabu search algorithm. The output is a PDB file representing each LDP in the longest path of the MST as a C\u03b1 atom.\n         \nMAINMAST -m [density map file in situs format] (options) > path.pdb\nOptions in version 2.0:\n         \n-Tree: Show MST mode\n-Graph: Show graph mode\nParameters for the mean shift clustering algorithm:\n         \n-gw: Bandwidth of the gaussian filter; default = 2.0, sigma = 0.5 \u00d7 [float]\n-Dkeep: Keep edge where distance < [float], default = 0.5\n-t: Threshold of density values, default = 0.0\n-allow: Max shift distance < [float], default = 10.0\n-filter: Filter of representative points, default = 0.1\n-merge: After the mean shift clustering, merge if distance < [float], default = 0.5\nParameters in Tabu-search:\n         \n-Nround: Number of iterations, default = 5000\n-Nnb: Number of neighbors, default = 30\n-Ntb: Size of tabu-list, default = 100\n-Rlocal: Radius of Local MST, default = 10-Const: Constraint of total length of edge, default = 1.01, Total(Tree) < [float] \u00d7 Total(MST)\nFor our example, we used default parameter values except for density contour level and Dkeep, which determines the edge weight threshold used in the mean shift clustering algorithm, for which we used 0.045 (author-recommended contour level \u00d7 0.5), and 1.5, respectively. The output of the first part is shown in Figure 6A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0006], which is a C\u03b1 model representing the LDPs on the longest path of the MST. This model is used as an input for the next step.\n\u00a0\n         \ntable:\n\ufeff0\nMAINMAST -i zoned_A.mrc -c 20 -t 0.045 -k 1.5 -R 10 > path.pdb\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/8646bc1d-87c6-47b7-a790-baca7dfe1cf8/cpz1494-fig-0006-m.jpg</p>\nFigure 6\nMAINMAST output of EMD-2513, chain A. (A) The longest path of the MST i.e., path.pdb, (B) the LDPs generated by the mean shift clustering algorithm, (C) MST generated using tree mode in MAINMAST, (D) all edges using graph mode in MAINMAST.\n7. Run the second part of the MAINMAST program, which is called ThreadCA, to map the amino acid sequence on the longest path of the MST, as shown below. You can determine the direction of threading the protein amino acid sequence on the longest path in the refined tree graph from MAINMAST. The output of ThreadCA is a C\u03b1 model of the predicted protein chain.\n         \nThreadCA -i [output file from MAINMAST] -a [20AA.param] -spd [*.spd3] (options)\nOptions in version 1.0:\n         \n-i: Result file of MAINMAST\n-a: 20AA.param\n-spd: Resulted file of SPIDER2\n-fw: Filter width, default = 1.0\n-Ab: Average length of CA-CA Bond, default = 3.5\n-Wb: Weight of Bond score, default = 0.9\n-r: Reverse mode, reverse protein main chain orderFor chain A, we assigned the amino acid sequence in reverse order and used 1.3 and 3.4 for the parameters fw (filter width) and Ab (average length of C-C\u00a0bonds), respectively. Default values were used for the other parameters. The output of ThreadCA, which is the C\u03b1 model of chain A is shown in Figure 7A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0007].\n\u00a0\n         \ntable:\n\ufeff0\nThreadCA -i path.pdb -a 20AA.param -spd 4CI0_A.spd3 -fw 1.3 -Ab 3.4 -Wb 0.9 -r >A_CA_reversed.pdb\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/76f0ede3-4511-4b28-8952-4d0366168a63/cpz1494-fig-0007-m.jpg</p>\nFigure 7\nThe modeled protein structure by MAINMAST from EMD-2513, chain A. (A) The native structure of 4CI0, chain A in yellow and the C\u03b1 model by MAINMAST in cyan, (B) the refined full-atom modeled protein by MAINMAST in magenta and the native structure in yellow, (C) the native and modeled protein structures fitted to chain A segmented map of EMD-2513.\n8. Run PULCHRA on the output of ThreadCA, C\u03b1 model, to generate a full atom model. The full atom model can be then refined using any refinement methods such as Rosetta Relax (https://www.rosettacommons.org/[href=https://www.rosettacommons.org/]) or MDFF (https://www.ks.uiuc.edu/Research/mdff/[href=https://www.ks.uiuc.edu/Research/mdff/]).\nVisualizing MAINMAST results\n9. To visualize a cryo-EM map, open PyMOL and run the following commands in the PyMOL command line:\n         \nbg_color white\nset normalize_ccp4_maps, 0\nload xxxx (replace xxxx with map_file_name)\nisosurface xxxx_isosurface, emd_xxxx, reference_contour_level (replace reference_contour_level by the contour level used for the map)\n10. To visualize the longest path of the MST generated by MAINMAST (Fig. 6A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0006]), run the bondmk.pl script, which takes as input the path PDB file generated in step 6 and outputs a PyMOL session file. Then, open the PyMOL session file of bondtree .pl using PyMOL:\n         \nbondmk.pl path.pdb > path_session.txt\npymol -u path_session.txt\n11. To visualize the LDPs of the MST built in the density map (Fig. 6B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0006]):\n         \nRun MAINMAST in tree mode and save the output to the tree.pdb fileOpen PyMOL and run the following commands in the PyMOL command line:\n               \nload tree.pdb\nset transparency, 0.4\nhide cartoon, tree\nshow spheres, tree\nset sphere_scale, 0.4, tree\nspectrum b, selection = SEL, tree (color a molecule based on B-Factors)\n12. To visualize the MST generated by MAINMAST using the tree mode (Fig. 6C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0006]) or to visualize all the possible connections, i.e., edges, on the EM map generated by MAINMAST using the graph mode (Fig. 6D[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.494#cpz1494-fig-0006]), run the bondtree.pl script, which takes as input the output PDB file from MAINMAST and generates a PyMOL session file. Then, open the output file of bondtree.pl using PyMOL:\n         \nbondtree.pl mainmast_output.pdb > pymol_session.txt\npymol -u pymol_session.txt", "Step-by-step method details\nStep-by-step method details\nMetabolic labeling\nTiming: 1\u20132\u00a0weeks\nTo allow quantitative comparison of cell proteomes between the two studied conditions, cell populations are expanded in the presence of amino acids containing different stable isotopes until most (\u226595%) of their proteins have incorporated them.\nHarvest cells of the two lines being analyzed and split them equally in two tubes.\nPellet cells by centrifugation at 500 g for 3\u00a0min.\nResuspend half of the control cell pellet with light SILAC medium, and the other half with heavy SILAC medium. Apply the same procedure for the test cell pellet. This will result in a total of 4 samples (i.e., cL, cH, tL, tH) that correspond to two replicates of the experiment with reciprocal labeling (i.e., cLtH and cHtL).\nAlternatives: It is highly recommended to perform this experiment with reciprocal labelling, i.e. have both control and test cell lines separately labelled with light and heavy isotopes, so that they are mixed in two different samples in which the only difference is the order of labelling. However, for preliminary experiments, and given the cost of amino acids containing heavy isotopes, it is possible to use a single labelling, with faster growing lines (typically control cells) being labelled with heavy medium and the other line with light medium. This way, the heavy medium is more effectively used to label cells, as incorporation goes to higher completion more quickly, and fewer medium exchanges are required.\nPlate the cells in an adequate vessel, such as a 6-well plate or a T75 flask.Note: The number of cells used to start the labelling depends on their doubling time. For fast growing cells such as HEK 293 or HeLa (doubling time approx. 24 h), it may be convenient to start with low number of cells (e.g. 0.3\u00a0\u00d7\u00a0106) as the population will expand during the labelling procedure according to   N =  N 0   e   t  ln \u2061  ( 2 )   / \u03c4    , where    N 0    is the cells seeded and   \u03c4   the doubling time. If enrichment of a subcellular compartment is required, its proportion should be considered (e.g. if mitochondria are to be isolated and the cell line of interest has a low amount of these, then a larger number of cells are needed). Typically, 12-15 15\u00a0cm plates of HEK 293 cells for each labelling condition are sufficient for one experiment with mitochondrial isolation.\nAllow the cell populations to expand in their respective SILAC medium for at least 7 doubling times, refreshing the medium every 2\u00a0days.\nCritical: It is crucial that the majority of the cell\u2019s proteins are properly labelled. Since the regular culture medium contain naturally occurring isotopes, which are mostly the light counterparts, the labelling completion becomes an issue only for the populations incubated in the heavy medium. If the appropriate time to achieve >95% labelling has not been established, it is possible to monitor heavy isotope incorporation into cellular proteins by taking a sample while passaging and analyzing the whole cell lysate by mass spectrometry.Note: It is suggested that the amount of SILAC medium needed is determined before starting the experiment, to make sure there are enough resources available. Strategies to minimize the use of these could involve starting the labeling with a small number of cells and, attending to their doubling time, splitting them throughout labeling to obtain the desired number of cells at the end of this procedure.\nHarvesting and sample mixing\nTiming: 60\u201390\u00a0min\nOnce sufficient (>95%) labeling of cells is achieved, labeled cells are collected and mixed. Mixing of cell suspensions in equal proportions is one of the most crucial steps in the protocol. Refer to Figure\u00a01[href=https://www.wicell.org#fig1] for schematic of sample mixing.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3128-Fig1.jpg\nFigure\u00a01. Schematics of sample mixing\nControl (c)\u00a0and test (t)\u00a0cells are separately labeled with SILAC media containing amino acids labeled with light (L) and heavy (H) isotopes. It is preferable that all cells are labeled simultaneously, but it is also possible to prepare one half of the full experimental set of cells at a time (i.e., cL and tH, or cH and tL). After expanding the cell populations, they are harvested and total protein quantified in each sample. The volume corresponding to the same amount of total protein in each pair is determined and mixed (i.e., cLtH, or cHtL).\nThe day before cell harvesting, place all required solutions, ultracentrifuge rotors and homogenizer at 4\u00b0C overnight.\nOn the day of the experiment, refrigerate the centrifuges and ultracentrifuges.\nHarvest each cell population (cL, cH, tL, tH) into separate tubes.\nAdherent cell lines: detach the monolayer using trypsin or another routine dissociation method.\nSemi-adherent cell lines (e.g., HEK 293): detach the monolayer by pipetting directly on the monolayer.\nSuspension cell lines: centrifuge the suspension.Retrieve cells from the suspension by centrifugation in a 50\u00a0mL conical tubes (500 g, 3\u20135\u00a0min, 4\u00b0C).\nDiscard the supernatant, resuspend the cell pellet in cold PBS, pellet cells, and discard supernatant.\nNote: Remove as much cell culture medium as possible as it contains proteins that may behave as contaminants in later stages of the procedure.\nResuspend the cell pellets in 50\u00a0mL conical tubes in 10\u00a0mL of cold PBS. Keep on ice.\nSave a small fraction of cells labeled with SILAC heavy medium (i.e., cH, tH) to determine the extent of incorporation of heavy isotopes in cellular proteins.\nResuspend the suspensions of cells labeled in heavy medium and immediately collect 10\u00a0\u03bcL into new, methanol washed, 1.5\u00a0mL microcentrifuge tubes.\nPellet the sample in 1.5\u00a0mL tubes by centrifugation in a benchtop centrifuge (500 g, 3\u00a0min, 4\u00b0C) and carefully discard the supernatant.\nFreeze the cell pellets.\nCritical: It is important to store a small aliquot of heavy samples before sample mixing. These will be analyzed by mass spectrometry and inform on the extent of the isotopic labelling by the SILAC heavy medium. This information can be used to optimize the labelling time. If the labelling is determined to be insufficient, the quantitative comparison between samples cannot be made, as the unlabeled pool of proteins from heavy-labelled cells will be indistinguishable from that originating from light-labelled cells.\nDetermine protein concentration of heavy and light labeled samples.\nResuspend cell suspensions of heavy and light labeled cells and immediately collect 10\u00a0\u03bcL of each into separate, new microcentrifuge tubes. Keep cell suspension in 50\u00a0mL conical tubes on ice.\nPellet samples in microcentrifuge tubes by centrifugation in a benchtop centrifuge (500 g, 3\u00a0min, 4\u00b0C) and carefully discard the supernatant.Add 10\u201320\u00a0\u03bcL of lysis buffer and mix vigorously by pipetting up-and-down. Avoid foaming.\nCritical: Although it is possible to use most protein quantification assays that provide good results for low concentrations, it is important to use a lysis buffer compatible with the assay. Check the interfering compounds (e.g. detergents, reducing agents) of the assay in advance to choose the adequate lysis buffer composition.\nClarify the lysate by centrifugation in a benchtop centrifuge (15,000 g, 5\u00a0min, 4\u00b0C).\nCarefully collect the supernatant into a new tube, avoiding transferring pelleted debris or chromatin.\nUsing a protein quantification of choice, determine the total protein concentration for each sample.\nIdentify the sample with the lowest total protein amount for the samples that will make up each mix (i.e., cLtH and cHtL), and determine the volume of the other counterpart that contains the same quantity of total protein.\nMix cell suspensions so that total protein quantities from each pair of control and test cell lines are the same.\nPellet cell mixes by centrifugation in a benchtop centrifuge (500 g, 3\u00a0min, 4\u00b0C) and carefully discard the supernatant. Keep the pellet on ice.\nCritical: It is important to keep cells on ice and perform all centrifugation steps at 4\u00b0C to limit degradation of biomolecules.\nSubcellular fractionation\nTiming: 2\u20133 h\nThis section describes purification of cellular components in order to enrich samples for the macromolecular complex of interest while simultaneously reducing contaminating species. Macromolecular complex refers to a stable, functional multi-component unit of proteins that can also contain non-protein molecules, such as nucleic acids.\nNote: This step can be tailored to the target of the study. Here, we describe the purification of mitochondria by differential centrifugation and a step/discontinuous density gradient.Note: Solutions used in this and following sections can be stored at 4\u00b0C for up to 1 year if protease and RNase inhibitors have not been added. For solutions containing carbohydrates, storage conditions will depend on their sterility.\nWeight the wet cell pellet in 50\u00a0mL conical tubes, using an empty tube as reference.\nGently resuspend the pellet in 3\u00a0mL of hypotonic buffer per 1\u00a0g of wet cell mass, and incubate on ice for 10\u00a0min. The hypotonic buffer causes cell swelling hence more efficient lysis upon homogenization (see next step).\nAssemble the Balch homogenizer with a suitable ball bearing (12\u00a0\u03bcm clearance for HEK 293 cells) and wash the inner chamber with hypotonic buffer using two 2.5\u00a0mL syringes.\nCritical: Use a ball bearing with an appropriate clearance in the homogenizer\u00a0\u2013 it must be just slightly smaller than the size of the used cells when in suspension. This may be verified using an optic microscope.\nNote: Make sure there are no air bubbles in the syringes or the inner chamber, as these will make the flow of the cell suspension more difficult.\nHomogenize the cell suspensions.\nPass 2\u00a0mL of cell suspension through the homogenizer, 3 times or until cells have been disrupted. Keep the homogenizer on ice at all times.\nNote: The weakest point of the Balch homogenizer is the connection with the syringes (Figure\u00a02[href=https://www.wicell.org#fig2]). Make sure to wear appropriate personal protective equipment, including goggles, and avoid applying excessive pressure on the plungers.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3128-Fig2.jpg\nFigure\u00a02. The assembly and use of Balch homogenizer\n(A and B) Parts of Balch homogenizer (A)\u00a0and assembled homogenizer (B). The weakest point of the homogenizer is the connection with the syringes (red circles).Note: The number of passages required to homogenize cells may require optimization. This would involve looking at the homogenate under a light microscope and proceed with further passages if there are still a considerable number of undisrupted cells in the suspension.\nTransfer the homogenate to a new 50\u00a0mL conical tube on ice, and immediately add 1.33\u00a0mL of 2.5x MSH.\nCritical: Avoid keeping the homogenate in hypotonic buffer for too long. If possible, before starting homogenization, add the required volume of 2.5x MSH to the pre-chilled tube where the homogenate will be dispensed.\nRepeat the homogenization steps until all suspension has been used.\nWash the homogenization chamber with 2.5x MSH and transfer the collected suspension to the tube containing the homogenate.\nMake the volume of the homogenate to 30\u00a0mL with 1x MSH.\nIf processing more samples (e.g., both cLtH and cHtL), disassemble the homogenizer, wash it in water and then 70% ethanol, and re-assemble it.\nRepeat homogenization for the next cell mix.\nAlternatives: The same procedure can be done using a Dounce homogenizer, or other preferred method for cell disruption. The main two points to consider are the efficiency of disruption and the integrity of the cell compartment of interest.\nRemove debris from the homogenate by centrifugation (1,000 g, 20\u00a0min, 4\u00b0C).\nTransfer the supernatant into a new 50\u00a0mL conical tube and keep on ice.\nPellet mitochondria by centrifugation (10,000 g, 20\u00a0min, 4\u00b0C). Troubleshooting 2[href=https://www.wicell.org#troubleshooting]\nDiscard the supernatant and resuspend the crude mitochondrial pellet in 1\u00a0mL of 1x MSH.\nPrepare the step density gradient to purify mitochondria.\nPipette 4\u00a0mL of 1.5\u00a0M sucrose buffer in a SW 40 Ti ultracentrifuge tube.\nGently layer 4\u00a0mL of 1.0\u00a0M sucrose buffer on top.Gently layer 3\u00a0mL of 0.5\u00a0M sucrose buffer on top of previous layer.\nLayer the crude mitochondrial suspension on top of the gradient column.\nTop up the tube with 1\u00a0mL of 1x MSH.\nBalance the tubes, and ultracentrifuge (85,200 g, 1 h, 4\u00b0C).\nLocate mitochondria in the gradient\u00a0\u2013 they should appear as a brownish-red disc between the 1.0\u00a0M and 1.5\u00a0M sucrose cushions (Figure\u00a03[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3128-Fig3.jpg\nFigure\u00a03. Expected results from purification of mitochondria by step/discontinuous density gradient ultracentrifugation\nThe photograph on the left was taken after ultracentrifugation finished, and the purified mitochondrial disc is highlighted by a white arrowhead. On the right, a scheme of the tube contents, with the location of the purified mitochondria disc highlighted by a black arrowhead. The tube shown in this figure is a 5\u00a0mL SW 60 Ti ultracentrifuge tube.\nRemove the 0.5\u00a0M and most of the 1.0\u00a0M layers by pipetting.\nCollect the mitochondria by pipetting and transfer them to a 15\u00a0mL conical tube.\nResuspend the mitochondrial pellet in 4 volumes of 1x MSH.\nPellet the purified mitochondria by centrifugation (10,000 g, 10\u00a0min, 4\u00b0C). Discard the supernatant. Troubleshooting 2[href=https://www.wicell.org#troubleshooting]\nContinuous density gradient ultracentrifugation\nTiming: 3\u20134 h\nIn this step, macromolecular complexes are separated by ultracentrifugation through a continuous density sucrose gradient. The conditions described here have been optimized for purification of mitochondrial ribosomes.\nNote: Separation is done according to the density of particles. Depending on the buffer used, it is possible to retrieve ribosomal subunits, or monosomes. This step is highly customizable and can be tailored to isolate other macromolecular complexes.\nLyse purified mitochondria.\nAdd 250\u00a0\u03bcL of lysis buffer to the mitochondrial pellet.\nPlace on a roller at 4\u00b0C for 15\u201320\u00a0min.\nMeanwhile, prepare the continuous density gradient.Prepare the 10% (w/v) and 30% (w/v) sucrose-TNM (Tris-NaCl-MgCl2), 2.5\u00a0mL of each solution per pair of gradients, and keep on ice.\nMark pairs of methanol-washed TLS-55 ultracentrifuge tubes using the appropriate block.\nFill each tube to the mark with 10% (w/v) sucrose-TNM.\nLoad 2.5\u00a0mL of 30% (w/v) sucrose-TNM into a syringe equipped with a blunt needle.\nInsert the needle to the bottom of the ultracentrifuge tube and load the solution from the bottom of the column, until the interphase reaches the mark.\nCap the tubes and carefully place them on the magnetic base of the gradient maker.\nRun the TLS-55 10\u201330 Sucrose Short program on the gradient maker.\nGently remove the tubes and keep them at 4\u00b0C until needed, limiting the manipulation of the tubes to avoid disruption of the formed continuous gradient.\nClarify the mitochondrial lysate by centrifugation (13,000 g, 5\u00a0min, 4\u00b0C).\nRemove 100\u00a0\u03bcL from the top of the gradient column in the ultracentrifugation tube and load 200\u00a0\u03bcL of clarified mitochondrial lysate by dispensing slowly against the inner wall of the tube.\nBalance the tubes, and ultracentrifuge (100,000 g, 2\u00a0h 15\u00a0min, 4\u00b0C).\nNote: The amount of material loaded on the continuous density gradient may require optimization. Avoid overloading the column (e.g. >2.5\u00a0mg of protein or >200\u00a0\u03bcL of sample in a TLS-55 ultracentrifuge tube) in order to obtain higher resolution of separation.\nAlternatives: The buffer composition, sucrose concentration and ultracentrifuge rotor can be adjusted to better suit the macromolecular complex being isolated.\nFractionate gradients (e.g., 100\u00a0\u03bcL per fraction) into methanol-washed 1.5\u00a0mL microcentrifuge tubes.Alternatives: Fractionation can be performed manually or automatically, from the top or the bottom of the tube. The smaller the volume of each fraction, the higher the resolution of the fractionation, but the probability of introducing technical artefacts is also higher.\nOptional: Transfer 5\u201310\u00a0\u03bcL of each fraction into new tubes for SDS-PAGE analysis. This will inform on the integrity of the gradient, and the distribution of the complex across fractions.\nPause point: Fractions can be stored at -80\u00b0C for several months until needed for mass spectrometric analysis.\nQuantitative mass spectrometry\nTiming: 3\u20134\u00a0days\nProteins in liquid fractions from the density gradient are precipitated, digested, and processed for mass spectrometric analysis using a quantitative pipeline for SILAC. The duration of this step will depend on the number of fractions collected, and the pre-processing employed. The output of this step is a list of identified proteins, and their respective abundance between samples and across density gradient fractions.\nTransfer an adequate volume of sample (e.g., 30\u00a0\u03bcL) to washed 2\u00a0mL microcentrifuge tubes.\nNote: The amount of material required for mass spectrometric analysis depends on the amount of total protein loaded on the density gradient, the amount of complex retrieved from it and the sensitivity of the mass spectrometric analysis used. In doubt, use as much volume as possible, as the processed material can be stored and used for additional mass spectrometric analyses.\nAdd 20 vol of cold ethanol to each fraction and incubate overnight (12\u201316 h) at -20\u00b0C.\nNote: Ethanol precipitation does not modify the protein residues while efficiently removing detergents with minimal protein loss, as well as allowing the easier resolubilisation of the precipitate. Alternative methods to concentrate proteins can be used.\nPellet the precipitate by centrifugation (16,000 g, 5\u00a0min, 4\u00b0C).Add 1% (w/w) trypsin in 50\u00a0mM NH4HCO3 to the pellets and incubate overnight (12\u201316 h) at 37\u00b0C.\nFractionate the obtained peptide digests by nano-scale reverse-phase liquid chromatography using a gradient of 5\u201340% acetonitrile in 0.1% (v/v) formic acid over 84\u00a0min, at a flow rate of 300 nL min-1. The eluate is transferred directly to the electrospray interface of the mass spectrometer.\nAcquire data from 400 to 1,600 m/z for precursor ions and set the ten most abundant multiply charged precursor ions in each spectrum to be fragmented by HCD with nitrogen. Precursor and fragment ion spectra are acquired with resolutions of 70,000 and 17,500, respectively.\nCritical: The settings and instruments used for mass spectrometric analysis can be flexibly adjusted. The key point is that each fraction is processed identically across the gradient, including their sequential analysis to minimize the effect of variations in instrument performance on sample analysis.\nAnalysis of raw mass spectrometry data\nTiming: 1\u201324 h\nIn this step peptides are identified and quantified from raw mass spectrometry data. The setup of the analysis should not take more than 1\u00a0h with the analysis running for approximately 8\u201312\u00a0h depending on the number of analyzed fractions. At the end, a table of all detected peptides matched to proteins is produced.\nUsing the Proteome Discoverer Deamon Utility submit the raw mass spectrometry files for all fractions for analysis by Proteome Discoverer.\nIdentify peptides by comparing the MS/MS spectra to the appropriate Mascot database with Proteome Discoverer.\nQuantify peptides using Proteome Discoverer.\nOpen a report from all fractions.\nExport the report of all peptides from all fractions as a single tab delimited text file (please refer to ComPrAn help files). Make sure that \u201cPeptides\u201d box is checked under Criteria. The stem \u201cpsms\u201d will be automatically added into the filename.Use file from step 48 as an input for analysis by the ComPrAn R package.\nNote: For the analysis of samples from HEK 293 cells, we used following parameters: proteins were searched against human UniProt database; mass tolerances of 10 ppm and 0.5\u00a0Da were used for precursor ions and fragmented ions, respectively; trypsin was selected as the protease with one missed cleavage allowed; dynamic modifications included N-terminal acetylation and formylation, oxidation of methionine and propionamide modification of cysteine; Mascot was configured to consider the possibility of the presence of the heavy-labelled lysine and arginine.\nAnalysis of peptide level data\nTiming: 1\u20132 h\nThis section describes analysis of peptide level data with the use of ComPrAn R package. As an output, a table of normalized protein data, lists of proteins detected only in one sample, a table of clustered proteins, as well as various visualizations of results are created. Analysis steps are illustrated in Methods Video S1[href=https://www.wicell.org#mmc1].\nNote: Two types of input data can be used in ComPrAn: i) peptide data or ii) normalized protein data. In this protocol steps 51 to 58 describe a workflow for processing peptide level data, that results in the generation of normalized protein data which are used from step 59 onwards.\nNote: ComPrAn contains internal example datasets: i) peptide dataset obtained with Proteome Discoverer version 1.4 (Thermo Fisher Scientific) in combination with Mascot database for peptide quantification and identification, respectively and ii) normalized protein dataset obtained by processing the example peptide dataset (i)\u00a0in ComPrAn. These datasets can be accessed, and the first 6 rows viewed by entering following code in R:\n# load ComPrAn package\nlibrary(ComPrAn)\n# locate peptide dataset\ninputFile <- system.file(\"extData\", \"data.txt\", package\u00a0= \"ComPrAn\")\n# read in example peptide dataset using the designated\n# ComPrAn import function\npeptides <- peptideImport(inputFile)# locate normalised protein dataset\ninputFileProtein <- system.file(\"extData\", \"dataNormProts.txt\",\npackage\u00a0= \"ComPrAn\")\n# read in file using the dedicated ComPrAn function, this\n# function automatically changes the structure of the table to\n# the required format\nproteins <- protImportForAnalysis(inputFileProtein)\n# display first 6 rows of peptides data\nhead(peptides)\n# display first 6 rows of protein data\nhead(proteins)\nStart R, load the ComPrAn package and launch the Shiny app by entering the following code.\n# load ComPrAn R package\nlibrary(ComPrAn)\n# launch the Shiny app\ncompranApp()\nImport the data by clicking on \u2018Import\u2019 tab and \u2018Browse\u2026\u2019 button; navigate to the file produced in step 48 and click Open.\nAfter progress bar shows \u201cUpload complete\u201d click \u2018Process data\u2019 button.\nSpecify names of the labeled (\u201cheavy\u201d) and unlabeled (\u201clight\u201d) samples in the appropriate boxes (these names will be used in plots later).\nSwitch to the \u2018Peptide-to-protein\u2019 tab and click on \u2018Summary\u2019 section in the list that will appear.\nNote: Summary plots contain the information about total number of peptides and number of peptides in the two studied samples that passed the initial filtering (removes peptides which were not assigned into a protein group, had missing precursor area value or had a \u201cRejected\u201d value of PSM (peptide spectrum match) ambiguity).\nSwitch to the \u2018Filter and Select\u2019 tab. Adjust the available setting and click \u2018Filter the data\u2019 button. Plots summarizing total number of proteins in samples are shown.\nIn the same tab, click \u2018Select peptides\u2019 button to select representative peptides that will represent each protein in the following analysis.\nSwitch to \u2018Rep Peptides\u2019 tab.Note: On this page all peptides for any given protein can be visualized with the option to highlight the peptide that was selected by the analysis software as representative of the protein. Additionally, lists of proteins that were detected in both or only one of the two samples can be downloaded.\nSwitch to \u2018Normalize\u2019 tab. Click on \u2018Normalize the data\u2019 button; all protein quantity values will be normalized to be between 0 and 1, a progress bar will appear in the bottom right corner, indicating processing of the data, once finished a message \u201cPart 1 analysis finished, you may proceed to Part 2\u201d will be shown.\nPause point: normalized data can be exported in tab delimited format and used as alternative input to continue with analysis straight from step 59.\nSwitch to \u2018Protein workflow\u2019 and click on \u2018Normalized Proteins\u2019 tab in the list that will appear. A protein can be selected from drop down menu for visualization of its quantitative comparison between labeled and unlabeled samples (Figures\u00a05[href=https://www.wicell.org#fig5]).\nNote: Each section of \u2018Protein workflow\u2019 provides a visualization option for the protein-level data to compare proteins and protein complexes quantitatively or qualitatively. Multiple options for customization of plots are provided, and each plot can be exported as a pdf file.\nSwitch to \u2018Heatmap\u2019 tab, click on \u2018Browse\u2026\u2019 and navigate to the file with information about the protein complex of interest. This produces a heatmap showing the quantitative comparison of protein profiles between samples (Figures\u00a05[href=https://www.wicell.org#fig5]).\nSwitch to \u2018Co-migration plots\u2019 tab, paste UniProt IDs of interest in a box to qualitatively compare the profile of one or two protein complexes.Switch to \u2018Cluster\u2019 tab. This section provides functionality for clustering analysis. Proteins are assigned into clusters, separately for each sample, based on correlation of protein migration profiles. Plots showing the number of proteins per cluster and table with proteins assigned into clusters can be downloaded.\nAlternatives: The analysis described here with the use of ComPrAn app can also be performed directly from the R command line with the use of ComPrAn functions. For step-by-step guidelines use ComPrAn R package vignettes that can be accessed by entering following commands into R: This will open a web page with a list of available vignettes. Command line analysis of peptide-level data is described in a \u201cSILAC complexomics\u201d vignette and analysis of protein-level data is described in \u201cProtein workflow\u201d vignette.\nbrowseVignettes(\"ComPrAn\")\n    Your browser does not support HTML5 video.\n  \n      Methods Video S1. Walkthrough of ComPrAn analysis\n    \nThis video shows the analysis of peptide-level data and visualization of protein data as described in steps 50 to 62 of the protocol.", "Step-by-step method details\nStep-by-step method details\nPrepare samples\nTiming: 4\u20135 h\n      The prepared herbal raw material powder will be extracted for active\n      ingredients. Subsequently, it will go through key steps such as\n      centrifugation, evaporation, redissolution and membrane filtration, and\n      finally become a sample ready for mass spectrometry detection.\n    \n        Weigh 1\u00a0g powder with an accuracy of more than 5% in a 50\u00a0mL plastic\n        centrifuge tube or in a 100\u00a0mL erlenmeyer flask.\n      \nNote: Lower amounts can be used as well,\n      but this is not advisable in view of the relatively higher weighing error.\n      It is recommended that there should be at least 0.1\u00a0g of powder. Here\n      steps 2\u20135 are used to illustrate the extraction method with the example of\n      the Chinese herb Gelsemium. Those with indole alkaloids as the main\n      active ingredients can refer to this extraction method, while others can\n      refer to the extraction methods in the relevant literature according to\n      the solubility of the main active ingredients.\n    \n        The dried powder (1 g) is extracted two times by ultrasonication with\n        80% alcohol (1:25, powder mass (1 g): extract liquid volume (25\u00a0mL)) for\n        0.5\u00a0h at 60\u00b0C. The twice-extracted solutions are combined.\n      \nNote: If the dried powder is less than 1\n      g, the use of 80% alcohol is reduced proportionally.\n    \nCritical: Ethanol is flammable and\n      should be used to avoid sources of ignition. And volatile, but also can\n      damage the skin, the experimenter should wear a mask and gloves. 80%\n      ethanol should be prepared when used, and unused sealed treatments can be\n      stored for 3\u20135\u00a0days.\n    \n        Centrifuge the sample and evaporate the ethanol from the\n        supernatant.20\u00b0C20\u00b0C.\n        \n            Centrifuge for 10\u00a0min at maximum speed (20,000\u00a0g for the centrifuge\n            tube) at 20.\n          \nPour the supernatant into a new centrifuge tube.volatilize the sample in chemical fume hood until no alcohol remains\n            at 20\u00b0C, which requires approximately 2 h.\n          \nCritical: If the volume of pure water\n      is too great in the ultrasonic bath, then there is a possibility of\n      tipping the Erlenmeyer flask, so control the volume of pure water added to\n      the ultrasonic bath. Alcohol is highly volatile, so open it only when\n      needed.\n    \n        Put 1\u00a0mL of the solution into a 5\u00a0mL centrifuge tube that is fastened to\n        a Thermovap Sample Concentrator. Use nitrogen gas to evaporate the\n        solution.\n      \nNote: When concentrating using liquid\n      nitrogen, the flow rate of nitrogen should be controlled slightly lower\n      than expected to prevent the liquid from splashing out. Centrifuge tube\n      specifications are optional, such as 5\u00a0mL or 10\u00a0mL. The tube is usually 3\n      to 5 times larger than the volume of the solution.\n    \n        Redissolve the sample.\n        \n            Add 1\u00a0mL of the mixture of the eluent using pipettor to the 5\u00a0mL\n            centrifuge tube.\n          \n            Close the lid and immediately vortex or dissolve in the\n            ultrasonicator for 20\u201330 s, number of times depending on the\n            solubility of the substance.\n          \n      (Advice: The ratio of the mixture of the eluent refers to the ratio\n      of the aqueous phase to the organic phase when compounds eluted are most\n      abundant. Because some plant alkaloids are less soluble, sonicate each\n      sample for 15\u201320\u00a0min at maximum frequency (40 kHz) continuously in a water\n      bath heated to 50\u00b0C\u201360\u00b0C when redissolving).\n    \n        Finally, use the solution that was filtered through a 0.22\u00a0\u03bcm membrane\n        filter for detection.\n      \nDetect samples\nTiming: 3\u20135 h\n      Prepared samples were detected using a liquid chromatography quadrupole\n      time-of-flight mass spectrometry (LC-QqTOF/MS) to obtain raw data. After\n      the pressure line of the mass spectrometer is stabilized, solvent blanksare run. A worklist is then created in the order of sample placement so\n      that the instrument runs according to the worklist.\n    \n        Place the prepared samples in trays inside the auto sampler during the\n        analysis series at 20\u00b0C20\u00b0C.\n      \nNote: In general, it is advisable to\n      prepare 3 replicates per tissue sample to ensure reliability and accuracy\n      of the data.\n    \n        Wait until each module is ready, and equilibrate the mass spectrometer\n        to stabilize the pressure line; run two solvent blanks.\n      \nNote: The four modules on the operator\n      interface appear green when ready, including HIP Sample, Binary Pump,\n      Column Comp. and Q-TOF. Solvent blank refers to the solution in which the\n      sample is dissolved, generally the same as the eluent.\n    \nRun two samples to check the pressure line and peak condition.\nNote: The pressure line should be stable,\n      without tilt and jagged. The mass spectrum peaks are independent and\n      complete, without overlap and tailing.\n    \n        Program the injection system to operate in sequential mode.\n        \n            Create a new worklist: in the drop-down menu of Worklist,\n            choose Add Multiple Samples.\nselect the location of the sample vials in order.\n            Edit the file name, save location, and run method of each sample.\n          \nNote: Please select a suitable injection\n      starting point and sequence in the selected fields of\n      Selection Origin and Block Increment. Note that the sample\n      file name must not be repeated, otherwise an error will occur and the\n      worklist cannot be run; The insertion of one quality control sample per\n      six samples is recommended, and the quality control sample can be either\n      an experimental sample or a solution prepared from the purchased standard.\n    \nCritical: The injection needle is\n      washed with 50% (v/v) methanol/ultra-pure water between injections. When\n      handling methanol, appropriate personal protective gear and thoroughventilation must be used. Gloves, goggles, and a laboratory coat should be\n      worn, and all containers should be labeled appropriately.\n    \n        Set the system to Standby automatically after finishing running of the\n        worklist. Click Script; ensure the selection of Project is\n        MH_Acq_Sript.exe. and the selection of Script is SCP_InstrumentStandby.\n      \nRun the worklist with the built method to obtain the raw data.\nPause point: Raw data can be stored\n      for an unlimited duration until data processing is initiated.\n    \nProcess data and characterize compounds\nTiming: 1\u20132\u00a0weeks\n    \n      The raw mass spectrometry data were analyzed using the Qualitative Mass\n      Hunter software. A table summing and recording the matching results were\n      first obtained by matching the raw data with the established PCDL in-house\n      database. Compounds were identified using MS and MS/MS data of the\n      compounds.\n    \n        Open the Qualitative Mass Hunter software (take version B.07.00 for\n        example).\n      \n        Select File -> Open Data File to select the data to analyze.\n        The total ion chromatography (TIC) can be retrieved from the spectral\n        file.\n      \nFind Compounds by Formula -> Find by Formula ->\n        Option, in Formula Source, select Database.\n        Select specified in-house database, set the maximum tolerance of mass\n        error to 10 ppm in Formula Matching, and click the start icon.\n      \nThere will be a table summing and recording the matching results.\nExport the table.\n      (Advice: Some unnecessary columns in the table can be deleted\n      before exporting the data. In general, the columns of\n      Lab, Name, Formula, Score, Mass, m/z and RT are necessary).\n    \n        Extract the extracted ion chromatogram (EIC) of compounds in the table.\n        \nSelect Chromatograms -> Extract Chromatograms.\nIn Type, select EIC.\n            On the MS Chromatogram tab, set the MS level to MS and\n            m/z value(s) (type in your values).\n          \n            On the Advanced tab, define the single m/z expansion to asymmetric (m/z) value, and the error range of m/z is\n            set to\u00a0\u00b110 ppm.\n          \nNote: If a compound in the table does not\n      have a completely separated peak or the peak area is too small when\n      extracting EIC, the compound should be removed from the table.\n    \n        Extract MS/MS spectra.\n        \n            Select Chromatograms -> Extract Chromatograms.\nIn Type, select TIC.\n            On the MS Chromatogram tab, set the MS level to MS/MS and m/z\n            value(s).\n          \nSelect or type your values.\nNote: Confirm the searched target\n      components by using the combination of accurate EIC and MS/MS spectra.\n      These searched results were assumed as target components only when these\n      components appeared from EIC and corresponding accurate MS/MS spectra that\n      were acquired at the same time.\n    \n        Manual confirmation based on the accurate EIC may be used to exclude\n        some repeat search results.\n      \nNote: If more than one accurate MS/MS\n      spectrum is acquired at a peak that appeared from EIC, these repeat\n      results can only be considered one compound.\n    \n        Obtain a more streamlined summary table with the elimination of\n        redundant information.\n      \n        Check whether the matching compound has a MS/MS spectra in the MS/MS\n        library.\n        \nIf so, directly determine the structure of the compound.\n            If not, determine the structure by analyzing the MS/MS fragments in\n            the raw data.\n          \nNote: If there are multiple matches\n      between the EIC and the database, they need to be determined by matching\n      the MS/MS of these compounds to the database.", "Step-By-Step Method Details\nStep-By-Step Method Details\nNote: all steps are performed in the statistical computing environment \u201cR\u201d. Example code for each step can be found at https://github.com/blumsteinm/Projecting_MAF_ClimateChange/STAR_Protocol_Example_Code.R[href=https://github.com/blumsteinm/Projecting_MAF_ClimateChange/blob/master/STAR_Protocol_Example_Code.R]\nCalculate Minor Allele Frequencies by Population for Loci of Interest\nTiming: 10\u201330\u00a0min depending on file sizes\nThis step uses R to pull information on the loci of interest from the genomic data files and uses it to calculate the minor allele frequency (MAF) by population.\nPull the allele information from the .bed files for each loci of interest using the \u201cread.plink\u201d function from the package snpStats in R.\nIf the allele information is in a large file, we recommend using \u201cfread\u201d from data.table instead of \u201cread.csv\u201d for faster loading.\nInstead of reading the whole .bed file into R, use the \u201cselect.snps\u201d parameter within \u201cread.plink\u201d to feed a list of loci of interest names or locations.\nCalculate the MAF by population.\nMerge the sample allele information with your population information so that you have a dataframe indicating the sample name, sample population, and what the samples\u2019 alleles at each loci are.\nOur species, Populus trichocarpa, is diploid. Thus our .bed files indicate any individual is either homozygous with 01 (AA) or 03 (BB) as the alleles value, or heterozygous with 02 (AB) as the alleles value. In our data, the minor allele is always 1/A.\nCreate a function for calculating the minor allele frequency by population.\nfreq <- function(alleles\u00a0= NULL){ converted_alleles <- sapply(as.numeric( alleles ), function(x) ifelse(x\u00a0== 1, 1, ifelse(x\u00a0== 2, 0.5, ifelse(x\u00a0== 3, 0, NA)))) allele_frequency <- sum(converted_alleles)/length(converted_alleles) return(allele_frequency) }\nUse \u201caggregate\u201d from base R and the \u201cfreq\u201d function from above to calculate the MAF (allele A) by population\nDefine the Major Axes of Climate Variation Using a Principal Component Analysis\nTiming: 10\u201320\u00a0minThis step takes the climatic data from the past Normals data and future GCM projections and puts them into principal components (PC) space. PCs pull out major axes of explanatory variation, which is particularly useful when many climate variables are highly correlated and thus difficult to use in a statistical model.\nOrder rows in both climate files by population to ensure they match. This is essential for the principal component analysis (PCA) predictions.\nUse \u201cprcomp\u201d from the vegan package to run a PCA of the past climate Normals data, inputting a matrix of all climate variables as the object.\nEnsure that both \u201cscale\u201d and \u201ccenter\u201d are equal to TRUE. If not, climate variables with large values will disproportionately drive axis variation.\nUse \u201cpredict\u201d and the future climate data as \u201cnewdata\u201d to project the future climate variables into PC space.\nCheck results using \u201cbiplot\u201d from the vegan package (Figure\u00a01[href=https://www.wicell.org#fig1]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/109-Fig1.jpg\nFigure\u00a01. Visualizing Current and Future Climate in Principle Components Space\nA visualization of (A) climate variable correlations and (B) climate principal components space. The (A) correlations plot shows positive (blue) and negative (red) correlations between climate variables, with circle size and depth of color indicating the strength of the correlation. The (B) PCA plot depicts climate variables (blue arrows) in the first two axes of pc space. Where populations fall in PC space are shown with the colored dots, with the closed dots representing the past 30-years of climate data and their open counterparts representing where populations are expected to fall in the PC space in 2080.\nFit a Canonical Correspondence Analysis to Current Allele Frequencies versus Climate\nTiming: 15\u201330\u00a0minThis step uses the climate PCs and the MAFs by population that we have tabulated to fit a canonical correspondence analysis (CCA) model. This model is the relationship between past climate and current MAFs. This CCA will be used to project MAFs into the future under climate change.\nMerge the past climate dataframe and MAFs dataframe using \u201cmerge\u201d in base R. We merged based on \u201cpopulation name\u201d, so that when data is put into models in subsequent steps, all columns are in the same order by population.\nRecord the index of the climate and MAF columns so that you can distinguish between the two when running the CCA.\nRun a Null Model CCA with no predictors using \u201ccca\u201d from package vegan.\nThis will be used for model comparisons in a future step. In this case, no explanatory predictors are put into the model, just allele data.\nWe ran ours with 4,000 permutations.\nRun a CCA with all climate predictors included using \u201ccca\u201d from package vegan.\nAgain, we ran this with 4,000 permutations.\nDrop environmental predictors that are collinear/non-significant via a step-wise model comparison using \u201cordistep\u201d in the vegan package.\nWe inputted the null CCA as the object, with the scope set to the full CCA predictor model.\nWe ran ours in both directions, forward and backward, then used the optimal model returned as our CCA model. Using \u201c$anova\u201d with the model object will return the significance of variables remaining in the model.\nFurther Assess CCA Model Fit and Accuracy\nTiming: 20\u00a0min\nThis step delves further into how much variation the CCA model explains and how well it does recapitulating our current data.\nCalculate the percent of variation in the MAF data that the model explains as the constrained inertial value divided by the total inertia value.Note: The \u201ctotal inertia\u201d is the total variance in allele frequency distributions. The \u201cconstrained inertia\u201d is the variance explained by the environmental variables.\nTest whether the model explains more variation in MAF than expected by chance (p\u00a0= 0.05) using \u201canova.cca\u201d from the vegan package. If model is not significant, you should reevaluate the climate predictors that you are including.\nFinally, visually compare to what degree predicted MAF values match actual MAF values, given the past climate Normals data (Figure\u00a02[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/109-Fig2.jpg\nFigure\u00a02. Predicted Minor Allele Frequencies under Past Climate Normals Plotted by Actual Current Minor Allele Frequencies\nText shows the R2 of each population\u2019s linear regression results. The black line indicates a 1:1 line, while gray lines are population-level fits.\nWe performed linear regressions by population to assess which populations were more poorly predicted than others.\nIn our example data, there are a few populations with non-significant p-values and low R2\u2019s because we do not use the full dataset. These populations may need to be dropped in a real analysis should you find similar results in your data as the model is to replicating the pattern better than chance.\nProject Future Minor Allele Frequencies Given Predicted Climate Change\nTiming: 10\u00a0min\nThis step uses the projected climate PCs and the CCA model formed in step 3 to predict MAF change in 60 years and calculate summary statistics.\nUse the \u201cpredict\u201d function with CCA model created in step 3 and the future climate PC dataframe in the \u201cnewdata\u201d parameter to predict MAF under future climate conditions.\nCalculate the average predicted MAF change across all loci between the climate of the past 30 years and projected climate in 2080 (Figure\u00a03[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/109-Fig3.jpg\nFigure\u00a03. The Proportion of Loci Missing the Minor Allele and the Average Predicted MAF Change by LatitudePlots of (A) the proportion of loci that have only one allele by population and (B) the average project MAF change by population. Both are plotted against population latitude.\nStart by subtracting current MAFs from projected MAFs. This should result in a matrix of differences for each population (rows) at each loci (columns).\nSum the absolute value of each row to get average change by population.\nNote: we chose to look at the absolute value of change as we were interested in capturing both large increases and decreases in allele frequency.\nCalculate the number of loci currently missing the minor allele by population (Figure\u00a03[href=https://www.wicell.org#fig3]).\nWe did this by setting all values in the current MAF matrix below 0.01 to 0 and all else to 1, then summing by populations (rows).", "Step-by-step method details\nStep-by-step method details\nPlasmodium chabaudi chabaudi infection and disease monitoring\nTiming: 7\u20138\u00a0days\nIn this section we describe Pcc infection and disease monitoring in C57BL/6J mice (12\u201314\u00a0weeks old).\nThe virulence of Pcc infection varies according to the number of times the parasites are passaged in mice, i.e., when the parasite life cycle stages naturally occurring in the mosquito and mouse liver are bypassed and mice are infected by sequential Pcc-infected blood transfusions.15[href=https://www.wicell.org#bib25] The first blood stage infection derived from the mosquito (i.e., passage 0, P0) is established after a liver stage of infection, and is characterized by the development of low parasitemia, rapid parasite clearance and low virulence, i.e., slight deviation in health parameters. As the number of blood passages increases, parasitemia and virulence increase accordingly. The values presented below for health parameters and parasitemia refer to 7 and \u223c30 passages (P30) after the initial mosquito bite and liver infection (Figure\u00a04[href=https://www.wicell.org#fig4]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2918-Fig4.jpg\nFigure\u00a04. Disease progression in Pcc-infected C57BL/6J mice\nMice were infected with different passages of Pcc AS parasites (P7, P33). Disease and parasite development were monitored daily for 15\u00a0days.\n(A\u2013F) (A) Parasitemia, i.e., percentage of infected RBC (iRBC); (B)\u00a0number of RBC per \u03bcL of blood (RBC/\u03bcL); (C)\u00a0pathogen load, i.e., number of iRBC RBC per \u03bcL of blood (iRBC/\u03bcL); (D)\u00a0weight loss, i.e., percentage of initial weight; (E)\u00a0core body temperature; (F)\u00a0glycemia. N\u00a0= 2 mice per group. Data as mean\u00a0\u00b1 SD in all plots.Note: Depending on the parasite and mouse strain used, time and severity of infection might differ. It is recommended to perform a disease progression analysis prior to the single-cell RNA sequencing experiment to identify the time point of interest. Protocol is described for adult male C57BL/6J mice maintained under specific pathogen free conditions (SPF), fed ad libitum with standard chow and maintained at 22\u00b0C room temperature with a light cycle of 12/12 h.\nDisease monitoring: Monitor mouse health parameters before the infection (Day 0) to define steady state and to calculate deviations from steady state during the course of infection.\nCore body temperature is measured using a rectal probe. Steady state values should range between 36\u00b0C and 38\u00b0C.\nBody weight is measured using a scale. Steady state values should range between 24\u201328\u00a0g for male C57BL/6J mice, at 12\u201314\u00a0weeks of age.\nGlycemia is measured in a drop of blood, collected as described for parasitemia determination using a Glucometer. Steady state values should range between 120\u2013180\u00a0mg/dL.\nNote: Steady state core body temperature and glycemia measurements are highly dependent on the mouse\u2019s stress status and vary accordingly. It is advisable to handle mice a few days, before starting the experiment, for acclimatization to manipulation, to prevent misleading steady-state measurements. Core body temperature and glycemia also vary in a circadian manner16[href=https://www.wicell.org#bib14],17[href=https://www.wicell.org#bib15] and as such it is advisable to monitor mice at same time of the day.\nPcc infection: Infect mice as described above in the experimental mice infection section. The day of infection is considered as day 0 (D0).\nDisease progression monitoring: Monitor mice daily, from day 3 onwards, for the parameters described above:Note: Core body temperature of Pcc-infected male C57BL/6J mice can decrease up to 29\u00b0C\u201332\u00b0C, at day 7 post infection (i.e., peak of infection), corresponding to a loss of \u223c6\u00b0C\u20139\u00b0C (10%\u201320%), relative to steady state. This is readily reversed in the days following the peak of infection, typically by day 9 all mice should have recovered normal core body temperature. Body weight of Pcc-infected male C57BL/6J mice can decrease up to 17\u201320\u00a0g at the peak of infection, corresponding to a loss of 5\u20138\u00a0g (20%\u201330%) of the initial body weight. Body weight recovery can be observed upon parasite clearance, typically at days 9\u201310 post-infection. Glycemia of Pcc-infected wild type C57BL/6J mice usually decreases up to 60\u2013100\u00a0mg/dL at the peak of infection, corresponding to a loss of 60\u2013120\u00a0mg/dL (50%\u201370%), relative to steady state. This represents a mild hypoglycemia, which is rapidly reversed after the peak of infection. Parasitemia: as described above (including flow cytometry and microscopical analysis). Typically, Pcc infection develops slowly in the first days, gaining momentum around D4-D7, where the percentage of iRBC can reach up to \u223c60%. Day 7 of infection is considered the peak of infection, after which parasites start to be cleared from circulation and parasitemia starts to decrease.\nCritical: The values presented here refer to male C57BL/6J mice infected with Pcc AS. These values are expected to vary according to the genetic background of the infected mice18[href=https://www.wicell.org#bib16] as well as in genetically-modified C57BL/6J mice. As an example of the latter, genetic loss-of-function of glucose 6 phosphatase c (G6pc1) specifically in hepatocytes is associated with the development of severe hypoglycemia (<40\u00a0mg/dL) as well as defective adaptive thermoregulation (24\u00b0C\u201328\u00b0C), leading to death.1[href=https://www.wicell.org#bib1]Critical: Mice should be monitored at the same time of the day, as some rodent Plasmodium strains, including Pcc, have a highly synchronous 24\u00a0h life cycle. This imparts that the parasite develops through the different life-cycle stages under circadian regulation. This in turn implies that disease severity also changes according to the hour of the day, which should be taken into consideration in the experimental design.\nSorting and preparation of iRBCs for single-cell RNA sequencing\nTiming: 3\u20134 h\nHere we describe how to prepare the iRBCs for single-cell RNA sequencing. This should be performed at the time-point of interest (e.g., when host or parasite present a specific phenotype) as determined by disease progression, as well as by visual observation of Plasmodium parasites in Giemsa-stained blood smears, monitored as described above.\nCritical: RBCs are very sensitive to shear stress. Accordingly, pipetting should be avoided whenever possible. If necessary, pipetting should be performed very gently to avoid damaging the RBC. Cell viability and transcriptional profiles are affected by the time of storage on ice. While parasitemia can be assessed up to 24\u00a0h after blood collection, the transcriptional profile might change significantly over time. Accordingly, it is advisable to start the single cell analysis as soon as possible after the sample preparation ideally, within 2\u00a0h after cell sorting. Low-binding plasticware is recommended for all the following steps regarding 10\u00d7 genomics libraries.\nAt the peak of infection (i.e., 6\u20138\u00a0days after inoculation with iRBC), blood is collected by cardiac puncture, as follows:\nTo euthanize mice by CO2 exposure, place the animal in a container, connected to a CO2 source allowing for air plus CO2 gas exchange (e.g., containing a vent that can be opened and closed).Start with a slow flow rate (3 L/min) of CO2 and an open vent to anaesthetize the mouse.\nClose the vent once the mouse is anaesthetized and increase the flow rate (7 L/min) until no breathing of the mouse is observed.\nNote: Euthanasia can be induced in different ways, for example by Ketamine/Xylazine. It should, however, be consistent between experiments to avoid variability.\nConfirm death by squeezing the hind paw. If there is no reaction proceed immediately with the cardiac puncture.\nPrior to the cardiac puncture, coat a 1\u00a0mL syringe with 0.5\u00a0M EDTA (RT), by filling the syringe with EDTA (without the needle) and then placing the needle (23G) on the syringe to eject the EDTA back into the original flask. The residual EDTA in the syringe and needle is enough to avoid blood clotting.\nPerform cardiac puncture according to previously described methods.3[href=https://www.wicell.org#bib3]\nTransfer blood into a polystyrene tube (1.5\u00a0mL) and place it on ice. Sorting should be started as soon as possible but not later than 1h after blood collection.\nPrior to sorting, confirm parasitemia, as described in the preparation of Pcc infection.\nDilute blood 1/75 vol/vol in PBS (1\u00d7) supplemented with FBS (1%). Prepare the dilution in a FACS tube to avoid multiple pipetting steps.\nNote: Cut the end of the pipette tip when pipetting blood to avoid RBC lysis. When using a 70\u00a0\u03bcm nozzle for cell sorting, RBC concentration must be adjusted to ensure the threshold rate is not higher than 22,000 events per second.\nSort iRBCs according to FSC-A, SSC-A and GFP signals, as detailed above for parasitemia determination.\nCollect sorted cells into polystyrene tubes (15\u00a0mL) containing 1\u00a0mL PBS (1\u00d7) supplemented with FBS (1%) to minimize cell lysis.Adjust the sensitivity of the FSC and SSC detectors to clearly visualize RBCs in the scatter plot. Define a region that includes RBCs but excludes white blood cells and debris (Figure\u00a01[href=https://www.wicell.org#fig1]).\nDisplay the RBCs in a new plot with FSC-A vs. FSC-H and/or FSC-A vs. FSC-W to exclude doublets, which have a higher, disproportional area, when compared with the second parameter.\nOn a third bivariate plot, visualize the GFP fluorescence of the \u201csinglet\u201d events previously gated out of the blue laser (using a BP530/30 optical filter), against an empty detector on\u00a0the yellow region out of the violet, blue or yellow green lasers (using a BP586/15 optical filter).\nIdentify the GFP+ events and define the region of the iRBC, excluding cell autofluorescence, which can be visualized in the empty detector.\nUse the defined gate to sort iRBCs using a Purity sorting mode.\nCritical: Do not set the sorting gate in a histogram plotting GFP signal, as this may lead to contamination of the sorted sample with GFP\u2013 cells.\nTo obtain an adequate number (>\u00a01.5\u00a0\u00d7\u00a0106) of iRBC, sort samples for at least 30\u00a0min.\nCentrifuge samples at 300\u00a0\u00d7\u00a0g, 4\u00b0C, 5\u00a0min. Pellet should resemble the one illustrated in Figure\u00a05[href=https://www.wicell.org#fig5].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2918-Fig5.jpg\nFigure\u00a05. Illustration of the sample before counting the cells\nDiscard the supernatant into a waste bin by inverting the tube. Avoid aspirating the supernatant by a vacuum pump as this can increase RBC loss.\nWash the sample by adding cold PBS (1\u00d7, 2\u00a0mL).\nRe-suspend the RBC pellet by inverting and flicking the tube. In case pipetting is needed, use a 1000\u00a0\u03bcL pipette and cut the pipette tip so that the opening is wider.\nRepeat step 7\u20138 once.\nResuspend the cells in PBS (1\u00d7; 1\u00a0mL)Count RBC using a Neubauer counting chamber (Figure\u00a06[href=https://www.wicell.org#fig6]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2918-Fig6.jpg\nFigure\u00a06. Neubauer chamber with iRBCs under the microscope\nMagnification 20\u00d7.\nPrepare the Neubauer chamber by placing a glass cover on the chamber.\nAdd the cell suspension (10\u00a0\u03bcL) on one edge of the glass cover, so that the liquid fills the chamber through capillarity. If air bubbles occur, disassemble the Neubauer chamber, clean it with ethanol (75%) and repeat the above-described steps.\nPlace the chamber under the microscope and count the cells of the 4 large corner squares.\nThe total number of RBC is calculated as follows:\nC\ne\nl\nl\ns\n/\nm\nL\n=\nT\no\nt\na\nl\nn\nu\nm\nb\ne\nr\no\nf\nc\no\nu\nn\nt\ne\nd\nc\ne\nl\nl\ns\nx\n10.000\nN\nu\nm\nb\ne\nr\no\nf\ns\nq\nu\na\nr\ne\ns\nc\no\nu\nn\nt\ne\nd\nRepeat step 7\u20138 and re-suspend the sample in cold PBS (1\u00d7) to reach a final concentration of 1200 cells/\u03bcL.\n10\u00d7 genomics single-cell sequencing of iRBC\nTiming: 3.5\u00a0days\nThe workflow, from cell encapsulation to library construction, is executed according to the recommended user guide (CG000204 Rev D\u00a0\u2013 Single Index; https://www.10xgenomics.com/support/single-cell-gene-expression/documentation/steps/library-prep/chromium-single-cell-3-reagent-kits-user-guide-v-3-1-chemistry[href=https://www.10xgenomics.com/support/single-cell-gene-expression/documentation/steps/library-prep/chromium-single-cell-3-reagent-kits-user-guide-v-3-1-chemistry]) retrieved on January 10th 2023.\nAdd the cell suspension (13.8\u00a0\u03bcL) to the prepared master mix (according to the user guide), keep on ice, to target a recovery of >10,000 cells (13.8\u00a0\u03bcL x 1200 cells/\u03bcL).\nLoad the master mix containing the cells, the unique molecular identifier (UMI) and partition oil onto Chromium Next GEM Chip G (Chromium\u2122 Single Cell Controller running Firmware 5.0 10\u00d7 Genomics, PN-120263).\nPerform reverse transcription of RNA into cDNA.Analyze the cDNA profile on a Fragment Analyzer System (Agilent Technologies) according to the High Sensitivity NGS Fragment kit instructions (Agilent Technologies).\nNote: This step is a crucial quality control for all subsequent procedures. The number of PCR cycles for cDNA amplification are adjusted according to the input cDNA mass on 10\u00a0\u03bcL of the total reaction, avoiding overamplification and possible artifacts. Library quality profile is also verified on the same instrument for both size determination and library quantification.\nSequence with NextSeq 500 (Illumina) using the NextSeq 500/550 High Output Kit v2.5 (150 Cycles) using a final diluted pool 1.8 pM with the following sequencing run settings: Read1: 28 cycles, i7 index: 8 cycles, i5 index: 0 cycles, Read2: 130 cycles, aiming a read depth of \u223c25,000 reads/cell.\nIncrease Read2 from the recommended 91 cycles to 130 due to cycle availability of the kit and to increase the mapping of the read to the reference transcriptome.\nAnalysis of scRNA-Seq data\nTiming: \u00a0>\u00a02\u00a0weeks\nHere we describe how to analyze the single-cell RNA sequencing data obtained with the protocol described above. In this section, the protocol starts by importing the filtered feature-barcode matrices, bad-quality cells and non-expressed genes are excluded, and the processed data is clustered into transcriptionally similar populations and projected onto the low-dimensional space. The cell populations identified are confirmed by inspecting conserved markers identified through differential gene expression (DGE).Note: Sequence alignment: Feature Count Matrix is created using Cellranger V7.0.0 (10\u00d7 Genomics) with STAR aligner V2.7.9a on an Ubuntu virtual machine. The reference genome of Pcc and annotation (Ensembl ftp://ftp.ensemblgenomes.org/pub/protists/release-50/protists/) is used for alignment through the \"mkref\" command from the Cellranger analysis pipeline. Raw reads from each sample and the reference genome prepared on the previous step were processed with the \"cellranger count\" pipeline resulting in the Counts Matrix for each sample. All the steps are performed following standard best practices from 10\u00d7 Genomics (https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger[href=https://support.10xgenomics.com/single-cell-gene-expression/software/pipelines/latest/what-is-cell-ranger]). QC, filtering and exploratory clustering, dimensional reduction and DEG: All following analyses are performed in a virtual machine with 10 cores and 64 GB of RAM running Ubuntu 18. All the R code are run in RStudio Server (v. 2022.07.1 Build 554) using R version 3.6.311[href=https://www.wicell.org#bib10] as Rmarkdown notebooks (v.2.519[href=https://www.wicell.org#bib17],20[href=https://www.wicell.org#bib18]). Code described herein to perform data analyses was deposited in the following github repository: https://github.com/inflammationlab/scRNAseq-malaria-STAR-protocol[href=https://github.com/inflammationlab/scRNAseq-malaria-STAR-protocol] in the hope of making analyses reproducible and reusable as well as to simplify the descriptions made herein which will be circumscribed to the most important steps for readability.\nSingle-cell analyses in R are performed using the Seurat package (v.4.0.08[href=https://www.wicell.org#bib7]). The Seurat package is forked and the \u2018DESCRIPTION\u2019 file modified (see: https://github.com/antonioggsousa/seurat/commit/3a9e57b499e9e417f09da3ebb0aeaa4cedfb9ef8[href=https://github.com/antonioggsousa/seurat/commit/3a9e57b499e9e417f09da3ebb0aeaa4cedfb9ef8]) to install Seurat v.4.0.0 in R v.3.6.3. The installation is performed after launching the R console as follows:\n> devtools::install_github(\u201cantonioggsousa/seurat\u201d)\nEach single-cell sample, hereby labeled as gt1 and gt2, are analyzed independently and the differences in parameters are highlighted be.\nImport the three files - \u2018barcodes.tsv.gz\u2019, \u2018features.tsv.gz\u2019, \u2018matrix.mtx.gz\u2019 - under the output folder \u2018filtered_feature_bc_matrix\u2019 comprising the filtered feature-barcode matrices MEX obtained in the previous step as a sparse matrix which was in turn converted into a S4 Seurat object. This step is done using two Seurat (v.4.0.08[href=https://www.wicell.org#bib7];) functions as described below:>set.seed(seed\u00a0= 1024) # to keep reproducibility\n>library(\"Seurat\", quietly\u00a0= TRUE) # import package\n# import 10x Gt1 sample\n>gt1_sample_dir <- \"../data/ftp01.igc.gulbenkian.pt/gt1_count_full/outs/filtered_feature_bc_matrix\"\n>gt1\u00a0<- Read10X(data.dir\u00a0= gt1_sample_dir)\n>gt1_seu <- CreateSeuratObject(counts\u00a0= gt1, project\u00a0= \"Gt1\", min.cells\u00a0= 3) # create Seurat object\n# import 10x Gt2 sample\n>gt2_sample_dir <- \"../data/ftp01.igc.gulbenkian.pt/Gt2_count_full/outs/filtered_feature_bc_matrix\"\n>gt2\u00a0<- Read10X(data.dir\u00a0= gt2_sample_dir)\n>gt2_seu <- CreateSeuratObject(counts\u00a0= gt2, project\u00a0= \"Gt2\", min.cells\u00a0= 3) # create Seurat object\nRemove genes expressed in less than three cells (\u2018CreateSeuratObject(..., min.cells\u00a0= 3\u2019)).\nInspected different single-cell data features in terms of the number of total UMIs (Unique Molecular Identifiers) or different number of genes expressed per cell.\nRemove Cells expressing less than 750 different genes or 1,500\u00a0UMIs.\n>params <- list(\"max_nFeature\"\u00a0= 750, \"max_nCount\"\u00a0= 1500) # parameters to apply the filtering\n>gt1_seu <- subset(gt1_seu, subset\u00a0= nFeature_RNA\u00a0<\u00a0params$max_nFeature\u00a0& nCount_RNA\u00a0<\u00a0params$max_nCount) # apply the filtering\n>gt2_seu <- subset(gt2_seu, subset\u00a0= nFeature_RNA\u00a0<\u00a0params$max_nFeature\u00a0& nCount_RNA\u00a0<\u00a0params$max_nCount)Note: By looking into its distribution, e.g., using the function \u2018VlnPlot(gt1_seu, features\u00a0= c(\"nFeature_RNA\", \"nCount_RNA\"))\u2019, and their linear relationship, e.g., plotting \u2018FeatureScatter(gt1_seu, feature1\u00a0= \"nCount_RNA\", feature2\u00a0= \"nFeature_RNA\")\u2019 other cell features that might indicate cell integrity are often used as well as filtering criteria such as the percentage of expression of mitochondrial and/or ribosomal genes.21[href=https://www.wicell.org#bib19] These features were not considered in this protocol as they are difficult to disentangle from the distinct life-cycle stages where the variation in gene expression of mitochondrial/ribosomal genes is likely to happen. These thresholds fit above the upper whisker (outliers) when the distribution of these features is plotted, likely representing doublets or multiples. Cells expressing low number of genes or UMIs might represent non-intact cells, dying cells or cells with broken membranes, and they should be excluded as well at this stage (e.g., \u2018subset(gt1_seu, subset\u00a0= nFeature_RNA\u00a0>\u00a0100\u00a0& nCount_RNA\u00a0>\u00a0150)\u2019). As the number of genes detected and UMIs obtained for the different Plasmodium life-cycle stages diverges,22[href=https://www.wicell.org#bib20] and smaller cells have lower mRNA content23[href=https://www.wicell.org#bib21] thresholds may not be applied or applied with caution as they may exclude resting, less active stages. The thresholds applied always need to be adapted to each study by looking into the distribution of the different cell features.\nFiltered gene expression count tables are normalized with the function: \u2018NormalizeData()\u2019 using the \u2018normalization.method\u00a0= \"LogNormalize\"\u2019 and \u2018scale.factor\u00a0= 10000\u2019.\n>gt1_seu <- NormalizeData(gt1_seu, normalization.method\u00a0= \"LogNormalize\", scale.factor\u00a0= 10000)\n>gt2_seu <- NormalizeData(gt2_seu, normalization.method\u00a0= \"LogNormalize\", scale.factor\u00a0= 10000)\nSelect the top 750 highly variable features across the datasets with the function \u2018FindVariableFeatures()\u2019 using the following parameters: \u2018selection.method\u00a0= \"vst\"\u2019 and \u2018nfeatures\u00a0= 750\u2019.\n> gt1_seu <- FindVariableFeatures(gt1_seu, selection.method\u00a0= \"vst\", nfeatures\u00a0= 750) # find variable features\n>gene_names <- rownames(gt1_seu) # retrieve gene names 4554>gt1_seu <- ScaleData(object\u00a0= gt1_seu, features\u00a0= gene_names) # run scaling\n>gt2_seu <- FindVariableFeatures(gt2_seu, selection.method\u00a0= \"vst\", nfeatures\u00a0= 750) # find variable features\n>gene_names <- rownames(gt2_seu) # retrieve gene names 4554\n>gt2_seu <- ScaleData(object\u00a0= gt2_seu, features\u00a0= gene_names) # run\nNote: Usually, 2k features are chosen for data sets with more than 10k genes, which represents less than 20% of the genes. Therefore, 750 variable features are chosen to represent \u223c16% of all variable features for Pcc samples. Then all the features are scaled with the \u2018ScaleData()\u2019 function.\nA Principal Component Analysis (PCA) is performed using the scaled top highly variable genes with the function \u2018RunPCA()\u2019. This PCA is in turn used to cluster and build the non-linear Uniform Manifold Approximation and Projection (UMAP) later.\n> gt1_seu <- RunPCA(object\u00a0= gt1_seu, features\u00a0= VariableFeatures(object\u00a0= gt1_seu)) # run PCA\n>gt2_seu <- RunPCA(object\u00a0= gt2_seu, features\u00a0= VariableFeatures(object\u00a0= gt2_seu)) # run PCA\nThe importance of the different principal component regarding the contribution of variance is inspected through an elbow plot and based on the result the top 10 PCs were selected for clustering and UMAP projection (Figure\u00a07[href=https://www.wicell.org#fig7]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2918-Fig7.jpg\nFigure\u00a07. Representative elbow plot\nShowing the relationship of the (standard) variation across the first 50 principal components.\n>library(\u201cggplot2\u201d)\n>elbow_plot <- ElbowPlot(gt1_seu, ndims\u00a0= 50)\u00a0+\ntheme_minimal()\u00a0+\ngeom_vline(xintercept\u00a0= 10, linetype\u00a0= \"dashed\")\u00a0+\ntheme(axis.text\u00a0= element_text(size\u00a0= 14, color\u00a0= \"black\"),\naxis.title\u00a0= element_text(size\u00a0= 14, color\u00a0= \"black\"),\n\u00a0\u00a0text\u00a0= element_text(size\u00a0= 14, color\u00a0= \"black\"))\u00a0+\nannotate(\"rect\", xmin\u00a0= 0, xmax\u00a0= 10, ymin\u00a0= 0, ymax\u00a0= 10,\nalpha\u00a0= .25, fill\u00a0= \"#E64B35FF\")\u00a0+\nannotate(geom\u00a0= \"text\", x\u00a0= 5, y\u00a0= 7.5, label\u00a0= paste0(\"10\nPCs\"), size\u00a0= 4.5, color\u00a0= \"blue\")\n>print(elbow_plot) # print\n>elbow_plot <- ElbowPlot(gt2_seu, ndims\u00a0= 50)\u00a0+theme_minimal()\u00a0+\ngeom_vline(xintercept\u00a0= 10, linetype\u00a0= \"dashed\")\u00a0+\ntheme(axis.text\u00a0= element_text(size\u00a0= 14, color\u00a0= \"black\"),\naxis.title\u00a0= element_text(size\u00a0= 14, color\u00a0= \"black\"),\ntext\u00a0= element_text(size\u00a0= 14, color\u00a0= \"black\"))\u00a0+\nannotate(\"rect\", xmin\u00a0= 0, xmax\u00a0= 10, ymin\u00a0= 0, ymax\u00a0= 10,\nalpha\u00a0= .25, fill\u00a0= \"#E64B35FF\")\u00a0+\nannotate(geom\u00a0= \"text\", x\u00a0= 5, y\u00a0= 7.5, label\u00a0= paste0(\"10\nPCs\"), size\u00a0= 4.5, color\u00a0= \"blue\")\n>print(elbow_plot) #\nUse the first 10 PCs for clustering with \u2018FindNeighbors()\u2019 based on the plots described in the previous section.\nUse the following values of resolution with the function \u2018FindClusters()\u2019: 0.20, 0.25, 0.30, 0.35, 0.40, 0.45, 0.50, 0.55, 0.60.\nCreate a UMAP using the function \u2018RunUMAP()\u2019 using the first 10 PCs.\nNote: Resolution is one important parameter and crucial to determine the number of clusters obtained.\n## Gt1 sample\n>set.seed(1024)\n>gt1_seu <- FindNeighbors(gt1_seu, dims\u00a0= 1:10)\n>res_2_iter <- seq(0.2,0.6, by\u00a0= 0.05) # resolutions to test\n# iterate over the different resolutions and return a list of Seurat objects to work with\n>gt1_seu_list <- list()\n>for ( res in res_2_iter ) {\n\u00a0\u00a0seu <- paste0(\"gt1_seu_\", res)\n\u00a0\u00a0set.seed(1024)\n\u00a0\u00a0gt1_seu_list[[seu]] <- FindClusters(gt1_seu, resolution\u00a0= res)\n}\n# check how many clusters by resolution\n>no_clusters <- lapply(gt1_seu_list, function(x) {\nlength(levels(x$seurat_clusters))\n})\n>clust_df <- data.frame(\"Seurat_obj\"\u00a0= names(gt1_seu_list),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Resolution\"\u00a0= res_2_iter,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"No_clusters\"\u00a0= unlist(no_clusters))\n# iterate over the list\n>gt1_seu_umap <- list()\n>for ( seu in names(gt1_seu_list) ) {\nset.seed(1024)\n\u00a0\u00a0gt1_seu_list[[seu]] <- RunUMAP(gt1_seu_list[[seu]], dims\u00a0= 1:10)\n\u00a0\u00a0gt1_seu_umap[[seu]] <- DimPlot(gt1_seu_list[[seu]], reduction\u00a0= \"umap\", label\u00a0= TRUE)\u00a0+\nggtitle(paste0(\"Resolution: \", clust_df[clust_df$Seurat_obj\u00a0== seu, \"Resolution\"]))\n}\n## Gt2 sample\n>set.seed(1024)\n>gt2_seu <- FindNeighbors(gt2_seu, dims\u00a0= 1:10)\n>res_2_iter <- seq(0.2,0.6, by\u00a0= 0.05) # resolutions to test\n# iterate over the different resolutions and return a list of Seurat objects to work with\n>gt2_seu_list <- list()>for ( res in res_2_iter ) {\n\u00a0\u00a0seu <- paste0(\"gt2_seu_\", res)\n\u00a0\u00a0set.seed(1024)\n\u00a0\u00a0gt2_seu_list[[seu]] <- FindClusters(gt2_seu, resolution\u00a0= res)\n}\n# check how many clusters by resolution\n>no_clusters <- lapply(gt2_seu_list, function(x) {\nlength(levels(x$seurat_clusters))\n})\n>clust_df <- data.frame(\"Seurat_obj\"\u00a0= names(gt2_seu_list),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"Resolution\"\u00a0= res_2_iter,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\"No_clusters\"\u00a0= unlist(no__clusters))\n># iterate over the list\n>gt2_seu_umap <- list()\n>for ( seu in names(gt2_seu_list) ) {\n\u00a0\u00a0set.seed(1024)\n\u00a0\u00a0gt2_seu_list[[seu]] <- RunUMAP(gt2_seu_list[[seu]], dims\u00a0= 1:10)\n\u00a0\u00a0gt2_seu_umap[[seu]] <- DimPlot(gt2_seu_list[[seu]], reduction\u00a0= \"umap\", label\u00a0= TRUE)\u00a0+\nggtitle(paste0(\"Resolution: \", clust_df[clust_df$Seurat_obj\u00a0== seu, \"Resolution\"]))\n}\n# retrieve the seurat object to work with\n>gt1_seu <- gt1_seu_list$gt1_seu_0.35\n>gt2_seu <- gt2_seu_list$gt2_seu_0.4\nSelect the resolution values, e.g., 0.35 and 0.4 for gt1 and gt2, respectively, and the corresponding clustering are used downstream.\nNote: At this stage, the processed samples can be submitted to a doublet identification algorithm, such as DoubletFinder,24[href=https://www.wicell.org#bib22] in order to remove potential doublets (for instructions see the documentation at: https://github.com/chris-mcginnis-ucsf/DoubletFinder[href=https://github.com/chris-mcginnis-ucsf/DoubletFinder]).24[href=https://www.wicell.org#bib22] As the identification of doublets generated from transcriptionally similar cell subtypes - homotypic doublets - is challenging, this step should be applied with caution depending on the type of data being analyzed, particularly, if it includes multicellular stages as found with Plasmodium. In this protocol, a more conservative approach is followed and, thus, doublet identification and removal is not performed at the cost of keeping some potential doublets.\nApply the function \u2018FindAllMarkers()\u2019 to find positive and negative markers for all with the following options: minimum percentage of cells that a gene needs to be expressed (\u2018min.pct\u00a0=\u00a00.25\u2019) and a log2 fold change threshold of 0.25 (\u2018logfc.threshold\u00a0= 0.25\u2019). The same function is used for positive markers (\u2018only.pos\u00a0= TRUE\u2019).\n## Gt1 sample\n# retrieve all the markers\n>set.seed(1024)\n>gt1_markers_all <- FindAllMarkers(gt1_seu, min.pct\u00a0= 0.25,\nlogfc.threshold\u00a0= 0.25)\n# retrieve only positive markers\n>set.seed(1024)\n>gt1_markers_pos <- FindAllMarkers(gt1_seu, only.pos\u00a0= TRUE,min.pct\u00a0= 0.25, logfc.threshold\u00a0= 0.25)\n## Gt2 sample\n# retrieve all the markers\n>set.seed(1024)\n>gt2_markers_all <- FindAllMarkers(gt2_seu, min.pct\u00a0= 0.25,\nlogfc.threshold\u00a0= 0.25)\n# retrieve only positive markers\n>set.seed(1024)\n>gt2_markers_pos <- FindAllMarkers(gt2_seu, only.pos\u00a0= TRUE,\nmin.pct\u00a0= 0.25, logfc.threshold\u00a0= 0.25)\nExport the main R objects to continue the analyses below.\n## Gt1 sample\n# create dir\n>r_objs_folder <- \"../results/gt1/R_objects\"\n>if( ! dir.exists(r_objs_folder) ) dir.create(r_objs_folder)\n# save R objects\n>saveRDS(object\u00a0= gt1_seu, file\u00a0= paste(r_objs_folder,\n\"gt1_seu.rds\", sep\u00a0= \"/\"))\n## Gt2 sample\n# create dir\n>r_objs_folder <- \"../results/gt2/R_objects\"\n>if( ! dir.exists(r_objs_folder) ) dir.create(r_objs_folder)\n# save R objects\n>saveRDS(object\u00a0= cre3_seu, file\u00a0= paste(r_objs_folder, \"cre3_seu.rds\", sep\u00a0= \"/\"))\nNote: To integrate, cluster, perform dimensional reduction, DEG and functional enrichment the next section continues by importing the processed data to integrate the two samples in order to harmonize/align shared cell types across conditions. The result is an integrated data set which is then used to identify and visualize cell populations through unsupervised clustering and projection onto the low-dimensional space, respectively. Finally, DGE between cell populations and across conditions is performed to identify conserved cluster markers and genes affected by the condition, respectively. The functions enriched based on the list of differentially expressed genes found are explored. The integration method performed is the Seurat CCA (Canonical Correlation Analysis).\nImport the objects produced above.\n# Import packages\n>library(\"dplyr\", quietly\u00a0= TRUE)\n>library(\"Seurat\", quietly\u00a0= TRUE)\n>library(\"ggplot2\", quietly\u00a0= TRUE)\n>library(\"tidyr\", quietly\u00a0= TRUE)\n# Import Seurat objects of Gt2 and Gt1\n>gt2_seu <- readRDS(file\u00a0= \"../results/gt2/R_objects/gt2_seu.rds\")\n>gt1_seu <- readRDS(file\u00a0= \"../results/gt1/R_objects/gt1_seu.rds\")\n>seu_list <- list(\n\u00a0\u00a0\"gt2\"\u00a0= gt2_seu,\n\u00a0\u00a0\"gt1\"\u00a0= gt1_seu\n)\nNormalize both samples with \u2018NormalizeData()\u2019 and variable features determined with \u2018FindVariableFeatures()\u2019 using the \u2018vst\u2019 method (\u2018selection.method\u00a0= \"vst\"\u2019) with the top 750 most highly variable features.\nIntegrated features are selected with \u2018SelectIntegrationFeatures()\u2019.## Normalization and Feature selection independently for each data set\n>params <- list(n_var_features\u00a0= 750) # list of parameters to use throughout the analysis\n>seu_list <- lapply(X\u00a0= seu_list, FUN\u00a0= function(x) { # apply norm\u00a0& var\n\u00a0\u00a0x <- NormalizeData(x)\n\u00a0\u00a0x <- FindVariableFeatures(x, selection.method\u00a0= \"vst\", nfeatures\u00a0= params$n_var_features)\n})\n# select features that are variable across both data sets\n>params[[\"var_features\"]] <- SelectIntegrationFeatures(object.list\u00a0= seu_list)\nPerform integration with \u2018IntegrateData()\u2019 by finding shared anchors across samples with \u2018FindIntegrationAnchors()\u2019 using the integrated variable features from before.\nScale the integrated data by (\u2018ScaleData()\u2019), PCA (\u2018RunPCA()\u2019) and UMAP (\u2018RunUMAP()\u2019) (Figure\u00a08[href=https://www.wicell.org#fig8]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2918-Fig8.jpg\nFigure\u00a08. Representative UMAP plot of integrated Pcc samples\nAdapted from Ramos, Ademolue et\u00a0al. (2022).1[href=https://www.wicell.org#bib1]\nPerform graph-based clustering with \u2018FindNeighbors()\u2019 using the PCA method.\nDefine clusters with \u2018FindClusters()\u2019 using a resolution value that yields a similar amount of clusters as in the separate analysis.\n## Find anchors, integrate, cluster, PCA and UMAP\n# find anchors\n>params[[\"anchors\"]] <- FindIntegrationAnchors(\nobject.list\u00a0= seu_list,\nanchor.features\u00a0= params[[\"var_features\"]]\n)\n# integrate labels\n>seu <- IntegrateData(anchorset\u00a0= params[[\"anchors\"]])\n# define default assay, assay integrated\n>DefaultAssay(seu) <- \"integrated\"\n# cluster, PCA and UMAP\n>params[[\"n_pcs\"]] <- 12; params[[\"mth\"]] <- \"pca\"; params[[\"res\"]] <- 0.39;\n>seu <- ScaleData(seu, verbose\u00a0= FALSE)\n>seu <- RunPCA(seu, npcs\u00a0= params$n_pcs, verbose\u00a0= FALSE)\n>seu <- RunUMAP(seu, reduction\u00a0= params$mth, dims\u00a0= 1:params$n_pcs)\n>seu <- FindNeighbors(seu, reduction\u00a0= params$mth, dims\u00a0= 1:params$n_pcs)\n>seu <- FindClusters(seu, resolution\u00a0= params$res)\nFind conserved markers across the two samples with the function \u2018FindConservedMarkers()\u2019 on the original RNA assay for each independent cluster as follows.\n# Find conserved markers\n# create folder to save conserved markers\n>conserved_markers_folder <- \"../results/int_28_05_21/tables/conserved_markers\"\n>if ( ! dir.exists(conserved_markers_folder) ) dir.create(conserved_markers_folder, recursive\u00a0= TRUE)\n# change default assay for DGE\n>DefaultAssay(seu) <- \"RNA\"\n# Run conserved markers>conserved_markers <- list() # to save results into a list\n>clts <- seu@meta.data$seurat_clusters %>% levels(.) # all the clts to loop over\n>for ( clt in clts ) { # loop over the clts and get conserved markers\nclt_name <- paste0(\"clt_\", clt)\n\u00a0\u00a0clt_no <- as.numeric(clt)\n\u00a0\u00a0cell_no_gt1\u00a0<- seu@meta.data %>%\n\u00a0\u00a0\u00a0\u00a0filter(orig.ident\u00a0== \"Gt1\"\u00a0& seurat_clusters\u00a0== clt) %>%\n\u00a0\u00a0\u00a0\u00a0nrow(.)\n\u00a0\u00a0cell_no_gt2\u00a0<- seu@meta.data %>%\n\u00a0\u00a0\u00a0\u00a0filter(orig.ident\u00a0== \"Gt2\"\u00a0& seurat_clusters\u00a0== clt) %>%\n\u00a0\u00a0\u00a0\u00a0nrow(.)\n\u00a0\u00a0if ( (cell_no_gt1\u00a0>= 3)\u00a0& (cell_no_gt2\u00a0>= 3) ) {\n\u00a0\u00a0\u00a0\u00a0set.seed(1024)\nconserved_markers[[clt_name]] <- FindConservedMarkers(seu, ident.1\u00a0= clt_no, grouping.var\u00a0= \"orig.ident\", verbose\u00a0= FALSE)\n\u00a0\u00a0write.table(x\u00a0= cbind(\"Gene_id\"\u00a0= rownames(conserved_markers[[clt_name]]), conserved_markers[[clt_name]]), file\u00a0= paste(conserved_markers_folder, paste0(clt_name, \"_conserved_markers.tsv\"), sep\u00a0= \"/\"), sep\u00a0= \"\u2216t\", row.names\u00a0= FALSE, quote\u00a0= FALSE)\n}\n## Join conserved markers into one table\n>conserved_markers_tbl <- NULL\n>for ( clt in names(conserved_markers) ) { # loop over clusters\n\u00a0\u00a0sub_df <- conserved_markers[[clt]]\n\u00a0\u00a0col_names <- colnames(sub_df)\n\u00a0\u00a0sub_df[,\"Gene_id\"] <- rownames(conserved_markers[[clt]])\n\u00a0\u00a0sub_df[,\"Cluster\"] <- clt\n\u00a0\u00a0col_order <- c(\"Cluster\", \"Gene_id\", col_names)\n\u00a0\u00a0sub_df <- sub_df[,col_order]\n\u00a0\u00a0# check if colnames match to just rbind\n\u00a0\u00a0if ( is.null(conserved_markers_tbl) ) {\n\u00a0\u00a0\u00a0\u00a0conserved_markers_tbl <- sub_df\n\u00a0\u00a0} else {\n\u00a0\u00a0\u00a0\u00a0stopifnot( all( colnames(conserved_markers_tbl)\u00a0== col_order ) )\n\u00a0\u00a0\u00a0\u00a0conserved_markers_tbl <- rbind( conserved_markers_tbl, sub_df )\n\u00a0\u00a0}\n}\nUse tables of conserved markers to define clusters identity:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2918-Fig9.jpg\nFigure\u00a09. UMAP plots showing Pcc stage-specific gene expression\nBased on the expression pattern different clusters are assigned to specific stages of Pcc. Figure\u00a0reprinted with permission from Ramos, Ademolue et\u00a0al. (2022).1[href=https://www.wicell.org#bib1]Note: To understand how the transcriptomes are segregated, enquire their identity in each cluster of conserved markers in both samples. This will allow, for example, ascribing different clusters to different life cycle stages and probe for effects on Plasmodium life cycle progression and development. For this, we relied on the publicly available information on Plasmodium stage specific gene expression to enquire whether the topmost expressed genes per cluster were specifically expressed in a particular stage of the life cycle (Figure\u00a09[href=https://www.wicell.org#fig9]).\nIdentify the top 25 genes expressed in each cluster.\nNote: We found this number to be enough to provide a defined signature of specific blood stages of Plasmodium development. Of note, some transcripts are expressed in most developmental stages, while others are specific for certain stages (e.g., gametocytes). However, the number of genes analyzed to assure a correct ascribing of specific developmental stages to each cluster will depend on each dataset. Most of the publicly available data for Plasmodium stage-specific gene expression, generated from either bulk or single cell RNAseq, refers to P.\u00a0falciparum, P.\u00a0vivax or P.\u00a0berghei in\u00a0vitro. To analyze the stage-specificity of Pcc gene sets, it is required to find the orthologues in these parasite species. Use available platforms, e.g., PlasmoDB (www.plasmodb.org[href=http://www.plasmodb.org]), to identify P.\u00a0falciparum or P.\u00a0berghei. orthologues of the genes of interest. Assess gene expression throughout the Plasmodium life cycle. Use publicly available data to enquire on stage-specificity of gene expression. We found that Malaria Cell Atlas (www.malariacellatlas.org[href=http://www.malariacellatlas.org]) to provide an excellent compilation of gene expression from bulk (smatseq2) or single cell (Chromium 10\u00d7) data acquired from different life cycle stages of Plasmodium spp., including P.\u00a0falciparum or P.\u00a0berghei.\nConfirm Plasmodium stage-specific gene expression for each cluster.Critical: Some clusters may have genes expressed typically in different Plasmodium developmental stages, indicating: i) Developmental stage transition (e.g., from rings to trophozoite where ring-stage specific genes are co-expressed with metabolic genes, typically highly expressed in trophozoites); ii) Sexual commitment (e.g., gene expression signature corresponding to asexual stages overlaps with gene signatures associated with parasite sexual commitment, suggesting a transition from merozoites, rings or trophozoites into gametocytes).\nAnalyze differential gene expression (DGE) across samples for each cluster using the function \u2018FindMarkers()\u2019.\nUse the identity of the cells belonging to each one of the two samples, Gt2 or Gt1, by cluster to compare genes differentially expressed across each cluster.\nNote: By default, this function uses the method Wilcoxon Rank Sum test on the normalized gene expression data.\n## DGE: Gt2 vs Gt1\n# Add metadata field to hold sample type and cluster id\u00a0& change the identity to this\n>stopifnot(DefaultAssay(seu)\u00a0== \"RNA\")\n>seu[[\"treat\"]] <- paste(seu$orig.ident, Idents(seu), sep\u00a0= \"_\")\n>seu[[\"clusters\"]] <- Idents(seu)\n>Idents(seu) <- seu$treat\n# create folder directory to save files\n>dge_table_folder <- \"../results/int_28_05_21/tables/dge_tables\"\n>if ( ! dir.exists(dge_table_folder) ) dir.create(dge_table_folder)\n# loop over clusters\u00a0& do DGE for each cluster among samples\n>dge <- list()\n>no_int_clts <- levels(seu$clusters)\n>for ( clt in no_int_clts ) { # loop over cluster\n\u00a0\u00a0cell_no_gt1\u00a0<- seu@meta.data %>%\n\u00a0\u00a0\u00a0\u00a0filter(orig.ident\u00a0== \"Gt1\"\u00a0& clusters\u00a0== clt) %>%\n\u00a0\u00a0\u00a0\u00a0nrow(.)\n\u00a0\u00a0cell_no_gt2\u00a0<- seu@meta.data %>%\n\u00a0\u00a0\u00a0\u00a0filter(orig.ident\u00a0== \"Gt2\"\u00a0& clusters\u00a0== clt) %>%\n\u00a0\u00a0\u00a0\u00a0nrow(.)\n\u00a0\u00a0if ( (cell_no_gt2\u00a0>= 3)\u00a0& (cell_no_gt1\u00a0>= 3) ) {\n\u00a0\u00a0\u00a0\u00a0clt_name <- paste0(\"clt_\", clt) # name of the current cluster\n\u00a0\u00a0\u00a0\u00a0ctrl_cells <- paste0(\"Gt1_\", clt) # control/reference name of the current groups of cells to compare\n\u00a0\u00a0\u00a0\u00a0treat_cells <- paste0(\"Cre3_\", clt) # treatment name of the current groups of cells to compare\n\u00a0\u00a0\u00a0\u00a0set.seed(1024)dge[[clt_name]] <- FindMarkers(seu, ident.1\u00a0= treat_cells, ident.2\u00a0= ctrl_cells, verbose\u00a0= FALSE) # do dge\n\u00a0\u00a0\u00a0\u00a0write.table(x\u00a0= cbind(\"Geneid\"\u00a0= rownames(dge[[clt_name]]), dge[[clt_name]]), file\u00a0= paste(dge_table_folder, paste0(clt_name, \"_Gt2_vs_Gt1_dge_table.tsv\"), sep\u00a0= \"/\"), sep\u00a0= \"\u2216t\", quote\u00a0= FALSE, row.names\u00a0= FALSE\n\u00a0\u00a0\u00a0\u00a0)\n\u00a0\u00a0}\n}\n# restore Seurat identity to clusters\n>Idents(seu) <- seu$clusters\n## Merge DGE tables in order to plot them below\n>dge_all <- NULL\n>for ( clt in names(dge) ) {\n\u00a0\u00a0sub_df <- dge[[clt]]\n\u00a0\u00a0col_names <- colnames(sub_df)\n\u00a0\u00a0sub_df[,\"Geneid\"] <- rownames(dge[[clt]])\n\u00a0\u00a0sub_df[,\"Cluster\"] <- clt\n\u00a0\u00a0col_order <- c(\"Cluster\", \"Geneid\", col_names)\n\u00a0\u00a0sub_df <- sub_df[,col_order]\n\u00a0\u00a0if ( is.null(dge_all) ) {\n\u00a0\u00a0\u00a0\u00a0dge_all <- sub_df\n\u00a0\u00a0} else {\n\u00a0\u00a0\u00a0\u00a0stopifnot( all( colnames(sub_df)\u00a0== colnames(dge_all) ) )\n\u00a0\u00a0\u00a0\u00a0dge_all <- rbind(dge_all, sub_df)\n\u00a0\u00a0}\n}\n>write.table(x\u00a0= dge_all, file\u00a0= paste(dge_table_folder, \"Cre3_vs_Lox2_dge_table_for_all_clusters.tsv\", sep\u00a0= \"/\"), sep\u00a0= \"\u2216t\", quote\u00a0= FALSE, row.names\u00a0= FALSE)\nHighlight the results in volcano plots and heatmaps.\nUse gprofiler2\u00a0R package to perform functional enrichment analysis on the differentially expressed genes (i.e., up- and down-regulated) between samples by cluster (v.0.2.07[href=https://www.wicell.org#bib6]), an interface to the g:Profiler web browser tool.\nApply the function \u2018gost()\u2019 to perform functional enrichment analysis, based on genes up- or down-regulated, between each pairwise comparison, against the annotated genes (\u2018domain_scope\u00a0= \u201cannotated\u201d\u2019) of the organism P.\u00a0chabaudi (\u2018organism\u00a0= \"pchabaudi\"\u2019).\nOrder gene lists by increasing adjusted p-value (\u2018ordered_query\u00a0= TRUE\u2019) to generate a GSEA (Gene Set Enrichment Analysis) style p-values.\nNote: This allows to start the enrichment testing from the topmost biological relevant genes with subsequent tests involving larger sets of genes. In addition, only statistically significant (\u2018user_threshold\u00a0= 0.05\u2019) enriched functions are returned (\u2018significant\u00a0= TRUE\u2019) after multiple testing corrections with the default method g:SCS (\u2018correction_method\u00a0= \"g_SCS\"\u2019).Add evidence codes to the final result (\u2018evcodes\u00a0= TRUE\u2019). Functional databases available to P. chabaudi are the following: Gene Ontology (GO or by branch GO:MF, GO:BP, GO:CC), KEGG.\nSubject all the clusters to functional enrichment analysis.\nParse the DGE results for gprofiler2 as follows:\n## Get up and down gene lists and parse them\n>reg_gene_list <- list()\n>for ( clt in names(dge) ) { # select dge by cluster and retrieve name\n\u00a0\u00a0sub_df <- dge[[clt]]\n\u00a0\u00a0sub_df[,\"Geneid\"] <- rownames(sub_df)\n\u00a0\u00a0gene_ids_up <- sub_df %>%\n\u00a0\u00a0\u00a0\u00a0filter(p_val_adj\u00a0<\u00a00.05\u00a0& avg_log2FC\u00a0>\u00a00) %>%\n\u00a0\u00a0\u00a0\u00a0arrange(p_val_adj) %>%\n\u00a0\u00a0\u00a0\u00a0pull(Geneid)\n\u00a0\u00a0gene_ids_down <- sub_df %>%\n\u00a0\u00a0\u00a0\u00a0filter(p_val_adj\u00a0<\u00a00.05\u00a0& avg_log2FC\u00a0<\u00a00) %>%\n\u00a0\u00a0\u00a0\u00a0arrange(p_val_adj) %>%\n\u00a0\u00a0\u00a0\u00a0pull(Geneid)\n\u00a0\u00a0reg_gene_list[[clt]] <- list()\n\u00a0\u00a0reg_gene_list[[clt]][[\"up\"]] <- gsub(pattern\u00a0= \"-\", replacement\u00a0= \"_\", x\u00a0= gene_ids_up)\n\u00a0\u00a0reg_gene_list[[clt]][[\"down\"]] <- gsub(pattern\u00a0= \"-\", replacement\u00a0= \"_\", x\u00a0= gene_ids_down)\n}\n# create folders\n>r_object_folder <- \"../results/int_28_05_21/R_objects\"\n>if ( ! dir.exists(r_object_folder) ) dir.create(r_object_folder)\n>func_enrich_folder <- \"../results/int_28_05_21/tables/functional_enrichment\"\n>if ( ! dir.exists(func_enrich_folder) ) dir.create(func_enrich_folder)\nSubmit the list of differentially up- and down-regulated genes by cluster to g:profiler2 as follows:\n### Functional enrichment of DEG\n# Run gprofiler2\n>func_enrich <- list()\n>set_base_url(\"https://biit.cs.ut.ee/gprofiler_archive3/e102_eg49_p15\")\n>for ( clt in names(reg_gene_list) ){ # loop over list and do functional enrichment\n\u00a0\u00a0#print(get_base_url())\n\u00a0\u00a0func_enrich[[clt]] <- list()\n\u00a0\u00a0set.seed(1024)\n\u00a0\u00a0func_enrich[[clt]][[\"up\"]] <- gost(query\u00a0= reg_gene_list[[clt]][[\"up\"]], organism\u00a0= \"pchabaudi\", ordered_query\u00a0= TRUE, multi_query\u00a0= FALSE, significant\u00a0= TRUE, exclude_iea\u00a0= FALSE, measure_underrepresentation\u00a0= FALSE, evcodes\u00a0= TRUE, user_threshold\u00a0= 0.05, correction_method\u00a0= \"g_SCS\", domain_scope\u00a0= \"annotated\", custom_bg\u00a0= NULL, numeric_ns\u00a0= \"\", sources\u00a0= NULL)\n\u00a0\u00a0if ( ! is.null(func_enrich[[clt]][[\"up\"]]$result) ) {\n\u00a0\u00a0\u00a0\u00a0sub_df_up <- func_enrich[[clt]][[\"up\"]]$result %>%\n\u00a0\u00a0\u00a0\u00a0apply(X\u00a0= ., MARGIN\u00a0= 2, FUN\u00a0= function(x) as.character(x))\n\u00a0\u00a0\u00a0\u00a0write.table(x\u00a0= sub_df_up,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0file\u00a0= paste(func_enrich_folder, paste0(clt, \"_up_functional_enrichment_table.tsv\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sep\u00a0= \"/\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0quote\u00a0= FALSE, sep\u00a0= \"\u2216t\", row.names\u00a0= FALSE, col.names\u00a0= TRUE)\nset.seed(1024)func_enrich[[clt]][[\"down\"]] <- gost(query\u00a0= reg_gene_list[[clt]][[\"down\"]], organism\u00a0= \"pchabaudi\", ordered_query\u00a0= TRUE, multi_query\u00a0= FALSE, significant\u00a0= TRUE, exclude_iea\u00a0= FALSE, measure_underrepresentation\u00a0= FALSE, evcodes\u00a0= TRUE, user_threshold\u00a0= 0.05, correction_method\u00a0= \"g_SCS\", domain_scope\u00a0= \"annotated\", custom_bg\u00a0= NULL, numeric_ns\u00a0= \"\", sources\u00a0= NULL)\n\u00a0\u00a0if ( ! is.null(func_enrich[[clt]][[\"down\"]]$result) ) {\n\u00a0\u00a0\u00a0\u00a0sub_df_down <- func_enrich[[clt]][[\"down\"]]$result %>%\n\u00a0\u00a0\u00a0\u00a0apply(X\u00a0= ., MARGIN\u00a0= 2, FUN\u00a0= function(x) as.character(x))\n\u00a0\u00a0\u00a0\u00a0write.table(x\u00a0= sub_df_down,\nfile\u00a0= paste(func_enrich_folder, paste0(clt, \"_down_functional_enrichment_table.tsv\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0sep\u00a0= \"/\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0quote\u00a0= FALSE, sep\u00a0= \"\u2216t\", row.names\u00a0= FALSE, col.names\u00a0= TRUE)\n\u00a0\u00a0\u00a0\u00a0}\n}\n# Create R object\n>saveRDS(object\u00a0= func_enrich, file\u00a0= paste(r_object_folder, \"func_enrich.rds\", sep\u00a0= \"/\"))\nThe results were explored through tables and plots.\nThe main integrated object was exported.\n>saveRDS(object\u00a0= seu, file\u00a0= paste(r_object_folder, \"seu.rds\", sep\u00a0= \"/\"))\nExplore the results through tables and plots.\nExport the main integrated object.\nNote: To perform RNA velocity analysis, which uses the ratio of spliced/unspliced transcripts to derive cell state transitions and infers pseudotime and cell trajectory the required inputs need to be obtained. The two input requirements are: spliced/unspliced matrices from the alignment sample files and the UMAP embedding. The result is the inference of cell state transitions projected onto UMAP space that allows to explore the cell development trajectory.\nPerform RNA velocity analysis of scRNA-seq samples of Pcc with RNA by projecting the integrated UMAP obtained with Seurat analysis described above.\nPerforme RNA velocity analysis in two steps, following the guidelines from https://github.com/basilkhuder/Seurat-to-RNA-Velocity[href=https://github.com/basilkhuder/Seurat-to-RNA-Velocity]:\nGenerate the \u2018.loom\u2019 file with the spliced and unspliced count matrices\nEstimate the RNA velocity from the spliced and unspliced matrices by using the velocyto (v.0.17.178) CLI (Command-Line Interface) tool and running the bash script:\n> ./velocyto_script.sh\u00a0&> velocyto_log.log\nThe content of the \u2018velocyto_script.sh\u2019 bash script is summarized below:\n## Variables\n# PATH\n>AFS_PATH=/afs/igc.gulbenkian.pt/folders/UBI/PROJECTS/UBI-2021/ongoing/2012_miguel_elisa# GTF and bam files for Gt2 and Gt1 samples\n>SAMPLES='Gt2 Gt1'\n>bam_gt2=${AFS_PATH}/data/ftp01.igc.gulbenkian.pt/Gt2_count_full/outs/possorted_genome_bam.bam\n>bam_gt1=${AFS_PATH}/data/ftp01.igc.gulbenkian.pt/Gt1_count_full/outs/possorted_genome_bam.bam\n>GTF=${AFS_PATH}/data/ftp01.igc.gulbenkian.pt/Pchabaudi2/genes/genes.gtf\n# activate conda first\n>source /home/agsousa/miniconda3/etc/profile.d/conda.sh\n>conda activate velocyto\n>parallel -v \u2216\n\u00a0\u00a0velocyto run10x -@ 8 --samtools-memory 5000 \u2216\n\u00a0\u00a0${AFS_PATH}/data/ftp01.igc.gulbenkian.pt/{}_count_full \u2216\n\u00a0\u00a0$GTF ::: $SAMPLES >> velocyto_script.log 2>&1\n>conda deactivate\nProcess both samples in parallel with GNU parallel (v.2016122225[href=https://www.wicell.org#bib23]).\nNote: Sample folders generated with the cellranger pipeline are provided as input, which contain the folder \u2018outs\u2019 with the \u2018.bam\u2019 alignment file and the filtered barcodes. In addition, the GTF file of Pcc is needed.\nBefore estimating velocities, export the identity of the cells, clusters as well as the UMAP embeddings from the integrated single-cell data to project the velocities. This data is exported in R as follows:\n## Import packages\n>library(\"Seurat\", quietly\u00a0= TRUE)## Import integrated Seurat object: Gt2 and Gt1\n# import obj integrated\n>seu <- readRDS(file\u00a0= \"../results/int/R_objects/seu.rds\")\n# Seurat: split objects by sample\n>sobjList <- SplitObject(seu, split.by\u00a0= \"orig.ident\")\n# dir to save\n>seurat_data <- \"../results/velocyto/seurat_data\"\n>if ( ! dir.exists(seurat_data) ) dir.create(seurat_data, recursive\u00a0= TRUE)\n# save cell_ids for both samples\n>write.csv(Cells(sobjList$Gt2), file\u00a0= paste(seurat_data, \"gt2_samp_cellID_obs.csv\", sep\u00a0= \"/\"), row.names\u00a0= FALSE)\n>write.csv(Cells(sobjList$Gt1), file\u00a0= paste(seurat_data, \"gt1_samp_cellID_obs.csv\", sep\u00a0= \"/\"), row.names\u00a0= FALSE)\n# save UMAP embeddings for both samples\n>write.csv(Embeddings(sobjList$Gt2, reduction\u00a0= \"umap\"), file\u00a0= paste(seurat_data,\"gt2_UMAP_cell_embeddings.csv\", sep\u00a0= \"/\"))\nwrite.csv(Embeddings(sobjList$Gt1, reduction\u00a0= \"umap\"), file\u00a0= paste(seurat_data,\"lox2_UMAP_cell_embeddings.csv\", sep\u00a0= \"/\"))\nEstimate RNA velocity with the stochastic model option from the spliced and unspliced matrices using the python package scvelo (v.0.2.29) through jupyter notebook.\nProject velocities onto the integrated UMAP embeddings as described below.\n# save cluster information\n>write.csv(sobjList$Gt2@meta.data[,\"seurat_clusters\", drop\u00a0= FALSE],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0file\u00a0= paste(seurat_data,\"gt2_clusters.csv\", sep\u00a0= \"/\"))\n>write.csv(sobjList$Gt1@meta.data[,\"seurat_clusters\", drop\u00a0= FALSE],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0file\u00a0= paste(seurat_data,\"gt1_clusters.csv\", sep\u00a0= \"/\"))\n# export the colors used in Seurat UMAP clusters\n# Gt2\n>gt2_umap_colors <- DimPlot(sobjList$Gt2)>gt2_umap_colors <- ggplot2::ggplot_build(gt2_umap_colors)\n>gt2_umap_colors <- gt2_umap_colors$data[[1]]\n>gt2_umap_colors <- gt2_umap_colors[,\"colour\", drop\u00a0= FALSE]\n>gt2_umap_colors[,\"cluster\"] <- row.names(sobjList$Gt2@meta.data)\n>write.csv(gt2_umap_colors, file\u00a0= paste(seurat_data,\"gt2_umap_colors.csv\", sep\u00a0= \"/\"), row.names\u00a0= FALSE)\n# Gt1\n>gt1_umap_colors <- DimPlot(sobjList$Gt1)\n>gt1_umap_colors <- ggplot2::ggplot_build(gt1_umap_colors)\n>gt1_umap_colors <- gt1_umap_colors$data[[1]]\n>gt1_umap_colors <- gt1_umap_colors[,\"colour\", drop\u00a0= FALSE]\n>gt1_umap_colors[,\"cluster\"] <- row.names(sobjList$Gt1@meta.data)\n>write.csv(gt1_umap_colors, file\u00a0= paste(seurat_data,\"gt1_umap_colors.csv\", sep\u00a0= \"/\"), row.names\u00a0= FALSE)\n# Import packages\n>import os\n>import random\n>import anndata\n>import scvelo as scv\n>import pandas as pd\n>import numpy as np\n>import matplotlib as plt\n#%load_ext rpy2.ipython\n# Keep reproducibility\n>random.seed(1024)\n## import loom files\n# path to the loom files\n>gt2_loom_file\u00a0= \"../data/ftp01.igc.gulbenkian.pt/Gt2_count_full/velocyto/Gt2_count_full.loom\"\n>gt1_loom_file\u00a0= \"../data/ftp01.igc.gulbenkian.pt/Gt1_count_full/velocyto/Gt1_count_full.loom\"\n>sample_gt2\u00a0= anndata.read_loom(gt2_loom_file)\n>sample_gt1\u00a0= anndata.read_loom(gt1_loom_file)\n## parse cell ids from loom files to match the Seurat cell ids\n# gt2\n>cell_ids_gt2\u00a0= sample_gt2.obs_names.to_list()\n# remove the prefix 'Gt2_count_full:' and sufix 'x' from the cell ids of the loom file\n# add the sufix \"-1_1\"\n>cell_ids_gt2_parsed\u00a0= [ cell[16:32]\u00a0+ \"-1_1\" for cell in cell_ids_gt2 ]\n>sample_gt2.obs_names\u00a0= cell_ids_gt2_parsed\n# gt1\n>cell_ids_gt1\u00a0= sample_gt1.obs_names.to_list()\n# remove the prefix 'Gt1_count_full:' and sufix 'x' from the cell ids of the loom file\n# add the sufix \"-1_2\"\n>cell_ids_gt1_parsed\u00a0= [ cell[16:32]\u00a0+ \"-1_2\" for cell in cell_ids_gt1 ]\n>sample_gt1.obs_names\u00a0= cell_ids_gt1_parsed\n## import seurat metadata\n# cell ids\n>seurat_data\u00a0= \"../results/velocyto/seurat_data\"\n>sample_gt2_obs\u00a0= pd.read_csv(seurat_data\u00a0+ \"/\"\u00a0+ \"gt2_samp_cellID_obs.csv\")\n>sample_gt1_obs\u00a0= pd.read_csv(seurat_data\u00a0+ \"/\"\u00a0+ \"gt1_samp_cellID_obs.csv\")\n## import umap embeddings\n>umap_cord_gt2\u00a0= pd.read_csv(seurat_data\u00a0+ \"/\"\u00a0+ \"gt2_UMAP_cell_embeddings.csv\")\n>umap_cord_gt1\u00a0= pd.read_csv(seurat_data\u00a0+ \"/\"\u00a0+ \"gt1_UMAP_cell_embeddings.csv\")\n## import cell_clusters\n>cell_clusters_gt2\u00a0= pd.read_csv(seurat_data\u00a0+ \"/\"\u00a0+ \"gt2_clusters.csv\")\n>cell_clusters_gt1\u00a0= pd.read_csv(seurat_data\u00a0+ \"/\"\u00a0+ \"gt1_clusters.csv\")\n## filter loom files based on Seurat cell ids (filtered)\n>sample_gt2\u00a0= sample_gt2[np.isin(sample_gt2.obs.index, sample_gt2_obs[\"x\"])] # 15543 cells to 15537\n>sample_gt1\u00a0= sample_gt1[np.isin(sample_gt1.obs.index, sample_gt1_obs[\"x\"])] # 5204 cells to 5176\n### Parse data\n## rename column\n# Gt2\n>sample_gt2_index\u00a0= pd.DataFrame(sample_gt2.obs.index)>sample_gt2_index\u00a0= sample_gt2_index.rename(columns\u00a0= {0:'Cell ID'})\n# Gt1\n>sample_gt1_index\u00a0= pd.DataFrame(sample_gt1.obs.index)\n>sample_gt1_index\u00a0= sample_gt1_index.rename(columns\u00a0= {0:'Cell ID'})\n# rename column from UMAP df\n>umap_cord_gt2\u00a0= umap_cord_gt2.rename(columns\u00a0= {'Unnamed: 0':'Cell ID'})\n>umap_cord_gt1\u00a0= umap_cord_gt1.rename(columns\u00a0= {'Unnamed: 0':'Cell ID'})\n# merge UMAP and index data\n>umap_ordered_gt2\u00a0= sample_gt2_index.merge(umap_cord_gt2, on\u00a0= \"Cell ID\")\n>umap_ordered_gt1\u00a0= sample_gt1_index.merge(umap_cord_gt1, on\u00a0= \"Cell ID\")\n# remove 1st 'Cell ID' column and add UMAP coordinates\n# Gt2\n>umap_ordered_gt2\u00a0= umap_ordered_gt2.iloc[:,1:]\n>sample_gt2.obsm['X_umap']\u00a0= umap_ordered_gt2.values\n# Gt1\n>umap_ordered_gt1\u00a0= umap_ordered_gt1.iloc[:,1:]\n>sample_gt1.obsm['X_umap']\u00a0= umap_ordered_gt1.values\n## add cluster colors\n# Gt2\n>cell_clusters_gt2\u00a0= cell_clusters_gt2.rename(columns\u00a0= {'Unnamed: 0':'Cell ID'})\n>cluster_ordered_gt2\u00a0= sample_gt2_index.merge(cell_clusters_gt2, on\u00a0= \"Cell ID\")\n>cluster_ordered_gt2\u00a0= pd.Series(cluster_ordered_gt2['seurat_clusters'].apply(str).values, index\u00a0= cluster_ordered_gt2['Cell ID'])\n#cluster_ordered_gt2\u00a0= cluster_ordered_gt2.iloc[:,1:]\n>sample_gt2.obs['clusters']\u00a0= cluster_ordered_gt2\n>sample_gt2.obs['clusters']\u00a0= sample_gt2.obs['clusters'].astype('category')\n>sample_gt2.uns['Cluster_colors']\u00a0= cluster_ordered_gt2.values\n# Gt1\n>cell_clusters_gt1\u00a0= cell_clusters_gt1.rename(columns\u00a0= {'Unnamed: 0':'Cell ID'})\n>cluster_ordered_gt1\u00a0= sample_gt1_index.merge(cell_clusters_gt1, on\u00a0= \"Cell ID\")\n>cluster_ordered_gt1\u00a0= pd.Series(cluster_ordered_gt1['seurat_clusters'].apply(str).values, index\u00a0= cluster_ordered_gt1['Cell ID'])\n#cluster_ordered_gt1\u00a0= cluster_ordered_gt1.iloc[:,1:]\n>sample_gt1.obs['clusters']\u00a0= cluster_ordered_gt1\n>sample_gt1.obs['clusters']\u00a0= sample_gt1.obs['clusters'].astype('category')\n>sample_gt1.uns['Cluster_colors']\u00a0= cluster_ordered_gt1.values\n## RNA velocity\n# define colors for both\n>color\u00a0= [\"#F8766D\", \"#D39200\", \"#93AA00\", \"#00BA38\", \"#00C19F\", \"#00B9E3\", \"#619CFF\", \"#DB72FB\", \"#FF61C3\"]\n# Gt2\n>scv.set_figure_params(facecolor=\"white\", figsize=(8, 8))\n>random.seed(1024)\n>scv.pp.filter_and_normalize(sample_gt2)\n>scv.pp.moments(sample_gt2)\n>scv.tl.velocity(sample_gt2, mode\u00a0= \"stochastic\")\n>scv.tl.velocity_graph(sample_gt2)\n>scv.set_figure_params(facecolor=\"white\", figsize=(8, 8), fontsize\u00a0= 10, dpi\u00a0= 100, dpi_save\u00a0= 300)\n>scv.pl.velocity_embedding(sample_gt2, basis\u00a0= 'umap', arrow_size\u00a0= 30, arrow_length\u00a0= 3, save\u00a0= \"gt2_RNA_velocity_cells_UMAP.svg\")\n>scv.pl.velocity_embedding_stream(sample_gt2, layer=['velocity'], palette\u00a0= color, size\u00a0= 30, save\u00a0= 'gt2_RNA_velocity_stream_UMAP.svg')\n# Gt1\n>scv.set_figure_params(facecolor=\"white\", figsize=(8, 8))\n>random.seed(1024)\n>scv.pp.filter_and_normalize(sample_gt1)\n>scv.pp.moments(sample_gt1)\n>scv.tl.velocity(sample_gt1, mode\u00a0= \"stochastic\")\n>scv.tl.velocity_graph(sample_gt1)\n>scv.set_figure_params(facecolor=\"white\", figsize=(8, 8), fontsize\u00a0= 10, dpi\u00a0= 100, dpi_save\u00a0= 300)\n>scv.pl.velocity_embedding(sample_gt1, basis\u00a0= 'umap', arrow_size\u00a0= 30, arrow_length\u00a0= 3, save\u00a0= \"gt1_RNA_velocity_cells_UMAP.svg\")\n>scv.pl.velocity_embedding_stream(sample_gt1, layer=['velocity'], size\u00a0= 30, palette\u00a0= color,save\u00a0= 'lox2_RNA_velocity_stream_UMAP.svg')", "Step-by-step method details\nStep-by-step method details\nGenerally speaking, the execution of the KDBBN project is composed of three stages, data standardization, data preprocessing, and model implementation. To illustrate these stages, we offer an illustrative example based on taking Dataset 1 as the training and test sets as well as Dataset 2 as the validation set.\nData standardization\nTiming: 1 h\nThe following steps explain the process of transforming an original dicom-formatted CT dataset into the dataset used in a deep learning model. In the original CT dataset, several images contain 4 subfigures (Figure\u00a01[href=https://www.wicell.org#fig1]), in which only the thin-slice CT image (Figure\u00a02[href=https://www.wicell.org#fig2]) is needed. Subfigure extraction has also been included at this stage.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig1.jpg\nFigure\u00a01. An example CT image containing 4 subfigures\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig2.jpg\nFigure\u00a02. Extracted subfigure\nCheck if there are some images containing 4 subfigures in the dataset, and extract the needed subfigure. Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], 3[href=https://www.wicell.org#sec6.5], and 4[href=https://www.wicell.org#sec6.7].\nOpen the \u2018readDCM.py\u2019 in the \u2018./lung_code/trans_data\u2019 folder.\nNote: The script has three custom parameters, in which the \u2018path\u2019 is the path of the original input dataset, \u2018resultpath1\u2019 and \u2018resultpath2\u2019 are two empty folders prepared for saving the outputs. There are no subordinate relationships between the three folders.\nRun the \u2018readDCM.py\u2019 after setting the custom parameters, or in command line run:\n>python readDCM.py -p [path] -r1 [resultpath1] -r2 [resultpath2]\nOpen the \u2018crop4.py\u2019 in the \u2018./lung_code/trans_data\u2019 folder. Run the \u2018crop4.py\u2019 after setting the \u2018path\u2019 the same as \u2018resultpath2\u2019, or in command line run:\n>python crop4.py -p [resultpath2]\nTransform dicom files to standardized jpg files and change the dataset division. The original dataset is divided by the patient id (Figure\u00a03[href=https://www.wicell.org#fig3]), while the dataset used for the deep learning model should be divided according to the adenocarcinoma categories (Figure\u00a04[href=https://www.wicell.org#fig4]). Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], 3[href=https://www.wicell.org#sec6.5], and 4[href=https://www.wicell.org#sec6.7].Open the \u2018trans2jpg.py\u2019 in the \u2018./lung_code/trans_data\u2019 folder.\nNote: The script has four custom parameters, in which the \u2018metadata\u2019 is the path of the dataset metadata, and \u2018file_dir1\u2019 is the same as \u2018resultpath1\u2019, while \u2018file_dir2\u2019 is the same as \u2018resultpath2\u2019 in step 1, and \u2018re_dir\u2019 is one empty folder prepared for saving the outputs. There is no subordinate relationship between the two folders.\nRun the \u2018trans2jpg.py\u2019 after setting the custom parameters, or in command line run:\n>python trans2jpg.py -m [metadata] -f1 [resultpath1] -f2 [resultpath2] -r [re_dir]\nNote: The standardized dataset will be located in the \u2018re_dir\u2019.\nRepeat steps 1 and 2 to standardize other datasets (e.g., validation dataset).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig3.jpg\nFigure\u00a03. Originally organized dataset, divided by patient id\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig4.jpg\nFigure\u00a04. Standardized dataset, classified by lung adenocarcinoma categories\nData preprocessing\nTiming: 2 h\nThese steps correspond to the preprocessing stage in the original publication (Chen et\u00a0al., 2022[href=https://www.wicell.org#bib1]), including the segmentation unit and rebalancing unit. The parameters miu and miut in step 4 correspond to the parameters \u03bc and \u03c1 in the original publication.\nRun the three extractors in the segmentation unit (Figure\u00a05[href=https://www.wicell.org#fig5]). Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], and 3[href=https://www.wicell.org#sec6.5].\nOpen the \u2018./lung_code/preprocessing\u2019 folder.\nOpen the \u2018contour.py\u2019 (corresponding to the black SROI images) in the folder.\nNote: The script has two custom parameters, in which \u2018re_dir\u2019 is the same as that in step 2, and \u2018data_dir2\u2019 is one empty folder prepared for saving the outputs.\nRun the \u2018contour.py\u2019 after setting the custom parameters, or in command line run:\n>python contour.py -r [re_dir] -d [data_dir2]\nOpen the \u2018background.py\u2019 (corresponding to the white SROI images) in the folder.\nNote: The script has two custom parameters, in which \u2018data_dir2\u2019 is the same as that in step b, and \u2018data_dir3\u2019 is one empty folder prepared for saving the output.Run the \u2018background.py\u2019 after setting the custom parameters, or in command line run:\n>python background.py -d2 [data_dir2] -d3 [data_dir3]\nOpen the \u2018crf.py\u2019 (corresponding to the CRF extractor) in the folder.\nNote: The script has two custom parameters, in which \u2018re_dir\u2019 is the same as that in step 2, and \u2018data_dir1\u2019 is one empty folder prepared for saving the output.\nRun the \u2018crf.py\u2019 after setting the custom parameters, or in command line run:\n>python crf.py -r [re_dir] -d1 [data_dir1]\nOpen the \u2018cut.py\u2019 (corresponding to the Crop Background extractor) in the folder.\nNote: The script has four custom parameters, in which the \u2018re_dir\u2019 is the same as that in step 2, \u2018cut_dir\u2019 is an empty folder prepared for output, while \u2018miu\u2019 and \u2018miut\u2019 are two parameters to finetune the extractor. Create three new paths \u2018data_dir4\u2019, \u2018data_dir5\u2019, and \u2018data_dir6\u2019 for three empty folders.\nRun the \u2018cut.py\u2019 after setting the \u2018cut_dir\u2019 the same as \u2018data_dir4\u2019 while setting miu as 180 and miut as 100. Or in command line run:\n>python cut.py -r [re_dir] -c [data_dir4] -m 180 -mt 100\nRun the \u2018cut.py\u2019 after setting the \u2018cut_dir\u2019 the same as \u2018data_dir5\u2019 while setting miu as 200 and miut as 100. Or in command line run:\n>python cut.py -r [re_dir] -c [data_dir4] -m 200 -mt 100\nRun the \u2018cut.py\u2019 after setting the \u2018cut_dir\u2019 the same as \u2018data_dir6\u2019 while setting miu as 230 and miut as 160. Or in command line run:\n>python cut.py -r [re_dir] -c [data_dir4] -m 230 -mt 160\nOpen the \u2018mix.py\u2019.Note: The script has seven paths, in which \u2018data_dir1\u2019, \u2018data_dir2\u2019, \u2018data_dir3\u2019, \u2018data_dir4\u2019, \u2018data_dir5\u2019 and \u2018data_dir6\u2019 are the same as those in the previous steps, and \u2018target_dir\u2019 is one empty folder prepared for saving the output. Moreover, there are six parameters in finetuning the corresponding rebalance unit in the original publication, default values have been set while the users can change them accordingly.\nRun the \u2018mix.py\u2019 after setting the custom parameters, or in command line run:\n>python mix.py -t [target_dir] -d1 [data_dir1] -d2 [data_dir2] -d3 [data_dir3] -d4 [data_dir4] -d5 [data_dir5] -d6 [data_dir6]\nNote: There are no subordinate relationships between the following folders, \u2018re_dir\u2019, \u2018data_dir1\u2019, \u2018data_dir2\u2019, \u2018data_dir3\u2019, \u2018data_dir4\u2019, \u2018data_dir5\u2019, \u2018data_dir6\u2019 and \u2018target_dir\u2019.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig5.jpg\nFigure\u00a05. Visualization of extracted ROI from the lung CT image, for more details please refer to the original publication.\n(A) The ROI extracted by CRF extractor.\n(B) The black ROI extracted by SROI extractor.\n(C) The white ROI extracted by SROI extractor.\n(D\u2013F) are generated by Crop-Background extractor with different miu and miut: (D)\u00a0miu=180, miut>100 (E)\u00a0miu=200, miut>100 (F)\u00a0miu=230, miut>160.\nModel implementation\nTiming: 6\u00a0h or more\nThese steps show details of implementing the KDBBN including training and testing on the bilateral-branch network as well as validating the deep network through a knowledge distillation procedure.\nOpen the \u2018./lung_code\u2019 folder.\nTrain and test the bilateral-branch network. Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], and 3[href=https://www.wicell.org#sec6.5].\nOpen the \u2018implement_concat.py\u2019. If there exists any GPU for training, set \u2018os.environ[\u201cCUDA_VISIBLE_DEVICES\u201d]\u2019 as the serial number of the GPU.\nSet \u2018re_dir\u2019 and \u2018target_dir\u2019 the same as those in the steps before.\nSet \u2018TRAIN\u2019 as TRUE, and create one new path \u2018filepath\u2019 of one empty folder. Run the \u2018implement_concat.py\u2019 after setting the three paths, or in the command line run troubleshooting 5[href=https://www.wicell.org#sec6.9].\n>python implement_concat.py -r [re_dir] -t [target_dir]Note: The best bilateral-branch network model after the specific training epochs is stored in the path \u2018filepath\u2019. The training accuracy of the bilateral-branch network is obtained in this step as well.\nSet \u2018TRAIN\u2019 as FALSE, and set \u2018modelpath\u2019 the same as \u2018filepath\u2019, or any other path of the predicting model. Run the \u2018implement_concat.py\u2019 after setting the \u2018modelpath\u2019, or in the command line run troubleshooting 5[href=https://www.wicell.org#sec6.9].\n>python implement_concat.py -m [modelpath]\nNote: The testing accuracy of the chosen bilateral-branch network model and the confusion matrix of the testing dataset is obtained in this step.\nCritical: The hyperparameters, such as the \u2018batch_size\u2019, \u2018nb_epoch\u2019, \u2018alpha\u2019, etc., have been finetuned and set the best choices as the default values. Or you can set your custom values.\nValidate the bilateral-branch network through a knowledge distillation procedure. Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], and 3[href=https://www.wicell.org#sec6.5].\nOpen the \u2018KD.py\u2019.\nThere are three custom paths. Set \u2018valpath\u2019 as the path of the standardized validation dataset. The \u2018unpath\u2019 is the path of the standardized unlabeled dataset. The \u2018Tmodelpath\u2019 is the path of the best saved bilateral-branch network model in step 6.\nNote: The validation dataset can be any labeled dataset different from the training and test datasets. In the original publication, it can be Dataset 2 or 3 or the validation part in Dataset 1. The unlabeled dataset can be any unlabeled lung adenocarcinoma CT images with lesion area, in the original publication, it is Dataset 4.\nRun the \u2018KD.py\u2019 after setting the paths, or in the command line run troubleshooting 5[href=https://www.wicell.org#sec6.9].\n>python KD.py -v [valpath] -u [unpath] -m [Tmodelpath]\nNote: The validation accuracy of the KDBBN and the confusion matrix of the validation dataset are obtained in this step.\nResults display through CAM heatmap\nTiming: 1 hThe following steps explain the process of visualizing the high-risk area in the CT images through CAM or Grad-CAM heatmap (Selvaraju et\u00a0al., 2017[href=https://www.wicell.org#bib3]). Both CAM and Grad-CAM are designed to locate category-related areas in the image. The final convolution layer output in CAM must be connected to the GAP Layer, which restricts the model construction and may reduce the model accuracy. However, Grad-CAM does not have this restriction and it can be regarded as the generalization of CAM.\nDraw the heatmap through CAM or Grad-CAM (Figure\u00a06[href=https://www.wicell.org#fig6]). Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], and 3[href=https://www.wicell.org#sec6.5].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig6.jpg\nFigure\u00a06. We present the original labels of the CT images which were diagnosed by skilled doctors through pathological examinations, and the corresponding probability scores for 3 categories\nThe detected high-risk area by the framework is also shown by the CAM heatmaps and detection results, for more details please refer to the original publication.\nOpen the \u2018./lung_code\u2019 folder.\nOpen the \u2018cam.py\u2019 for CAM heatmap or \u2018apply_gradcam.py\u2019 for Grad-CAM heatmap.\nThere are three paths in the scripts. Set \u2018inputpath\u2019 as the path of any standardized dataset you want to display, while \u2018modelpath\u2019 is the path of the chosen model. Create one new path \u2018outputpath\u2019 for one empty folder.\nRun the \u2018cam.py\u2019 or \u2018apply_gradcam.py\u2019 chosen in step b after setting the paths, or in command line run:\n>python apply_gradcam.py -i [inputpath] -m [modelpath] -o [outputpath]", "Step-by-step method details\nStep-by-step method details\nCell culture and transfection\nTiming: 1\u00a0day\nADAR fusion plasmids are introduced into cells by transient transfection.\nNote: The following steps were optimized for adherent mouse embryonic fibroblasts and human osteosarcoma cells but may need to be adjusted depending on cell type. We recommend transient transfection of 0.5\u20131 million cells and FACS sorting of 10k GFP+ DAPI\u2013 cells so that adequate amount of RNA (>100ng) can be acquired per sample before moving onto library preparation.\nFor each sample, prepare 10\u00a0cm dishes: 3\u00d710\u00a0cm dishes for mCherry-ADAR control, 3\u00d7 10\u00a0cm dishes for RBP-ADAR fusions, and one dish of un-transfected or mock transfected cells as a negative control. This final dish is useful for FACS gating and for downstream analysis of SNPs present in the parental cell line.\nNote: RNA isolation is performed in triplicate for each sample, this serves as a backup in case one sample has low RNA integrity or if there is a failure during library preparation. While the extra sample can be omitted, we have found it to be helpful for those attempting the protocol for the first time.\nNote: All dishes should be passed in the same manner and for equal number of passages relative to one another, this will help minimize the dish-to-dish variability. This consistency allows for RNA editing and gene expression comparisons to confidently be made between dishes (and eventually RNA sequencing libraries).\nCulture the cells of interest using standard sterile tissue culture protocols, refer to American Type Culture Collection (ATCC) standards if needed. For our TRIBE experiments, mammalian cells are cultured and maintained in a humidified 37\u00b0C incubator with 5% CO2.Mouse embryonic fibroblasts (MEFs) and human osteosarcoma (U2OS) cells are cultured in DMEM 4.5\u00a0g glucose, glutamine, supplemented with 1% penicillin streptomycin (P/S) and 10% heat-inactivated fetal bovine serum.\nOnce cell lines have been thawed, it is advisable to passage the cells at least two times before using for experiments. Cell passages should be tracked and cells with minimal passage number should be used.\nFor each RBP of interest, transfect the appropriate number of cells (see above Notes). For adherent MEFs or U2Os cells we use triplicate 10\u00a0cm dishes (70\u201380% confluence gives best results). Follow best practices for transfection reagent of choice and refer to troubleshooting[href=https://www.wicell.org#troubleshooting] section, problem 1[href=https://www.wicell.org#sec7.1] if issues occur.\nOptional: Before beginning next step, check transfection efficiency of each dish using GFP fluorescence. Additionally, for mCherry ADAR control plasmids, check RFP fluorescence. The ADAR p2A GFP design helps check that the reading frame of the RBP upstream of ADAR is correct.\nOptional: If transfection efficiency is high (> 75%) it is possible to omit the following FACS sorting step and proceed directly to RNA isolation (Eg. HEK cell transfection). If the cells are not uniformly transfected, the WT unedited RNA will dampen the RNA editing levels below the threshold of detection.\nNote: Prolonged expression of RNA editing constructs can lead to increasing levels of cellular death. Therefore, transfection should be performed the day before RNA isolation and cells should be sorted within 24\u00a0h of transfection as editing reaches steady state within that period (Biswas et\u00a0al., 2020[href=https://www.wicell.org#bib2]).\nAlternatives: The following transfection reagents have been tested to be effective:\nHEK293T cells (70\u201390% transfection efficiency) \u2013 Calcium Phosphate, Lipofectamine 2000, Lipofectamine 3000, JetPrime (Polyplus), JetOptimus (Polyplus)\nSV40 Mouse embryonic fibroblasts (5\u201310% transfection efficiency), Human U2OS (25\u201350% transfection efficiency) \u2013 JetPrime (Polyplus), JetOptimus (Polyplus)\nFACS sortingTiming: 4 h\nFor cell lines with lower transfection efficiency, FACS sorting allows isolation of live cells (DAPI negative) transfected with ADAR (GFP positive). Sorting is performed directly into TriZol or equivalent RNA isolation buffer.\nNote: The following steps were optimized for adherent mouse embryonic fibroblasts (MEFs) and human osteosarcoma cells (U2OS) but may need to be adjusted depending on cell type. We recommend FACS sorting of \u223c10k GFP+ DAPI\u2013 cells so that >100ng of RNA can be acquired per sample.\nCritical: TriZol can cause serious chemical burns and is toxic upon inhalation, appropriate precautions should be taken, and reagent should be handled within chemical hood or class II biosafety cabinet.\nBefore removing cells (U2OS, MEFs) from incubator, prepare materials.\nPlace FACS buffer inside tissue culture hood.\nPre-warm tissue culture materials to 37\u00b0C (tissue culture media, DPBS, trypsin).\nPre-label 15\u00a0mL Falcon tubes, one corresponding to each FACS sample.\nPrepare sterile 1.5\u00a0mL Eppendorf tubes corresponding to the same number of FACS samples, place 500\u03bcL of TriZol in each tube, keep tubes on ice until ready to use.\nFor each 10\u00a0cm dish, place one sterile 15\u00a0mL Falcon tube in the tissue culture hood.\nDissociate cultured cells from their 10\u00a0cm plates.\nWash each plate twice with 7\u00a0mL pre-warmed DPBS.\nUse 1\u00a0mL pre-warmed trypsin to dissociate cells.\nPlace cells in incubator to complete dissociation, use gentle tapping of the dish to promote detachment.\nRe-suspend the dissociated cells in complete media (with FBS) to neutralize trypsin activity and place each sample into pre-labeled a 15\u00a0mL conical Falcon tube.\nPreparation of FACS samples: prepare three samples for each RBP-ADAR sample, three samples for the mCherry-ADAR control and one sample of un-transfected cells as a negative control (Figure\u00a01[href=https://www.wicell.org#fig1]).Spin the re-suspended cells gently for 5\u00a0min at 500 g.\nRemove the supernatant from the pelleted cells being careful not to disturb the cell pellets.\nRe-suspend the cell pellets in 200\u2013500\u03bcL of FACS buffer.\nPass the re-suspended cells through a mesh strainer to promote dissociation.\nNote: The density of cells and size of the cell pellet is dependent on cell type. It may be necessary to optimize the amount of FACS buffer to re-suspend in so that sorting can be expedited efficiently.\nPause Point: Cells can remain on ice for one hour prior to sorting. However, prolonged incubation on ice may cause increased death, decreasing RNA yield and quality.\nKeep FACS samples on ice and proceed immediately to FACS sorting, aim for 10,000 cells per sample with 1,000 cells as a minimum requirement.\nGate the cells using the negative sample to isolate un-transfected cells (Figure\u00a02[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/762-Fig2.jpg\nFigure\u00a02. Representative FACS plots\nPlots show stepwise selection of DAPI negative, GFP positive cell population for downstream RNA isolation and library preparation. Top plots show gate definition; it is important to identify gates that will prevent un-transfected cells from being included in the final population. Associated population counts can also be used to estimate transfection efficiency.\nSelect single cells using forward scatter and side scatter ranges. Use cells that are DAPI negative (alive) and GFP positive (transfected).\nEnsure that the FACS solution is no more than 10% of total TriZol volume, start with at least 500uL of TriZol in each of the tubes and add more TriZol after sort if necessary.\nCap the tubes containing the samples in TriZol, mix well and store on ice until ready to proceed to RNA isolation.\nOptional: After sorting, TriZol containing cell lysates can be stored at \u221280 until ready to proceed with RNA isolation.RNA isolation and quality control\nTiming: 4 h\nHigh quality RNA is crucial to the process of library preparation. We routinely isolate RNA from TriZol, however other methods such as RNA minipreps, spin columns and direct lysis can be used to isolate total RNA. The protocol is also compatible with nascent RNA isolation (McMahon et\u00a0al., 2016[href=https://www.wicell.org#bib16]).\nCritical: TriZol and chloroform can cause serious chemical burns, appropriate precautions should be taken, and reagent should be handed within chemical hood.\nRNA isolation using TRIZol is the preferred method when dealing with FACS sorted cells. If material is not limiting, RNA miniprep is an acceptable alternative.\nPerform RNA isolation according to manufacturer instructions.\nIsolated RNA should be placed into PCR tubes.\nPerform DNAse digestion of the RNA samples using Turbo DNA Free Kit (Ambion AM1907).\nAdd 0.1 volume of 10\u00d7 DNAse buffer to the isolated RNA.\nAdd 1uL of DNAse to the RNA and mix gently.\nIncubate at 37\u00b0C (in thermocycler) for 30\u00a0min.\nWhile incubating \u2013 thaw out the DNAse inactivation resin.\nInactivate with 0.1 volumes of the inactivation resin (3.5uL resin for 30uL of RNA) and leave at RT for 2\u00a0min.\nSpin in PCR tubes for 30 s, careful not to disturb the resin.\nTransfer as much as possible to a fresh tube, use remainder for qubit (2uL into 500uL eppendorf tubes) and Bioanalyzer (3uL into PCR strip).\nNote: Measuring the concentration of RNA with a Nanodrop will overestimate the amount in the sample. It is highly recommended that more specific assays such as Qubit be utilized so that libraries can be prepared with equal amounts of RNA input.\nMeasure concentration of RNA using QuBit RNA high sensitivity assay according to manufacturer instructions.\nPerform RNA integrity analysis using RNA bio analyzer (Figure 3[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/762-Fig3.jpgFigure\u00a03. Examples of RNA integrity measured after RNA isolation\nHigh quality RNA is required (Left). RNA integrity is evaluated using ribosomal RNA peaks which constitute the majority of RNAs within the cell. Do not proceed with samples that have a low RIN score and are degraded (Right). X-axis, fragment size in nucleotides, y-axis, arbitrary\u00a0fluorescence units\u00a0(FU).\nAfter sending out samples for quality control (QuBit and Bioanalyzer) we are left with approximately 30uL of RNA. 100\u00a0ng of total RNA is ideal for library preparation, however as little as 10\u00a0ng can be used. Proceed using samples with RIN scores > 8 (Figure 3[href=https://www.wicell.org#fig3]).\nOptional: Tape station can also be used for RNA integrity analysis.\nPause Point: Once RNA samples have been purified, 2uL and 3uL aliquot can be made for qubit and Bioanalyzer, respectively. All aliquots can be stored at -80 until the next step.\nRNA sequencing library preparation\nTiming: 1\u20133\u00a0days\nCritical: Do not proceed with library preparation until sufficient, high quality RNA samples are available.\nNote: Library preparation times can vary widely depending on the number of samples and the individual pause points within the protocol.\nAlternatives: Stranded, paired end RNA seq library preparation kits are ideal and allow for accurate mapping of transcripts. When dealing with limiting input amounts, we have had satisfactory performance from kits that have a lower required input (such as the NEB ultra II directional RNA seq library prep kit which can use as little as 10ng of total RNA input).Either rRNA depletion or polyA selection can be used to purify total RNA before moving to library preparation. The decision to use rRNA depletion or poly-A selection should be guided by the model organism and biological question. Caveats of rRNA depletion include that the primers needed are often organism specific and finalized libraries may require increased sequencing depth to achieve an equivalent coverage.\nAlternative mRNA seq library preparation kits that have been used in the literature for TRIBE include the TruSeq stranded mRNA from Illumina and the KAPA Stranded mRNA-Seq Kit from Roche.\nDepending on the amount of RNA available, different library prep methods may be used. We have found that NEB provides robust kits and routinely use their Ultra II Directional RNA library prep kit. However, many other, equivalent kits are available.\nUsing the NEB Ultra 2 Kit with sample preparation beads \u2013 prepare stranded RNA seq samples according to manufacturer instructions. In addition to the kit, the following is required: NEB Index primers, polyA isolation module or rRNA depletion, a magnet appropriate for PCR tubes.\nIsolate mRNA with poly T-beads (or rRNA depletion).\nFragment the RNA.\nCritical: Fragmentation time is critical, be sure to follow manufacturer\u2019s instructions for proper library size. If using PE 150bp reads, it is recommended that fragmentation aims for \u223c300bp fragments. RNA fragment size can be checked using Bioanalyzer or Tape station.\nFirst and second strand synthesis (after this step the DNA is stable).\nPause Point: There are multiple pause points and the protocol can be split into two or three days. DNA can be stored after second strand synthesis, can also be stored after end prep and adapter ligation and finally is stable after library cleanup.\nEnd prep and adapter ligation, choose index primers for each sample.Note: NEB Index primers each contain a specific six nucleotide barcode sequence. Each index primer must be chosen as to not interfere with downstream de-multiplexing. If libraries are going to be sequenced in a single pool, each library within that pool must receive a unique index number for downstream demultiplexing of the sequence data. Library preparation kit manufacturers will sell index oligonucleotides in sets (of 12 or 24 indices).\nPCR amplification, double library cleanup to remove primer dimers.\nNote: We recommend using the minimal amount of PCR cycles necessary to minimize the amount of PCR duplicates that remain in the final library.\nCheck proper fragmentation and DNA library size using Bioanalyzer (Figure 4[href=https://www.wicell.org#fig4]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/762-Fig4.jpg\nFigure 4. Proper cDNA library preparation\nDNA libraries should be correctly sized and absent of adapter dimer peaks X-axis, fragment size in nucleotides, y-axis, arbitrary\u00a0fluorescence units\u00a0(FU). See troubleshooting[href=https://www.wicell.org#troubleshooting], problem 2[href=https://www.wicell.org#sec7.3] for example of libraries requiring extra purification.\nOptional: Agilent TapeStation can also be used for DNA library size analysis.\nRNA sequencing and mapping\nTiming: 1\u00a0day\nThe major data generation step for any TRIBE experiment is RNA sequencing. We currently use Illumina next generation sequencers; in the future this approach may be extended to other platforms. For mammalian cells, it is important to achieve the requisite sequencing depth for high confidence SNP calling, we have found that individual lanes of a HiSeq4000 or NovaSeq equivalents running in PE150 mode provide sufficient depth (40Gb per library) at a reasonable cost for a single experiment, where a single experiment contains duplicate RBP samples, duplicate mCherry controls and an untransfected library. Please refer to Figure 5[href=https://www.wicell.org#fig5] for a summary of the downstream bioinformatic analysis.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/762-Fig5.jpg\nFigure 5. Overview of HyperTRIBE computational analysisFlow chart showing steps of data analysis and script paths that need to be changed throughout the protocol. Shell scripts are designated \u201c.sh\u201d and highlighted in bold.\nNote: The total number of bases output is important for TRIBE (with an ideal of 40Gb per mammalian library. Longer, paired-end reads and higher depth are both useful in performing mapping and SNP calling respectively. We recommend using no shorter than 100bp paired end reads for sequencing and analysis. Long paired end libraries that are sequenced to an appropriate depth will facilitate accurate RNA editing identification.\nNote: Sequencing depth should be at least 50 million 150bp PE reads per sample when working with human and mouse samples. More reads will be necessary if using 100bp or shorter read lengths.\nOperate the sequencer according to manufacturer\u2019s instructions.\nDemultiplex the sequencing reads using index primers that were added during library preparation as per manufacturer instructions.\nConcatenate data from different lanes together, if applicable.\ncat data1_read1.fq.gz data data1_read2.fq.gz\nOptional: Multiple mCherry-ADAR control samples should be concatenated together to make a single, high coverage background file. When concatenated, control editing sites found in either sample are used to call background RNA editing events.\nQuality control and mapping of sequenced reads using STAR:\nKnow the path to the following previously downloaded genome annotations (refer to before beginning section).\nPrepare the shell scripts (Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/762-Mmc1.zip]) using the previously defined paths (refer to install and compile software dependencies[href=https://www.wicell.org#sec1.2] section).\nStar_indices\nTRIMMOMATIC_JAR\nPICARD_JAR\nGENOME_FAI_FILE\nMove the modified trim_and_align_PE.sh file (Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/762-Mmc1.zip]) into the directory with your paired end RNA seq data.\nUse the trim_and_align script to perform quality control, adapter trimming and mapping of the paired-end data using STAR. For each piece of data:\nnohup trim_and_align_PE.sh data1_read1.fq.gz data1_read2.fq.gz &Repeat the above command for each sample. Note that this process may be limited by the amount of system memory, for human or mouse samples ensure that the machine has 32GB of RAM per sample that is to be run.\nPlease refer to troubleshooting, problem 3[href=https://www.wicell.org#sec7.5] for common issues running shell scripts.\nWhile the alignment is running, the output will be stored in the newly created \u201cnohup.out\u201d file, you can track the progress of the alignment using the following command:\ntail nohup.out\nThe following files will be created from a successful alignment:\nLog.out \u2013 log of alignment process.\nLog.final.out \u2013 contains important mapping quality metrics.\nSort.bam \u2013 when used with the .bam.bai file can be loaded into IGV to visualize coverage.\nSort.bam.bai\nSort.sam \u2013 used for downstream TRIBE analysis.\nLoading alignments into MySQL database:\nnohup /path_from_root/HyperTRIBE/CODE/load_table.sh sam_filename mysql_tablename expt_name replicate/timepoint &\nexample: nohup /home/jbiswas/RNA/HyperTRIBE/hg38/load_table_hg38.sh wtRNA.sort.sam testRNA rnalibs 2 &\nWait for nohup.out file to be updated, this may take several hours depending on the machine resources and sequencing depth. Refer to troubleshooting[href=https://www.wicell.org#troubleshooting], problem 3[href=https://www.wicell.org#sec7.5] if running into issues with script permissions.\nDetermination of RNA editing sites from alignments\nNavigate to your working directory and copy over the script (Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/762-Mmc1.zip]) to call RNA editing sites.\ncd /directory_of_choice/\ncp /home/jbiswas/RNA/HyperTRIBE/hg38/rnaedit_wtRNA_RNA_hg38.sh .\nModify the heading of the copied script file (Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/762-Mmc1.zip]) to direct the script to the appropriate path to the CODE as well as the path to the genome annotation.\nHyperTRIBE_DIR=\"/home/jbiswas/RNA/HyperTRIBE/hg38\"\nAnnotationfile=\"/home/jbiswas/RNA/index/Homo_sapiens/UCSC/hg38/Annotation/Archives/archive-current/Genes/refFlat.txt\"\nModify the script (Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/762-Mmc1.zip]) to include your choice of table name, control sample (wtRNA), and different experimental samples or timepoints.\nwtRNAtablename=\"testRNA\"\nwtRNAexp=\"rnalibs\"\nwtRNAtp=\"2\"\nRNAtablename=\"testRNA\"\nRNAexp=\"rnalibs\"\ntimepoint=(3 4 5)Optional: Editing thresholds can be changed within the script. Recommended thresholds are 1% for the control sample, and 5% for each of the experimental samples. This provides stringent filtering of RNA editing sites.\nRun the modified script to generate files containing output from a single replicate (Expected Outcomes, Table 2[href=https://www.wicell.org#tbl2]).\ntable:files/protocols_protocol_762_3.csv\nAfter running the process_editing_sites.sh script, an excel file containing the background subtracted intersection of replicates from both samples is generated:\nnohup./rnaedit_wtRNA_RNA.sh &\nTRIBE analysis \u2013 Postprocessing of data output\nTiming: 1\u00a0day\nOnce mapped, the mapped files can be used for other standard quantification such as differential gene expression analysis. This section of the protocol will discuss further processing of the mapped data to identify SNPs specific to RNA editing events in both control and experimental samples.\nCopy the process_editing_sites.sh file into your current working directory.\ncp /home/jbiswas/RNA/HyperTRIBE/process_editing_results.sh.\nModify the location of the HyperTRIBE files and the file prefix.\nHyperTRIBE_DIR=\"/home/jbiswas/RNA/HyperTRIBE/\nfile_prefix=\u201dMCP_TRIBE\u201d\nUse process_editing_results.sh script to find the intersection of both experimental replicates and remove the background sites (Expected Outcomes, Table 3[href=https://www.wicell.org#tbl3]).\ntable:files/protocols_protocol_762_4.csv\nThree inputs are required for the script:\nBedgraph file containing WT editing sites with 1% editing or more.\nBedgraph file containing Replicate 1 editing sites with 5% editing or more.\nBedgraph file containing Replicate 2 editing sites with 5% editing or more.\nAlternatively, bedtools intersect can be used to perform these functions:\nintersect replicates\nremove control editing sites\nVisualize the editing sites by importing the editing bedgraph files into IGV (Figure 6[href=https://www.wicell.org#fig6]). It is often useful to simultaneously visualize coverage using the .bam and .bam.bai files generated after alignment.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/762-Fig6.jpg\nFigure\u00a06. RNA editing can be visualized with IGV and occurs both at and adjacent to the site of RNA bindingFigure\u00a0modified with permission from Biswas et\u00a0al., iScience 2020. \u03b2-actin gene, focusing on the MBS array (x-axis) showing (from top to bottom). MS2 sites at genomic locus (ground truth): blue boxes represent the known location of the MS2 stem loops. MCP-TRIBE alignment with multimapping: reads are depicted in gray (scale bar for number of reads on right), editing sites are depicted as red bars. Uniquely mapped read alignment: mRNA coverage without multimapping depicted in blue (scale bar for number of reads on right). MCP-TRIBE sites both replicates uniquely mapped: editing events as indicated by dark blue bars where height corresponds to the average editing percentage across both replicates at that nucleotide (scale to right). Light blue shading indicates location of the stem loop nucleotides.\nPlease refer to troubleshooting[href=https://www.wicell.org#troubleshooting], problem 4[href=https://www.wicell.org#sec7.7] for issues with editing site discovery.\nDetermine intersections of gene symbols with other datasets (such as Hi-C, CLIP or RIP).\nThe gene symbols output from the TRIBE pipeline can be compared to gene symbols from other datasets using free online tools (http://genevenn.sourceforge.net/[href=http://genevenn.sourceforge.net/]).\nDetermine relative positions of TRIBE sites to known binding motifs or published CLIP peaks.\nThe .bedgraph files from the TRIBE experiment can be intersected with bed files output from published CLIP datasets (https://www.encodeproject.org/[href=https://www.encodeproject.org/]) using bedtools function \u201cintersect\u201d https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html[href=https://bedtools.readthedocs.io/en/latest/content/tools/intersect.html].\nDistances can be measured using the bedtools function \u201cclosest\u201d (https://bedtools.readthedocs.io/en/latest/content/tools/closest.html[href=https://bedtools.readthedocs.io/en/latest/content/tools/closest.html]).", "Step-by-step method details\nStep-by-step method details\nChecking AD studies summary statistics\nTiming: 10\u00a0min\nThis section allows one to browse the overall summary statistics, including species, disease condition, brain regions, and gender. The researcher can then make an informed decision of which dataset of interest one should navigate to.\nThe current release of the scREAD collected datasets from 15 studies in total. Based on the metadata provided from the original papers, we constructed the original samples into 73 datasets, each of which corresponds to a specific species (human or mouse), gender (male or female), brain region (entorhinal cortex, prefrontal cortex, superior frontal gyrus, cortex, cerebellum, subventricular zone, superior parietal lobe, or hippocampus), disease or control, and age stage (7\u00a0months, 15\u00a0months, or 20\u00a0months for mice, and 50\u2013100+ years old for human).\nNavigate to https://bmbls.bmi.osumc.edu/scread/[href=https://bmbls.bmi.osumc.edu/scread/], and the dataset summary should be listed (Figures 1[href=https://www.wicell.org#fig1]A and 1B).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fig1.jpg\nFigure\u00a01. Overview of the scREAD homepage\n(A) The pie charts represent four factors of distribution: species, control/disease condition, brain region, and gender from the left side to the right side, respectively. Each color in each pie chart represents one element, and the number represents the distribution ratio for each element under each factor for 73 datasets.\n(B) The table shows the general information of all 73 datasets. Users can select filters and the table will be updated accordingly. Clicking a row in the table will pop up the dataset overview panel, and users can navigate to the dataset details page through the link.\nYou can either click on one of the pie charts or select filters from the dataset summary table, including species, sample condition, brain region, and gender. The content of the table will be updated accordingly.You can click any row in the table, and a dataset overview panel will pop. You can further navigate to the dataset details page through the link.\nSearching genes of interest from the differential gene expression (DGE) analysis results\nTiming: 20\u00a0min\nThis section allows one to search a gene of interest from DGE analysis results across multiple comparisons. For detailed gene information, the researcher can check from the link to the dataset ID of interest (Figure\u00a02[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fig2.jpg\nFigure\u00a02. Example DGE analysis searching result of the GAD1 gene, a marker gene of inhibitory neurons\nUsers can select filters and the table will be updated accordingly.\nTo search genes of interest from DGE analysis results across multiple datasets, type the gene symbol in the search box, and click on the search button. You can select filters to specify dataset sources or decide which comparison types should be displayed.\nThe query results are returned from multiple DGE analyses, including cell-type-specific genes, subcluster specific genes, AD vs control differentially expressed genes (DEGs), or AD vs AD DEGs.\nCell-type-specific genes were identified by performing DGE analysis between the cell type of interest and the average of the remaining cell types.\nSubcluster-specific genes were identified by performing DGE analysis between the subcluster of interest and the average of the remaining subclusters from the same cell type.\nAD vs control DGE analysis was performed within the same cell type, brain region, and gender. For example, Male-AD-Prefrontal cortex vs Male-Control-Prefrontal cortex.\nAD vs AD DGE analysis was performed within the sample cell type while based on different sample conditions. For example, one AD vs AD comparison can be Male-AD-Prefrontal cortex vs Male-AD-Entorhinal Cortex or Male-AD-Prefrontal cortex vs Female-AD-Prefrontal cortex.Note: The \u2018multiple comparison types\u2019 in the comparison type selection box include disease vs control performed within the same dataset and disease vs disease performed from two different datasets. A positive log foldchange (FC) value indicates the gene expressions are higher in the first group. The log FC is returned in the natural logarithm.\nChecking cell clustering results\nTiming: 20\u00a0min\nIn this section, we used a dataset from scREAD as an example to show the analysis result (ID: AD00103), https://bmbls.bmi.osumc.edu/scread/AD00103[href=https://bmbls.bmi.osumc.edu/scread/AD00103]. This dataset consists of 6,629 cells isolated from a human AD female prefrontal cortex sample (Mathys et\u00a0al., 2019[href=https://www.wicell.org#bib11]). As we know, not all cells collected from AD patient samples are malignant, and some healthy cells may be included in the cell populations, which were defined as healthy-like cells in Granja et\u00a0al.\u2019s study (Granja et\u00a0al., 2019[href=https://www.wicell.org#bib4]). We applied this concept to all AD datasets in scREAD and defined these healthy-like cells as control-like cells. These control-like cells maintain distinct regulatory mechanisms and gene expression patterns compared to AD cells, and they will disturb the accurate identification of AD cell types. Thus, we removed these control cells from disease datasets and identify AD-associated cells. Here, scREAD filtered out 950 control-like cells and kept 5,679 AD-associated cells for the downstream analysis. See the quantification and statistical analysis[href=https://www.wicell.org#quantification-and-statistical-analysis] section for more details about how scREAD filtered out control-like cells.\nChecking cell clustering results (Figure\u00a03[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fig3.jpg\nFigure\u00a03. Cell clustering and gene expression result from the scREAD homepage\nCell clustering results including UMAP plot colored by cell types or subclusters (left), and searching gene expression using the same UMAP coordinates (right). The darker the color is in this UMAP, the higher the expression value of the gene.By default, all the cell types will be selected and the corresponding Adjusted Rand Index (ARI) will be displayed. The ARI score is used to evaluate the similarity of our predicted cell types compared with the original cell labels in the original paper.\nNote: The ARI score will not be displayed when the cell labels were not provided from the original paper, and a silhouette score will be displayed instead. Meanwhile, the ARI score will be hidden if users did not select all cell types.\nChoose one of these cell types, the following Uniform Manifold Approximation and Projection (UMAP)(Becht et\u00a0al., 2019[href=https://www.wicell.org#bib1]) will change to the UMAP of predicted subclusters for this specific cell type.\nA sliding bar is used for controlling the size of each point in the following UMAP. It ranges from 1 to 10, i.e., the bigger the number is, the larger the point size is.\nThis function bar contains several quick buttons for graphic operations.\nHovering the cursor on cell points will display cell type, cell name, and the UMAP coordinates.\nThe legend of this UMAP plot will be displayed based on the genes selected in the drop-down bar. The darker the color is in this UMAP, the higher the expression value of the gene.\nCritical: Rendering gene expression scatter plot can be slow due to network speed or a large number of cells data need to process, please be patient while scREAD is fetching data from the backend server.\nChecking differential expression (DE) results and performing functional enrichment analysis\nTiming: 20\u00a0min\nIn this section, we used the same example data from checking cell clustering results to illustrate DGE analysis results and to perform functional gene set enrichment analysis based on DEGs.First, apply the necessary filtering criteria from the DGE analysis results panel (Figure\u00a04[href=https://www.wicell.org#fig4]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fig4.jpg\nFigure\u00a04. Differential gene expression (DGE), and functional gene set enrichment based on DEGs\n(A) Differential gene expression analysis panel, the comparison groups include cell-type-specific genes, subcluster specific genes, and DEGs from the cross-dataset comparison.\n(B) KEGG pathway, GO biological process, molecular function, and cellular component analysis using the DEGs from (A), an enriched genes example were displayed on the 5th GO cellular component term.\nDGE analysis groups for browsing cell-type-specific genes, subcluster specific genes, and DE genes from the cross-dataset comparison.\nChoose the cell type of interest in DGE analysis.\nChoose the log fold-change ranges. (default\u00a0= 0.5; ranges from 0 to 5).\nThe adjusted p value ranges. (default\u00a0= 0.05; ranges from 10\u02c6\u22126 to 1).\nThe DE direction can filter by all DE genes, only up-regulated genes, only down-regulated genes (default\u00a0= \u2019all\u2019).\nYou can search for genes that you are interested in, and then the following table will return the matching result.\nDownload the currently listed table.\nGeneCards database (https://www.genecards.org/[href=https://www.genecards.org/]) is linked to each gene in the table.\nNote: Adjusting any parameters above will immediately affect the displayed DEGs table.\nPerforming functional gene set enrichment (Figure\u00a04[href=https://www.wicell.org#fig4]B)\nKEGG pathway, GO biological process, GO molecular function, and GO cellular component analysis using the DEGs from above, an example of enriched genes are displayed on the 4th GO cellular component term. You can also search for a specific item by entering the content that you want to search in the search box.Critical: The functional gene set analysis results are calculated in real-time by sending the DEGs to the Enrichr (Kuleshov et\u00a0al., 2016[href=https://www.wicell.org#bib9]) web server (https://maayanlab.cloud/Enrichr/[href=https://maayanlab.cloud/Enrichr/]). Thus, changing DEG log FC or p value cutoffs can significantly change the results of enrichment analysis. Considering Enrichr does not provide options to submit a custom background and the results could be potentially misleading (Timmons et\u00a0al., 2015[href=https://www.wicell.org#bib13]). We also provide a link to another enrichment analysis tool, g: Profiler (https://biit.cs.ut.ee/gprofiler/[href=https://biit.cs.ut.ee/gprofiler/]), which allows users to submit custom background gene sets.\nIdentifying cell-type-specific regulons\nTiming: 20\u00a0min\nIn this section, we describe the process of identifying cell-type-specific regulons (CTSRs) using IRIS3.\nIn the DE section, when you select the \u201cCell-type-specific genes\u201d item in the \u201cGroup\u201d select box, cell-type-specific regulon analysis will be performed. CTSRs results are displayed at the bottom of the screen. Click on the \u201cCell-type-specific regulons\u201d bar, detailed CTSRs information will be shown.\nThe CTSRs are displayed in a reactive table in the DE section, each row shows that for each cell type, a set of genes are regulated by a specific transcription factor (TF). Clicking on the \u201cRegulon overview\u201d panel, a table in the panel summarizes the overall cell number and regulon number in each cell cluster (Figure\u00a05[href=https://www.wicell.org#fig5]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fig5.jpg\nFigure\u00a05. Cell-type-specific regulon (CTSR) analysis results from IRIS3\n(A) Overview table of identified CTSRs.\n(B) An example of CTSR (p value\u00a0< 0.05), stars indicate differential expressed gene within the cell type, the corresponding matched TF is linked to the HOCOMOCO database.\n(C) UMAP plot colored by regulon activities in each cell.You can navigate into the IRIS3 to see the detailed results of this job by clicking the 'Open cell-type-specific regulon result page in the new tab' button. In the table, the index number will be given to represent CTSRs (Figure\u00a05[href=https://www.wicell.org#fig5]B).\nBoth gene compositions of regulons and their expression values across different cell types can be intuitively displayed in a heatmap. Regulons are ranked in increasing order of the empirical p values of regulon specificity scores (RSS) as described above, and a regulon is named as CTn-Rm with 'n' representing the index of cell type and 'm' represents the regulon rank. Due to the space limitation, only the top ten regulons and their corresponding genes are showcased in the heatmap, and the component genes of each regulon are indicated as green rectangles. The heatmap records the log-transformed expression level of each top-ten-regulon-covered gene across all cells.\nRegulon results are separately showcased in each cell type. Click on the \"CT#\" button to switch to see results in other cell types. A scatter plot shows the distribution of the RSS of each regulon. CTSRs are ranked top and marked as blue dots with their representative TF names, and insignificant regulons are marked as grey dots. For each regulon, genes and the corresponding TF are presented (Figure\u00a05[href=https://www.wicell.org#fig5]B) with several actions that link to showing heatmap, functional gene set enrichment analysis, and regulon activities in the UMAP plot (Figure\u00a06[href=https://www.wicell.org#fig6]C). A more detailed interpretation of each regulon can be found on the IRIS3 website, https://bmbl.bmi.osumc.edu/iris3/tutorial.php#3example&q=2[href=https://bmbl.bmi.osumc.edu/iris3/tutorial.php#3example%26q=2].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fig6.jpg\nFigure\u00a06. Overlapping genes and annotated cell type (ct) from the example dataset\n(A) The up-regulated overlapping genes in Human Entorhinal Cortex Astrocytes (ast).\n(B) The log FC, dataset source, and rankings from the overlapping genes table in (A).(C) A UMAP plot of the control dataset example with six cell types was annotated from the scREAD workflow.\n(D) A UMAP plot of the disease dataset example with six cell types was transferred from the example reference control dataset.\nCritical: The CTSRs results are only available when you selected the cell-type-specific option in the DEG group box.\nOptional step: calculating overlapping DEGs from multiple comparisons\nTiming: 1 h\nIn this section, we provide a workflow for calculating overlapping DEGs from multiple comparisons. Suppose we have   m   AD vs control comparisons from a cell type of interest in a specific brain region. For each comparison, we select top   t   DEGs based on the ranked log FC. We define an \"overlapping gene\" as the gene that appears at least   n   times in   m   comparisons (  n \u2264 m  ).   t ,  n  are parameters set by the users.\nHere, we provide two approaches, you can follow the code example below on your local R environment; For users wishing to avoid setup procedures, you can use the following link from Google Colab, which is an interactive computational environment that combines live code, visualizations, and explanatory text, https://colab.research.google.com/drive/1lInXa6jD4yc7RGJc0EWDfy5NNoXT1qye?usp=sharing[href=https://colab.research.google.com/drive/1lInXa6jD4yc7RGJc0EWDfy5NNoXT1qye?usp=sharing].\nIf you wish to perform the calculation in your local computer, first, load the R packages, scREAD data, and predefined functions in your R local environment:\nlibrary(tidyverse)\nlibrary(RVenn)\nlibrary(rlist)\nlibrary(knitr)\ntryCatch({\n\u00a0\u00a0load(\n\u00a0\u00a0\u00a0\u00a0url(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'https://bmbl.bmi.osumc.edu/downloadFiles/scread/protocol/scread_db.rdata[href=https://bmbl.bmi.osumc.edu/downloadFiles/scread/protocol/scread_db.rdata]'\n\u00a0\u00a0\u00a0\u00a0)\n\u00a0\u00a0)\n}, error = {\n\u00a0\u00a0load(\n\u00a0\u00a0\u00a0\u00a0url(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0'https://github.com/OSU-BMBL/scread-protocol/raw/master/overlapping_genes/scread_db.rdata[href=https://github.com/OSU-BMBL/scread-protocol/raw/master/overlapping_genes/scread_db.rdata]'\n\u00a0\u00a0\u00a0\u00a0)\n\u00a0\u00a0)\n})\nTo calculate overlapping genes, these parameters are needed\nThe number of genes to be selected in each AD vs control DEG results (default\u00a0= 100)\nSpecies (default\u00a0= Human)\nBrain region (e.g., Entorhinal Cortex)\nDE direction (e.g., up)Overlap threshold (For example, A gene is an overlapping gene if A should at least appear 3 times in total 4 comparisons, here the threshold is 3)\nWe can then process some of our metadata:\nREGION_LIST <- sort(unique(dataset$region))\nCT_LIST <- sort(unique(cell_type_meta$cell_type))\nCT_SHORT_LIST <- CT_LIST\nCT_SHORT_LIST[CT_LIST==\"Oligodendrocyte precursor cells\"] <- \"opc\"\nCT_SHORT_LIST <- tolower(substr(CT_SHORT_LIST, 1, 3))\nBelow are the necessary settings to calculate the overlapping genes.\nWe use the top 100 DE genes in each AD vs control comparison:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fx1.jpg\nSpecies should be either 'Human' or 'Mouse':\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fx2.jpg\nSpecify our brain region of interest, here we selected the 5th brain region in REGION_LIST variable, i.e., Entorhinal Cortex':\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fx3.jpg\nDE direction should either 'up' or 'down', 'up' means we select DE genes that are expressed higher in the disease dataset (the first group):\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fx4.jpg\nThe OVERLAP_THRES should be manually defined based on your interest and the total number of comparisons in scREAD. For example, scREAD have 4 total AD vs control datasets comparisons, we set the threshold to 3, meaning that we want to find overlapping genes that are at least appeared in 3 comparisons:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fx5.jpg\nNow, we can calculate the overlapping genes based on the parameters above, the results are stored in a list variable:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fx6.jpg\nTwo tables can be generated by accessing the result variable:\nThe overlapping genes in the selected brain region (Figure\u00a06[href=https://www.wicell.org#fig6]A)\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fx7.jpg\nThe detailed information, including rankings, log FC, dataset source information from the overlapping genes (Figure\u00a06[href=https://www.wicell.org#fig6]B)\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/973-Fx8.jpg\nOptional section 7: Running the scREAD backend analysis workflow locally\nTiming: 2 h\nIn this section, we present how to run the scREAD workflow to process a custom dataset. The workflow can be used in the Unix command-line environment with R installed.Download the scREAD workflow and an example dataset from https://github.com/OSU-BMBL/scread-protocol/tree/master/workflow[href=https://github.com/OSU-BMBL/scread-protocol/tree/master/workflow], the folder should contain the following files (5\u00a0min):\ncustom_marker.csv: A manually created marker gene list file used for identified cell types.\nfunctions.R: Visualization functions used in R.\nbuild_control_atlas.R: build control cells atlas Seurat object from count matrix file.\ntransfer_cell_type.R: filter out control-like cells in disease dataset.\nrun_analysis.R: run analysis workflow, and export tables in the scREAD database format.\nexample_control.csv. The example control dataset.\nexample_disease.csv. The example disease dataset.\nBuild the control atlas file from the raw gene expression matrix (5\u00a0min).\nPrepare your control gene expression data. In the data frame, the first column should be gene symbols and other columns as cell labels. Put all code and data in a working directory. (e.g., PATH_TO_WD), in this protocol, we will run example_control.csv.\nbuild_control_atlas.R takes three parameters:\nWorking directory path.\nControl data path.\nOutput data ID.\nNext, run the following command, remember to change PATH_TO_WD to your working directory path:\ncd PATH_TO_WD\nRscript build_control_atlas.R PATH_TO_WD example_control.csv control_example\nThe expected output for this step contains four files:\ncontrol_example.rds: The Seurat R object storing example control data.\ncontrol_example_expr.txt: Filtered gene expression matrix.\ncontrol_example_cell_label.txt: The first column is the cell name, the second column is the cell type information.\ncontrol_example_umap.png: UMAP plot of example control data colored by cell types (Figure\u00a06[href=https://www.wicell.org#fig6]C).\nTransfer cell types based on control atlas, the goal of this step is to annotate cell type using the control atlas as the reference, onto the disease gene expression matrix file (\u223c5\u00a0min).\nPut all code and data in a working directory. (e.g., PATH_TO_WD) after you have generated the control atlas file (control_example.rds).\ntransfer_cell_type.R takes four parameters:\nWorking directory path.\nControl atlas Seurat object file name.\nDisease gene expression matrix name.\nOutput disease data ID.\nNext, run the following commandcd PATH_TO_WD\nRscript transfer_cell_type.R PATH_TO_WD control_example.rds example_disease.csv disease_example\nThe expected output for this step contains four files:\ndisease_example.rds: The Seurat R object storing example disease data.\ndisease_example_expr.txt: Filtered gene expression matrix.\ndisease_example_cell_label.txt: The first column is the cell name, the second column is the cell type information.\ndisease_example_umap.png: UMAP plot for disease data colored by cell types (Figure\u00a06[href=https://www.wicell.org#fig6]D).\nRun data analysis, the goal of this step is to identify cell-type-specific genes, DEGs from two example datasets (60\u00a0min).\nPut all code and data in a working directory. (e.g., PATH_TO_WD) after you have generated the control atlas file (control_example.rds), and the disease file (disease_example.rds)\nrun_analysis.R takes three parameters:\nWorking directory path.\nControl Seurat object file name.\nDisease Seurat object file name.\nNext, run the following command:\ncd PATH_TO_WD\nRscript run_analysis.R PATH_TO_WD control_example disease_example\nThe expected output for this step contains three folders:\n/de. Differential gene expression analysis results.\nCell-type-specific genes.\nSub-cluster specific genes.\nDEGs between two conditions.\n/dimension: UMAP coordinates for two datasets.\n/subcluster_dimension: UMAP coordinates for each sub-clusters in two datasets.\nIdentify CTSRs using IRIS3 (2 h).\nNavigate to https://bmbl.bmi.osumc.edu/iris3/submit.php[href=https://bmbl.bmi.osumc.edu/iris3/submit.php], submit two jobs for the two example datasets:\nupload control_example_expr.txt and control_example_cell_label.txt\nupload disease_example_expr.txt and disease_example_cell_label.txt\nThe expected output for IRIS3 contains these files:\nLists\nCTSR gene list\nMarker gene list\nGene module list\nMotif list\nTranscription factor list\nTables\nPredicted cell types\nBulk ATAC peak enrichment\nTAD association\nFigures (only display on the website)\nCTSR active UMAP\nCTSR gene heatmap\nTrajectory\nUMAP\nCritical: You need to open the advanced options tab in the IRIS3 submission page to upload a custom cell label.", "R is one of the most widely used programming languages for bioinformatics. Numerous packages for statistical data analysis and visualization have been created by R developers. In order to make Perseus more powerful by making these functions available from within the software, a package for integrating R scripts into Perseus was developed\u2014PerseusR (Rudolph & Cox, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0016]). With this, all custom tools originally scripted in R can now be used within Perseus. In this basic protocol, a simple example of an R-only plugin, extracting the head (top rows) of a matrix, will be presented to illustrate how the data transfer between Perseus and R functions. This example will be run through the command line style interface. The code of this example is available at: https://github.com/JurgenCox/perseus-plugin-programming/blob/master/scripts/head.R[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/scripts/head.R].\nNecessary Resources\nHardware\nA computer running Windows 8 (64 bit) or higher, or Windows Server 2008 or higher\n4 GB RAM minimum\nAt least a quad core processor is recommended\nSoftware\nPerseus 1.6.13 or higher:\u2013can be downloaded from https://maxquant.org/perseus[href=https://maxquant.org/perseus]\nR: Please use a version \u2265 3.5.0. The Rscript executable has to be listed in the PATH environment variable of the operating system. Please refer to the \u201cTroubleshooting\u201d if Perseus cannot find your R installation, which is indicated in the command line style interface.\nPerseusR: available at https://github.com/cox-labs/PerseusR[href=https://github.com/cox-labs/PerseusR], where installation instructions are provided\ninstall.packages(\u201cdevtools\u201d)\nlibrary(devtools)\ninstall_github(\u201ccox-labs/PerseusR\u201d)\nR-supported editor like Visual Studio, RStudio or Notepad++\nInput files\nThis protocol requires no extra input files. The outlined plugin works with a randomly generated matrix, which can be done using the dice button in Perseus.\n1. Parse command line arguments from Perseus.\nThe communication between Perseus and R works through temporary files containing the data, their location being specified by fixed index command line arguments. Therefore, these command line arguments sent from Perseus need to be parsed first.args = commandArgs(trailingOnly=TRUE)\nif (length(args) != 2) {\n\u2003stop(\"Do not provide additional arguments!\", call.=FALSE)\n}\ninFile <- args[1]\noutFile <- args[2]\nSince the arguments from Perseus are input file and output file, the length of arguments should be 2. The order of the arguments is fixed: the first in this case is for input file and the last one is for the output file.\n2. Use PerseusR to read the data matrix written by Perseus.\n         \nlibrary(PerseusR)\nmdata <- read.perseus(inFile)\nPerseusR is the package that bridges between Perseus and R. It needs to be imported first. Afterwards, the matrix from Perseus can be read into a special matrixData object by read.perseus.\n3. Get the main matrix of Perseus for data processing.\nThe matrix in Perseus is composed of annotation rows/columns and the main data columns. In order to extract the main matrix for analysis, the function \u2212main() needs to be used.\n         \ncounts <- main(mdata)\n4. Execute the main custom code for data analysis or modification.\nAfter obtaining the main matrix, the custom analysis steps and modifications can be done. In this protocol, the head of the matrix is extracted (\u201c15 rows\u201d is assigned).\n         \nmdata2 <- head(counts, n=15)\nSince the number of rows is reduced for the main matrix, the annotation columns need to be shortened for the output matrix as well. To get the annotation columns, use annotCols().\n         \naCols <- head(annotCols(mdata), n=15)\n5. Export the output matrix to Perseus with correct format.After finishing all data-processing steps, the data needs to be converted back to the Perseus txt format and written to the predefined temporary output file. For generating the final output, a new matrixData object consisting of main matrix (main), annotation columns (annotCols). and annotation rows (annotRows) needs to be created. Similar to annotCols(), the function annotRows() extracts the content of Perseus annotation rows. Since the annotation rows are not changed by the plugin, they are reused from the original matrix.\n         \nmdata2 <- matrixData(main=mdata2, annotCols=aCols, annotRows=annotRows(mdata))\nTo generate the temporary file, write.perseus() is used with the matrixData object and the outputfile location, which was read in the first step of this protocol.\n         \nprint(paste(`writing to', outFile))\nwrite.perseus(mdata2, outFile)\nThese are all the basic elements of an R-only Perseus plugin. If no further arguments are required, the developers only have to put their custom code in step 4.\n6. Apply the plugin in Perseus.\n         \nOpen Perseus and import the matrix/load a session file.\nA random matrix is used for testing the plugin in this tutorial.\nExecute the plugin.\nIn the \u201cProcessing\u201d block, click \u201cExternal\u201d \u2013> \u201cMatrix \u2013> R\u201d. If the button \u201cselect\u201d is green, it means that Perseus recognized your R installation and PerseusR (Figure 5A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0005]), otherwise navigate to your Rscript.exe or add it to your systems PATH variable. Afterwards, specify the R script that you want to execute and click OK (Fig. 5B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0005]).In order to make a script more flexible and useful, additional parameters are usually required. With the above example of extracting the head of a matrix (Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0001]), it will be much more convenient if the number of rows can be defined by the users. The following steps will provide the details of how to add parameters to the plugins. The script, including all steps, can also be found at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/scripts/head_add_argument.R[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/scripts/head_add_argument.R].\nNecessary Resources\nSame as Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0001]\n1. Install the argparser library (https://cran.r-project.org/web/packages/argparser/[href=https://cran.r-project.org/web/packages/argparser/]) with install_packages(\"argparser\") and parse command line arguments from Perseus.\n         \nargv <- commandArgs(trailingOnly=TRUE)\nlibrary(\"argparser\")\np <- arg_parser(description = \"Head processing\")\np <- add_argument(p, `input', help=\"path of the input file\")\np <- add_argument(p, `output', help=\"path of the output file\")\np <- add_argument(p, `--nrow', type=\"numeric\", default=15, help=\"the number of row\")\nargp <- parse_args(p, argv)\nThe first two arguments \"input\" and \"output\" are required arguments for storing the input and output files, respectively. An additional optional argument for the number of rows \"--nrow\" is added. Its default value is set to 15. Please refer to the argparser manual for more details.\n2. Use PerseusR to read the data in Perseus.\nIn the same was as for the scripts without parameters, PerseusR needs to be imported, and read.perseus is used for converting the matrix from Perseus to R format.\n         \nlibrary(PerseusR)\nmdata <- read.perseus(inFile)\n3. Get the main matrix of Perseus for the data processing.\nUse main() to obtain the main matrix that is needed for the following data processing.\n         \ncounts <- main(mdata)\n4. Execute the main part for data analysis or modification.\nThe information from the additional parameter was already parsed and stored. The extraction of the head of the matrix can be performed based on the number of rows that the user assigned.\n         \nmdata2 <- head(counts, n=num)aCols <- head(annotCols(mdata), n=num)\n5. Export the output matrix to Perseus with correct format.\nThis step is the same as last section. Generate matrixData() and write to Perseus by using write.perseus().\n         \nmdata2 <- matrixData(main=mdata2, annotCols=aCols, annotRows=annotRows(mdata))\nprint(paste(`writing to', outFile))\nwrite.perseus(mdata2, outFile)\n6. Apply the plugin in Perseus.\nNow the plugin is ready to be executed. The number of extracted rows can be assigned from \u201cAdditional arguments\u201d in the pop-up window (Fig. 5B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0005]). For instance, writing \u201c--nrow 10\u201d in \u201cAdditional arguments,\u201d only the first 10 rows will remain in the output matrix in Perseus.In recent years, many useful Python packages have been developed for computational biology and data visualization. Moreover, an annual conference (SciPy) provides a platform where up-to-date Python tools are released and presented. Perseuspy builds a bridge to integrate Python libraries into Perseus as plugins (Rudolph & Cox, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0016]). In this section, we provide the basic steps for generating Python-only plugins through the command line style interface. The code for this example is available at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/scripts/head.py[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/scripts/head.py].\nNecessary Resources\nHardware\nA computer running Windows 8 (64 bit) or higher or Windows Server 2008 or higher\n4 GB RAM minimum\nAt least a quad core processor is recommended\nSoftware\nPerseus 1.6.13 or higher: can be downloaded from https://maxquant.org/perseus[href=https://maxquant.org/perseus].\nPython: use version higher than 3.7.0. The Python executable has to be listed in the PATH environment variable of the operating system. Please refer to Troubleshooting if Perseus cannot find your Python installation, which is indicated in the command line\u2212style interface.\nPerseuspy: available at https://github.com/cox-labs/perseuspy[href=https://github.com/cox-labs/perseuspy]. An installation guide and required dependencies can be found in the repository. Also available on PyPI (https://pypi.org/project/perseuspy/[href=https://pypi.org/project/perseuspy/]).\nPython supported editor like Visual Studio, PyCharm or Notepad++\nInput files\nThis protocol requires no extra input files. The outlined plugin works with a randomly generated matrix, which can be generated using the dice button in Perseus.\n1. Import the required packages.\n         \nimport sys\nfrom perseuspy import pd\nPerseuspy builds on top of the widely-used pandas package to handle matrix processing (Mckinney, 2010[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0012]). The nested namespace pd contains this extended version of pandas.\n2. Parse command line arguments from Perseus.\nThe communication between Perseus and Python works through temporary files containing the data, their location being specified by fixed index command line arguments. Therefore, these command line arguments sent from Perseus need to be parsed first.\n         \n_, infile, outfile = sys.argvBy default, the name of the Python script occupies the first argument of sys.argv and the input and output file are always the last two arguments. Thus, the order of the arguments is: the name of Python script, input file, and output file.\n3. Read the data from Perseus.\nUsing the new pandas function read_perseus the data matrix from Perseus can be read from the input file directly into a pandas data frame object.\n         \ndf = pd.read_perseus(infile)\n4. The main custom code for data analysis or modification.\nThis part is for the custom desired data analysis. In this example, the code for extraction of the head of the matrix is placed in this position.\n         \ndf2 = df.head(15)\nBased on the code, only the first 15 rows will be kept in the matrix.\n5. Export the output matrix to Perseus with correct format.\nWhen all the steps of the data processing are done, the final matrix needs to be exported to Perseus in the correct format. For this, the second new function in pandas, to_perseus, can be used.\n         \ndf2.to_perseus(outfile)\n6. Apply the plugin in Perseus.\n         \nOpen Perseus and import the matrix or load a session.\nA random matrix is used for testing the plugin in this tutorial.\nExecute the plugin.\nIn the \u201cProcessing\u201d block, click \u201cExternal\u201d \u2013> \u201cMatrix \u2013> Python\u201d. If the button \u201cselect\u201d is green, it means that Perseus recognized your Python installation and perseuspy (Fig. 6A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0006]); otherwise, navigate to your python.exe or add it to your systems PATH variable. Afterwards, specify the Python script that you want to execute and click OK (Fig. 6B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0006]).For a more elaborate analysis, Python plugins can also be passed additional arguments just like R plugins. The following steps will demonstrate the steps needed for adding parameters to plugins. The example the number of rows to obtain from the top of the matrix can be specified by the user. The script is available at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/scripts/head_add_argument.py[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/scripts/head_add_argument.py].\nNecessary Resources\nSame as Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0003]\nAdditionally, the package argparse needs to be installed in the Python environment\n1. Import the required packages.\n         \nimport argparse\nfrom perseuspy import pd\n2. Parse command line arguments from Perseus.\n         \nparser = argparse.ArgumentParser(\"Head processing\")\nparser.add_argument(\"input\", help=\"path of the input file\")\nparser.add_argument(\"output\", help=\"path of the output file\")\nparser.add_argument(\"--nrow\", type=int, default=15, help=\"the number of row\")\narg = parser.parse_args()\nThis part is similar to \u201cParse command line arguments from Perseus\u201d in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0002]. The first two arguments \"input\" and \"output\" are positional arguments for storing the input and output files, respectively. The third argument, \"--nrow\" is for the number of rows.\n3. Read the data from Perseus.\n         \ndf = pd.read_perseus(arg.input)\n4. Retrieve the user-define arguments (--nrow) to modify the matrix.\nThe number of rows stored in \"--nrow\", can now be used for the extraction.\n         \ndf_head = df.head(arg.nrow)\n5. Export the output matrix to Perseus with correct format.\n         \ndf2.to_perseus(arg.output)\n6. Apply the plugin in Perseus.\nFor using the newly added parameter as an input, the assignment needs to be written in \u201cAdditional arguments\u201d in the pop-up window (Fig. 6B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0006]). If \u201cAdditional arguments\u201d is filled in with \u201c--nrow 10\u201d, the output matrix will be the first 10 rows.Even better integrated plugins with an automatically generated graphical user interface for the parameters can be generated when using C#, which is the original programming language of Perseus. The architecture of Perseus plugins is systematic and well structured. Numerous C# plugins can be found at https://github.com/JurgenCox/perseus-plugins[href=https://github.com/JurgenCox/perseus-plugins]. All the scripts can be recycled and modified by users. The same basic example as in the previous sections will be used to explain how to generate a C# plugin. The script of the example can be seen at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Head_c_sharp.cs[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Head_c_sharp.cs].\nNecessary Resources\nHardware\nA computer running Windows 8 (64 bit) or higher, or Windows Server 2008 or higher\n4 GB RAM minimum\nAt least a quad core processor is recommended\nSoftware\nPerseus 1.6.13 or higher: can be downloaded from https://maxquant.org/perseus[href=https://maxquant.org/perseus]\nFor editing C# code, Visual Studio Community Edition is recommended (https://www.visualstudio.com/downloads/[href=https://www.visualstudio.com/downloads/]). Please select the \u201c.Net Desktop Development workflow\u201d in the installer to install everything required. To ensure version compatibility, please use .NET Framework <= 4.7.2 or .NET Core <= 2.1.\n.NET packages BaseLibS and PerseusAPI: Both of these can be installed by using \u201cManage NuGet Packages\u201d in Visual Studio, which is explained in step 2 of the protocol\n1. Create a C# project.\nDue to the internal connection between the plugin and Perseus, the project name should use \u201cPlugin\u201d as prefix and the type of project should be \u201cClass Library (.NET framework).\u201d In this demonstration, the Project name was set as \u201cPluginTutorial\u201d.\n2. Add the dependencies.\nPerseusAPI and BaseLibS need to be installed for generating plugins. For installation of these two packages, right-click on the \u201cPluginTutorial\u201d solution and choose \u201cManage NuGet Packages\u2026.\u201d Afterwards, search for PerseusApi in the \u201cBrowse\u201d tab and install it for \u201cPluginTutorial\u201d. The PerseusAPI and its dependency BaseLibS will be added.\n3. Import packages and define namespace with a class.In the Class1.cs file, remove all default packages and code generated by Visual Studio, and instead import the packages as shown here:\n         \nusing System.Linq;\nusing BaseLibS.Graph;\nusing BaseLibS.Param;\nusing PerseusApi.Document;\nusing PerseusApi.Generic;\nusing PerseusApi.Matrix;\nnamespace PluginTutorial\n{\n\u2003public class PluginHead : IMatrixProcessing\n\u2003{\n\u2003}\n}\nThe project name (PluginTutorial) should be placed in namespace, and the user-defined class (PluginHead) should inherit IMatrixProcessing, which is created for processing the matrix from Perseus.\n4. Generate basic structure of the C# plugin.\nTo make IMatrixProcessing work, a number of methods and variables need to be implemented.\n         \nnamespace PluginTutorial\n{\n\u00a0public class PluginHead : IMatrixProcessing\n\u2003{\n\u2003public bool HasButton => false;\n\u2003public string Description => \"extract the header of the matrix.\";\n\u2003public string HelpOutput => \"extract the header of the matrix.\";\n\u2003public string[] HelpSupplTables => new string[0];\n\u2003public int NumSupplTables => 0;\n\u2003public string Name => \"Head CS only\";\n\u2003public string Heading => \"Tutorial\";\n\u2003public float DisplayRank => 6;\n\u2003public string[] HelpDocuments => new string[0];\n\u2003public int NumDocuments => 0;\n\u2003public string Url => null;\n\u2003public Bitmap2 DisplayImage => null;\n\u2003public bool IsActive => true;\n\u2003public int GetMaxThreads(Parameters parameters)\n\u2003{\n\u2003\u2003return 1;\n\u2003}\n\u2003public void ProcessData(IMatrixData mdata, Parameters param, ref\n\u2003IMatrixData[] supplTables,\n\u2003ref IDocumentData[] documents, ProcessInfo processInfo)\n\u2003{\n\u2003}\n\u2003public Parameters GetParameters(IMatrixData mdata, ref string errorString)\n\u2003{\n\u00a0}\n\u00a0}\n}To customize the plugin, the modification of several parameters and methods is required. Name defines the name of the plugin as it will appear in Perseus (Fig. 7A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007]). Heading defines the name of the drop-down menu the plugin will be added to (Fig. 7A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007]). If the name of the menu does not yet exist, a new one will be created automatically without requiring further changes in other places. Optional parameter changes are the following: the Description of the plugin, which will be shown when hovering over the plugin in Perseus (Fig. 7A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007]); the Url of the plugin's documentation, which can be opened by clicking the ghost icon besides the \u201cOK\u201d button (Fig. 7B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007]); and DisplayImage, which is the icon appearing next to the plugin name in the menu. If more threads need to be used for the plugin, it can be changed in GetMaxThread. The code for actual data processing should be placed inside ProcessData. The additional input parameters required are defined in Parameters. The other parameters defined in the code above are not relevant for custom-generated plugins, but need to be defined for the class to be compiled successfully.\n5. Add parameters.\nThe method GetParameters returns a C# object (Parameters) containing all the parameters that the user has to provide in the interface.\n         \npublic Parameters GetParameters(IMatrixData mdata, ref string errorString)\n{\n\u2003return new Parameters(new IntParam(\"Number of rows\", 15)\n\u2003{\n\u2003\u2003Help = \"The number of rows for the header needs to be kept.\"\n\u2003});\n}In this example, an integer parameter was created (IntParam). The name of the parameter is \"Number of rows,\" and the default is 15. A description of the parameter can be added in Help. This description will appear when hovering over the parameter text (Fig. 7B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007]). If multiple parameters need to be added, the user just needs to add them one by one in Parameters, as shown below. A complete example can be seen at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Head_c_sharp_two_params.cs[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Head_c_sharp_two_params.cs].\n         \npublic Parameters GetParameters(IMatrixData mdata, ref string errorString)\n{\n\u2003return new Parameters(new IntParam(\"Number of rows\", 15)\n\u2003\u2003\u2003{Help = \"The number of rows to retain.\"},\n\u2003\u2003\u2003new IntParam(\"Number of columns\", 2)\n\u2003\u2003\u2003{Help = \"The number of columns to retain.\"}\n\u2003);\n}\n6. Generate the code for data processing.\nThe main data processing is defined in ProcessData. In this example, the first n rows of the matrix are extracted.\n         \npublic void ProcessData(IMatrixData mdata, Parameters param, ref IMatrixData[]\n\u2003supplTables, ref IDocumentData[] documents,\n\u2003ProcessInfo processInfo)\n{\n\u2003int lines = param.GetParam<int>(\"Number of rows\").Value;\n\u2003int[] head = Enumerable.Range(0, lines).ToArray();\n\u2003mdata.ExtractRows(head);\n}\nparam.GetParam<int>(\"Number of rows\").Value will get the value stored in the integer parameter called \"Number of rows\", which was created in GetParameters before. The other lines use this number to extract the head of the matrix. mdata is the object that stores all the information on the Perseus matrix and ExtractRows can be used to extract the rows of the matrix based on the list of indices generated from the user input.\n7. Compile the plugin and copy the dll.Since C# is a compiled language, the next step is to build the project, which will generate a .dll file. If the build was successful, PluginTutorial.dll will be saved in the \u201cbin/Debug\u201d folder of the project directory, which you can open by right-clicking the project in the Solution Explorer \u2013> \u201cOpen Folder in File Explorer\u201d (PluginTutorial\\bin\\Debug, Fig. 7C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007]). For adding the newly created plugin to Perseus, this dll must be copied to the \u201cbin\u201d folder of Perseus (Perseus/bin, Fig. 7D[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007]). Afterwards, the plugin can be used in Perseus after a re-start (Fig. 7A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007] and 7B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007]).\n8. Apply the plugin in Perseus.\n         \nOpen Perseus and import the matrix or open a session.\nA random matrix is used for testing the plugin in this tutorial.\nExecute the plugin.\nClick \u201cTutorial\u201d \u2013> \u201cHead CS only\u201d in \u201cProcessing\u201d block (Fig. 7A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007]). Then, specify the number of rows for extraction and click OK (Fig. 7B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0007]).\nResource for C# Plugins\nFor more examples and source codes of C# plugins, please check the repository: https://github.com/JurgenCox/perseus-plugins[href=https://github.com/JurgenCox/perseus-plugins].Although C# can generate a user-friendly interface for the plugins, R and Python packages are still not able to be integrated into Perseus with the native C# interface. To combine the flexibility of R and Python with the graphical user interface generated by C#, the C# package PluginInterop was created (Rudolph & Cox, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0016]). Here, the basic methods of PluginInterop needed to create an R plugin with C# interface will be presented step by step. The R script can be found at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Resources/head_c_sharpR.R[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Resources/head_c_sharpR.R], and the C# script at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Head_with_r.cs[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Head_with_r.cs].\nNecessary Resources\nAll requirements of Basic Protocols 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0001] and 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0005]\nAdditionally, the C# package PluginInterop is required: this can also be installed by using \u201cManage NuGet Packages\u201d in a Visual Studio project as described in step 2.\n1. Create a C# project.\nSee step 1 of Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0005].\n2. Add the dependencies.\nBesides PerseusAPI and BaseLibS, PluginInterop also needs to be installed. The steps are the same as step 2 of Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0005]. Right-click on you project solution (PluginTutorial) and choose \u201cManage NuGet Packages\u2026.\u201d, then search for PluginInterop in the \u201cBrowse\u201d tab and install it to your project solution.\n3. Import packages and define namespace with a class.\nSince PluginInterop is the bridge between R and C#, it needs to be used in the wrapper. Additionally, the R script needs to be placed into the resource folder of the project (will be descripted at step 6). Thus, PluginTutorial.Properties needs to be used as well.\n         \nusing BaseLibS.Param;\nusing PerseusApi.Matrix;\nusing System.IO;\nusing PluginInterop;\nusing System.Text;\nusing PluginTutorial.Properties;\nnamespace PluginTutorial\n{\n\u2003public class HeadR : PluginInterop.R.MatrixProcessing\n\u2003{\n\u2003}\n}\nHere, the class HeadR inherits from PluginInterop.R.MatrixProcessing, which is the method for managing the matrix and parameter transfer between C# and the R script.\n4. Override methods in the class.PluginInterop.R.MatrixProcessing is a template for generating a C# plugin that can connect to R. For applying it to a user-specific project, some methods and variables need to be overridden.\n         \npublic class HeadR : PluginInterop.R.MatrixProcessing\n{\n\u2003public override string Heading => \"Tutorial\";\n\u2003public override string Name => \"Head with R\";\n\u2003public override string Description => \"extract the header of the matrix\";\n\u2003protected override bool TryGetCodeFile(Parameters param, out string\n\u2003codeFile)\n\u2003{\n\u2003\u2003\u2003byte[] code = (byte[])Resources.ResourceManager.GetObject(\"head_c_sharpR\");\n\u2003\u2003\u2003codeFile = Path.GetTempFileName();\n\u2003\u2003\u2003File.WriteAllText(codeFile, Encoding.UTF8.GetString(code));\n\u2003\u2003\u2003return true;\n\u2003}\n\u2003protected override string GetCommandLineArguments(Parameters param)\n\u2003{\n\u2003\u2003\u2003var tempFile = Path.GetTempFileName();\n\u2003\u2003\u2003param.ToFile(tempFile);\n\u2003\u2003\u2003return tempFile;\n\u2003}\n\u2003protected override Parameter[] SpecificParameters(IMatrixData mdata, ref string errString)\n\u2003{\n\u2003\u2003\u2003if (mdata.ColumnCount < 3)\n\u2003\u2003\u2003{\n\u2003\u2003\u2003errString = \"Please add at least 3 main columns to the matrix.\";\n\u2003\u2003\u2003return null;\n\u2003\u2003}\n\u2003\u2003return new Parameter[]\n\u2003\u2003{\n\u2003\u2003\u2003new IntParam(\"Number of rows\", 15)\n\u2003\u2003\u2003{\n\u2003\u2003\u2003Help = \"The number of rows for the header needs to be kept.\"\n\u2003\u2003\u2003}\n\u2003\u2003};\n\u2003}\n}\nHeading, Name and Description should be changed to match the needs of the project (the details of these methods were mentioned at step 4 of Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0005]). TryGetCodeFile() is for getting the R script from the project resources. The only thing that needs to be edited here is the name of the R script in (byte[])Resources.ResourceManager.GetObject(), omitting the file extension (head_c_sharpR). GetCommandLineArguments() will convert the parameters specified at SpecificParameters() and filled in by the user to a temporary file which can be read inside the R script. This method definition does not require further editing. SpecificParameters() is similar to GetParameters(), which was introduced at step 5 of Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0005]. It needs to return an array of parameter definitions, which will be used to generate the interface. For definition of several parameters please refer to Support Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0007], which shows a more advanced example.5. Generate R script for data processing.\nAt this point, the C# side of the interface has been established. Next, the R code for the actual data processing is added. The code is very similar to the script presented in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0002]. The only difference is that the additional parameters are transferred as one file from the C# interface, rather than as individual arguments. The order of the command line arguments is now parameter file, input file and output file.\n         \nargs = commandArgs(trailingOnly = TRUE)\nparamFile <- args[1]\ninFile <- args[2]\noutFile <- args[3]\nAfterwards, the user input has to be extracted by a function called \u201cparseParameters\u201d and several type-specific functions like intParamValue(), or boolParamValue(). Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-tbl-0001] summarizes the most commonly used functions and their C# counterparts. In this demonstration, the type of the parameter is integer and the name is \u201cNumber of rows.\u201d\n         \nparameters <- parseParameters(paramFile)\nnum_row <- intParamValue(parameters, `Number of rows')\nThe remainder of the R script is the same as in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0002].\n6. Store R script to resource folder.\nAs mentioned in steps 2 and 3, the R script needs to be placed in the resource folder of the C# project. To do so follow these steps:\n         \nRight-click \u201cProperties\u201d under the project (PluginTutorial) in Visual Studio.\nClick \u201cResources.\u201d\nClick \u201cAdd Resource\u201d and navigate to the target R script (head_c_sharpR.R). Then save it.\n7. Build the solution and place the required files to the bin folder of Perseus.Just as with a C#-only plugin, the code needs to be compiled and moved to the Perseus bin folder. Please refer to step 7 of Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0005] for this. The only difference is that the .pdb file also has to be transferred, as it contains the R script. Thus, PluginTutorial.dll and PluginTutorial.pdb need to be copied. The plugins can be used after re-starting Perseus (Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0001]).UMAP (Becht et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0002]; McInnes, Healy, & Melville, 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0011]) is a powerful dimensionality-reduction algorithm that is widely used for many different studies. It will be extremely useful to add UMAP to Perseus. This section will take UMAP as an advanced example to demonstrate how powerful the new Perseus plugin interface is for data analysis. The C# script can be found at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/UmapAnalysis_with_r.cs[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/UmapAnalysis_with_r.cs], and the R script is saved at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Resources/Umap_R.R[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Resources/Umap_R.R].\nNecessary Resources\nAll of the resources listed in Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006]\nUMAP: a dimensionality-reduction method. The R version of UMAP can be found at CRAN (https://cran.r-project.org/web/packages/umap/index.html[href=https://cran.r-project.org/web/packages/umap/index.html])\nInput files\nThe samples for the example of UMAP analysis can be downloaded at PRIDE (PXD003710) (Bailey, McDevitt, Westphall, Pagliarini, & Coon, 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0001]). Additionally, the MaxQuant (Cox & Mann, 2008[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0005]; Sinitcyn, Rudolph, & Cox, 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0017]; Tyanova, Temu, & Cox, 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0020]; Yu, Kiriakidou, & Cox, 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0023]) proteinGroup table of this dataset is also provided at https://github.com/JurgenCox/perseus-plugin-programming/tree/master/dataset[href=https://github.com/JurgenCox/perseus-plugin-programming/tree/master/dataset]. The values are normalized and transformed by logarithm, and the unreliable protein groups (reversed, only identified by site, contaminant, containing more than 30% missing values) are all removed from the table. Moreover, the data is well annotated by experimental design. This table can be directly used for the advanced example.\n1. Create a C# project.\nPlease see step 1 of Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0005].\n2. Add the dependencies.\nPlease see step 2 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006].\n3. Import packages and define namespace with a class.\nPlease see step 3 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006].\n4. Override methods in the class.\nIn general, the practical procedures are the same as in step 4 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006]. The only requirement is to modify several methods and variables to match this example. First, the Head, Name, and Description need to be changed.\n         \npublic override string Heading => \"Tutorial\";public override string Name => \"Umap analysis with R\";\npublic override string Description => \"Applying Umap to cluster the data\";\nSecondly, the R script of UMAP analysis has to be assigned properly.\n         \nbyte[] code = (byte[])Resources.ResourceManager.GetObject(\"Umap_R\");\nThird, all the parameters should be added to SpecificParameters() one by one with their corresponding data type.\n         \nprotected override Parameter[] SpecificParameters(IMatrixData mdata, ref string errString)\n{\n\u2003if (mdata.ColumnCount < 3)\n\u2003{\n\u2003\u2003errString = \"Please add at least 3 main data columns to the matrix.\";\n\u2003\u2003return null;\n\u2003\u2003}\n\u2003return new Parameter[]\n\u2003{\n\u2003\u2003new IntParam(\"Number of neighbors\", 15)\n\u2003\u2003{\n\u2003\u2003\u2003\u2003\u2003\u2003Help = \"The number of neighbors.\"\n\u2003\u2003},\n\u2003\u2003new IntParam(\"Number of components\", 2)\n\u2003\u2003{\n\u2003\u2003\u2003\u2003\u2003\u2003Help = \"The number of components.\"\n\u2003\u2003},\n\u2003\u2003new IntParam(\"Random state\", 1)\n\u2003\u2003{\n\u2003\u2003\u2003\u2003\u2003\u2003Help = \"Set seed for reproducibility.\"\n\u2003\u2003},\n\u2003\u2003new DoubleParam(\"Minimum distance\", 0.1)\n\u2003\u2003{\n\u2003\u2003\u2003\u2003\u2003\u2003Help = \"Set minimum distance between the data point.\"\n\u2003\u2003},\n\u2003\u2003new SingleChoiceParam(\"Metric\")\n\u2003\u2003{\n\u2003\u2003\u2003\u2003\u2003\u2003Values= new[] { \"euclidean\", \"manhattan\", \"cosine\", \"pearson\",\"pearson2\"},\n\u2003\u2003\u2003\u2003\u2003\u2003Help = \"The method of metric for doing clustering.\"\n\u2003\u2003}\n\u2003};\n}\nThe structure of SingleChoiceParam() is different from IntParam() or DoubleParam(). SingleChoiceParam() has to contain an array called Values to store all the options for the users. The first element of Values is the default one, which will appear on the dropdown list of the Perseus plugin.\n5. Generate R script for data processing.\nThe basic rules for generating an R script are the same as in step 5 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006]. Hence, the most important thing is to obtain the information of parameters from C#. The other parts of the R script can be done like normal R programming and are not shown here.\n         \nargs = commandArgs(trailingOnly = TRUE)\nparamFile <- args[1]\ninFile <- args[2]\noutFile <- args[3]\nparameters <- parseParameters(paramFile)\nn_neighbor <- intParamValue(parameters, \"Number of neighbors\")\nn_component <- intParamValue(parameters, \"Number of components\")seed <- intParamValue(parameters, \"Random state\")\nmetric <- singleChoiceParamValue(parameters, \"Metric\")\nm_dist <- intParamValue(parameters, \"Minimum distance\")\nintParamValue() can also be used to receive the value with the double-precision data type because R can handle the conversion at the first assignment of the variables.\n6. Store R script to resource folder.\nPlease see step 6 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006].\n7. Build the solution and place the required files to the bin folder of Perseus\nPlease see step 7 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006].\n8. Run UMAP and plot the result.\nIn this section, the dataset from Bailey, D. J., et\u00a0al. will be applied to test the newly developed plugin of UMAP (Bailey et\u00a0al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0001]).\n         \nOpen Perseus and load proteinGroup.txt.\nSince proteinGroup.txt is already pre-processed and grouped, it can be directly loaded into Perseus by clicking the green icon of arrow in the block of \u201cLoad.\u201d\nRun R plugin of UMAP.\nClick \u201cTutorial\u201d \u2212> \u201cUmap analysis with R\u201d to specify the parameters and run the plugin (Fig. 2A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0002] and 2B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0002]). After running the plugin of UMAP, the matrix will be transposed and the main values will be changed to components.\nPlot the result of UMAP.\nUse scatter plot (with columns) to view the result of the UMAP analysis. The outcome shows that the data points are clustered based on cell types (Fig. 2C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0002]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/77385475-46ea-4a8e-a862-73bdb5fc5ca4/cpbi105-fig-0002-m.jpg</p>\nFigure 2\nC# + R plugin for running UMAP analysis. (A) Menu item for running UMAP plugin. The pop-up window with the arguments of the plugin is presented in (B). (C) Shows the results of UMAP analysis in a scatter plot in Perseus.This protocol will continue to demonstrate how to generate Python plugins with C# interface using the same examples as Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006]. The C# script of the basic example can be found at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Head_with_py.cs[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Head_with_py.cs], and the Python code can be found at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Resources/head_c_sharpPy.py[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Resources/head_c_sharpPy.py].\nNecessary Resources\nAll requirements of Basic Protocols 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0003] and 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0005]\nAdditionally, the C# package PluginInterop is required: this can also be installed by using \u201cManage NuGet Packages\u201d in a Visual Studio project as described in step 2\n1. Create a C# project.\nPlease see step 1 of Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0005].\n2. Add the dependencies.\nPlease see step 2 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006].\n3. Import packages and define namespace with a class.\nThe only difference between this step and step 3 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006] is that the inheritance has to be edited for connecting Python instead of R to C#.\n         \npublic class HeadPy : PluginInterop.Python.MatrixProcessing\nLike PluginInterop.R.MatrixProcessing, PluginInterop.Python.MatrixProcessing is for managing the matrix of Perseus and the communication between Python and C#. Of course, the name of the class can be changed as well (from HeadR to HeadPy).\n4. Override methods in the class.\nThe majority of the code is the same as step 4 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006]. Only several commands need to be modified to match the needs of integrating the Python script.\n         \npublic override string Heading => \"Tutorial\";\npublic override string Name => \"Head with Python\";\npublic override string Description => \"extract the header of the matrix\";\nprotected override bool TryGetCodeFile(Parameters param, out string codeFile)\n{\n\u2003byte[] code = (byte[])Resources.ResourceManager.GetObject(\n\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\u2003\"head_c_sharpPy\");\n\u2003codeFile = Path.GetTempFileName();\n\u2003File.WriteAllText(codeFile, Encoding.UTF8.GetString(code));\n\u2003return true;\n}\nBased on the above code, Heading, Name and Description were changed from R to Python. Additionally, the name of Python script was assigned to (byte[])Resources.ResourceManager.GetObject().\n5. Generate Python script of data processing.For the basic principles of generating Python plugins, please refer to Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0003]. In order to gain the information about the parameters assigned through the C# interface, a variable needs to be created for receiving the file storing the parameter values.\n         \n_, paramfile, infile, outfile = sys.argv\nparse_parameters() is a function for parsing the parameters from the parameter file. Based on different data types, all the values of parameters can be used for the Python script by applying corresponding functions like intParam(). Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-tbl-0001] summarizes the most commonly used functions and their C# counterparts.\n         \nparameters = parse_parameters(paramfile)\nhead = intParam(parameters, \"Number of rows\")\n6. Store Python script to resource folder.\nPlease see step 6 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006].\n7. Build the solution and place the required files to the bin folder of Perseus\nPlease see step 7 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006].\nIf all the procedures are done, the plugin will be shown in Perseus (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0003])\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/cf3c6bda-6ba6-4143-9a93-3014d4e63a2e/cpbi105-fig-0003-m.jpg</p>\nFigure 3\nC# + Python plugin in Perseus. (A) Menu item for running C# + Python plugin. The pop-up window containing the parameters of the plugin is shown in (B).Since UMAP is also available in Python, the same analysis can be used as an advanced example to show the power of Perseus Plugins for data analysis. The C# and Python scripts are listed at https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/UmapAnalysis_with_py.cs[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/UmapAnalysis_with_py.cs], and https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Resources/Umap_Py.py[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/PluginTutorial/Resources/Umap_Py.py], respectively.\nNecessary Resources\nAll of the requirements listed in Basic Protocol 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0008].\nUMAP: a dimensionality-reduction method. The Python version of UMAP can be found at PyPI (https://pypi.org/project/umap-learn/[href=https://pypi.org/project/umap-learn/]).\nInput files\nSame as Support Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0007]\n1. Create a C# project.\nPlease see step 1 of Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0005].\n2. Add the dependencies.\nPlease see step 2 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006].\n3. Import packages and define namespace with a class.\nPlease see step 3 of Basic Protocol 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0008].\n4. Override methods in the class.\nThe most important thing is to specify the correct names of plugin and Python script.\n         \npublic override string Heading => \"Tutorial\";\npublic override string Name => \"Umap analysis with Python\";\npublic override string Description => \"Applying Umap to cluster the data\";\nprotected override bool TryGetCodeFile(Parameters param, out string codeFile)\n{\n\u2003byte[] code = (byte[])Resources.ResourceManager.GetObject(\"Umap_Py\");\n\u2003codeFile = Path.GetTempFileName();\n\u2003File.WriteAllText(codeFile, Encoding.UTF8.GetString(code));\n\u2003return true;\n}\n5. Generate Python script for data processing.\nSince this part of data processing will be done in Python script, all the parameters need to be transferred to Python variables.\n         \nn_neighbor = intParam(parameters, \"Number of neighbors\")\nn_component = intParam(parameters, \"Number of components\")\nseed = intParam(parameters, \"Random state\")\nm_dist = doubleParam(parameters, \"Minimum distance\")\nmetric = singleChoiceParam(parameters, \"Metric\")As in step 5 of Support Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0007], intParam() can be used for handling the double-precision data type because Python can automatically adjust for the conversion. Additionally, the Perseus matrix has to be transposed due to the input format of UMAP. Therefore, the category rows need to be extracted for the modification as well. Similar to annotRows in PerseusR, read_annotation can return a matrix containing all category rows.\n         \nannotations = read_annotations(infile)\nFurthermore, Perseuspy has a function called main_df(), which is similar to main() in PerseusR, for extracting the main matrix in Perseus. Based on this, only the values of the main matrix will be used for the UMAP analysis, which is not shown here.\n         \nnewDF1 = main_df(infile, df)\n6. Store Python script to resource folder.\nPlease see step 6 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006].\n7. Build the solution and place the required files in the bin folder of Perseus\nPlease see step 6 of Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0006].\n8. Run UMAP and plot the results.\nPlease see step 8 of Support Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0007]. Figure 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0004] shows the outcome of the plugin and the results of UMAP with the data points grouped based on cell types.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/eddc7107-4b73-494e-b773-c872c8d4e312/cpbi105-fig-0004-m.jpg</p>\nFigure 4\nC# + Python plugin for running UMAP analysis. (A) Highlights the menu item for running the UMAP plugin. The pop-up window including the arguments of the plugin is presented in (B). (C) Shows the results of UMAP analysis in a scatter plot.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/746bd63a-1767-412f-80e9-a66466d9a6a1/cpbi105-fig-0005-m.jpg</p>\nFigure 5\nR plugin in Perseus. (A) The labeled menu item is for running an R plugin from an external script. (B) Popup window for specifying the path of the script and additional parameters of the plugin.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/3d232650-b7f7-44f3-a474-e0cc41c559b2/cpbi105-fig-0006-m.jpg</p>\nFigure 6Python plugin in Perseus. (A) The labeled menu item is for running a Python plugin from an external script. (B) Popup window for specifying the path of the script and additional parameters of the plugin.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d301a265-5bbd-4b58-bb71-0d47427554fb/cpbi105-fig-0007-m.jpg</p>\nFigure 7\nC# plugin in Perseus. (A) The labeled menu item is for running a C# plugin. The description specified in the C# code is shown when the mouse hovers over it. (B) The popup window for specifying the values of parameters. The explanation specified in the script is displayed when hovering with the mouse. For adding plugins in Perseus, the selected files in (C) (the folder of $PROJECT_NAME/bin/Debug) need to be copied to the folder in (D) (Perseus/bin).Based on the above protocols, Perseus plugins can be generated according to the user's needs. In order to demonstrate the benefits that Perseus can offer for data analysis, a basic workflow for the analysis of label-free quantification (LFQ) will be presented in this section. The UMAP plugin generated via Support Protocols 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0007] and 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0009] can be applied to this analysis. The samples are from a part of the dataset in a Proteome Informatics Research Group (iPRG) 2015 Study (Choi et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-bib-0003]). The proteinGroup table used for this example can be downloaded from https://github.com/JurgenCox/perseus-plugin-programming/blob/master/dataset/proteinGroups_LFQ.txt[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/dataset/proteinGroups_LFQ.txt].\nNecessary Resources\nAll of the requirements listed in Basic Protocol 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0008] and Support Protocol 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-prot-0009]\nInput files\nproteinGroups_LFQ.txt from https://github.com/JurgenCox/perseus-plugin-programming/blob/master/dataset/proteinGroups_LFQ.txt[href=https://github.com/JurgenCox/perseus-plugin-programming/blob/master/dataset/proteinGroups_LFQ.txt]. This dataset contains three samples named as 1, 2, and 3. Moreover, each sample has three technical replicates labeled as A, B, and C.\nThe workflow, plugins, and settings for the basic analysis are shown in Figure 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0008]. The results of most commonly used statistics methods\u2013differential expression analysis (ANOVA test is used) and dimensionality reduction (UMAP is applied) are presented in Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0009] and 10[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.105#cpbi105-fig-0010]. This example only demonstrates a basic workflow. Perseus contains numerous useful plugins and parameters. The user can change the settings based on different requirements.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/eb71ed74-7715-4724-a23e-946f79cd71c5/cpbi105-fig-0008-m.jpg</p>\nFigure 8\nWorkflow and settings of a basic analysis. The rectangle blocks represent the steps for the analysis. The blue paths are the plugins used for the steps. The green statements are the settings that need to be changed and results of the plugins; the rest of the parameters can remain as the default.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/72514129-032f-4cec-9187-e92e08fd4f4e/cpbi105-fig-0009-m.jpg</p>\nFigure 9\nVolcano plots of the example LFQ dataset. (A, B, and C) show the volcano plots of sample 1 versus 2, sample 1 versus 3, and sample 2 versus 3, respectively. The red squares represent the differential expressed proteins.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/ed6b1761-8d91-4e96-a3bd-19970388e7bb/cpbi105-fig-0010-m.jpg</p>Figure 10\nUMAP plot of the example LFQ dataset. The blue, red, and green squares represent the sample 1, 2, and 3, respectively.", "This Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0001] is intended as a first contact with the BioGateway Cytoscape app, focusing on the use of the Human data. As data for several additional species is now being added to the BioGateway triple store, the protocols also can be used for other taxa, with the exception of steps that involve relationships of transcription factors and their target genes. Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0001] explains how to import one node into an empty Cytoscape canvas, explore the available information for that node, and use it to create a small protein-protein interaction network together with Gene Ontology annotations. The protocol illustrates the exploration of RASK (UNIPROT ID: P01116), a protein with an important role in regulation of cell proliferation. The protocol will start by importing a node representing the protein RASK and the gene coding for it, followed by the identification of all proteins interacting with RASK and the GO Biological Processes that RASK is involved in.\nBefore starting, it is important to note that (1) BioGateway does differentiate between proteins and genes, and treats them as separate entities that require specific queries; and, (2) for reasons of accuracy, the naming of nodes representing proteins (boxes) and genes (ovals) always uses the GENE name. Because every gene encodes many different protein isoforms, we have chosen to treat the main entity protein as a \u201cbag\u201d representing all isoforms that may exist for that protein, and name that \u201cbag\u201d with the name of its gene.\nThus, the aim of this protocol is to show users how to import nodes of interest (protein, gene, Gene Ontology term, etc.) into the Cytoscape canvas and find the relations going to/from that node and its neighbors.\nNecessary Resources\nHardwareThis protocol requires users to have an up-to-date Macintosh, Unix/Linux, or Windows computer capable of running Cytoscape version 3.0 or later (https://cytoscape.org/manual/Cytoscape2_8Manual.html#System%20requirements[href=https://cytoscape.org/manual/Cytoscape2_8Manual.html#System%20requirements]). Finally, it is necessary to have a stable broadband connection to the Internet in order to smoothly execute the protocols.\nSoftware\nCytoscape and the BioGateway Cytoscape plugin are required to follow this protocol. The Support Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0006] describes how to download and install the required software. During the writing of this protocol, version 3.8.0 of Cytoscape and version 3.2.2 of the BioGateway plugin were used. However, the users can follow this protocol by installing the latest versions of both Cytoscape and the BioGateway plugin.\n1. Launch Cytoscape, and create an empty network (File > New Network > Empty). Name your network \u201cNetwork 1.\u201d\n2. Open the BioGateway configuration panel by clicking on the BioGateway tab in the Cytoscape control panel, expand the Active Taxa selection, and make sure that only the taxon Homo sapiens is activated (Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0001]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/cf81dc0b-0732-4fed-95da-f2008a85a46c/cpbi106-fig-0001-m.jpg</p>\nFigure 1\nThe BioGateway Tab in the Cytoscape Control Panel showing some of the possible Active Properties to use when querying. The Active Taxa tree allows users to select what taxa they want to work with.\n3. Right click anywhere in the empty Cytoscape canvas and select in the pop-up window the option BioGateway. Next, click Add BioGateway node (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0002]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/81eff77c-7682-4ee3-903d-03ac4c6daec3/cpbi106-fig-0002-m.jpg</p>\nFigure 2\nThe BioGateway menu displayed when right clicking anywhere on the Cytoscape Canvas.\n4. This will open the BioGateway Node Lookup dialog box, where the user can search for the name of a protein, gene, GO term, taxon, or disease of interest. The default name to search for is set to \u201cProtein.\u201d Enter \u201cRASK\u201d in the query box and click the Search button.If multiple species are active, the search will produce more results, and the search time may accordingly be longer.\n5. The result will be shown in the Results Panel of the dialog. Select the protein of interest and import by clicking the Use Selected Node button (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0003]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/39402e45-ba11-493a-abce-ccf93349f621/cpbi106-fig-0003-m.jpg</p>\nFigure 3\nThe Query Result window after looking for the protein \u201cRASK.\u201d\n6. The Cytoscape canvas will now contain a square-shaped node representing the RASK protein. As noted above, it is important to remember that BioGateway treats genes and proteins as different entities. To illustrate the information that BioGateway has about this node, right click on the KRAS node, select BioGateway Fetch relations TO node, and observe several options: Search for all relation types, molecularly interacts with, and encodes. For each, the origin of the relationship is indicated. Select the option Search for all relation types (Fig. 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0004]). This will query the BioGateway server for all relationships that include the protein RASK as target node.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/fc9c6f51-b065-4b10-bc9e-ecd7bad23df8/cpbi106-fig-0004-m.jpg</p>\nFigure 4\nThe BioGateway menu displayed when right-clicking on a BioGateway node in the Cytoscape Canvas.7. A dialog will open showing the results of the query launched in step 6. There are many different relation types returned, and they can be sorted by clicking on the column header relation type. Alternatively, by typing a search string in the Filter results box, the results will be limited to those matching the typed characters. By highlighting one or more types of relationships (in one or more steps), the desired relationships can be imported to a network by clicking the Import Selected button. First select first all relationships of the type \u201cencodes,\u201d and import to the canvas by clicking Import Selected (Fig. 5A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0005]). Next, select all relationships of the type \u201cmolecularly interacts with\u201d and \u201cinvolved in,\u201d and import. Notice that all new nodes are superimposed in the canvas (because of a Cytoscape \u201cfeature\u201d), and press F5 to prompt Cytoscape to render a deconvoluted network (Fig. 5B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0005]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/20c35b64-98b7-40ae-a702-f121b33399b9/cpbi106-fig-0005-m.jpg</p>\nFigure 5\nFinding all relationships leading to the selected node, filtering results, and importing them. (A) The Query Result window after filtering the results containing \u201cencodes\u201d in any of the fields. (B) The resulting network after importing the results shown in (A). BioGateway displays proteins and genes as separate entities, where square-shaped nodes represent protein nodes and oval-shaped nodes represent genes.8. Note that alternatively you can select the \u201cmolecularly interacts with\u201d relationships for RASK, involving Protein\u2212 Interactions (PPIs) that RASK is involved in, by right clicking on the KRAS protein node and selecting the option BioGateway > Fetch relations FROM node > IntAct: molecularly interacts with. This will query the BioGateway server for all PPIs that include RASK (note that the same results will be obtained when clicking Fetch relations TO node, as PPIs do not have a direction). Likewise, the \u201cinvolved in\u201d relationships can be found by BioGateway > Fetch relations FROM node > GOA: involved in, which fetches all GOA annotations with biological process terms, for RASK. For these two separate queries, a dialog will open; select all relations and import them by clicking the Import Selected button.\n9. Observe that the network around the RASK protein now has diamond-shaped nodes with GO term names for biological process terms and box-shaped nodes representing proteins that the RASK protein is connected to (Fig. 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0006]). The information about these nodes is shown in the Cytoscape node table; the information about the relationships can be seen in the edge table. This network can now be further extended either by right-clicking any of the nodes in this network and launching new searches as above, or by using the query builder (see below).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d114a1bc-ef2b-46d0-90c7-dffc3a733a1d/cpbi106-fig-0006-m.jpg</p>\nFigure 6\nThe resulting network showing proteins as square-shaped nodes, genes as oval-shaped nodes, and GO terms as diamond-shaped nodes.This protocol builds on Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0001] and shows users how to produce the same network as above by using the Query Builder. After replicating these results some additional information will be added to the network, introducing the user to the building of queries that can be specified further in a step-by-step mode using multiple query lines. This approach allows a user either to broaden a network or to further select the network results by specifying additional criteria that the results should comply with. The protocol also shows the power of using identifiable wildcards to find the intersection between several query results.\nThe Query Builder is one of the most important ways of interacting with the BioGateway repository. It allows the creation of complex queries in an intuitive manner by \u201cstacking\u201d desired criteria. Every single criterion is represented as a line in the Query Builder, where each line is a statement that constitutes a Subject-Relationship-Object structure (similar to RDF). Figure 7A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0007] shows the Query Builder, where the three top fields marked in red represent the Subject, the Relationship, and the Object. Both the Subject and the Object box have the same setting options (Fig. 7B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0007]). From left to right, these are:\n         \n1.Entity or Set: To specify if the node to use as Subject/Object will be a specific named entity (e.g., Protein, Gene, Gene Ontology term, etc.,) or a set (wildcard).\n2.Entity type: To define if the Subject/Object is a protein, gene, GO term, or Disease. The remaining terms will not be used in the use cases covered in this protocol.\n3.Entity name: To insert the desired protein, gene, etc. As shown in Figure 7B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0007], an autocomplete function is started when the user starts typing in the text of interest.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/1578ade7-5589-4f25-9a07-f5aedc5b02af/cpbi106-fig-0007-m.jpg</p>\nFigure 7The BioGateway Query Builder. (A) The Query Builder Window: red boxes indicate the Subject, Relation, Object, and Autocomplete results. (B) Closer detail of the Subject part of the Query Builder after typing \u201cTP5\u201d: typing text into the search field triggers the autocomplete function, which provides all matches with the introduced text. Note that the Subject and the Object have the same structure. (C) Closer detail of the Relation part of the Query Builder: a drop-down menu allows the users to select the desired relation type.\nThe Relation (Fig. 7C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0007]) states the type of relationship to search for between the Subject and the Object. The possible relationships are TF-TG interactions, protein-protein Interactions (PPIs), gene encoding for a protein, Gene Ontology Annotations (GOAs), involvement in a disease, and orthology. The app provides user support to flag queries that do not make sense: for instance, if the query aims to fetch relationships between a transcription factor (by definition a protein) and a gene, the entity types that do not match the query will be shown in red.\nNecessary Resources\nSee Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0001]\n1. Introduce a new network in Cytoscape (File > New Network > Empty), and name it \u201cNetwork 2.\u201d\n2. Select the Biogateway Control Panel and open the Query Builder (Fig. 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0008]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/5f825803-a1d3-4b2a-a6e3-053828c0ee9f/cpbi106-fig-0008-m.jpg</p>\nFigure 8\nOpening the BioGateway Query Builder from the BioGateway Tab in the Cytoscape Control Panel.3. This will open the Query Builder box that allows the user to compose queries line by line, by specifying a first entity type [either a specific one or a group (e.g., Set A)]; a relation type (to be chosen from a drop-down list); and a second entity (again either a specific one or a Set). After each query line, the results can be observed by pressing the Run Query button at the bottom of the query box. Based on the volume and details of the results, the user may decide either to apply additional selections in a next query line, by mentioning the same entities of sets, or add additional network information by defining new entities or sets.\n4. The first entity is by default \u201cprotein,\u201d and typing RASK in the box next to it will launch the autocomplete function. Note that with each character the autocomplete matches become more specific. Select the autocomplete suggestion: KRAS: KRAS2, RASK2, RASK_HUMAN, P01116\u2212Homo Sapiens\u2013GTPase KRas. Note that the autocomplete function returns multiple gene names (KRAS, KRAS2, RASK2) as it supports synonym searches, and that for the protein the UniProt identifier (RASK_HUMAN) and UniProt accession number (P01116) are provided.\n5. The default relationship is set to \u201cIntAct: molecularly interacts with,\u201d so this can be left as is.\n6. Define the entity that RASK should interact with, by selecting \u201cSet A.\u201d This query line will now search for all protein-protein interactions that include the RASK protein. Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0009] shows the Query Builder after building the query defined so far.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/49e4d363-353a-481b-8a91-77d8c533942d/cpbi106-fig-0009-m.jpg</p>\nFigure 9\nThe Query Builder with a one-line query asking for all proteins interacting with RASK.\n7. Note that when selecting Run Query, the Query Results window will open showing the PPIs that contain RASK (Fig. 10[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0010]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/aeb68acb-bb65-44f1-b92b-6ec5d726d6d7/cpbi106-fig-0010-m.jpg</p>\nFigure 10The Query Result Tab after launching a query through the Query Builder.\n8. Next, go back to the Query Builder interface by clicking the Build Query button, and press Add Line (bottom of pane). Set the relation type to \u201cGOA: involved in,\u201d and again specify the KRAS protein as first entity. Select \u201cSet B\u201d as the second entity, specifying the query to fetch all GOA biological process annotations for KRAS.\n9. Add one more query line to fetch the gene encoding the RASK protein: specify \u201cgene\u201d as first entity, and allow for multiple hits (Set C). Select \u201cprotein\u201d as second entity, specify \u201cRASK,\u201d and select the RASK protein from the autocomplete result. Note that the gene needs to be specified first, as the \u201cencodes\u201d relationship has the TO direction (Fig. 11[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0011]). Your query is now identical to the selection steps performed in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0001]. Click Run Query, select all the results and Import to selected Network.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/68354afe-0d0c-4f0d-9e85-007efed2605d/cpbi106-fig-0011-m.jpg</p>\nFigure 11\nThe Query Builder showing the query that will yield the same network as Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0001].\n10. Cytoscape now has two networks, comparison will show that Network 1 is identical to Network 2 (Fig. 12[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0012]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/dfc8daf4-0228-4a45-bacd-839bea3ccda7/cpbi106-fig-0012-m.jpg</p>\nFigure 12\nThe network obtained through the Query Builder is the same as the one obtained in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0001].\n11. We will now refine the network by limiting the results to RASK interacting proteins that have the same biological process annotations as KRAS. Add a new query line, specify \u201cSet A,\u201d set the relationship to \u201cGOA: involved in,\u201d and specify the GO term as \u201cSet B.\u201d Run the query and observe that the number of relationships has now significantly dropped. Select all results and click Import to new Network.12. The new network (Fig. 13[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0013]) has only 25% of the nodes and half of the relationships, specifying a protein-protein interaction network around the RASK protein in which all pairs of interactors share the same biological process annotations.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/535e6b2d-0292-4fcc-a76d-bed7154844db/cpbi106-fig-0013-m.jpg</p>\nFigure 13\nThe resulting network after adding extra lines in the Query Builder.\n13. In a last step, we will fetch the genes for all the proteins in the network and select transcription factors that regulate them. Because there will be many TFs, we will select only those that regulate both the RASK gene and one of the other genes, and we further select them to share the same annotations as the proteins in the network. The steps to do this are:\n         \nFetch the genes coding for the proteins in set A: add a new query line and set the Subject Entity to \u201cSet D\u201d and its entity type to Gene. Next, set the relation type to \u201cencodes,\u201d and set the Object to \u201cSet A\u201d and its Entity type to Protein.\nFetch the transcription factors regulating Set D: add a new query line and set the Subject Entity to \u201cSet E\u201d and its Entity type to Protein. Now, set the relation type to \u201cinvolved in regulation of\u201d and set the Object Entity to \u201cSet D\u201d and its Entity Type to Gene.\nFetch the transcription factors regulating the RASK gene (which is not a member of set A!): add a new query line and set the Subject Entity to \u201cSet E\u201d and its Entity type to Protein. Next, select the relation type to \u201cinvolved in regulation of.\u201d Finally, set the Object Entity to \u201cRASK\u201d and its entity type to Gene.Limit set E to TFs with annotations shared by PPI pairs: add a new query line and select \u201cSet E\u201d in the Subject Entity and Protein as its Entity type. Next, select \u201cinvolved in\u201d as the relation type, select \u201cSet B\u201d as the Object Entity, and set \u201cGO term\u201d as its Entity type. The resulting Query can be seen in Figure 14[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0014].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/57ff3ce4-bfcf-415d-8ba1-621f7570914f/cpbi106-fig-0014-m.jpg</p>\nFigure 14\nThe Query Builder with the further expanded query.\n14. Queries can be saved and loaded by clicking the Save Query and Load Query buttons, respectively. Save the query and then click Run Query. Select all the results and Import to new Network.\n15. After pressing F5, the network will contain genes and proteins characterized by GO-BP terms Signal transduction and Ras signal transduction. In addition to the RASK protein, it contains other TFs interacting with it, the genes whose transcription they regulate, and the proteins encoded by these genes (Fig. 15[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0015]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/40250b72-9dae-4dd0-86b6-a5f5deb559bf/cpbi106-fig-0015-m.jpg</p>\nFigure 15\nThe resulting network with proteins interacting with RASK, the genes whose expression they regulate, the proteins encoding by these genes, and the GO Biological Process terms.This protocol will take users deeper into building queries through the BioGateway Query Builder by constructing a multiline query. The overarching biological questions will be \u201cWhat are some proteins that are relevant in both Breast and Colorectal Cancer? What Transcription Factors regulate the expression of the genes coding for these proteins?\u201d. Through this, we will also introduce the users to the UniProt Disease graph, which stores annotations of proteins and their roles in different diseases. Thus, in this protocol, users will learn to create a network that satisfies several criteria in one query. The protocol will start from the Query Builder dialog. To open this dialog, please refer to Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0002], step 2.\nNecessary Resources\nSee Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0001]\n1. Open the BioGateway Query Builder.\n2. Create a line asking for all the proteins annotated to be involved in Breast Cancer: set the subject Entity to \u201cSet A.\u201d Now, select \u201cDisease: involved in\u201d as the relation type and type \u201cBREAST CANCER\u201d in the search field for the Object. In the autocomplete results, select \u201cBREAST CANCER: BREAST CANCER, FAMILIAL.\u201d\n3. Add a second line to the query asking for all proteins annotated to be involved in Colorectal Cancer: add a query line and set the subject to \u201cSet A.\u201d Now, set the relation type to \u201cDisease: involved in\u201d as the relation type. Finally, type \u201cCOLORECTAL CANCER\u201d in the search field for the Object and select \u201cCOLORECTAL CANCER: COLON CANCER, CRC\u201d in the autocomplete results.\n4. Add a third line querying for all TFs regulating the expression of the genes found in lines 1 and 2 of the query: set the subject to \u201cSet B\u201d and select the relation type to \u201cinvolved in regulation of.\u201d Next, set the Object to \u201cSet A.\u201d Figure 16[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0016] shows the resulting query.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d09bc282-7cb7-4477-b898-d8f9fe5b91f5/cpbi106-fig-0016-m.jpg</p>\nFigure 16The Query Builder window after building the specified query.\n5. Run the query by clicking the Run Query button.\n6. In the Query Result window, select all results and import them to the Cytoscape canvas by clicking the Import to New Network button.\n7. Press F5 and explore the resulting network (Fig. 17[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0017]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/bb496161-a576-4d6e-b021-7c2f0b9333f8/cpbi106-fig-0017-m.jpg</p>\nFigure 17\nThe resulting network from Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0003].With this protocol, we aim to set a first example where users employ both the Query Builder and the point-and-click-based interactions. It will start with making a two-line query in the Query Builder to create a network. Next, we will further expand the network by directly interacting with some of its nodes. This example will introduce a feature allowing users to import only relations between already existing nodes. The biological aim in this example will be to find proteins with kinase activity that are involved in colorectal cancer, the genes encoding for them, the interactome around the found proteins, and potentially relevant TF-TG interactions.\nNecessary Resources\nSee Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0001]\n1. Build the first line of the query asking for proteins annotated with the Gene Ontology term \u201cprotein kinase activity\u201d: set the Subject entity to \u201cSet A\u201d and the Relation type to \u201cGOA: enables.\u201d Type \u201cprotein kinase activity\u201d in the Object search field and select the appropriate result.\n2. Add a second line to the query by clicking the Add Line button. Set the Subject entity to \u201cSet A\u201d and the Relation type to \u201cmolecularly interacts with.\u201d Now, set the Object entity to \u201cSet B.\u201d\n3. Add another line to the query by clicking the Add Line button. Set the Subject entity to \u201cSet C\u201d and the Relation type to \u201cencodes.\u201d Finally, set the Object entity to \u201cSet A.\u201d\n4. Add a last line asking for genes from line 3 that are involved in Colorectal Cancer: click the Add Line button. Set the Subject entity to \u201cSet C\u201d and the Relation type to \u201cDisease: involved in.\u201d Now, type \u201cCOLORECTAL CANCER\u201d in the Object search field and select the result \u201cCOLORECTAL CANCER: COLON CANCER, CRC.\u201d Figure 18[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0018] displays the resulting query in the Query Builder.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4a238b6f-7a90-432d-b0e5-279c2e844226/cpbi106-fig-0018-m.jpg</p>\nFigure 18The Query Builder after building the specified query.\n5. Run the query by clicking the Run Query button.\n6. In the Query Result tab, select all results and import them by the Import to New Network button.\n7. In the network, Select all nodes in the network by pressing Ctrl/Cmd + A. Now, right click anywhere on the canvas and select BioGateway > Fetch relations FROM selected > tfact2gene: involved in regulation of. In the Query Result dialog, select all results and import them by clicking the Import selected button.\n8. Select all nodes in the network by pressing Ctrl/Cmd + A. Right click anywhere in the Cytoscape canvas and select Biogateway > Fetch relations FROM selected > UniProt Gene: encodes. This query will ask for the proteins encoded by the genes present in the network. Select all results in the Query Result dialog and import by clicking the Import selected button.\n9. Select all nodes in the network by pressing Ctrl/Cmd + A. Right click anywhere on the Cytoscape canvas and select Biogateway > Fetch relations FROM selected > IntAct: molecularly interacts with. In the Query Result dialog, click the Import relations between existing nodes (Fig. 19[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0019]). This will import only the interactions involving nodes that are already present in the network. Figure 20[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0020] shows the resulting network.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/988158f2-57dd-4e45-8ced-89375fd3cb0e/cpbi106-fig-0019-m.jpg</p>\nFigure 19\nThe Query Result window when launching a right-click query. Users can import only the relationships involving nodes already present in the network by clicking the Import relations between existing nodes button (marked in red).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f4ab436e-5aaa-4515-b4bd-803699bf86f9/cpbi106-fig-0020-m.jpg</p>\nFigure 20\nThe resulting network after launching the Query from Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0004].In this protocol, we will explore an advanced feature from the Query Builder that allows the users to Filter the results of their query and create a subnetwork from the filtered results. Through this, users can create wider queries, which will then be explored and curated in the Query Result window. The biological questions in this case are \u201cWhat are the potential downstream effects after targeted inhibition of MAP3K7 and AKT1? Can we find a connection between the two affected proteins?.\u201d To answer these questions, the protocol will start with the creation of a 5-line query on the Query Builder. Once getting the results, users will be guided through a series of steps to learn how to curate the results based on a text search and create a subnetwork based on the filtering.\nNecessary Resources\nSee Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0001]\n1. Build a query line asking for the proteins interacting with MAP3K7: type \u201cMAP3K7\u201d in the Subject search field and select the first result from the autocomplete results. Next, set the relation type to \u201cmolecularly interacts with\u201d and the Object to \u201cSet A.\u201d\n2. Add a new line asking for the proteins interacting with AKT1 from the set of proteins found in step 1: add a new query line and type \u201cAKT1\u201d in the search field for the Subject. Select the first result from the autocomplete function and set the relation type to \u201cmolecularly interacts with.\u201d Now, set the Object to \u201cSet A.\u201d\n3. Add a new line querying for the TGs of the TFs found in lines 1 and 2 of the query: add a new query line and set the Subject to \u201cSet A.\u201d Now, set the relation type to \u201cinvolved in regulation of\u201d and the Object to \u201cSet B.\u201d4. Add a new line in order to get the proteins encoded by the genes found in step 3: add a new query line and set the Subject to \u201cSet B.\u201d Next, set the relation type to \u201cencodes\u201d and the Object to \u201cSet C.\u201d\n5. Add a new line to find the GO Biological Process annotations for the proteins found in step 4: add a query line. Set the Subject to \u201cSet C\u201d and the relation type to \u201cinvolved in.\u201d Finally, set the Object to \u201cSet D.\u201d Figure 21[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0021] shows the Query Builder window after building the described query.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/c2483bb4-2003-46a3-beb2-e13436549c49/cpbi106-fig-0021-m.jpg</p>\nFigure 21\nThe Query Builder after building the query described in Basic Protocol 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0005].\n6. Run the query by clicking the Run Query button.\n7. In the Query Result window, type \u201cDNA damage\u201d in the text field named \u201cFilter results.\u201d This will display only the results containing the entered text (Fig. 22[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0022]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/eeeac6ac-9a0d-4367-9205-bd286a43523e/cpbi106-fig-0022-m.jpg</p>\nFigure 22\nThe Query Results tab. After selecting a specific set of rows, users can click the \u201cSelect paths of selection\u201d button (marked in red) to select only the relations leading to the selected rows.\n8. Select all displayed results by pressing Ctrl/Cmd \u00b1 A.\n9. Click the Select paths of selection button to find all nodes and relations leading to the filtered results found in step 10.\n10. Click the Import selected nodes to new network button. Figure 23[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0023] displays the resulting network.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4b19eefd-91e5-4e88-b2d6-aee42b04cf01/cpbi106-fig-0023-m.jpg</p>\nFigure 23\nThe resulting network from Basic Protocol 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-prot-0005].This protocol will require a working installation of Cytoscape. The Cytoscape installer and instructions can be found at https://cytoscape.org/download.html[href=https://cytoscape.org/download.html]. We will showcase how to install the BioGateway Cytoscape App by two different means. The first one, which we recommend, will demonstrate how to install the plugin through the Cytoscape built-in App Manager. The second one displays how to install the plugin from source, which can be useful if the users are not able to find the BioGateway Cytoscape App through the first option, or if they want to test a development version. More information for both alternative installations can be found at https://www.biogateway.eu/app/[href=https://www.biogateway.eu/app/].\nInstallation through the Cytoscape App Manager\n1. Open the Cytoscape App Manager by clicking on the Apps menu. Here, users will have to select the App Manager option (Fig. 24[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0024]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/638dd03c-1c4c-4a67-8c38-73ad5a95d549/cpbi106-fig-0024-m.jpg</p>\nFigure 24\nthe App Manager can be accessed by navigating to the Apps menu at the top bar and selecting the App Manager\u2026 option.\n2. In the App Manager, under the Install Apps tab, type \u201cBiogateway\u201d in the search field.\n3. Select the BioGateway Cytoscape plugin and click on the Install button (Fig. 25[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpbi.106#cpbi106-fig-0025]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/da6246e7-8000-43ad-9bb9-da7b8a92891b/cpbi106-fig-0025-m.jpg</p>\nFigure 25\nThe Cytoscape App Manager.\nInstallation from source\n4. Download the BioGateway plugin source file from https://www.biogateway.eu/app/[href=https://www.biogateway.eu/app/].\n5. In Cytoscape, open the App Manager by clicking on the Apps menu.\n6. In the App Manager, under the Install Apps tab, click on the button named Install from File \u2026\n7. In the new dialog, select the file downloaded in step 1.", "Step-By-Step Method Details\nStep-By-Step Method Details\nIterative Application of OptFill to the Exophiala dermatitidis Draft Models to Development of the Final Model\nTiming: days to weeks\nNow that the draft model is in a state such that OptFill can be applied, this section details the iterative application of OptFill to the second draft model. This section specifically discusses the applications of OptFill to the second, third, and fourth drafts of the iEde2091 model to expand the functionality of the reconstructions. Detailed in this section is the construction of each database, the application of OptFill, the inclusion of a chosen solution to the previous draft model to produce the next draft model, and the application of BLASTp using NCBI\u2019s BLAST API to determine the evidence for the selected OptFill solution. This method is used in Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24] in model reconstruction.\nBuilding the first database. From the analysis of the overlap of enzymes present in the four Apsergillus models, the set of enzymes which is common to any three of those models and absent from the fourth should be used as the basis of the first database (from analysis done in Before You Begin step 17). These enzymes should be allowed with all compartmentalizations which are already present in the second draft genome-scale Exophiala dermatitidis model. This list of enzymes with appended compartmentalization (such as \u201c1.1.1.1[c]\u201d for alcohol dehydrogenase in the cytosol) can then be converted to a list of reactions with stoichiometry using the \u201cEnzymes_to_rxns.pl\u201d Perl language code (yellow arrow in Figure\u00a02[href=https://www.wicell.org#fig2]) which automatically adds compartmentalization to each reaction stoichiometry based on the compartmentalization of the enzyme. The list of reactions and stoichiometries produced by this step is defined as the first database for the application of OptFill.Applying OptFill for the first time. Using the database defined in step 1, OptFill should then be applied to the second draft model of Exophiala dermatitidis. The process of the application of OptFill is the same as that described in the \u201cGeneral steps on how to apply OptFill\u201d section of this protocol, and therefore will not be repeated here (Figure 5[href=https://www.wicell.org#fig5] and Figure 6[href=https://www.wicell.org#fig6] are taken from the results of this application of OptFill as a method for checking results obtained). OptFill produces two types of results, first the results of the TIC-Finding Problem (TFP) and second, the results of the Connecting Problems (CPs) which provide sets of reactions to be added to the model to fill metabolic gaps. See the \u201cGeneral steps on how to apply OptFill\u201d section of this protocol for how to read and interpret OptFill results.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/183-Fig5.jpg\nFigure\u00a05. Screenshot of an Example of the Output of the Standard TIC-Finding Problem (TFP) from the First Application of OptFill to the Second Draft Exophiala dermatitidis Model\nBlack text is the output, blue text, lines, and boxes serve to highlight important features of the output so that it may be better understood.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/183-Fig6.jpg\nFigure\u00a06. Screenshot of an Example of the Output of the Standard CPs from the First Application of OptFill to the Second Draft Exophiala dermatitidis Model\nBlack text is the output, blue text, lines, and boxes serve to highlight important features of the output so that it may be better understood.Selecting and applying the first filling solution. In this instance, it is recommended to select and implement the first CPs solution produced by OptFill as this solution improves the model yet not at the expense of adding unnecessary additional reactions. The stoichiometries for the reactions selected by the first CPs solution (taken from the first database file) should be added to a copy of the second draft Exophiala dermatitidis model in order to make the third draft E.\u00a0dermatitidis model.\nNote: Other solutions of the CPs could be selected at this stage but would result in a different model and may result in different results of analyses than those obtained through the iEde2091 model.Performing BLASTp to support the first solution. From the OptFill solution selected, a list of enzymes which catalyze these reactions should be manually gathered from the KEGG database and placed into a list in the file \u201cEClist_1.txt\u201d. Using this list and the code \u201cBidirectionalBLAST.pl\u201d (input files \u201cEClist_1.txt\u201d and \u201cBlastSpecs.txt\u201d) an automated search should be made to determine if there is genetic evidence to support this OptFill solution using the \u201cBidirectionalBLAST.pl\u201d code. This code uses an input list of enzymes; specifications as to acceptable cutoffs values for percent positive substitution of residues and expect value (E value) related to sequence similarity; and a list of related species from which to take the search sequences for the given enzyme. This code works as follows. First, for each enzyme on the list, this code searches KEGG for amino acid sequences for genes known to produce that enzyme from an acceptably related organism (these organisms are specified in the \u201cBlastSpecs.txt\u201d file). This sequence is then used as the query in a BLASTp search, utilizing the BLAST Application Programming Interface (API), against only the Exophiala dermatitidis genome in the NCBI database. It should be noted that the performance of the BLASTp analyses themselves are based on the sample Perl code provided by NCBI for using the BLAST API, available in the developer information of the BLAST API documentation (BLAST Developer Information, n.d.[href=https://www.wicell.org#bib3]). The time need to receive BLAST results depends not on the BLAST code, but rather the volume of demand for the BLAST tool at the time. All matches are then exported to various text files to store these \u201cforward\u201d BLAST results.This code then evaluates each match in terms of the cutoffs provided, and if the match passes, performs a BLASTp of the sequence found in the target genome against the sequence provided by the reference genome. Should this \u201cbackward\u201d BLAST also pass the provide criteria, the match is accepted. The recommended cutoffs could be 60% positive substitution (e.g., 60% of residues are conserved or substituted by a similar amino acid) and an expect value of 1E-30. This should result in a CSV file which details genome-based support for the the inclusion of the reactions included in the OptFill results.Critical: Note the formatting of input files \u201cBlastSpecs.txt\u201d and \u201cEClist_1.txt\u201d, provided in the GitHub associated with Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24], as no effort was made to allow for different formatting of input files in the \u201cBidirectionalBLAST.pl\u201d code.\nCritical: A loss of internet connection while the \u201cBidirectionalBLAST.pl\u201d code is running could cause the program to terminate prematurely.\nNote: If the \u201cBidirectionalBLAST.pl\u201d code no longer functions, consult the BLAST Developer information web page or other BLAST API documentation to determine if the API has changed or been updated.\nNote: It is suggested by the authors to run the \u201cBidirectionalBLAST.pl\u201d code such that times of peak BLAST usage are avoided (or minimized). In the experience of the authors, it is best to run such code overnight or over the weekend.\nNote: Depending on how the supercomputing facilities which the reader uses works, this code may need to be run on a personal or work device (such as a laptop or desktop computer) rather than the supercomputing cluster as operations which directly interact with outside websites may not be allowed.\nBuilding the second database. The second database should be constructed in the same manner as the first database (detailed in step 1); however, the enzymes used for the construction of this database are those common to two of four of the Aspergillus models analyzed, allowing all compartmentalizations currently in the third draft E.\u00a0dermatitidis model for each enzyme.\nApplying OptFill for the second time. OptFill should then be applied to the third draft Exophiala dermatitidis model (serving as the model) with the second database built in step 5 serving as the database for this application of OptFill, applying the procedure detailed in \u201cGeneral steps on how to apply OptFill\u201d. The results of this application are described in Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24].Selecting and applying the second filling solution. As with step 3, the best solutions should be selected from the second application of OptFill, and the stoichiometries of the reactions in the optimal CPs solution should be added to a copy of the third draft Exophiala dermatitidis model to produce the fourth draft E.\u00a0dermatitidis model.\nPerforming BLASTp to support the second solution. As in step 4 a list of enzymes which catalyze the reactions which participate in the OptFill solution selected in step 7 is created, and the \u201cBidirectionalBLAST.pl\u201d code is applied to this list to attempt to find some genomic support for this second OptFill solution.\nBuilding the third database. The third database is built in the same manner as the first and second databases (detailed in steps 1 and 5); however, the enzymes which should be used for the construction of the database are those unique to one of the Aspergillus models analyzed, allowing all compartmentalizations currently in the fourth draft E.\u00a0dermatitidis model for each of these enzymes.\nApplying OptFill for the third time. As in steps 2 and 6, OptFill should then be applied for the third time using the fourth draft E.\u00a0dermatitidis model as the model file, the third database (constructed in step 9) as the database, and following the procedure outlined in the \u201cGeneral steps on how to apply OptFill\u201d section of this protocol. The results of this application are described in Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24].Selecting and applying the third filling solution. As with steps 3 and 7, the best solutions should be selected from the third application of OptFill, and the stoichiometries of the reactions in the optimal CPs solution can then be added to a copy of the fourth draft Exophiala dermatitidis model to produce the fifth draft E.\u00a0dermatitidis model.\nPerforming BLASTp to support the third solution. As in steps 4 and 8, a list of enzymes which catalyze the reactions which participate in the OptFill solution of step 11 should be created, and the \u201cBidirectionalBLAST.pl\u201d code is applied to this list to find some genomic support for this third OptFill solution.\niEde2091 Model: Shadow Price Analyses\nTiming: hours to daysAfter reconstructing the Exophiala dermatitidis model iEde2091, it is desired to use this model in analysis of E.\u00a0dermatitidis metabolism. As the defensive pigments of E.\u00a0dermatitidis are of particular interest for their ability to confer polyextremotolerant properties to an organism, shadow price analysis is selected as a tool for metabolic investigation of these molecules. Shadow price represents the cost to the objective (growth in this model) to produce one more unit (here mmol/gDW\u2219h) of a particular metabolite. The analysis is performed using the dual form of the FBA optimization problem to determine the shadow price for each metabolite. It can then be used to determine the cost of particular metabolic phenotypes to the organism (here melanogenesis and carotenogensis) to give greater insight into an organism\u2019s metabolism. The shadow price analysis of metabolites, specifically defensive pigments, is one of the key analyses of Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24]. Here is a step-by-step description of how the shadow price analysis was performed on the fifth draft Exophiala dermatitidis model, which could hopefully aid others in their uses of shadow price analysis. Further, this analysis resulted in some additional curation of the fifth draft Exophiala dermatitidis model, which finally resulted in the iEde2091 model.\nGet all requisite files. Files which are required for the shadow price analysis are included in the GitHub associated with Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24] (doi:10.5281/zenodo.3608172[href=https://github.com/ssbio/E_dermatitidis_model]) and indicated by light blue-colored arrows in Figure\u00a02[href=https://www.wicell.org#fig2]. These files include \u201ciEde2091.txt\u201d (the model file), \u201cconvertModel.py\u201d (the file which creates necessary input files from the model file), and \u201cget_shadow_prices.gms\u201d (the file which performs the shadow price analyses).Note: So as not to upload many duplicates of the \u201cconvert\u2217.py\u201d (where \u201c\u2217\u201d represents some descriptive word like \u201cModel\u201d or \u201cDatabase\u201d), the \u201cconvertModel.py\u201d code contains a generic placeholder \u201cModel.txt\u201d as the input file in line 20. For this subprocedure, replace this text with \u201ciEde2091.txt\u201d.\nSetting up appropriate file architecture. For this protocol, it is assumed that all three files mentioned in step 13 are in the same file folder.\nRun code generating requisite input files for shadow price analysis. For Windows users, open the command prompt and for Mac and Unix users, open the terminal. Navigate the working directory to where the files are located from all previous steps (the iEde2091 should be present in this direcotyr). To run the code, simply type \u201cpython convertModel.py >output.txt\u201d into the terminal/prompt followed by selecting enter on the keyboard. The \u201c>output.txt\u201d portion should place what would normally be written to the command prompt or terminal to a text file named \u201coutput.txt\u201d for later viewing.\nCritical: The format of the model file, here \u201ciEde2091.txt\u201d, is important to how the convert file reads the model and the proper creation of the input files required for the shadow price analysis. The format is as follow (Perl- and python- format regular expressions are used to describe the format, items which will be described in greater detail are bracketed by \u201c<>\u201d):\n\u201c<ReactionLabel>\\t(<#>\\s<MetaboliteLabel>(\\s\\+\\s)?)\u2217(\\->|<\\->|<\\-)\\s (<#>\\s<MetaboliteLabel>(\\s\\+\\s)?)\u2217\u201d\nExample reactions and reaction formats are shown in Figure 7[href=https://www.wicell.org#fig7] with the format described below.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/183-Fig7.jpg\nFigure\u00a07. Screenshot from the \u201ciEde2091.txt\u201d Model Files Showing an Example of the Formatting of the iEde2091 ModelThe first line shows an example comment inside the model (comments start with a pound sign \u201c#\u201d). The formatting is such that the reaction label is written (for instance \u201cR10965[c]\u201d), followed by a tab, followed by the stoichiometry of the reaction. Forward (reaction \u201cR02907[c]\u201d), backward (reaction \u201cR02906[c]\u201d) and reversible (reaction \u201cR10965[c]\u201d) reactions are shown, where the direction of the reaction is written into the stoichiometry of the model by the format of the arrow. Further, custom reaction labels (such as \u201cR900[c]\u201d and \u201cR901[c]\u201d) and metabolite labels (\u201cX00001[e]\u201d and \u201cC17937DHN[e]\u201d) are shown in this example. The latter custom label is made from the KEGG identifier for melanin (C17937) combined with a label for the type of melanin (\u201cDHN\u201d) and the metabolite compartmentalization (\u201c[e]\u201d).\n\u201c<ReactionLabel>\u201d: label of the reaction. While it can be anything, but in this work the reaction label is composed of the KEGG reaction identifier, if applicable, followed by square brackets surrounding a short code indicating subcellular compartment. Exchange, transport, and reactions not in the KEGG database but in the Exophiala dermatitidis metabolism generally have custom reaction labels, which uses three digits after the character \u201cR\u201d (such as \u201cR900[c]\u201d and \u201cR901[e]\u201d in Figure 7[href=https://www.wicell.org#fig7]), as opposed to five digits in the KEGG identifiers.\n\u201c<#>\u201d: stoichiometric coefficient of the metabolite in the given reaction. Not to be confused with a line beginning with \u201c#\u201d which denotes a comment in the model file (such as the first line of text in Figure 7[href=https://www.wicell.org#fig7]). These comments are ignored by the python code \u201cconvert\u2217.py\u201d so that the comments aid in the organization of the file yet have no effect on the actual function of the model.\u201c<Metabolite label>\u201d: label of a metabolite. While it can be anything, in this work the metabolite label is composed of the KEGG reaction identifier, if applicable, followed by square brackets surrounding a short code indicating subcellular compartment. Metabolites which do not have KEGG identifiers have custom labels beginning with \u201cX\u201d, such as X00001[c] in Figure 7[href=https://www.wicell.org#fig7] which stands in for 1,8-dyhidroxypaphthalene which is not a compound in the KEGG database.\nNote: Any changes made to the model, such as curation or the addition of reactions, should be made to the \u201ciEde2091.txt\u201d model file, then repeating this step (specifically running the \u201cconvertModel.py\u201d code) will automatically update all input files required for the shadow price analysis.\nRun shadow price analysis code. Run the \u201cget_shadow_prices.gms\u201d code using the command \u201cgams get_shadow_prices.gms\u201d. The mathematics related to this analyses are described in detail in Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24]; however, it may be summarized that the shadow price is the cost to the objective function (in this case, rate of biomass production) that would be incurred by producing one more mmol/gDW\u00b7h of a given metabolite. This creates several output files. First is \u201crxn_rates_out.csv\u201d, which is a CSV file which stores the reaction flux (in mmol/gDW\u00b7h) for each FBA performed in the shadow price analysis. The second output file is \u201cshadow_price.csv\u201d, which stores the shadow prices calculated for metabolites which are metabolically close to carotenoids and melanins. The next file is \u201cshadow_price_MCoA.csv\u201d which stores the calculated shadow prices for malonyl-CoA and its precursor metabolites. Finally, \u201cshadow_price_biomass.csv\u201d stores the shadow price of all biomass precursor for each FBA analysis.Study shadow price analysis results. The majority of shadow price values should be negative, indicating that producing extra of any metabolite detracts from biomass production, except for metabolites which may be biomass-coupled. A discussion on biomass-coupling can be found in Burgard, Pharkya and Maranas (2003)[href=https://www.wicell.org#bib4]. This study is generally the most time-consuming step of the procedure of shadow price analysis applied to the iEde2091 model.\nModel curation using shadow price analysis. When applying shadow price analysis to the fifth draft Exophiala dermatitidis model, it may be the case that some compounds have positive shadow prices (indicating that if more is made then more biomass is also made) and others may have shadow prices which vary between the high, medium, and low limiting nutrient availability conditions. These issues are likely indicators of mass or charge imbalance is some reaction involving that particular metabolite, or a metabolite upstream of that metabolite in the reaction network, or that a particular metabolite is coupled with biomass production. The latter is unlikely unless the model is of a particular strain designed to have biomass production be coupled with metabolite production. The former can be corrected by manual curation which addresses reaction balances. The shadow price analysis may then be run again until neither of these indicators remain present in the analysis. In Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24], this served as the final curation step to turn the fifth draft Exophiala dermatitidis model into the final iEde2091 model.\nGeneral Steps on How to Apply OptFill\nTiming: minutes to 1\u00a0weekShould a reader wish to apply OptFill to a Genome-Scale Model (GSM) of an organism of choice, as opposed to the iEde2091 model as described in this workflow, this section will describe, in more general terms, the step-by-step procedure to apply OptFill using GAMS. This section describes the required files, directory structure, and modifications to code provided in the GitHub repositories related to this work which will be necessary to apply OptFill to an organism and model of the user\u2019s choice.\nGetting all requisite files. For the general application of OptFill, several input files are required, and the number of required files varies to some extent by the procedure used. Here it will be assumed that a similar procedure is used in the general application of OptFill as in the specific applications of OptFill to build the iEde2091 model. Therefore, required files include the following (marked by dark blue arrows in Figure\u00a02[href=https://www.wicell.org#fig2]):\nOne model file \u2013 the model whose metabolic gaps are to be filled using the OptFill method (in this case the second draft Exophiala dermatitidis model is marked).\nOne database file \u2013 the database of reactions representing metabolic functionalities to use in the filling of metabolic gaps of the model using the OptFill method (the \u201c3of4DB.txt\u201d file is marked as this is what was used with the second draft Exophiala dermatitidis model).\nCritical: These files should have the same formatting as described in step 15.Two \u201cconvert\u2217.py\u201d files \u2013 Line 20 of each of these \u201cconvert\u2217.py\u201d files should be changed so that one file converts the provided model file, while the other converts the provided database file. Further, lines 23 through 32 in these files must be somewhat changed so that unique sets of output files are made for each file to which as \u201cconvert\u2217.py\u201d code is applied.\nOne \u201cprep_for_optfill.pl\u201d file \u2013 This file runs both of the \u201cconvert\u2217.py\u201d and creates certain input files for the OptFill code, such as the set of all metabolites and the set of all reactions in both the model and database.\nCritical: Attention should be paid to updating the input files in the \u201cconvert\u2217.py\u201d and \u201cprep_for_optfill.pl\u201d code based on what the codes in this step were named (specifically lines 9 and 10 of this code) and what the output files of step 15 were named (specifically lines 15, 22, and 98 should reference the output files of the convert code which converted the model file, and lines 28, 34, and 103 should reference the output files of convert code which converted the database file).\nOne OptFill file \u2013 The OptFill code file which will fill the metabolic gaps in the model file using the database file.\nCritical: Attention should be paid to updating the input files in the code based on the name of output files of previous steps, specifically step 15. Lines particularly important to examine are 31, 34, 37, 40, 43, 46, 49, 58, 61, 64, 67, 79, 82, and 85.\nSetting up appropriate file architecture. These codes assumed that all other codes are contained in the same directory.\nNote: The authors set up the directory on the Crane computing cluster of the Holland Computing Center which was used in this work.Run code generating requisite input files for OptFill. This code can be run in two steps: first run the \u201cprep_for_optfill.pl\u201d code (command is \u201cperl prep_for_optfill.pl\u201d provided the terminal working directory is the directory in step 20).\nCritical: Format the model and database files (format shown in Figure 7[href=https://www.wicell.org#fig7]); otherwise code will not run properly.\nNote: This step will create a large number of files.\nRun OptFill. OptFill should be run on the hardware as advance as possible which allows for a long runtime. Depending on the quality and size of the model and database used, the runtime may be between a few seconds and several days.\nNote: In both the Schroeder and Saha (2020)[href=https://www.wicell.org#bib25] and Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24] works, the Crane computing cluster was used. In order to have an uninterrupted run time, jobs were submitted using the SLURM research manager which is used by all Holland Computing Center computing clusters.\nNote: A discussion about the needed quality of the model and how quality influence runtime can be found in Schroeder and Saha (2020)[href=https://www.wicell.org#bib25]. Should the model itself has a large number of inherent TICs, see the general steps on how to apply the TFP. This can greatly increase the runtime, and the TFP may need to be coupled with manual curation to address model quality issues.\nUnderstanding OptFill solutions part 1: the TFP. An example of results for the TFP is shown in Figure 5[href=https://www.wicell.org#fig5] and will be used to show what a typical output would look like (important features highlighted in blue). The substeps below describe feature of the output of the TFP.\nLines 1 through 5 represent the standard output header, which is always at the top of the output file.Lines 6 through 9 is the standard output block which the TIC-Finding Problem (TFP) returns when there are no more TICs of a given size (where the size in indicated by the value of phi) left to find. In this case, there are no TICs of size 1 (which only occur when there are duplicate reactions in the database and model) or of size 2.\nLines 18 through 28 and 31 through 41 show two examples of the information provided by the TFP once a TIC is found.\nOn lines 18 and 31, it is shown that each new TIC is assigned a new, whole number, label.\nOn lines 19\u201320 and 32\u201333, the objective value of the number of reactions in the TIC are both reported. As discussed in Schroeder and Saha (2020)[href=https://www.wicell.org#bib25], the objective function is the minimization of the number of reactions participating in the TIC, and that this number is also fixed by the value of phi, rendering the objective function moot, yet one is still required for optimization. Each pair of objective function and number of reaction pairs should have the same value. If they do not, this indicates that the relaxations allowed by the solver may need to be tightened.\nLines 21\u201322 and 34\u201335 serve as headers to make the output more human-readable and indicates the start of the table which describes the TIC found in detail.\nAt the start of each line in the table is the reaction label as provided in the model file.The next column in the table is an arrow indicating the direction of flux through the reaction when the reaction is participating in the TIC. Each reaction is allowed to proceed either forward or backward. The direction of the reaction is also indicated by the binary variables alpha and beta reported in the last two columns where alpha indicates forward while beta indicates backward.\nThe third column of the table indicates where the reaction resides, either in the model (M) or database (DB).\nThe fourth column reports on the binary variable eta, which simply reports whether (value 1) or not (value 0) a reaction is participating in the found TIC. All values of this column should be 1, and if this is not the case, consider tightening the relaxations allowed by the solver.\nThe fifth column indicates the flux through the reactions participating in the TIC. The values will always belong to the set [\u22121, \u22121E \u22125]\u222a[1E \u22125, \u22121]. This is because allowing smaller magnitude flux rates or more orders of magnitude for the range in flux rates may cause issues with the relaxations used by optimization problem solvers. Generally, the magnitude of the reported flux rate is unimportant, rather the ratios between flux rates is more enlightening. See Schroeder and Saha (2020)[href=https://www.wicell.org#bib25] for a discussion on this issues.Understanding OptFill solutions part 2: the CPs. An example of the results which may be returned from the CPs is shown in Figure 6[href=https://www.wicell.org#fig6]. Note that as there are several output sections for each CPs solution, only one example solution is shown, and some lines have been removed from that solution (replaced with ellipses) for the brevity of the figure. It should be noted that, for some portions of the output of the solutions to the CPs, formatting is not as neat as that of the TIC-Finding Problem.\nLine 2,577 is the heading for the start of the current CPs solution.\nLines 5,279, 5280, and 5,281 report the objective solution for the first, second, and third CPs respectively. These problems sought to maximize the number of metabolites which can be produced, minimize the number of reactions, and maximize the number of reactions added reversibly respectively.\nLines 5,283 through 5,305 is the first result table of the CPs solution and summarizes which reactions are added in the solution and how those reactions are added. The meaning of each column is detailed in Figure 6[href=https://www.wicell.org#fig6].\nLines 5,308 through 7,142 is the same table as lines 5,283 through 5,305, though these lines detail the results for all reactions contained in the database, hence the much larger size. This is mostly used for the purposes of debugging. Reactions not included in the CPs solution are labeled with a direction of \u201cXX\u201d.\nLines 7,145 to 1,776 list the metabolites which can now be produced by the OptFilled model which the model could not previously produce.\nLines 7,179 through 9,507 produces a table of all metabolites in both the model and the database, and the binary variable, x(i), which determines whether or not they are produced.Lines 9,510 through 11,119 show the results of a FBA of the OptFilled model. The alignment of text is not as high quality as with other tables produced by OptFill, but this can be remedied by copying-and-pasting this table into a Microsoft Excel worksheet and using the text import wizard tool to delimit the cells by spaces. This will result in a more readable table of FBA results. This table is mostly used for the purposes of debugging.\nLines 11,121 through 11,123 report the model and solver statuses of the solutions, as well as the number of iterations needed for the solver to reach these solutions. Should the model or solver statuses not have a value of 1 at any stage, this would be an indication of a need for debugging the code.\nLine 11,124 again states the number of reversible reactions in the CPs solution.\nLine 11,125 states the growth/biomass rate of the OptFilled model, as often fixing metabolic gaps can have an effect on the rate of biomass production.\nLine 11,126 finally reports the time, in seconds, which the solver takes to reach this optimal solution.\nSelect Solution. A set of unique filling solutions will be provided by the OptFill code. Users should carefully review the filling solutions produced by the OptFill code and select the solution based on criteria such as how optimal the solution is determined by OptFill (e.g., order of solutions), which metabolites are fixed (literature evidence for fixed metabolites), which reactions are added (literature evidence for metabolic functions), or other user-defined criteria.\nGeneral Steps on How to Use the TIC-Finding Problem of OptFill for Identifying Inherent TICs\nTiming: minutes to 1\u00a0weekThis section describes how to apply the modified TFP of OptFill to identify inherent TICs utilizing GAMS. This is useful in order to identify TICs inherent to a model to improve the quality of the reconstruction. This section describes the required files, directory structure, and modifications to code provided in the GitHub repositories related to this work which will be necessary to apply the modified TFP to an organism and model of the user\u2019s choice.\nGetting all requisite files. For the general application of the TFP of OptFill, a few input files are required, and the number of required files varies to some extent by the procedure used. Here it will be assumed that a similar procedure is used in the general application of OptFill as in the specific applications of OptFill to build the iEde2091 model. Therefore, required files include the following (marked by orange arrows in Figure\u00a08[href=https://www.wicell.org#fig8]):\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/183-Fig8.jpg\nFigure\u00a08. Screenshot of a Sub-folder of the OptFill GitHub Repository (https://doi.org/10.5281/zenodo.3518501 or https://github.com/ssbio/OptFill[href=https://github.com/ssbio/OptFill]) with code highlighted which can be used to perform the TIC-Finding Problem only on the iJR904 model to identify TICs inherent to the model. This code should be adaptable to other genome-scale models as well.\nOne model file \u2013 the model to which the TFP will be applied. Here, the iJR904 model file (\u201ciJR904.txt\u201d) is marked as the system to which the inherent TIC-Finding will be applied.\nCritical: These files should have the same formatting as described in step 15.\nOne \u201cconvert\u2217.py\u201d file \u2013 A Python code which makes several input files necessary for the TFP. Line 20 of this \u201cconvert\u2217.py\u201d file should be changed so that one file converts the provided model file. (Here, the file is \u201cconvert_iJR904.py\u201d).One TIC-Finding Problem file \u2013 The code which performs the TIC-Finding Problem on the input model file (here labeled \u201cOptFill_TFP_only_iJR904.gms\u201d).\nSetting up appropriate file architecture. These codes assumed that all other codes are contained in the same directory.\nRun code generating requisite input files for OptFill. The \u201cconvert\u2217.py\u201d code from step 26b should be run to create all necessary input files.\nRun TFP Code. The TFP should be run on the hardware as advance as possible which allows for a long runtime. Depending on the quality and size of the model and database used, the runtime may be between a few seconds and several days.\nNote: In both the Schroeder and Saha (2020)[href=https://www.wicell.org#bib25] and Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24] works, the Crane computing cluster was used. In order to have an uninterrupted run time, jobs were submitted using the SLURM research manager which is used by all Holland Computing Center clusters.\nAnalyze Solution. TICs inherent to the model will be included in the solution of the TFP code. These solutions will include both the reactions participating in TICs and their directions in the participating TIC. Inherent TICs generally need to be addressed through manual curation that involve changing reaction directions or removing reactions.\nGeneral Steps on How to Use the iEde2091 Model Including Accompanying Codes for Basic Analyses (FBA and FVA)\nTiming: seconds to minutes\nThis section describes how to use the iEde2091 model for general analyses such as FBA and FVA using codes provided by the authors. The code focused on in this section utilize GAMS.\nGetting all requisite files. To run FBA and FVA codes on the iEde2091 code, four files are required (indicated by purple arrows in Figure\u00a02[href=https://www.wicell.org#fig2]).\n\u201ciEde2091.txt\u201d file \u2013 The model file for the iEde2091 model.\u201cconvertModel.py\u201d \u2013 Converts the iEde2091 model file into several files required by FBA and FVA code.\n\u201cFBA.gms\u201d file \u2013 Code which runs Flux Balance Analysis on the iEde2091 model.\n\u201cFVA.gms\u201d file \u2013 Code which runs Flux Variability Analysis on the iEde2091 model.\nSet up appropriate file architecture. These codes assume that all files are contained in the same directory.\nRun code which generates input files necessary for FBA and FVA. The \u201cconvertModel.py\u201d file should be run to create all requisite input files (the command is \u201cpython convertWLS.py\u201d).\nRun FBA and FVA. Flux Balance and Flux Variability Analyses may be run using the command \u201cgams F\u2217A.gms\u201d (where \u201c\u2217\u201d is substituted with \u201cB\u201d or \u201cV\u201d based on the desired analysis), and each will create output files with the results of these analyses.\niEde2091 Model: Comparison with Human Metabolism\nTiming: minutes to days\nAs mentioned in (Schroeder et\u00a0al., 2020[href=https://www.wicell.org#bib24]), E.\u00a0dermatitidis is a potential model defensive pigment producing organism due to it relatively small genome of 26.4 Mbp, compared to other model organisms including Saccharomyces cerevisea with a genome of 11.9 Mbp, Drosophilia melanogaster with a genome of 137.6 Mbp, Arabidopsis thaliana with a genome of 119.1 Mbp, and Homo sapiens with a genome of 2,893.9 Mbp (National Center for Biotechnology Information, n.d.[href=https://www.wicell.org#bib18]). Due to the size of the Exophiala dermatitidis genome and the importance of melanin to the organism, the similarities and differences between E.\u00a0dermatitidis and H.\u00a0sapiens was investigated to determine the potential of E.\u00a0dermatitidis as a model of H.\u00a0sapiens melanogenesis. This section describes the procedure used to compare human and Exophiala dermatitidis melanin metabolisms in the Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24]. This procedure includes comparing metabolic pathways and comparing human and E.\u00a0dermatitidis tyrosinase amino acid sequences.Comparing metabolic pathways of melanin synthesis. From the reconstruction of the iEde2091 model, the metabolic pathway of melanin synthesis should be known through the reconstruction process. Now the metabolic pathways for the synthesis of melanins, specifically pheomelanin and eumelanin, must be investigated. Several sources were identified in Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24] which detailed eumelanin and pheomelanin synthesis pathways. This allows for a comparison of reaction pathways such as is shown in Figure 3A of Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24].\nEnsuring that all tyrosinase enzymes have been identified in Exophiala dermatitidis. The key enzyme in the synthesis of eumelanin (and also the synthesis of pheomelanin in humans) is tyrosinase (EC number 1.14.18.1). In Chen et al. (2014)[href=https://www.wicell.org#bib6], this four gene copies encoding the tyrosinase enzyme are listed with NCBI accessions of XP_009160170.1, XP_009156893.1, XP_009157733.1, and XP_009155657.1. The former two are said to be gene copies unique to E.\u00a0dermatitidis, whereas the latter two are said to be conserved from Aspergillus homologs. A quick check should be done to ensure that no new tyrosinase gene copies have been identified.\nPerform a search in NCBI using the string \u201ctyrosinase Exophiala dermatitidis\u201d, which should produce a short list of all enzymes annotated as tyrosinase in Exophiala dermatitidis. In Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24], these were the only four gene copies in the list.Perform a BLASTp search to determine if there are as yet unidentified tyrosinase gene copies in the Exophiala dermatitidis genome. Do this by performing a BLASTp of the known E.\u00a0dermatitidis gene copies against the E.\u00a0dermatitidis genome. This can be done by searching for the accession number in NCBI and selecting the protein page result matching that accession number. Then, selecting the \u201cFASTA\u201d link on that page (just below the heading). This will give the amino acid sequence of the tyrosinase gene copy.\nThe amino acid sequence can then be copied-and-pasted into the query sequence textbox in the the BLASTp tool (see Figure 9[href=https://www.wicell.org#fig9]). Figure 9[href=https://www.wicell.org#fig9] shows the BLASTp settings used. Figure 10[href=https://www.wicell.org#fig10] shows composite results of the BLAST results performed on 04/23/2020. From this, it can be shown that, at present, there is no genetic evidence for additional gene copies of tyrosinase in E.\u00a0dermatitidis using this method. Interestingly, some tyrosinase genes are so dissimilar, at least on the whole amino acid sequence, that some gene copies do not match to all other gene copies of tyrosinase.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/183-Fig10.jpg\nFigure\u00a010. BLASTp Results for the Query of Each Exophiala dermatitidis Tyrosinase Gene Copy against the Exophiala dermatitidis Genome when Searching for Previously Unidentified Tyrosine Gene Copies\n(A) Search results for comparing E.\u00a0dermatitidis protein XP_009156893.1 (tyrosinase) to the rest of the E.\u00a0dermatitidis genome utilizing the settings shown in Figure\u00a09[href=https://www.wicell.org#fig9].\n(B) Search results for comparing E.\u00a0dermatitidis protein XP_009157733.1 (tyrosinase) to the rest of the E.\u00a0dermatitidis genome utilizing the settings shown in Figure\u00a09[href=https://www.wicell.org#fig9].\n(C) Search results for comparing E.\u00a0dermatitidis protein XP_009160170.1 (tyrosinase) to the rest of the E.\u00a0dermatitidis genome utilizing the settings shown in Figure\u00a09[href=https://www.wicell.org#fig9].(D) Search results for comparing E.\u00a0dermatitidis protein XP_009155657.1 (tyrosinase) to the rest of the E.\u00a0dermatitidis genome utilizing the settings shown in Figure\u00a09[href=https://www.wicell.org#fig9].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/183-Fig9.jpg\nFigure\u00a09. Setup of the BLASTp Query Used to Map Exophiala dermatitidis Tyrosinase Gene Copy XP_0099155657.1 Back onto the Exophiala dermatitidis Genome to Search for as yet Unidentified Tyrosinase Gene Copies\nThis search was repeated for each gene copy of Exophiala dermatitidis.\nComparison of human and tyrosinase enzymes through BLASTp. As one method for evaluating the similarity of human and Exophiala dermatitidis melanin synthesis, the similarity between tyrosinase enzymes of the two species should be evaluated. The first method of evaluation is a BLASTp analysis, which can be performed similarly to steps 4, 8, and 12, except that the search is limited to the Homo sapiens genome. The combined results for this search can be seen in Figure 11[href=https://www.wicell.org#fig11]. In summary, no tyrosinase gene copy from Exophiala dermatitidis has high sequence similarity to that of Homo sapiens.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/183-Fig11.jpg\nFigure\u00a011. BLASTp Result for the Query of Each Exophiala dermatitidis Tyrosinase Gene Copy against the Homo Sapiens Genome to Determine if the Tyrosinase Gene of Humans Are Sufficiently Similar to that of E.\u00a0dermatitidis to Produce a Significant BLASTp Match\n(A) BLASTp results comparing E.\u00a0dermatitidis protein XP_009156893.1 (tyrosinase) to the human (H.\u00a0sapiens) genome.\n(B) BLASTp results comparing E.\u00a0dermatitidis protein XP_009157733.1 (tyrosinase) to the human (H.\u00a0sapiens) genome.\n(C) BLASTp results comparing E.\u00a0dermatitidis protein XP_009160170.1 (tyrosinase) to the human (H.\u00a0sapiens) genome.\n(D) BLASTp results comparing E.\u00a0dermatitidis protein XP_009155657.1 (tyrosinase) to the human (H.\u00a0sapiens) genome.Comparing human and tyrosinase enzymes through COBALT. A BLASTp analysis generally looks a whole-sequence similarity; however, the whole amino acid sequence of an enzyme is generally not critical to enzyme function. Rather, those amino acids in the active site or sites of an enzyme are most critical; therefore, this step is focused on evaluating the similarity and conservation of the active site using the Constraint-based Multiple Alignment Tool (COBALT) (Papadopoulos and Agarwala, 2007[href=https://www.wicell.org#bib20]). The COBALT is available through NCBI (url: https://www.ncbi.nlm.nih.gov/tools/cobalt/re_cobalt.cgi[href=https://www.ncbi.nlm.nih.gov/tools/cobalt/re_cobalt.cgi]) and is relatively simple to use, as described below to compare human tyrosinase, human tyrosinase related proteins, and Exophiala dermatitidis tyrosinase gene copies.\nTo compare human tyrosinase related proteins, human tyrosinase alleles, and Exophiala determatitidis, use the settings and search query shown in Figure\u00a012[href=https://www.wicell.org#fig12] (note that since this screenshot was taken on 04/23/2020, the COVID-19 banner was removed from the image so as to show what is normally seen), then select \u201cAlign\u201d.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/183-Fig12.jpg\nFigure\u00a012. Input to the COBALT Tool Used when Comparing the Amino Acid Sequences of Human Tyrosinase-Related Proteins 1 and 2, Three Different Human Alleles for Tyrosinase, and the Four Gene Copies of Tyrosinase in E.\u00a0dermatitidis\nOnce the alignment is complete, the enzymes may be compared, particularly in term of conserved sequences. For this analysis, 3-bit conservation settings should be used (this can be changed by changing the drop-down menu labeled \u201cConservation Setting:\u201d under the heading \u201cAlignments\u201d).The active sites of tyrosinase and tyrosinase related proteins, CuA and CuB (standing for Copper-binding domains A and B respectively) can be identified using information on the residue positions of these active sites from sources such as Garc\u00eda-Borr\u00f3n and Solano (2002)[href=https://www.wicell.org#bib12], Spritz et al. (1997)[href=https://www.wicell.org#bib26], and Furumura et al. (1998)[href=https://www.wicell.org#bib11]. It should be assumed that the sequences from E.\u00a0dermatitidis genes copies of tyrosinase which align with these active sites are the active sites of these tyrosinases. In summary, these results should show relatively well-conserved active sites and particularly well-conserved key active site residues.\nComparing E.\u00a0dermatitidis tyrosinase gene copies to hidden Markov Models (hMMs) using Pfam. To further evaluate how similar Exophiala dermatitidis gene copies are to other tyrosinase sequences, the sequence of each gene copy should be evaluated against the hidden Markov Models (hMMs) of various protein families using the Pfam tool. As with the COBALT tool, it is fairly simple to use and the steps are described below.\nThe Pfam tool can be found at pfam.xfam.org[href=http://pfam.xfam.org]. To submit a sequence to query against hMMs of protein families, select the link \u201cSequence Search\u201d in the middle of the page.\nThis will result in a text box appearing in the middle of the page. Copy-and-past the amino acid sequence of the desired protein into that text box and then select \u201cGo\u201d (note that this should be done one sequence at a time).In general, the only protein family to which Exophiala dermatitidis tyrosinase gene copies map is tyrosinase, showing that the important portions of the sequence are well conserved (expect values of 2E-38 or better). Two of these gene copies, XP_0091601701.1 and XP_009156893.1, also had weak matches to the Tyrosinase C hMM as well. A combination of the results for each tyrosinase gene copy of E.\u00a0dermatitidis is shown in Figure\u00a013[href=https://www.wicell.org#fig13].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/183-Fig13.jpg\nFigure\u00a013. Results of the Analysis of Each Tyrosinase Gene Copy in Exophiala dermatitidis in a Hidden Marko Model Analysis Done by the Pfam Tool\nAs shown here, each of these sequences align well with the tyrosinase hidden Markov Model.\n(A) Result of comparison of E.\u00a0dermatitidis protein XP_009160170.1 (tyrosinase) against the baseline Hidden Markov Chain of the tyrosinase family using the Pfam tool.\n(B) Result of comparison of E.\u00a0dermatitidis protein XP_009156893.1 (tyrosinase) against the baseline Hidden Markov Chain of the tyrosinase family using the Pfam tool.\n(C) Result of comparison of E.\u00a0dermatitidis protein XP_009157733.1 (tyrosinase) against the baseline Hidden Markov Chain of the tyrosinase family using the Pfam tool.\n(D) Result of comparison of E.\u00a0dermatitidis protein XP_009155657.1 (tyrosinase) against the baseline Hidden Markov Chain of the tyrosinase family using the Pfam tool.Analyze all data gather to this point on the similarity of Human and Exophiala dermatitidis melanin synthesis. To this point, a comparison of the reaction pathway which produces melanins has been made (step 35), the key enzyme (tyrosinase) is compared between the two species (steps 37, 38, and 39), and tyrosinase gene copies of E.\u00a0dermatitidis are compared to the hidden Markov Models of tyrosinase. At this stage, comparisons of human and Exophiala dermatitidis eumelanin and pheomelanin metabolism may be made. An example of such an analysis is provided in Schroeder et\u00a0al. (2020)[href=https://www.wicell.org#bib24].", "This protocol describes the basic profiling routine of mOTUs for metagenomic and metatranscriptomic short-read (e.g., Illumina) sequencing data with default settings. For metagenomic data, the output is a profile of microbial community members and their relative abundances. By default, the microbial community compositions are summarized into abundances of mOTUs, i.e., phylogenetic marker gene-based operational taxonomic units at species-level resolution (Mende et\u00a0al., 2013[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-bib-0008]; Sunagawa et\u00a0al., 2013[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-bib-0010]). For metatranscriptomic data, the output follows the same format, but reflects the relative taxonomic composition of transcriptionally active community members.\nNecessary Resources\nHardware/Software\nThe mOTU profiler is a command-line tool that requires a 64-bit Linux or MacOS system with at least 16 GB of RAM and its runtime scales almost linearly with the number of input sequences (see Strategic Planning). Installation instructions are provided in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0003].\nInput Files\nPaired-end or single-end short-read sequencing data (see Strategic Planning for more details) in FASTQ format. For demonstration purposes we have deposited sequencing files on Zenodo (https://zenodo.org/record/5012695[href=https://zenodo.org/record/5012695]).\nOutput Files\nThe output of mOTUs is a two-column, tab-separated table. The first column contains taxonomic identifiers, by default at the rank of mOTUs (i.e., species level), and the values in the second column correspond to their relative abundances in the profiled sample. The profile can also be generated at other taxonomic ranks (see Support Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0005]). Profiles from multiple samples can be merged into a single file with taxonomic identifiers in the first column and abundances of the profiled samples in the following columns.\n1. Install mOTUs as described in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0003].\n2. Create a work folder and download the sequencing reads from Zenodo:\n         \nmkdir basic_protocol_1\ncd basic_protocol_1\nwget https://zenodo.org/record/5012695/files/protocol_profile.zip\nunzip protocol_profile.zip\ncd protocol_profile\nPrecomputed results of all steps shown in this protocol can be found in the protocol_profile/bp1_precomputed folder.3. Generate a mOTUs profile with paired-end sequencing read files from a single sample:\n         \nmotus profile -f input/ERR479298s.1.fq.gz -r\ninput/ERR479298s.2.fq.gz -n ERR479298s -o ERR479298s-default.motus\nUnpaired reads can also be profiled and/or added to the command using the -s option. Multiple sequencing files can be used as input for a single profiling run. The order, orientation, and naming are required to follow the conventions described in Strategic Planning.\nThe optional argument -n adds the sample name to the profile. This is important if profiles are merged to retain sample names as column names.\n4. The output of mOTUs is a two-column, tab-separated table. The first column lists the identifiers of all mOTUs and the second column shows the relative abundance of each mOTU in the profiled sample. For example, meta_mOTU_v3_12429 has a relative abundance of 16%:\n         \ngrep \"meta_mOTU_v3_12429\" ERR479298s-default.motus\nPrevotella species incertae sedis [meta_mOTU_v3_12429] 0.1597971398\n5. Individual profiles can be merged into a single file to facilitate comparative downstream analyses using the motus merge command. Here, we merge the profile created in the previous step with a precomputed profile:\n         \nmotus merge -i ERR479298s-default.motus,bp1_precomputed/ERR479045- default.motus -o merged.motus\nAlternatively, all profile files can be placed into a single folder and the -i option can be replaced by the -d option, setting the folder name as value.\n6. The format of the merged profile is identical to the single sample profile with additional columns for each sample added:\n         \ngrep \"meta_mOTU_v3_12429\" merged.motus\nPrevotella species incertae sedis [meta_mOTU_v3_12429]\u2003 0.1597971398\u2003\u20030.0000000000\n7. The profiling and merging routines have finished successfully if and when the output file (e.g., ERR479298s-default.motus or merged.motus) is created and the tool reports that no error has occurred. A guideline on how to interpret the profiling results can be found in Guidelines for Understanding Results.The mOTU profiler can be used to generate metagenomic single nucleotide variation (SNV) profiles of the mOTUs marker genes and provides a computationally efficient alternative to generating SNV profiles from whole-genome sequences. These profiles are subsequently used to produce between-sample distances for individual mOTUs, providing access to strain population diversity patterns for as many as 33,750 species. To this end, mOTUs will first align metagenomic sequencing reads against its marker gene database and then process the alignments using the metaSNV package (Costea et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-bib-0007]).\nNecessary Resources\nHardware/Software\nThe mOTU profiler is a command-line tool that requires a 64-bit Linux or MacOS system with at least 16 GB of RAM and its runtime scales almost linearly with the number of input sequences (see Strategic Planning). Installation instructions are provided in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0003].\nInput Files\nMetagenomic SNV profiling is a pipeline split into two parts:\n               \nmotus map_snv requires paired-end or single-end short read sequencing data (see Strategic Planning for more details) in FASTQ format\nThe BAM alignment files produced by motus map_snv serve as input for the motus snv_call routine\nFor demonstration purposes we have deposited sequencing and alignment files on Zenodo (https://zenodo.org/record/5012568[href=https://zenodo.org/record/5012568])\nOutput Files\nThe motus map_snv command will create one BAM file per execution\nThe motus snv_call command will create an output folder with four files/directories:\n               \nTwo files containing the read coverage information for each mOTU, both horizontal, i.e., the percentage of bases covered by at least one read (*.perc.tab file), and vertical, i.e., the average number of reads covering each base (*.cov.tab file)\nPer mOTU filtered allele frequencies of identified SNVs across samples (filtered-* directory)\nPer mOTU genetic distances between samples (distances-* directory)For the files containing distance information, both regular Manhattan (*mann.dist files) and dominant allele (*allele.dist files) distances are provided. The dominant allele distance is a Manhattan distance that only takes positions with a frequency change above 50% (i.e., change of the dominant allele) into account.\nThe SNV profiling routine is split into two steps (motus map_snv and motus snv_call) that need to be executed sequentially. The first step aligns sequencing reads of a single sample against a reduced mOTUs centroid database. The second step uses alignments from multiple samples to calculate metagenomic SNV profiles and distances using metaSNV.\n1. Install mOTUs as described in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0003].\n2. Create a work folder and download the sequencing reads from Zenodo:\n         \nmkdir basic_protocol_2\ncd basic_protocol_2\nwget https://zenodo.org/record/5012568/files/protocol_snv.zip\nunzip protocol_snv.zip\ncd protocol_snv\nPrecomputed results of all steps shown in this protocol can be found in the protocol_snv/bp2_precomputed folder.\n3. Execute the motus map_snv command to align reads from a paired-end sequencing sample against the mOTUs centroid database:\n         \nmotus map_snv -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-default.bam\nUnpaired reads can also be profiled and/or added to the command using the -s option. Multiple sequencing files can be used as input for a single profiling run. The order, orientation, and naming are required to follow the conventions described in Strategic Planning.\n4. The motus snv_call command takes a folder containing BAM files as input. Execute the following command to run motus snv_call:\n         \nmotus snv_call -d bp2_precomputed/bams/ -o motus-snvcall-default\nBAM files produced by other tools or those created by the motus profile command cannot be used as an input for the motus snv_call command. Only BAM files produced by the motus map_snv command can be used.motus snv_call requires at least two BAM files as input. We provide a set of 20 precomputed BAM files for this example.\n5. Execute the following command to inspect the marker gene-based distances between metagenomic SNV profiles, which can be found in the distances-m2-d5-b80-c5-p0.9 folder:\n         \ncat motus-snvcall-default/distances-m2-d5-b80-c5-\np0.9/meta_mOTU_v3_12235.filtered.mann.dist\n\u2003\u2003\u2003\u2003\u2003\u2003SAMEA2466942-default.bam\u2003\u2003\u2003\u2003SAMEA2467017-default.bam\ntable:\n\ufeff0,1,2\nSAMEA2466942-default.bam,0.0,0.6728930543446672\nSAMEA2467017-default.bam,0.6728930543446672,0.0\n6. The motus map_snv step has finished successfully when the output alignment file is created (e.g., ERR479298-default.bam). The motus snv_call step has finished successfully when all result files are present and the tool reports \u201cSuccessfully finished\u201d in the command line. A guideline on how to interpret the SNV results can be found in Guidelines for Understanding Results.The mOTU profiler is written in Python 3 and can be executed on a 64-bit Linux or MacOS system. There are external dependencies that need to be pre-installed. This can be done either by manual installation or, preferably, by using the conda package manager.\nNecessary Resources\nHardware\nAs described in Strategic Planning\nSoftware\nBWA, to align sequencing reads against the mOTUs database. Tested with version BWA-0.7.17 (r1188); https://github.com/lh3/bwa[href=https://github.com/lh3/bwa].\nSAMtools, to read the alignment files produced by BWA. Tested with version 1.6 or higher; https://github.com/samtools/samtools[href=https://github.com/samtools/samtools].\nPython as the runtime for the mOTU profiler. Tested with version 3.6.5 or higher; https://www.python.org/downloads/[href=https://www.python.org/downloads/].\nOptional: metaSNV, for SNV calling and processing. Tested with version 1.0.3; http://metasnv.embl.de/[href=http://metasnv.embl.de/]. The metaSNV package is only needed for the snv_call command (Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0002] and Support Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0006]).\nOptional: miniconda, for installation using conda; https://docs.conda.io/en/latest/miniconda.html[href=https://docs.conda.io/en/latest/miniconda.html]\nFiles\nThe latest mOTUs package (tar.gz file); https://github.com/motu-tool/mOTUs[href=https://github.com/motu-tool/mOTUs]\nOption A: Installation using the Conda Package Manager\n1a. The installation using the conda package manager is generally preferable, as it encapsulates the entire installation process into a single command once conda is installed. Execute the following command to install conda:\n         \ncurl -O https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh Miniconda3-latest-Linux-x86_64.sh\nconda config --add channels defaults\nconda config --add channels bioconda\nconda config --add channels conda-forge\nIf conda is already installed on the system, only the config steps need to be executed.\nIf working on a MacOS system, the download link has to be replaced by: https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh[href=https://repo.anaconda.com/miniconda/Miniconda3-latest-MacOSX-x86_64.sh].\n2a. Install the latest version of mOTUs using conda:\n         \nconda install -c bioconda motus=3.0.0\nOption B: Manual installation1a. Both mandatory dependencies (BWA and SAMtools) can be downloaded from their respective GitHub repositories. Up-to-date installation instructions are provided by the developers. The metaSNV source code can be downloaded from http://metasnv.embl.de/[href=http://metasnv.embl.de/] along with installation instructions. Python (version 3.6.5 or higher) should be installed following the instructions on the official website (https://www.python.org/downloads/[href=https://www.python.org/downloads/]).\n2a. The manual installation consists of three steps. In step 1, the release files are downloaded from the official GitHub repository. The installation script in step 2 will download the database. The test routine in step 3 checks whether dependencies are installed correctly and whether profiling a test dataset yields the expected results. Execute the following commands to download and install mOTUs:\n         \nwget https://github.com/motu-tool/mOTUs/archive/refs/tags/3.0.0.tar.gz\ntar -xzvf 3.0.0.tar.gz\ncd mOTUs-3.0.0\npython setup.py\npython test.py\necho export PATH=`pwd`:$PATH >> \u223c/.bashrc\nsource \u223c/.bashrc\nThis example shows the installation of version 3.0.0 which was released in June 2021. The installation of other versions (https://github.com/motu-tool/mOTUs/releases[href=https://github.com/motu-tool/mOTUs/releases]) works accordingly.\nMacOS users can use curl -OL as a drop-in replacement for wget.The motus profile routine is an analysis pipeline that consists of three steps, namely map_tax, calc_mgc, and calc_motu, which are internally executed in that order. Each step produces the output files that are needed for the next step, and therefore it can be useful to run the commands individually, e.g., to test different parameter settings, without the need to run the entire pipeline. This is especially useful, as the first step (map_tax) accounts for most of the CPU time needed to calculate profiles.\nNecessary Resources\nHardware/Software\nThe mOTU profiler is a command-line tool that requires a 64-bit Linux or MacOS system with at least 16 GB of RAM and its runtime scales almost linearly with the number of input sequences (see Strategic Planning). Installation instructions are provided in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0003].\nInput Files\nPaired-end or single-end short-read sequencing data (see Strategic Planning for more details) in FASTQ format. For demonstration purposes we have deposited sequencing files on Zenodo (https://zenodo.org/record/5012695[href=https://zenodo.org/record/5012695]).\nOutput Files\nThe output of mOTUs is a two-column, tab-separated table. The first column contains taxonomic identifiers, by default at the rank of mOTUs (i.e., species level), and the values in the second column correspond to their relative abundances in the profiled sample. The profile can also be generated at other taxonomic ranks (see Support Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0005]). Profiles from multiple samples can be merged into a single file with taxonomic identifiers in the first column and abundances of the profiled samples in the following columns.\nThe intermediate output files produced by mOTUs contain:\n               \nalignments of reads against the mOTUs database in BAM format\nmarker gene abundances in a two-column, tab-separated table\n1. Install mOTUs as described in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0003].\n2. Create a work folder and download the sequencing reads from Zenodo:\n         \nmkdir support_protocol_2\ncd support_protocol_2\nwget https://zenodo.org/record/5012695/files/protocol_profile.zip\nunzip protocol_profile.zipcd protocol_profile\nPrecomputed results of all steps shown in this protocol can be found in the protocol_profile/sp2_precomputed folder.\n3. Execute the following command to align paired-end sequencing reads against the mOTUs database and store alignments in a BAM file:\n         \nmotus map_tax -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-default.bam\nUnpaired reads can also be profiled and/or added to the command using the -s option. Multiple sequencing files can be used as input for a single profiling run. The order, orientation, and naming are required to follow the conventions described in Strategic Planning.\nThe optional argument -n adds the sample name to the profile. This is important if profiles are merged to retain sample names as column names.\n4. Parse the alignments in the BAM file and produce an intermediate file that assigns each insert to its marker gene cluster:\n         \nmotus calc_mgc -i ERR479298s-default.bam -n ERR479298s -o ERR479298s-default.mgc\n5. The output of the calc_mgc step is a two-column, tab-separated table with the first column representing the name of the marker gene cluster and the second column containing the insert count (see Background Information and Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-fig-0001]):\n         \n\u2003\u2003\u2003\u2003head -n 10 ERR479298s-default.mgc\n# map_tax 3.0.0 | gene database: nr3.0.0 | 84 | calc_mgc 3.0.0 -y insert.scaled_counts -l 75\ntable:\n\ufeff0,1\nERR479298s,\nCOG0495.mOTU.v25.0000273,476.9980996151712\nCOG0552.mOTU.v2.0001907,52.97275139363133\nCOG0533.mOTU.v2.0001890,312.27285022688824\nCOG0525.mOTU.v25.0000325,363.7120130887338\nCOG0215.ext_mOTU_v3_003085,48.85735226138667\nCOG0172.mOTU.v2.0002099,101.5497931033204\nCOG0525.mOTU.v2.0001728,3.4031882043690063\nCOG0525.mOTU.v25.0000424,2.255959601538731\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/b725575c-71f7-4705-97c4-3a7db16fc5f4/cpz1218-fig-0001-m.jpg</p>\nFigure 1Marker gene clusters. Marker genes (MGs) represent the basic unit to which metagenomic and metatranscriptomic sequencing reads are mapped using the motus map_tax command. Marker genes (represented as circles) from the same species (in this example, all COG0012 genes from Prevotella copri) are clustered into marker gene clusters (MGC). The execution of motus calc_mgc quantifies the abundance of each of up to 10 MGCs per mOTU using a predefined metric (see Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-fig-0003]). For each MGC, one centroid sequence (in blue) is selected to generate a reduced set (only centroid sequences) of MGs that is used as the reference database for the motus map_snv command.\n6. To calculate the abundance of each mOTU, execute the motus calc_motu command, which generates the mOTUs output file:\n         \nmotus calc_motu -i ERR479298s-default.mgc -n ERR479298s -o ERR479298s-default.motus\n7. The output of mOTUs is a two-column, tab-separated table. The first column lists the identifiers of all mOTUs and the second column shows the relative abundance of each mOTU in the profiled sample. For example, meta_mOTU_v3_12429 has a relative abundance of 16%:\n         \ngrep \"meta_mOTU_v3_12429\" ERR479298s-default.motus\nPrevotella species incertae sedis [meta_mOTU_v3_12429]\u2003\u20030.1597971398\n8. The routines finished successfully if and when the output file (e.g., ERR479298s-default.motus) is created and the tool reports that no error has occurred. A guideline on how to interpret the profiling results can be found in Guidelines for Understanding Results.Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0001] and Support Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0004] illustrate the core functionality of the profiling routine using default settings for the most common metagenomic short read sequencing approaches. These settings were chosen to provide a balanced tradeoff between sensitivity and specificity. However, mOTUs offers flexibility in changing these settings according to user preference. This section describes all available options and their effects on the resulting profiles. It also introduces how profiles can be exported in BIOM format, and how results can be aggregated at different taxonomic ranks. The last section of this protocol shows how user-generated profiles can be merged with over 11,000 precomputed profiles from publicly available metagenomes and metatranscriptomes.\nNecessary Resources\nHardware/Software\nThe mOTU profiler is a command-line tool that requires a 64-bit Linux or MacOS system with at least 16 GB of RAM and its runtime scales almost linearly with the number of input sequences (see Strategic Planning). Installation instructions are provided in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0003].\nInput Files\nPaired-end or single-end short-read sequencing data (see \u201cStrategic Planning\u201d for more details) in FASTQ format. For demonstration purposes we have deposited sequencing files on Zenodo (https://zenodo.org/record/5012695[href=https://zenodo.org/record/5012695]).\nOutput Files\nThe output of mOTUs is a two-column, tab-separated table. The first column contains taxonomic identifiers, by default at the rank of mOTUs (i.e., species level), and the values in the second column correspond to their relative abundances in the profiled sample. The profile can also be generated at other taxonomic ranks (see Support Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0005]). Profiles from multiple samples can be merged into a single file with taxonomic identifiers in the first column and abundances of the profiled samples in the following columns.\nAlternative output formats supported by mOTUs are Bioboxes (http://bioboxes.org/[href=http://bioboxes.org/]) and BIOM (https://biom-format.org/[href=https://biom-format.org/]).\nThe intermediate output files produced by mOTUs contain:alignments of reads against the mOTUs database in BAM format\nmarker gene abundances in a two-column, tab-separated table\nThe optional arguments of the motus profile command modify its runtime, the way sequencing reads are filtered and counted, and the format of the reported output.\nNOTE: This protocol describes how to set optional arguments to change the default behavior of mOTUs. Each analysis step in this protocol is therefore self-contained and does not rely on output data of any other analysis step.\n1. Install mOTUs as described in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0003].\n2. Create a work folder and download the sequencing reads from Zenodo:\n         \nmkdir support_protocol_3\ncd support_protocol_3\nwget https://zenodo.org/record/5012695/files/protocol_profile.zip\nunzip protocol_profile.zip\ncd protocol_profile\nPrecomputed results of all steps shown in this protocol can be found in the protocol_profile/sp3_precomputed folder.\n3. Runtime: Most of the runtime of the motus profile command is spent computing alignments of input sequences against the marker gene database using the BWA aligner. Using multiple cores for the alignment step will lead to a near-linear decrease in runtime if the number of cores is lower than 16. Further increasing the number of cores will still improve runtime but at a lower rate. The number of cores used by mOTUs for the alignment step can be adjusted with the -t option. Here we run mOTUs with 1, 8, or 16 cores, which leads to a decrease in runtime (1600 s, 300 s, 191 s):\n         \ntime motus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-t_1.motus -t 1\ntime motus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-t_8.motus -t 8\ntime motus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-t_16.motus -t 164. Length filtering: The length and similarity of a sequencing read aligned to a marker gene reference sequence determine whether the read is considered for the subsequent counting step. The default minimum length cutoff is set to 75 bases. If a substantial fraction of the sequencing reads is shorter than 75 bases, the -l option can be used to reduce the minimum alignment length to increase the fraction of mapped reads. Here we set the -l argument to a minimum alignment length of 50:\n         \nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-l_50.motus -l 50\nUsing a significantly lower cutoff than the average read length (e.g., 50 for reads with an average length of 100) will increase the mapping rate and also the probability of retaining misalignments for the downstream steps. The combined use of other settings, such as using a higher value for the option -g (see next section), can compensate for potential misalignments.\n5. Presence/absence: The mOTU profiler provides an option (-g) to adjust the precision and recall of species identification. Each mOTU consists of a minimum of 6 and up to 10 marker gene clusters (MGCs, see Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-fig-0001]). To avoid the spurious detection of taxa, the number of detected MGCs can be used to adjust the confidence level. The -g option in motus profile (and in motus calc_motu) controls the minimum number of detected MGCs (to which reads are mapping) required to call a mOTU as being present in a sample. The default value is set to 3, representing half the minimum number of MGCs per mOTU. Execute the following commands to profile with different values for the -g option:\n         \nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-g_3.motus -g 3\nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-g_1.motus -g 1motus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-g_6.motus -g 6\nHigher values will increase precision at the cost of recall. Lowering this value will identify more species (-g 1 = 723, -g 3 = 335, -g 6 = 193), albeit at lower precision. Values above 6 should be used with caution, as some mOTUs may become undetectable if they contain fewer MGCs than the set value (see Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-fig-0002]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6962c162-b97f-48e2-ae6a-7dbf5b811734/cpz1218-fig-0002-m.jpg</p>\nFigure 2\nAbundance estimation by the mOTUs profiler. Metagenomic or metatranscriptomic reads mapped against the mOTUs database containing marker gene sequences from 10 gene families (COG0012, COG0016, etc.) are retained for the quantification of mOTUs. Each mOTU consists of a minimum of 6 marker genes. Marker genes not associated with any mOTU (unlinked) are grouped into a single cluster (Unassigned). The minimum number of detected MGCs define the detection threshold for a mOTUs. This value (default = 3) can be adjusted to avoid the spurious detection of taxa using the -g option. For each mOTUs passing the cutoff, MGC abundances are quantified (see Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-fig-0003]) and the median of MGC with non-zero counts abundances used as a basis for generating the output profile.\n6. Quantification: There are three modes to quantify the abundance of mOTUs, namely base.coverage, insert.raw_counts, and insert.scaled_counts (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-fig-0003]). These alternatives can be selected using the -y option in the motus profile (and motus calc_mgc) command. The abundance of each mOTU is calculated as the median across all its MGCs with non-zero values. Execute the following commands to profile with different values for the -y option.\n         \nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-y_insert.raw_counts.motus -y insert.raw_countsThis simplest counting algorithm quantifies MGCs by the number of inserts mapping to their associated marker gene members. The resulting counts are affected by gene-length differences, as longer genes recruit more reads. This option should therefore be used with caution.\nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-y_base.coverage.motus -y base.coverage\nTo account for gene length differences and for varying read lengths, this counting algorithm quantifies marker gene clusters using the mean coverage. The total alignment length of all reads mapping against the marker genes of a MGC cluster are divided by the length of the associated marker genes.\nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-y_insert.scaled_counts.motus -y insert.scaled_counts\nThe default counting algorithm normalizes the number of mapped inserts by marker gene length and additionally rescales them to the same range as the initial counts. This is done by multiplying by a scaling factor calculated as formula:\n$\\Sigma \\ ( {inserts} )/\\Sigma \\ ( {inserts/lengt{h_{gene}}} )$\n, i.e., the sum of all mapped inserts divided by the sum of all gene length\u2212normalized insert counts. The read counts are processed in this way to remove the effects of gene length differences, while ranging on a scale comparable to read counts.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/8a1c6eac-9fa0-44ec-a0c6-578ed9492859/cpz1218-fig-0003-m.jpg</p>\nFigure 3Illustration of mOTU quantification methods. The different quantification methods of mOTUs are illustrated with two genes (Gene1 and Gene2). Gene2 is three times longer than Gene1. Both genes recruit the same number of inserts (6), which are depicted by horizontal blue lines with diamond-shaped arrowheads. If insert.raw_counts is selected, the number of inserts is summed up for each gene. If base.coverage is selected, the total number of bases that were mapped is divided by the length of the respective gene. For the default setting, insert.scaled_counts, insert.raw_counts are first normalized by length and then scaled by total abundance. For example, assuming length(Gene1) = 1000, length(Gene2) = 3000, and total_inserts = 12, then insert.scaled_counts(Gene1) = 12 * (6/1000)/(6/1000 + 6/3000) = 12 * \u00be = 9.\n7. Output formats: The command motus profile creates by default a tab-separated file in which the columns correspond to the taxonomic annotation of each mOTU and its abundance. The mOTU profiler also supports the BIOM (https://biom-format.org/[href=https://biom-format.org/]) output format, which can be invoked using the flag -B. The option -C can be used to generate an output file compliant with the CAMI/Bioboxes (http://bioboxes.org/[href=http://bioboxes.org/]) format. Execute the following commands to generate profiles with either BIOM or Bioboxes format:\n         \nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-B.biom -B\nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-C_precision.cami -C precision\nThe -C option should be used for reproducing the challenge results (Meyer et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-bib-0002]). Further details can be found at: https://github.com/motu-tool/mOTUs[href=https://github.com/motu-tool/mOTUs].8. Taxonomy: The mOTU profiler uses the standard taxonomic hierarchy with seven ranks (kingdom, phylum, class, order, family, genus, and mOTU), where the typical species level is substituted by the equivalent mOTU level, which is reported by default. The flags -p and -q will add the NCBI taxonomy ID to the profile or report the full taxonomy, respectively. The profiles can be further aggregated at a different taxonomic level using the -k option. Execute the following command to add the associated NCBI taxonomy ID (here: 1262806) as a new column to the profile:\n         \nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-p.motus -p\ngrep \"meta_mOTU_v3_12886\" ERR479298s-p.motus\nClostridium sp. CAG:433 [meta_mOTU_v3_12886]\u2003\u20031262806\u2003\u20030.0184576392\nExecute the following command to report the full taxonomy path:\n         \nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-q.motus -q\ngrep \"meta_mOTU_v3_12886\" ERR479298s-q.motus\nmeta_mOTU_v3_12886 k__Bacteria|p__Firmicutes|c__Clostridia|o__Clostridiales|f__Clostridiaceae|g__Clostridium|s__Clostridium sp. CAG:433 0.0184576392\nExecute the following command to aggregate the taxonomic path at a different taxonomic level (here: phylum):\n         \nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-k_phylum.motus -k phylum\ngrep \"Actinobacteria\" ERR479298s-k_phylum.motus\nActinobacteria\u2003\u20030.0079714597\n...\n9. Counts/relative abundance: \nmotus profile reports, by default, relative abundances of each mOTU. The relative abundances are directly calculated from the counts (bases or inserts; see Quantification). The output format can be modified using the -c flag. Execute the following commands to either report relative abundances (default) or to report counts (-c) instead:\n         \nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-default.motus\ngrep \"meta_mOTU_v3_12886\" ERR479298s-default.motus\nClostridium sp. CAG:433 [meta_mOTU_v3_12886]\u2003\u20030.0184576392\nmotus profile -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-c.motus -c\ngrep \"meta_mOTU_v3_12886\" ERR479298s-c.motus\nClostridium sp. CAG:433 [meta_mOTU_v3_12886]\u2003\u20034010. Merge profiles: Users who generated profiles in the default mOTUs output format and using the -c flag (reporting counts instead of relative abundances, see above) can add their profiles to precomputed profiles to contextualize and compare the taxonomic profiles of their samples with previously published ones. For the release of version 3.0.0 of mOTUs, we provided precomputed profiles from more than 11,000 public metagenomes and metatranscriptomes that are directly accessible via the motus merge command. Execute the following commands to merge the precomputed profile with either all mouse gut profiles, all mouse and dog gut profiles, or all public profiles, respectively.\n         \nmotus merge -i sp3_precomputed/ERR479298s-c.motus -a mouse -o merged.mouse.motus\nmotus merge -i sp3_precomputed/ERR479298s-c.motus -a mouse,dog -o merged.mouse.dog.motus\nmotus merge -i sp3_precomputed/ERR479298s-c.motus -a all -o merged.all.motusBasic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0002] explains the core functionality of the metagenomic SNV profiling mode using default settings. The preconfigured settings filter both metagenomic samples and genomic positions to offer a good tradeoff between sensitivity and specificity. However, some use cases may require custom settings instead. This section provides an in-depth explanation of all options and describes their effects to the resulting profiles.\nNecessary Resources\nHardware/Software\nThe mOTU profiler is a command-line tool that requires a 64-bit Linux or MacOS system with at least 16 GB of RAM and its runtime scales almost linearly with the number of input sequences (see Strategic Planning). Installation instructions are provided in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0003].\nInput Files\nMetagenomic SNV profiling is a pipeline split into two parts:\n               \nmotus map_snv requires paired-end or single-end short read sequencing data (see Strategic Planning for more details) in FASTQ format\nThe BAM alignment files produced by motus map_snv serve as input for the motus snv_call routine\nFor demonstration purposes we have deposited sequencing and alignment files on Zenodo (https://zenodo.org/record/5012568[href=https://zenodo.org/record/5012568])\nOutput Files\nThe motus map_snv command will create one BAM file per execution\nThe motus snv_call command will create an output folder with four files/directories:\n               \nTwo files containing the read coverage information for each mOTU, both horizontal, i.e., the percentage of bases covered by at least one read (*.perc.tab file), and vertical, i.e., the average number of reads covering each base (*.cov.tab file)\nPer mOTU filtered allele frequencies of identified SNVs across samples (filtered-* directory)\nPer mOTU genetic distances between samples (distances-* directory).\nFor the files containing distance information, both regular Manhattan (*mann.dist files) and dominant allele (*allele.dist files) distances are provided. The dominant allele distance is a Manhattan distance that only takes positions with a frequency change above 50% (i.e., change of the dominant allele) into account.The SNV profiling routine is composed of two steps: motus map_snv first aligns sequencing reads of a single sample against the reduced mOTUs centroid database, and motus snv_call subsequently processes alignments from multiple samples to calculate metagenomic SNV profiles and genetic distances. Details are described in Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0002].\nNOTE: This protocol describes how to set optional arguments to change the default behavior of mOTUs. Each analysis step in this protocol is therefore self-contained and does not rely on output data of any other analysis step.\n1. Install mOTUs as described in Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.218#cpz1218-prot-0003].\n2. Create a work folder and download the sequencing reads from Zenodo:\n         \nmkdir support_protocol_4\ncd support_protocol_4\nwget https://zenodo.org/record/5012568/files/protocol_snv.zip\nunzip protocol_snv.zip\ncd protocol_snv\nPrecomputed results of all steps shown in this protocol can be found in the protocol_snv/sp4_precomputed folder.\n3. Runtime: Most of the runtime of the SNV profiling routine is spent computing alignments of input sequences against the marker gene database using the BWA aligner. Using multiple cores for the alignment step will lead to a near-linear decrease in runtime (up to 16 cores, further increases will still improve runtime, but at a lower rate). Execute the following command to use 16 cores (-t 16) for alignment:\n         \nmotus map_snv -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-default.bam -t 16\n4. Length filtering: The length and similarity of a sequencing read aligned to a marker gene reference sequence determine whether the read is considered for the subsequent counting step. The default minimum length cutoff is set to 75 bases. If a substantial fraction of the sequencing reads is shorter than 75 bases, the -l option can be used to reduce the minimum alignment length to increase the fraction of mapped reads. Execute the following command to set the -l option to a minimum alignment length of 50:motus map_snv -f input/ERR479298s.1.fq.gz -r input/ERR479298s.2.fq.gz -o ERR479298s-l_50.bam -l 50\nUsing a significantly lower cutoff than the average read length (e.g., 50 for reads with an average length of 100) will increase the mapping rate and also the probability of retaining misalignments for the downstream steps. The combined use of other settings, such as using a higher value for the option -g (see next section), can compensate for potential misalignments.\n5. Sample support: The first step of the motus snv_call routine will calculate the vertical (average number of reads covering each base) and horizontal (percentage of bases covered by at least one read) coverage of each mOTU for each sample and then select only those mOTUs that fulfil the coverage-filtering criteria. By default these thresholds are 5\u00d7 vertical coverage and 80% horizontal coverage, and can be controlled by the -fd and -fb options, respectively. In addition, a mOTU will be selected only if at least a given number of samples meet the coverage criteria (2 samples by default). This number can be controlled with the -fm option. Execute the following commands to alter filtering criteria:\n         \nmotus snv_call -d bp2_precomputed/bams/ -o motus-snvcall-fd_10 -fd 10\nOnly mOTUs with a vertical coverage of 10 will pass the filter. Using this value will remove more mOTUs, as it is more conservative compared to the default (\u22655).\nmotus snv_call -d bp2_precomputed/bams/ -o motus-snvcall-fb_90 -fb 90\nOnly mOTUs with a horizontal coverage of \u226590% will pass the filter. Using this value will remove more mOTUs, as it is more conservative compared to the default (\u226580%).\nmotus snv_call -d bp2_precomputed/bams/ -o motus-snvcall-fm_3 -fm 3\nOnly mOTUs where at least 3 samples fulfill coverage conditions will be reported. Using this value will remove more mOTUs, as it is more conservative compared to the default (at least 2 samples).6. Base coverage: The previous filtering step selected mOTUs and filtered samples based on coverage values across whole mOTUs. This second filtering step will retain genomic positions for SNV calling within an individual mOTU. To that end, only positions in a mOTU that have enough coverage in at least a given proportion of filtered samples are retained. The base coverage threshold is controlled with the -fc option, and is set to a minimum of 5\u00d7 coverage. The proportion of samples is controlled with the -fp option and is set to a minimum of 90% of the samples by default. Execute the following commands to alter filtering criteria:\n         \nmotus snv_call -d bp2_precomputed/bams/ -o motus-snvcall-fc_10 -fc 10\nOnly positions with a vertical coverage of 10 will pass the filter. Using this value will remove more positions as it is more conservative compared to the default (5).\nmotus snv_call -d bp2_precomputed/bams/ -o motus-snvcall-fp_1.0 -fp 1.0\nOnly positions where all samples fulfill coverage criteria will be reported. Using this value will remove more positions, as it is more conservative compared to the default (at least 90% of the samples).\n7. Keep intermediate files: The arguments -K and -v allow users to control the reporting of the SNV profiling routine. First, -K will provide the user with all the intermediary files created by metaSNV. Second, -v controls the verbosity of the motus snv_call command according to four standardized levels of specificity. Execute the following command to keep intermediate files and enable verbose logging:\nmotus snv_call -d bp2_precomputed/bams/ -o motus-snvcall-k-v_4 -K -v 4", "Step-by-step method details\nStep-by-step method details\neVIP2 characterization of RNF43 variants with Kallisto files as input\nTiming: 30\u00a0min\nHere, we describe how to run eVIP2 on RNA-seq gene expression data from two RNF43 variants. We recommend using Kallisto, but gene quantification from other tools can be used as well, as shown in the sections below. The use of Kallisto is required for eVIP2 pathway analysis.\nWe prepared a GitHub repository and Docker image containing all the necessary example inputs, reference files, and scripts for the method presented below.\nThe repository is found on GitHub: https://github.com/BrooksLabUCSC/eVIP2[href=https://github.com/BrooksLabUCSC/eVIP2].\nThe docker image is found at Docker Hub: https://hub.docker.com/r/althornt/evip2_env[href=https://hub.docker.com/r/althornt/evip2_env].\nThe Docker image contains all files needed to run the following tutorial commands below in the directory named \u201cdocker_tutorial_files\u201d. Here we describe the files and how they are used.\nPreparation of RNA-seq data.\nNote: We recommend performing general quality control analysis on your data. For information on quality control for raw RNA-seq reads, refer to Conesa et\u00a0al. (Conesa et\u00a0al., 2016[href=https://www.wicell.org#bib5]). Based on our analysis from subsampling read depth, we recommend having at least 20 million reads per replicate.\nRun Kallisto on the RNA-seq fastq files with default parameters, or the parameters of your choice. Only default parameters have been tested. Kallisto creates an output directory for each sample, which contains various files, including \u201dabundance.tsv\u201d. A directory containing a Kallisto subdirectory for each sample will be used as input to run_eVIP2.py. For this protocol, a folder of Kallisto outputs (docker_tutorial_files/RNF43_kallisto_outputs) is provided in the Docker image and can be used as a guide to structure directories for new datasets.\nPreparation of input files. To run eVIP2, users need to provide the following required files, which are located within the Docker image and in the GitHub repository:--sig_info This tab delimited file indicates which samples are replicates of which conditions. The sample names listed under distil_id must match the corresponding name of each Kallisto output directory.\ntable:files/protocols_protocol_1938_1.csv\n-r The reference file is a tab delimited file that describes which WT to mutant comparisons to do.\ntable:files/protocols_protocol_1938_2.csv\n-c The control file is a tab delimited file to list the controls in the experiment. The control name should match the \u201csig_id\u201d in the -sig_info file. There must be at least one control.\nGFP\nNote: When running eVIP2 Pathways (by declaring -eVIPP) the following additional files are required and are provided on the Docker image:\n-gmt Gene set file in .gmt format. These can be downloaded from MSigDB http://www.gsea-msigdb.org/gsea/msigdb/collections.jsp[href=http://www.gsea-msigdb.org/gsea/msigdb/collections.jsp] (Liberzon et al., 2011[href=https://www.wicell.org#bib7]). Custom gene sets may also be created.\n-gtf A gtf file used to convert transcript counts to gene counts. In this tutorial we use the Ensembl GRCh38 version of the reference genome, but eVIP2 is compatible with any version (Howe et al., 2021[href=https://www.wicell.org#bib6]). The gtf file used with eVIP2 should be the same as the gtf used as input to Kallisto.\nSetting up eVIP2 repo and Docker container.\nClone the eVIP2 repo to the desired path on your machine:\n> git clone https://github.com/BrooksLabUCSC/eVIP2.git[href=https://github.com/BrooksLabUCSC/eVIP2.git]\nPull the Docker image.\n> docker pull althornt/evip2_env\nVerify the Docker image installation.\n> docker image ls\nWhich should display the althornt/evip2_env repository name.\nREPOSITORY\u00a0\u00a0\u00a0\u00a0TAG IMAGE ID\u00a0\u00a0\u00a0\u00a0CREATED\u00a0\u00a0\u00a0\u00a0SIZE\nalthornt/evip2_env\u00a0\u00a0\u00a0\u00a0latest\u00a0\u00a0\u00a0\u00a08f9af2abd32e\u00a0\u00a0\u00a0\u00a01\u00a0h ago\u00a0\u00a0\u00a0\u00a04.9GB\nRun the Docker container:\n-v mounts the locally cloned eVIP2 directory inside of the container.\n-ti makes the container interactive.\n> docker run -v /path/to/eVIP2:/eVIP2 -ti althornt/evip2_env\nRunning eVIP2 command from kallisto outputs.Now, we demonstrate the first of three independent eVIP2 applications. The following eVIP2 command recreates the overall and pathway analysis of the RNF43 variants presented in Thornton et\u00a0al. (Thornton et\u00a0al., 2021[href=https://www.wicell.org#bib11]). Enter the eVIP2 directory and run eVIP2 with pathway analysis. Table\u00a01[href=https://www.wicell.org#tbl1] provides an explanation of all parameters of the run_eVIP2.py command:\ntable:files/protocols_protocol_1938_4.csv\n> cd eVIP2\n> python2 run_eVIP2.py \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--input_dir ../docker_tutorial_files/RNF43_kallisto_outputs \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--out_directory tutorial_files/eVIP2_output_from_kallisto \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--sig_info tutorial_files/RNF43_sig.info \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-c tutorial_files/controls.grp \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-r tutorial_files/comparisons.tsv \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--gmt tutorial_files/h.all.v6.0.symbols.gmt \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--gtf ../docker_tutorial_files/Homo_sapiens.GRCh38.87.gtf \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--num_reps 4 \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--use_c_pval \\\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0--eVIPP\neVIP2 characterization of RNF43 variants with a gene expression table as input\nTiming: 1\u00a0min\nRunning eVIP2 command from gene expression table.\nFor a second eVIP2 application, we demonstrate running eVIP2 using a gene expression table as input as an alternative to using the Kallisto inputs. At this time, pathway analysis has only been tested with Kallisto input; however, you can still run eVIP2 with a generic gene expression table without pathway analysis. To run the eVIP2 pipeline without pathway analysis, the \u2013input_gene_tpm or \u2013input_table parameters can be used as input with run_eVIP2.py.\nSince we are using the same data and experimental setup as in the previous application, we use many of the same input files and parameters files as above. In the following command, we use a gene expression table from the RNF43 experiment to get the overall eVIP2 predictions.\npython2 run_eVIP2.py \\\n\u00a0\u00a0--input_gene_tpm tutorial_files/RNF43_gene_exp.tsv \\\n\u00a0\u00a0--\u2013-out_directory tutorial_files/eVIP2_output_from_gene_exp_table \\\n\u00a0\u00a0--\u2013-sig_info tutorial_files/RNF43_sig.info \\\n\u00a0\u00a0-c tutorial_files/controls.grp \\\n\u00a0\u00a0-r tutorial_files/comparisons.tsv \\\n\u00a0\u00a0--num_reps 4 \\\n\u00a0\u00a0--use_c_pval\neVIP2 RNF43 variants JuncBASE table\nTiming: 1\u00a0min\nRunning eVIP2 command from JuncBASE table.So far, we have used gene expression as input to predict variant impact. Now we demonstrate how tables representing other biological measurements can be used as well. Junction Based Analysis of Alternative Splicing Events (JuncBASE) is a tool to identify and quantify alternative splicing in RNA-seq data (Brooks et\u00a0al., 2011[href=https://www.wicell.org#bib3]).\nHere we use a table of quantification of alternative splicing events generated by JuncBASE to see how the RNF43 gene variants impact the splicing profiles using the run_eVIP2.py script. We use the --input_table parameter to use the JuncBASE table as input.\npython2 run_eVIP2.py \\\n\u00a0\u00a0--input_table tutorial_files/RNF43_JuncBASE_PSI_infile.txt \\\n\u00a0\u00a0--out_directory tutorial_files/eVIP2_output_from_juncBASE \\\n\u00a0\u00a0--sig_info tutorial_files/RNF43_sig.info \\\n\u00a0\u00a0-c tutorial_files/controls.grp \\\n\u00a0\u00a0-r tutorial_files/comparisons.tsv \\\n\u00a0\u00a0--num_reps 4 \\\n\u00a0\u00a0--use_c_pval", "Step-by-step method details\nStep-by-step method details\nStep 1: Installing PhosR\nTiming: 5\u00a0min\nFull installation of PhosR includes downloading the PhosR package from GitHub or BioConductor. To use the latest developmental version of PhosR, install from GitHub. An example of how to perform all steps of this protocol using real data is available on the project GitHub at https://pyanglab.github.io/PhosR/articles/web/PhosR_STAR_protocols.html[href=https://pyanglab.github.io/PhosR/articles/web/PhosR_STAR_protocols.html].\nInstall PhosR by running the following code:\n> if(!require(devtools)){\n> install.packages(\"devtools\") # If not already installed\n> }\n> devtools::install_github(\"PYangLab/PhosR\",\nbuild_opts\u00a0= c(\"--no-resave-data\", \"--no-manual\"),\nbuild_vignettes\u00a0= TRUE,\ndependencies\u00a0= TRUE)\n> library(PhosR)\nTo access the vignette directly from R console, run the following code after installation:\n> browseVignettes(\"PhosR\")\nCritical: To install all dependencies, you will need to update to the recommended R version (4.0.3 or above).\nNote: Note that all the necessary data needed to reproduce the code can be downloaded from https://github.com/PYangLab/PhosR_STAR_Protocols[href=https://github.com/PYangLab/PhosR_STAR_Protocols].\nStep 2: Creating a PhosphoExperiment object from a MaxQuant output\nTiming: 10\u00a0min\nTo increase the usability of PhosR functions, we implement a \u201cPhosphoExperiment\u201d (ppe) object based on the \u201cSummarizedExperiment\u201d class. To create the PhosphoExperiment object, you will be required to provide a quantification matrix where columns refer to samples and rows refer to phosphosites. Additional annotation labels for phosphosites should be provided alongside the matrix, including the phosphosite residue and position, \u201csequence window\u201d that captures the amino acids flanking the phosphorylation sites, and the official gene symbol of the host protein in capital letters.\nHere, we will show the basic steps for generating a PhosphoExperiment object for an exemplar MaxQuant-processed phosphoproteomic data from two liver cell lines, FL83B and Hepa 1-6 cells (Humphrey et\u00a0al., 2015[href=https://www.wicell.org#bib6]). This dataset contains the phosphoproteomic quantifications of mouse hepatocyte cell lines that were treated with either PBS (mock) or insulin. Each condition includes six biological replicates.Load the txt output file to an R environment\n> phospho_hepatocyte_raw <- read.delim(\"Data/PXD001792_raw_hepatocyte.txt\", header\u00a0= TRUE)\nClean and process the raw txt files to extract quantifications.\nTo delete reverse matches and potential contaminants.\n> del <- which(phospho_hepatocyte_raw[,\"Reverse\"]==\"+\" | phospho_hepatocyte_raw[,\"Potential.contaminant\"]==\"+\")\n> phospho_hepatocyte_clean <- phospho_hepatocyte_raw[-del,]\nWe subset the raw data to select columns with \u201cIntensity\u201d values.\n> PXD001792_raw_hepatocyte <- phospho_hepatocyte_clean[,grep(\"Intensity\", colnames(phospho_hepatocyte_clean))]\nCreate the PhosphoExperiment Object\nCreating the base PhosphoExperiment object\n> ppe <- PhosphoExperiment(assays\u00a0= list(Quantification\u00a0= as.matrix(PXD001792_raw_hepatocyte)))\nAdd site annotations to the object\n> GeneSymbol <- toupper(sapply(strsplit(as.character(phospho_hepatocyte_clean[,\"Gene.names\"]), \";\"), function(x){x[1]}))\n> Residue <- as.character(phospho_hepatocyte_clean [,\"Amino.acid\"])\n> Site <- as.numeric(phospho_hepatocyte_clean [,\"Position\"]) >\nSequence <- sapply(strsplit(as.character(phospho_hepatocyte_clean[,\"Sequence.window\"]), \";\"), function(x){x[1]})\n> ### add these annotations to respective ppe slots > ppe@GeneSymbol <- GeneSymbol\n> ppe@Residue <- Residue\n> ppe@Site <- Site\n> ppe@Sequence <- Sequence\nAlternatively, we can create PhosphoExperiment object as following\n> ppe <- PhosphoExperiment(assays\u00a0= list(Quantification\u00a0= as.matrix(PXD001792_raw_hepatocyte)), Site\u00a0= Site, GeneSymbol\u00a0= GeneSymbol, Residue\u00a0= Residue, Sequence\u00a0= Sequence)\nLastly add colData information\n> sample_name <- strsplit(gsub(\"\u02c6Intensity.\", \"\", colnames(ppe)), \"_\")\n> df <- S4Vectors::DataFrame(\n> cellline\u00a0= sapply(sample_name, \"[[\", 1),\n> condition\u00a0= sapply(sample_name, \"[[\", 2),\n> replicate\u00a0= sapply(sample_name, \"[[\", 3)\n> )\n> rownames(df) <- colnames(ppe)\n> SummarizedExperiment::colData(ppe) <- df\nHave a quick glance of the object\n> ppe\n> dim(ppe)\nAdditional arguments\nUniprotID: Uniprot ID of the protein which has the phosphosite\nLocalization: localization reliability of the phosphosite\nThe final output of \u201cStep 2: Creating a PhosphoExperiment object from a MaxQuant output[href=https://www.wicell.org#sec3.2]\u201d is a PhosphoExperiment object containing the quantification matrix and site annotations of all phosphosites.\nCritical: Different preprocessing software may output the key data fields required for the PhosphoExperiment in different ways. Please ensure that the format used conform to the requirements in PhosR. The specific requirements are outlined below.\nRequirements of the basic phosphosite featuresPosition of the phosphosite: An integer vector denoting the position of the phosphosite within the protein\nResidue of the phosphosite: A character vector denoting the residue of the phosphosite as a single letter (\u201cS\u201d, \u201cY\u201d, or \u201cT\u201d)\nGene symbol: A character vector containing gene symbols in upper case\nSequence window: A character vector denoting the sequence window, typically of 15\u201331 in length. The residues in the sequence window should be capitalized. Note that the predicted phosphosite should reside in the middle of the window. Any phosphosites that are found near the N or C terminus of the protein will require a placeholder (i.e., add \u201c_\u201d to either side of the flanking sequence) to position the phosphosite in concern at the middle of the sequence window (Most processing software will do this for you automatically).\nStep 3: Data pre-processing and differential phosphosite identification\nTiming: 10\u00a0min\nThe presence of missing values in quantitative phosphoproteomics reduces the completeness of data. Whilst imputation has been widely applied to handle missing values, it remains a major challenge when analyzing phosphoproteomic data and has significant impact on downstream analysis such as normalization. PhosR provides users with greater flexibility for imputation with imputation functions such as \u2018scImpute\u2019 and \u2018tImpute\u2019. Here, we will go through each function step by step to demonstrate their use in imputing phosphoproteomic data. We will call differentially phosphorylated sites between two cell lines FL83B and Hepa 1-6 to visualize the change in phosphorylation upon insulin simulation.\nLog transformation\nWe perform log2 transformation of the data\n> logmat <- log2(SummarizedExperiment::assay(ppe, \"Quantification\"))\n> # mark any missing values as NA\n> logmat[is.infinite(logmat)] <- NA\n> SummarizedExperiment::assay(ppe, \"Quantification\") <- logmat\nPerform filtering\nWe first extract the grouping information for cell type and condition\n> grps <- paste0(SummarizedExperiment::colData(ppe)$cellline, \"_\", SummarizedExperiment::colData(ppe)$condition)We filter for sites with at least 50% quantification rate (q \u2265 0.5) in one or more conditions\n> ppe <- selectGrps(ppe, grps, 0.5, n=1)\nCheck the filtering results\n> dim(ppe)\nNote: The \u2018Quantification\u2019 assay in PhosphoExperiment object is now filtered of phosphosites with high number of missing values. The user may wish to save the unfiltered matrix, `PXD001792_raw_hepatocyte, for future analysis.\nPerform imputation of missing values\nPhosR enables site- and condition-specific imputation. Here, for each phosphosite in each condition, we impute its missing values in that condition (if any) using site- and condition-specific imputation if the quantification rate within that condition is equal to or greater than a desired percentage (such as \u2265 50% in the example below).\n> set.seed(123)\n> ppe <- scImpute(ppe, 0.5, grps)\n> ppe\nLastly, we can impute the remaining sites using tail-based imputation\n> ppe <- tImpute(ppe, assay\u00a0= \"imputed\")\nNote: At this stage, the imputed quantification matrix is stored as an assay called \u2018imputed\u2019 in the PhosphoExperiment object.\nCentering data across their median\n> ppe <- medianScaling(ppe, scale\u00a0= FALSE, assay\u00a0= \"imputed\")\n> ppe\nNote: The scaled quantification matrix can be found as the \u2018scaled\u2019 matrix in the PhosphoExperiment object.\nCall differentially phosphorylated sites for the FL83B and Hepa 1-6 insulin stimulation dataset\nWe use limma package for calling for differentially phosphorylated sites between control and insulin-simulated conditions in the two cell types.\n> library(limma)\n> design <- model.matrix(~ grps - 1)\n> fit <- lmFit(ppe@assays@data$scaled, design)\n> contrast.matrix <- makeContrasts(grpsFL83B_Ins-grpsFL83B_Control, grpsHepa1.6_Ins-grpsHepa1.6_Control, levels=design)\n> fit2\u00a0<- contrasts.fit(fit, contrast.matrix)\n> fit2\u00a0<- eBayes(fit2)\nVisualize differentially phosphorylated sites using a volcano plot (Figure 1[href=https://www.wicell.org#fig1]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/701-Fig1.jpg\nFigure\u00a01. Volcano plots visualizing significantly up- and down-regulated phosphositesDifferential phosphosites can be called using normalized LFQ values and visualized using volcano plots. The above plots show up- (red) and down-regulated (blue) phosphosites in the two liver cell lines, respectively.\n> par(mfrow=c(1,2))\n> FL83B.DE <- topTable(fit2, coef=\"grpsFL83B_Ins - grpsFL83B_Control\", number\u00a0= Inf)\n> plot(FL83B.DE[,\"logFC\"], -log10(FL83B.DE[,\"adj.P.Val\"]), main=\"FL83B\", xlab=\"Log2 FC\", ylab=\"-Log10(Adjust P)\")\n> sel <- which(FL83B.DE[,\"adj.P.Val\"]\u00a0< 0.05 & FL83B.DE[,\"logFC\"] > 0)\n> points(FL83B.DE[sel,\"logFC\"], -log10(FL83B.DE[sel,\"adj.P.Val\"]), pch=16, col=\"red\")\n> sel <- which(FL83B.DE[,\"adj.P.Val\"]\u00a0< 0.05 & FL83B.DE[,\"logFC\"]\u00a0< 0)\n> points(FL83B.DE[sel,\"logFC\"], -log10(FL83B.DE[sel,\"adj.P.Val\"]), pch=16, col=\"blue\")\n> Hepa1.6.DE <- topTable(fit2, coef=\"grpsHepa1.6_Ins - grpsHepa1.6_Control\", number\u00a0= Inf)\n> plot(Hepa1.6.DE[,\"logFC\"], -log10(Hepa1.6.DE[,\"adj.P.Val\"]), main=\"Hepa 1-6\", xlab=\"Log2 FC\", ylab=\"-Log10(Adjust P)\")\n> sel <- which(Hepa1.6.DE[,\"adj.P.Val\"]\u00a0< 0.05 & Hepa1.6.DE[,\"logFC\"] > 0)\n> points(Hepa1.6.a.DE[sel,\"logFC\"], -log10(Hepa1.6.DE[sel,\"adj.P.Val\"]), pch=16, col=\"red\")\n> sel <- which(Hepa1.6.DE[,\"adj.P.Val\"]\u00a0< 0.05 & Hepa1.6.DE[,\"logFC\"]\u00a0< 0)\n> points(Hepa1.6.DE[sel,\"logFC\"], -log10(Hepa1.6.DE[sel,\"adj.P.Val\"]), pch=16, col=\"blue\")\nAfter imputation, we calculate the ratio of each value against the mean phosphosite values of the control samples. The ratios were calculated independently for each of the two cell lines (Figure 2[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/701-Fig2.jpg\nFigure\u00a02. Boxplot of quantifications before and after taking the ratio\nPhosphoproteomic data from label-free quantification (LFQ) are typically converted to ratios against the control samples to generate fold changes (FCs). The boxplots of phosphoproteomic quantifications demonstrate the change in distribution of values before and after conversion for each sample.\n> FL83B.ratio <- SummarizedExperiment::assay(ppe, \"scaled\")[, grep(\"FL83B_\", colnames(ppe))] -\nrowMeans(SummarizedExperiment::assay(ppe, \"scaled\")[,grep(\"FL83B_Control\", colnames(ppe))])\n> Hepa.ratio <- SummarizedExperiment::assay(ppe, \"scaled\")[, grep(\"Hepa1.6_\", colnames(ppe))] -\nrowMeans(SummarizedExperiment::assay(ppe, \"scaled\")[,grep(\"Hepa1.6_Control\", colnames(ppe))])\n> SummarizedExperiment::assay(ppe, \"ratio\")<- cbind(FL83B.ratio, Hepa.ratio)\n> par(mfrow=c(1,2))\n> boxplot(ppe@assays@data$scaled, ylab=\"Log2 LFQ\", main=\"Normalized LFQ data\", las=2, col=factor(rep(1:4, each=6)))\n> boxplot(ppe@assays@data$ratio, ylab=\"Log2 Fold Change\", main=\"Ratio data\", las=2, col=factor(rep(1:4, each=6)))\nLastly, we will save this processed ppe object for later use\n> PXD001792_ppe_hepatocyte <- ppe\n> save(PXD001792_ppe_hepatocyte, file\u00a0= \"Data/PXD001792_ppe_hepatocyte.RData\")Critical: After imputation, data from label-free quantification are typically converted to ratios before subsequent analysis. In contrast to label-free data, you do not need to take ratios of phosphoproteomic data derived from SILAC quantification since the values are inherently ratios (typically with respect to the control sample). The tail-based imputation (Beck et\u00a0al., 2015[href=https://www.wicell.org#bib1]) is designed specifically for label-free data (such as the one used in our example) and is not applicable to SILAC data.\nStep 4: Identifying stably phosphorylated sites\nTiming: 10\u201330\u00a0min\nSeveral commonly used data normalization approaches such as the 'removal of unwanted variation\u201d (RUV) (Gagnon-Bartsch and Speed, 2012[href=https://www.wicell.org#bib3]) require a set of internal standards whose expression are known to be unchanged in the samples measured. This is a challenge for phosphoproteomic data since phosphorylation is a highly dynamic biochemical process. Identifying a set of stably phosphorylated sites (SPSs) is a unique feature of PhosR which enables users to identify context-dependent sets of SPSs. We also included a set of 100 SPSs as a resource, identified from multiple high-quality datasets generated from different cell types and experimental conditions (Kim et\u00a0al., 2021[href=https://www.wicell.org#bib7]). As an example, we will use three datasets to demonstrate how SPSs can be identified from multiple phosphoproteomic datasets. Users may wish to replace the example datasets with their own collection of datasets.\nLoad and set up datasets\nLoad datasets\n> load(\"Data/PXD010621_ppe_ESC.RData\", verbose\u00a0= TRUE)\n> load(\"Data/PXD003631_ppe_adipocyte.RData\",\nverbose\u00a0= TRUE)\n> load(\"Data/phospho_ppe_adipocyte.RData\",\nverbose\u00a0= TRUE)\nSimplify names of datasets\n> ppe1\u00a0<- PXD010621_ppe_ESC\n> ppe2\u00a0<- PXD003631_ppe_adipocyte\n> ppe3\u00a0<- phospho_ppe_adipocyte\nMake a list of all PhosphoExperiment objects\n> ppe.list <- list(ppe1, ppe2, ppe3)\nMake a list of grouping information of each dataset\n> cond.list <- list(\ngrp1\u00a0= gsub(\"_.+\", \"\", colnames(ppe1)),\ngrp2\u00a0= gsub(\"_r[0-9]\", \"\", colnames(ppe2)),\ngrp3\u00a0= colnames(ppe3))Note: The regular expression used for defining \u2018cond.list\u2019 above is highly specific to the example dataset used in this tutorial. Users should adopt and modify these codes according to the specific experimental design and conditions in their own datasets.\nCritical: If inputs are not PhosphoExperiment objects, please transfer the data format to a PhosphoExperiment object (refer to the instructions in \u201cStep 2: Creating a PhosphoExperiment object from a MaxQuant output[href=https://www.wicell.org#sec3.2]\u201d). Example datasets are processed datasets after filtering, normalization, and ratio converting. Please refer to \u201cStep 4: Identifying stably phosphorylated sites[href=https://www.wicell.org#sec3.4]\u201d for data normalization guidance.\nIdentify SPSs by calling getSPS()\n> generate a vector of the selected assays in each of the PhosphoExperiment objects\n> assays <- \"Quantification\"\n> inhouse_SPSs <- getSPS(phosData\u00a0= ppe.list,\nassays\u00a0= assays,\nconds\u00a0= cond.list, num\u00a0= 100)\nNote: Please note that presence of sufficient overlapped phosphosites identified from the input datasets is critical to identify SPSs. You will receive an error if the number of overlapped sites is fewer than 200 in at least two datasets or fewer than 1,000 across all input datasets.\nRequired parameters:\nphosData: a list of users' PhosphoExperiment objects from which generate SPSs. For creating PhosphoExperiment objects, please refer to \u201cStep 2: Creating a PhosphoExperiment object from a MaxQuant output[href=https://www.wicell.org#sec3.2]\u201d. Please note that the datasets in the list require to be processed.\nconds: a list of vectors contains the conditions or time-course labels of each sample in the phosphoExperiment objects.\nassays: a vector indicating the assay to select, by default is \u201cQuantification\u201d.\nnum: the number of SPSs to identify, by default is 100.\nThe output of \u201cStep 4: Identifying stably phosphorylated sites[href=https://www.wicell.org#sec3.4]\u201d will be a set of stably phosphorylated sites identified across the input datasets that users provide. The SPSs will be used to perform batch correction.Step 5: Normalization and batch correction of data sets\nTiming: 10\u00a0min\nA common but critical step in phosphoproteomic data analysis is to correct for batch effect. Without batch effect correction, it is often not possible to analyze datasets in an integrative manner. To perform data integration and batch effect correction, we utilize a set of stably phosphorylated sites (SPSs) across a panel of phosphoproteomic datasets (defined from \u201cStep 4: Identifying stably phosphorylated sites[href=https://www.wicell.org#sec3.4]\u201d) and implement normalization using RUV-III (Molania et\u00a0al., 2019[href=https://www.wicell.org#bib8]). To demonstrate batch effect correction, we will perform RUVphospho to normalize a SILAC data from L6 myotubes treated with two factors: 1) AICAR, an analog of adenosine monophosphate (AMP) that stimulates AMPK activity and/or 2) insulin.\nCritical: RUV-III requires a complete data matrix. If you have not followed through the steps above, you will need to perform imputation of the missing values. The imputed values are removed by default after normalization but can be retained for downstream analysis if the users wish to use the imputed matrix.\nLoad PhosphoExperiment objects\n> load(\"Data/PXD019127_ratio_myoblast.RData\",\nverbose\u00a0= TRUE)\n> ppe <- PhosphoExperiment(assays\u00a0= list(Quantification\u00a0= as.matrix(PXD019127_ratio_myoblast)))\nAdd site annotations to PhosphoExperiment object\n> rowNames <- strsplit(rownames(ppe), \"~\")\n> ppe@GeneSymbol <- toupper(sapply(rowNames, \"[[\", 2))\n> ppe@Residue <- gsub(\"[0-9]\",\"\", sapply(rowNames, \"[[\", 3))\n> ppe@Site <- as.numeric(gsub(\"[A-Z]\",\"\", sapply(rowNames, \"[[\", 3)))\n> ppe@Sequence <- sapply(rowNames, \"[[\", 4)\nAdd colData information\n> sample_name <- strsplit(colnames(ppe), \"_\")\n>\n> df <- S4Vectors::DataFrame(\n> condition\u00a0= sapply(sample_name, \"[[\", 1),\n> replicate\u00a0= gsub(\"exp\", \"\", sapply(sample_name, \"[[\", 2))\n> )\n> rownames(df) <- colnames(ppe)\n> SummarizedExperiment::colData(ppe) <- df\nDiagnosing batch effect\nHierarchical clustering\n> plotQC(SummarizedExperiment::assay(ppe, \"Quantification\"), panel\u00a0= \"dendrogram\",\ngrps\u00a0= SummarizedExperiment::colData(ppe)$condition,\nlabels\u00a0= colnames(ppe))\u00a0+ ggplot2::ggtitle(\"before batch correction\")\nPCA plot\n> plotQC(SummarizedExperiment::assay(ppe, \"Quantification\"), panel\u00a0= \"pca\",\ngrps\u00a0= SummarizedExperiment::colData(ppe)$condition,\nlabels\u00a0= colnames(ppe))\u00a0+ ggplot2::ggtitle(\"before batch correction\")Correcting batch effect\nConstruct a design matrix by condition\n> design <- model.matrix(~ SummarizedExperiment::colData(ppe)$condition - 1)\n> head(design) # observe first 6 rows of the design matrix\nDefine negative controls sites\n> # Given that the rownames of a matrix ppe is in a format 'GENESYMBOL;RESIDUE;SITE;'\n> sites <- paste(\nsapply(ppe@GeneSymbol, function(x)x),\n\";\",\nsapply(ppe@Residue, function(x)x),\nsapply(ppe@Site, function(x)x),\n\";\",\nsep\u00a0= \"\")\n> data(SPSs)\n> ctl <- which(sites %in% SPSs)\nRun RUVphospho\n> ppe <- RUVphospho(ppe, M\u00a0= design, k\u00a0= 3, ctl\u00a0= ctl)\nQuality control to assess the removal of batch effect\nHierarchical clustering\n# plot before and after batch correction\n> p1\u00a0<- plotQC(SummarizedExperiment::assay(ppe,\"Quantification\"),\ngrps= SummarizedExperiment::colData(ppe)$condition,\nlabels\u00a0= colnames(ppe),\npanel\u00a0= \"dendrogram\")\n> p2\u00a0<- plotQC(SummarizedExperiment::assay(ppe,\"normalized\"),\ngrps= SummarizedExperiment::colData(ppe)$condition,\nlabels\u00a0= colnames(ppe),\npanel=\"dendrogram\")\n> ggpubr::ggarrange(p1, p2, nrow\u00a0= 1)\nPCA\n# plot before and after batch correction\n> p1\u00a0<- plotQC(SummarizedExperiment::assay(ppe,\"Quantification\"),\ngrps= SummarizedExperiment::colData(ppe)$condition,\nlabels\u00a0= colnames(ppe),\npanel\u00a0= \"pca\")\n> p2\u00a0<- plotQC(SummarizedExperiment::assay(ppe,\"normalized\"),\ngrps= SummarizedExperiment::colData(ppe)$condition,\nlabels\u00a0= colnames(ppe),\npanel=\"pca\")\n> ggpubr::ggarrange(p1, p2, nrow\u00a0= 1)\nWe can now save the final processed data for future use\n> PXD019127_ppe_myoblast\u00a0= ppe\n> save(PXD019127_ppe_myoblast,\nfile\u00a0= \"Data/PXD019127_ppe_myoblast.RData\")\nPause Point: This is an ideal pause point as we have generated a fully processed data. By now, you should have a good idea of the data quality and have performed the necessary processing to filter any suboptimal sites, imputed missing values (if present) and diagnosed and corrected any batch effect present in the data.\nStep 6: Predicting kinase substrates\nTiming: 15\u00a0minA key end-goal of phosphoproteomic data analysis is to identify kinases that are responsible for the phosphorylation of specific sites. The basic computational approach to annotate kinases to their substrates or phosphosites is to find consensus amino acid sequences around the phosphorylation site. We can go beyond this approach by considering cell type and/or treatment (perturbation) specificity of phosphorylation. PhosR implements a multi-step framework that contains two major components including (i) a kinaseSubstrateScore function which scores a given phosphosite using kinase recognition motif and phosphoproteomic dynamics, and (ii) a kinaseSubstratePred function which synthesize the scores generated from (i) for predicting kinase-substrate relationships using an adaptive sampling-based positive-unlabeled learning method (Yang et\u00a0al., 2018[href=https://www.wicell.org#bib11]).\nIn the original PhosR publication, we demonstrate the application of the scoring method to the myotube phosphoproteome and uncover potential kinase-substrate pairs and global relationships between kinases. We confirm well established substrates of AMPK in our publication: ACACA S79, AKAP1 S103, SMCR8 S488 (Hoffman et\u00a0al., 2015[href=https://www.wicell.org#bib4]) and MTFR1L S100 (Schaffer et\u00a0al., 2015[href=https://www.wicell.org#bib10]). Importantly, PhosR generates a list of potential candidates not included in the PhosphoSitePlus database for validation.\nPrepare inputs\nLoad the PhosphoExperiment object\n> load(\"Data/PXD019127_ppe_myoblast.RData\",\nverbose\u00a0= TRUE)\n> ppe <- PXD019127_ppe_myoblast\n> mat <- SummarizedExperiment::assay(ppe, \"normalized\")\nCritical: If you have not processed your data, please go through Steps 1\u20135 to perform the necessary processing prior to performing the downstream analysis in Steps 6 and 7.\nFilter for up-regulated phosphosites\n> # filter for up-regulated phosphosites\n> mat.mean <- meanAbundance(mat,\ngrps\u00a0= SummarizedExperiment::colData(ppe)$condition)\n> aov <- matANOVA(mat=mat,\ngrps\u00a0= SummarizedExperiment::colData(ppe)$condition)\n> idx <- (aov\u00a0< 0.05) & (rowSums(mat.mean > 0.5) > 0)\n> mat.reg <- mat[idx, ,drop\u00a0= FALSE]\nStandardize the matrix\n> mat.std <- standardize(mat.reg)\n> rownames(mat.std) <- sapply(strsplit(rownames(mat.std), \"~\"), function(x){gsub(\" \", \"\", paste(toupper(x[2]), x[3], \"\", sep=\";\"))})Critical: To calculate the profile matching score, we rely on the z-score transformed matrix to compare the profiles of phosphosites. Thus, the standardization step is critical.\nKinase substrate scoring step integrates information from both kinase recognition motif (i.e., motif matching score) and experimental perturbation (i.e., profile matching score) for prioritizing kinases that may be regulating the phosphorylation level of each site quantified in the dataset.\nThe kinase-substrate prediction step requires the following specific inputs:\n\u2018substrate.list\u2019 denotes an object that contains all kinases and their substrate peptide sequences from the PhosphoSitePlus database. This is used to compute the position-specific motif matching score.\n> data('KinaseMotifs')\n> head(PhosphoSite.mouse)\n\u2018mat\u2019 denotes the standardized matrix. PhosR uses a pre-defined number of substrates to compare the dynamic phosphorylation profiles of the substrate against that of their known kinases. Next, the profile matching scores of each phosphosite quantified in \u2018mat\u2019 are calculated by using Pearson\u2019s correlation with respect to the averaged profiles of known substrates of each of all kinases.\n> head(mat.std)\n\u2018seqs\u2019 denotes a vector containing all the phosphosites detected in the phosphoproteomic dataset at hand. Note that if any filtering was performed in the previous step, we must perform the same filtering here.\n> seqs <- ppe@Sequence[idx]\n> head(seqs)\nRun PhosR kinase-substrate prediction with the default parameters. PhosR generates the final combined scores of the motif matching score and the profile matching score by taking into account the number of sequences and substrates used for calculating the motif and profile of the kinase.\n> kssMat <- kinaseSubstrateScore(substrate.list\u00a0= PhosphoSite.mouse,\nmat\u00a0= mat.std,\nseqs\u00a0= seqs,\nnumMotif\u00a0= 5,\nnumSub\u00a0= 1)\nThere are two arguments that are set to default in \u2018kinaseSubstrateScore\u2019\n\u2018numMotif\u2019 denotes the minimum number of sequences used for compiling the motif for each kinase. The default value is set to 5.\u2018numSub\u2019 denotes the minimum number of phosphosites used for compiling the phosphorylation profile. The default value is set to 1.\nNote: Please ensure that there are no duplicate phosphosites by removing or aggregating them before running the kinase-substrate prediction (for a workaround please refer to Problem 5 in Troubleshooting).\nNote: Please be patient, as this step may take several minutes.\nAs the second and last step of kinase-substrate prediction, PhosR uses the \u2018kinaseSubstratePred\u2019 function to synthesize the scores generated from \u2018kinaseSubstrateScore\u2019 to predict the kinase-substrate relationships using an adaptive sampling-based positive-unlabeled learning method (Yang et\u00a0al., 2018[href=https://www.wicell.org#bib11]). This step prioritizes the kinases that are most likely to regulate a phosphosite.\nUse the out of \u2018kinaseSubstrateScore\u2019 to run kinase substrate prediction\n> set.seed(1)\n> predMat <- kinaseSubstratePred(kssMat, top\u00a0= 30)\nNote: While the default values in \u2018kinaseSubstratePred\u2019 provide reliable performance of PhosR, the user may wish to optimize the parameters for each phosphoproteomic dataset. Listed below are the parameters to choose with default values and tips for each selection provided.\nSelection of Additional Parameters\n\u2018ensembleSize\u2019 denotes the number of the support vector machine (SVM) classifiers to be used in an ensemble. The default value is set to 10. Decreasing the number may speed up the process but may compromise the accuracy.\n\u2018top\u2019 denotes the top number of kinase substrates to select. The top kinase substrates are selected based on the combined scores. They are used as positive examples for training SVMs for predicting substrates of that kinase. The default value is 50.\u2018cs\u2019 denotes the score threshold. The default value is 0.8. This argument is used to filter any of the top kinases that do not meet a certain threshold. This argument would override the \u2018top\u2019 argument in that any substrates among the top 50 that do not meet the score threshold are excluded.\n\u2018inclusion\u2019 denotes the minimal number of substrates required for a kinase to be selected. The default value is 20. Decreasing this number would increase the total number of kinases for which we generate kinase-substrate scores, and vice versa.\n\u2018iter\u2019 denotes the number of iterations for adaSampling. AdaSampling procedure is used to update the SVM training examples based on the model confidence on each phosphosite. The default value is 5.\n\u2018verbose\u2019 is set to TRUE by default, allowing the tracking of the analysis progress.\nPause Point: Once the desired parameters are used to successfully run the kinase-substrate prediction, you may find this to be a good place to pause and save the results before proceeding with the visualization steps.\nStep 7: Constructing signaling networks (signalomes)\nTiming: 10\u00a0min\nConstructing signaling networks referred to as \u201cSignalomes\u201d is a useful feature in PhosR that allows the users to obtain a global view of kinase regulation and to establish distinct modules of proteins that demonstrate similar kinase and dynamic regulation upon perturbation or across a time-course.An important feature of PhosR signalomes is that the resultant signaling modules denote a set of proteins and not phosphosites. Proteins are frequently phosphorylated at multiple sites and these sites may not necessarily be targeted by the same kinases. Site- and protein-centric analyses of phosphoproteomic data lie at opposite ends of the spectrum, with the former treating phosphosites on the same protein independently and ignoring the host protein information, and the latter focusing on a specific protein, losing information from individual phosphosites. Using our global kinase-substrate scoring of phosphosites, we generate signalomes wherein dynamic changes in phosphorylation within and across proteins are conjointly analyzed, allowing us to detect proteins that are co-regulated across multiple phosphosites.\nSignalome construction The signalome construction uses the outputs of \u2018kinaseSubstrateScore\u2019 and \u2018kinaseSubstratePred\u2019 functions for the generation of a visualization of the kinase regulation of discrete regulatory protein modules present in our phosphoproteomic data (Figure 3[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/701-Fig3.jpg\nFigure\u00a03. Visualizations of global kinase-substrate relationships and signalomes\n(A) A heatmap of global kinase-substrate relationship scores. A clustered heatmap of the combined score from kinase-substrate scoring function for the top three phosphosites of all kinases evaluated in this study. A higher combined score denotes a better fit to a kinase motif and kinase-substrate phosphorylation profile of a phosphosite. Kinases are annotated to which kinase group they belong.\n(B) Signalomes identified from the phosphoproteomic data. The branching nodes denote the kinases for which substrates have been predicted, and the stem nodes denote the protein modules that they regulate. Edges between nodes connect kinases to the protein modules they regulate.\nPrepare inputs\n\u2018KSR\u2019 denotes the output of the kinase-substrate relationship scoring function.\n\u2018predMatrix\u2019 denotes the output of the \u2018kinaseSubstratePred\u2019 function.\u2018exprsMat\u2019 denotes the matrix with rows corresponding to phosphosites and columns corresponding to samples. Users may wish to perform various processing steps such as filtering, imputations, and normalization.\n\u2018KOI\u2019 denotes a vector containing kinases of interest for which the expanded signalomes will be generated. The Signalomes function also outputs signalomes associated to any kinase of interest (referred to as extended signalome of a kinase). To facilitate assessment of proteins and phosphosites that are under similar regulation, the extended signalome of a kinase of interest combines cognate signalomes from other kinases that share a high degree of similarity in substrate regulation.\nThen to generate the signalomes, run:\n> kinaseOI\u00a0= c(\"AKT1\")\n> signalomesRes <- Signalomes(KSR\u00a0= kssMat,\npredMatrix\u00a0= predMat,\nexprsMat\u00a0= mat.std,\nmodule_res\u00a0= 6,\nKOI\u00a0= kinaseOI)\nThere are a number of important arguments in the \u2018Signalomes\u2019 function.\n\u2018module_res\u2019 denotes the argument to control the number of final modules.\n\u2018threskinaseNetwork\u2019 denotes the threshold used to select interconnected kinases for the expanded signalomes.\n\u2018signalomeCutoff\u2019 refers to the cutoff used to filter kinase-substrate relationships based on scores from the \u2018kinaseSubstratePred\u2019 function. A cutoff of 0.5 is used as default. By increasing the threshold, you can reduce the number of phosphosites used to generate the signalomes by selecting high confidence phosphosites only.\n\u2018filter\u2019 requires a Boolean argument. When set to TRUE, the function filters modules with fewer than 10 proteins.\nVisualization of the Signalome as a Balloon Plot\nWe can visualize the signalomes as a balloon plot. Using the resulting visualization, we are better able to compare the differences in kinase regulation of the modules and the varying proportions of regulation. In the balloon plot, the size of the balloons denotes the percentage magnitude of kinase regulation in each module.\nThe code to generate the balloon plot is:\n> ### generate palette> my_color_palette <- grDevices::colorRampPalette(RColorBrewer::brewer.pal(8, \"Accent\"))\n> kinase_all_color <- my_color_palette(ncol(kssMat$combinedScoreMatrix))\n> names(kinase_all_color) <- colnames(kssMat$combinedScoreMatrix)\n> kinase_signalome_color <- kinase_all_color[colnames(predMat)]\n> plotSignalomeMap(signalomes\u00a0= signalomesRes, color\u00a0= kinase_signalome_color)\nConstruction of the Kinase Network\nFinally, we can also plot the signalome network that illustrates the connectivity between kinase signalome networks.\n> plotKinaseNetwork(KSR\u00a0= kssMat,\npredMatrix\u00a0= predMat,\nthreshold\u00a0= 0.95,\ncolor\u00a0= kinase_all_color)\nThe edges refer to the Pearson\u2019s correlation of kinase regulation between the two nodes linked by the edge. The wider the link, the stronger the correlation. You can control the links shown in the visualization by changing the \u2018threshold\u2019. Decreasing this value, which is set to a default of \u20180.9\u2019, would reveal more links.", "This protocol provides complete instructions for performing the ancestry analysis using a pre-computed reference model and three alternative file formats as input files. First, we show how to perform the analysis using a target model .vcf (VCF) file as input (alternative a steps). A VCF file contains sequence variations, and can be obtained from NGS data using Support Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0005] or any other of several available genotyping tools (FreeBayes (Garrison & Marth, 2012[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-bib-0007]), GATK HaplotypeCaller (Poplin et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-bib-0012]), VarScan (Koboldt et\u00a0al., 2012[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-bib-0009]), or SAMtools/BCFtools (Danecek et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-bib-0004])). Then, we show how to perform ancestry analysis using a GDS file as target model input (alternative b steps). To manage GDS files, EthSEQ uses a set of functions provided by the SNPRelate and gdsfmt R packages. Also, in this case, a minimum required set of variables should be stored in the GDS file (see Strategic Planning). Last, we show how to perform the ancestry analysis using a list of .bam (BAM) files as input (alternative c steps). A BAM format file is the compressed version of a SAM format file, and contains NGS aligned reads. EthSEQ expects a list of BAM files representing control (non-tumor) WES or TS NGS DNA data files obtained from a set of individuals of interest. EthSEQ determines the genotype calls for all available reference models\u2019 SNPs for each individual from the corresponding BAM file, then merges the genotype calls of all individuals, and finally performs the ancestry analysis. Genotype information is extracted and computed by EthSEQ, exploiting the tool ASEQ. More specifically, EthSEQ downloads ASEQ and runs it automatically using its genotype mode to compute the genotype of all available reference model's SNPs across all input BAM files. Details about ASEQ are available at http://bcglab.cibio.unitn.it/aseq[href=http://bcglab.cibio.unitn.it/aseq].\nNecessary Resources\nHardwareA 64-bit computer with \u22658 GB RAM\nAn internet connection to download the reference model\nNote that the RAM required for using the tool is proportional to the size of the input VCF file (only alternative a steps). Usually, the maximum RAM required for processing a file with approximately 250,000 variants and 1000 individuals will not exceed 4.5 GB when performing the file conversion from VCF to GDS format. Using GDS or a list of BAM files as input files (alternative b and c steps, respectively) the minimum RAM required is 1 GB.\nSoftware\nThe library has been tested with R version 3.6.3+. The R software is free and can be downloaded from the official website https://cran.r-project.org/[href=https://cran.r-project.org/], where installation instructions are also available. Although EthSEQ is an R package and can be run across different operating systems (e.g., Windows, MacOS and Linux), the code provided in this protocol is designed to run under Linux systems.\nInput files\nTarget model: one of the following file formats must be provided as target model: VCF file format, GDS file format, or a list of BAM files. VCF files provided in input to EthSEQ should respect the following constraints:\n               \nGenotype field \u201cGT\u201d is used for the analysis and must be present;\nOnly positions with single reference and single alternative base are admitted;\nNo duplicated sample names are admitted;\nNo duplicated positions are admitted.\nGDS files must instead be formatted as explained in the Strategic Planning section.\nSample data\nInput data for the example analyses reported here are included and installed along with the EthSEQ package. The user can manually download and explore these data from https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata[href=https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata].Output results of the example analyses reported here are available at https://github.com/cibiobcg/EthSEQ_Data/tree/master/example_outputs/[href=https://github.com/cibiobcg/EthSEQ_Data/example_outputs/]. The user can explore and compare the expected results available on the GitHub page with the results obtained by running the protocol.\nR installation\nTo download and install R, refer to the documentation available at the official website https://cran.r-project.org/[href=https://cran.r-project.org/]. R is provided there as a precompiled binary for multiple operating systems.\nInstall and load EthSEQ\n1. Run R from the command line:\n         \n$ R\n2. Install EthSEQ for the first time:\n         \n> install.packages(\"EthSEQ\")\n3. Load the library:\n         \n> library(EthSEQ)\nThe user performs the ancestry analysis using the function ethseq.Analysis and, as previously described, must provide a reference model and a target model.\nThe user defines the reference model using the parameters model.available, model.assembly, and model.pop (see Strategic Planning). The model is automatically downloaded and used by EthSEQ. Then, the user must follow one of the alternative steps based on the target model format available. The user can provide a genotype data file in VCF format (alternative a steps), a GDS data file (alternative b steps), or a list of BAM files (alternative c steps). A different set of parameters must be used when running the ethseq.Analysis function, depending on the target model file format used.\na. Ancestry analysis using a VCF file as input\n4a. Run the analysis:\n         \n> ethseq.Analysis(target.vcf = system.file(\"extdata\", \"Samples.HGDP.10000SNPs.vcf\",\n\u00a0\u00a0\u00a0\u00a0package = \"EthSEQ\"),\nmodel.available = \"Gencode.Exome\",\nmodel.assembly = \"hg38\",\nmodel.pop = \"All\",\nout.dir = file.path(tempdir(),\"EthSEQ_Analysis/\"),\nverbose=TRUE,\ncores = 1,\ncomposite.model.call.rate = 1,\nspace = \"3D\")\nIn this mode, the function ethseq.Analysis takes as input the following parameters:\n         \ntarget.vcf: Path to the samples\u2019 genotype data in VCF format;\nmodel.available: String specifying the pre-computed reference model to use (use getModelsList() function to retrieve the list of all available reference models);model.assembly (default = \u2018hg38\u2019): Version of the human assembly used to build the reference model (\u2018hg19\u2019 or \u2018hg38\u2019);\nmodel.pop (default = \u2018All\u2019): Population of the samples to be included in the reference model; use \u201cAll\u201d to perform the ancestry analysis using all the superpopulation defined by the 1000 Genomes Project or set the parameter to a specific available superpopulation (e.g., \u201cEUR\u201d) to perform the analysis across the corresponding subpopulation groups (use getSamplesInfo() function to retrieve all available populations);\nout.dir (default = tempdir()): Path to the folder where the output of the analysis is saved;\nmodel.folder (default = tempdir()): Path to the folder where reference models are already present or downloaded when needed;\ncores (default = 1): Number of parallel cores used for the analysis;\nverbose (default = TRUE): Print detailed execution information;\ncomposite.model.call.rate (default = 1): Minimum SNP call rate to include a SNP in the Principal Component Analysis (PCA);\nspace (default = \u20182D\u2019): Dimensions of PCA space used to infer ancestry (2D or 3D). 2D will use the first two principal components to infer the ancestry and generate the plot. 3D will use the first three principal components to infer the ancestry and generate the plots.\nOf note, the function:\n         \n> system.file(\"extdata\", \"Samples.HGDP.10000SNPs.vcf, package=\"EthSEQ\")\nretrieves the path to the sample VCF data file included in the EthSEQ package. Importantly, the file can also be manually downloaded from https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata[href=https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata] and provided to the ethseq.Analysis function specifying the corresponding file system path.\nThe output of the analysis is written into the out.dir folder. EthSEQ produces a collection of textual and visual files reporting the inferred ancestries for all the target model's individuals. See Guidelines for Understanding Results for a detailed description of all EthSEQ outputs and their interpretation.\nb. Ancestry analysis using a GDS file as input\n4b. Run the analysis:> ethseq.Analysis(target.gds = system.file(\"extdata\", \"Samples.HGDP.10000SNPs.gds\",\n\u00a0\u00a0\u00a0\u00a0package=\"EthSEQ\"),\nmodel.available = \"Gencode.Exome\",\nmodel.assembly = \"hg38\",\nmodel.pop = \"All\",\nout.dir = file.path(tempdir(),\"EthSEQ_Analysis/\"),\nverbose= TRUE,\ncores = 1,\ncomposite.model.call.rate = 1,\nspace = \"3D\")\nIn this mode, the function ethseq.Analysis takes as input the following parameters:\n         \ntarget.gds: Path to the samples\u2019 genotype data in GDS format;\nmodel.available: String specifying the pre-computed reference model to use (use getModelsList() function to retrieve the list of all available reference models);\nmodel.assembly (default = \u2018hg38\u2019): Version of the human assembly used to build the reference model model (\u2018hg19\u2019 or \u2018hg38\u2019);\nmodel.pop (default = \u2018All\u2019): Population of the samples to be included in the reference model; use \u201cAll\u201d to perform the ancestry analysis with all the superpopulation defined by the 1000 Genomes Project or set the parameter to a specific available superpopulation (e.g., \u201cEUR\u201d) to perform the analysis across the corresponding subpopulation groups (use getSamplesInfo() function to retrieve all available populations);\nout.dir (default = tempdir()): Path to the folder where the output of the analysis is saved;\nmodel.folder (default = tempdir()): Path to the folder where reference models are already present or downloaded when needed;\ncores (default = 1): Number of parallel cores used for the analysis;\nverbose (default = TRUE): Print detailed execution information;\ncomposite.model.call.rate (default = 1): Minimum SNP call rate to include a SNP in the Principal Component Analysis (PCA);\nspace (default = \u20182D\u2019): Dimensions of PCA space used to infer ancestry (2D or 3D). 2D will use the first two principal components to infer the ancestry and generate the plot. 3D will use the first three principal components to infer the ancestry and generate the plots.\nAs before, the function:\n         \n> system.file(\"extdata\", \"Samples.HGDP.10000SNPs.gds, package=\"EthSEQ\")retrieves the path to the sample GDS data file included in the EthSEQ package. Importantly, the file can be also manually downloaded from https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata[href=https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata] and provided to the ethseq.Analysis function specifying the corresponding file system path.\nAlso, in this case the output of the analysis is written into the out.dir folder, producing the same collection of textual and visual files reporting the inferred ancestries for all the target model's individuals. See Guidelines for Understanding Results for a detailed description of all EthSEQ outputs formats and their interpretation.\nc. Ancestry analysis using a list of BAM files as input\n4c. Create a text file containing the list of paths to the BAM files to use in the analysis:\n         \n> write(c(file.path(tempdir(),\"HGDP00228.sub_GRCh38.bam\"),\nfile.path(tempdir(),\"HGDP01200.sub_GRCh38.bam\"),\nfile.path(tempdir(),\"HGDP01201.sub_GRCh38.bam\")),\nfile.path(tempdir(),\"BAMs_List.txt\"))\nOf note, in this example we rely on BAM files and their corresponding index files available in a temporary folder created with the function tempdir(). To download and save the files in the aforementioned temporary folder the user has to run the following R code:\n         \n>download.file(\"https://github.com/cibiobcg/EthSEQ_Data/raw/master/BAM/HGDP00228.sub_GRCh38.bam\",destfile = file.path(tempdir(),\"HGDP00228.sub_GRCh38.bam\"))\n>download.file(\"https://github.com/cibiobcg/EthSEQ_Data/raw/master/BAM/HGDP00228.sub_GRCh38.bam.bai\",destfile = file.path(tempdir(),\"HGDP00228.sub_GRCh38.bam.bai\"))\n>download.file(\"https://github.com/cibiobcg/EthSEQ_Data/raw/master/BAM/HGDP01200.sub_GRCh38.bam\",destfile = file.path(tempdir(),\"HGDP01200.sub_GRCh38.bam\"))\n>download.file(\"https://github.com/cibiobcg/EthSEQ_Data/raw/master/BAM/HGDP01200.sub_GRCh38.bam.bai\",destfile = file.path(tempdir(),\"HGDP01200.sub_GRCh38.bam.bai\"))\n>download.file(\"https://github.com/cibiobcg/EthSEQ_Data/raw/master/BAM/HGDP01201.sub_GRCh38.bam\",destfile = file.path(tempdir(),\"HGDP01201.sub_GRCh38.bam\"))\n>download.file(\"https://github.com/cibiobcg/EthSEQ_Data/raw/master/BAM/HGDP01201.sub_GRCh38.bam.bai\",destfile = file.path(tempdir(),\"HGDP01201.sub_GRCh38.bam.bai\"))\n5c. Run the analysis:\n         \n> ethseq.Analysis(bam.list = file.path(tempdir(),\"BAMs_List.txt\"),\nmodel.available = \"Gencode.Exome\",\nout.dir = file.path(tempdir(),\"EthSEQ_Analysis/\"),\nverbose = TRUE,\ncores = 1,\naseq.path = file.path(tempdir(),\"EthSEQ_Analysis/\"),\nrun.genotype = TRUE,\nmbq = 20,\nmrq = 20,\nmdc = 10,\ncomposite.model.call.rate = 1,\nspace = \"3D\",\nbam.chr.encoding = TRUE)\nIn this mode, the function ethseq.Analysis takes as input the following parameters:\n         \nbam.list: Path to a file containing a list of BAM files paths;\nmodel.available: String specifying the pre-computed reference model to use (use getModels() function to retrieve the list of all available reference models);\nmodel.assembly (default = \u2018hg38\u2019): Version of the human assembly used to build the reference model (\u2018hg19\u2019 or \u2018hg38\u2019);model.pop (default = \u2018All\u2019): Population of the samples to be included in the reference model; use \u201cAll\u201d to perform the ancestry analysis using all the superpopulation defined by the 1000 Genomes Project or set the parameter to a specific available superpopulation (e.g., \u201cEUR\u201d) to perform the analysis across the corresponding subpopulation groups (use getSamplesInfo() function to retrieve all available populations);\nout.dir (default = tempdir()): Path to the folder where the output of the analysis is saved;\nmodel.folder (default = tempdir()): Path to the folder where reference models are already present or downloaded when needed;\nrun.genotype (default = FALSE): Logical values indicating whether the ASEQ genotype should be run;\naseq.path (default = tempdir()): Path to the folder where ASEQ binary is available or is downloaded when needed;\nmbq (default = 20): Minimum base quality used in the pileup by ASEQ;\nmrq (default = 20): Minimum read quality used in the pileup by ASEQ;\nmdc (default = 10): Minimum read count acceptable for genotype inference by ASEQ;\ncores (default = 1): Number of parallel cores used for the analysis;\nverbose (default = TRUE): Print detailed execution information;\ncomposite.model.call.rate (default = 1): Minimum SNP call rate to include a SNP in the Principal Component Analysis (PCA);\nspace (default = \u20182D\u2019): Dimensions of PCA space used to infer ancestry (2D or 3D). 2D will use the first two principal components to infer the ancestry and generate the plot. 3D will use the first three principal components to infer the ancestry and generate the plots.\nbam.chr.encoding (default = FALSE): Logical value indicating whether input BAM files have chromosomes encoded with \"chr\" prefix.As previously mentioned, when NGS is used as input, EthSEQ invokes ASEQ to produce for each input BAM file a VCF file reporting the genotype calls for the reference model's SNPs. Specifically, ASEQ calls a heterozygous genotype for an SNP if the proportion of the coverage for the alternative base with respect to the total coverage is in the range [0.2,0.8]; otherwise, ASEQ calls a homozygous genotype, either for the reference or the alternative base. The precision of this approach is shown and discussed in Romanel et\u00a0al. (2015[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-bib-0015]). Although the execution of ASEQ and the processing of its output is managed automatically and transparently to the user by EthSEQ, when several BAM files are analyzed, the ASEQ processing step can take some time. In those cases, ASEQ could also be run externally (Support Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0005]) and its output could be passed in input to EthSEQ in a secondary moment, setting the run.genotype parameter to FALSE. Importantly, in this case the ASEQ VCF output files should be provided to EthSEQ in the same format and folder as EthSEQ would have done automatically.\nSetting the parameter run.genotype to FALSE is also useful when the user wants to re-run only the ancestry analysis (e.g., with different parameters) without running the entire ASEQ genotyping again.\nAlso in this case, the output of the analysis is written into the out.dir folder, producing the same collection of textual and visual files reporting the inferred ancestries for all the target model's individuals. See Guidelines for Understanding Results for a detailed description of all EthSEQ outputs formats and their interpretation.This protocol provides complete instructions to perform ancestry analysis with a user-specified GDS file as reference model. In this case, a minimum set of variables should be stored in the GDS file, as described in Strategic Planning. In this protocol we show how to perform the analysis using a target model VCF file as in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], a steps.\nNecessary Resources\nHardware\n64-bit computer with \u22651 GB RAM\nSoftware\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001]\nInput files\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001]\nSample data\nInput data for the example analyses reported here are included and installed along with the EthSEQ package. The user can manually download and explore these data from https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata[href=https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata].\nOutput results of the example analyses reported here are available at https://github.com/cibiobcg/EthSEQ_Data/tree/master/example_outputs/[href=https://github.com/cibiobcg/EthSEQ_Data/example_outputs/]. The user can explore and compare the expected results available on the GitHub page with the results obtained by running the protocol.\nThe user should start by running step 1 to 3 as described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001] and then:\n4. Run the analysis:\n         \n> ethseq.Analysis(target.vcf = system.file(\"extdata\", \"Samples.HGDP.10000SNPs.vcf\",\n\u00a0\u00a0\u00a0\u00a0\u00a0package = \"EthSEQ\"),\nmodel.gds = system.file(\"extdata\", \"Reference.Gencode.Exome.10000SNPs.gds\",\npackage = \"EthSEQ\"),\nout.dir = file.path(tempdir(),\"EthSEQ_Analysis/\"),\nverbose = TRUE,\ncores = 1,\ncomposite.model.call.rate = 1,\nspace = \"3D\")\nIn this mode, the function ethseq.Analysis takes as input the following parameters:\n         \ntarget.vcf: path to the samples\u2019 genotype data file in VCF format. Only one file is allowed as input in the analysis;\nmodel.gds: path to a GDS file specifying the reference model;\nout.dir (default = tempdir()): path to the folder where the output of the analysis is saved;\nverbose (default = TRUE): print detailed execution information;\ncores (default = 1): number of parallel cores used for the analysis;composite.model.call.rate (default = 1): SNP call rate used to run the Principal Component Analysis (PCA). The SNPs with a call rate lower than this value are not included in the analysis;\nspace (default = \u20182D\u2019): Dimensions of PCA space used to infer ancestry (2D or 3D). 2D will use the first two principal components to infer the ancestry and generate the plot. 3D will use the first three principal components to infer the ancestry and generate the plots.\nOf note, the functions:\n         \n> system.file(\"extdata\", \"Samples.HGDP.10000SNPs.vcf, package=\"EthSEQ\")\n> system.file(\"extdata\",\"Reference.Gencode.Exome.10000SNPs.gds,package=\"EthSEQ\")\nretrieve the paths to the sample data files included in the EthSEQ package. Importantly, these files can be also manually downloaded from https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata[href=https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata] and provided to the ethseq.Analysis function specifying the corresponding file system paths.\nAlso, in this case the output of the analysis is written into the out.dir folder, producing the same collection of textual and visual files reporting the inferred ancestries for all the target model's individuals. See Guidelines for Understanding Results for a detailed description of all EthSEQ outputs formats and their interpretation.\nThe same analysis can be performed using as target input a GDS file (similar to Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], b steps) or a list of BAM files (similar to Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], c steps). Specifically, using the procedure described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], b steps and Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], c steps, it is enough to replace the parameters model.available, model.assembly, and model.pop with the parameter model.gds to perform the analysis using a user-specified GDS reference model, as described above.This protocol provides complete instructions to perform an ancestry analysis using a VCF file as input (as Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], a steps), but exploiting a multi-step refinement procedure to better discern ancestry annotations across ancestrally close groups. This approach relies on a tree structure (Fig. 2A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-fig-0002]), provided in input to EthSEQ as a matrix encoding (Fig. 2B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-fig-0002]), that represents recursively the annotation of close ancestry groups that we want to refine. Specifically, given a tree of ancestry group sets such that sibling nodes have non-intersecting ancestry groups and child nodes have ancestry groups included in the parent node ancestry groups, ancestry of individuals is inferred following a pre-order traversal of the tree (Fig. 2C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-fig-0002]). At each node with ancestry groups S, annotations resulting from the analysis of the parent node are refined by reducing both reference and target models on individuals with annotations in S only, and performing again the PCA analysis on these individuals. Global annotation of all individuals is updated throughout the tree traversal.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/14535f45-82b3-4594-a0d4-91122d911b99/cpz1663-fig-0002-m.jpg</p>\nFigure 2\nMulti-step refinement analysis. (A) Example of a tree used in the multi-step refinement analysis with P1, P2, etc., representing different reference ancestry groups. (B) Traversal of the tree representing the ancestry refinement steps. (C) Matrix representation of the tree that is given to EthSEQ.\nNecessary Resources\nHardware\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001]\nSoftware\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001]\nInput files\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001]\nSample data\nInput data for the example analyses reported here are included and installed along with the EthSEQ package. The user can manually download and explore these data from https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata[href=https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata].Output results of the example analyses reported here are available at https://github.com/cibiobcg/EthSEQ_Data/tree/master/example_outputs/[href=https://github.com/cibiobcg/EthSEQ_Data/example_outputs/]. The user can explore and compare the expected results available on the GitHub page with the results obtained by running the protocol.\nThe user should start by running step 1 to 3 as described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001] and then:\n4. Prepare the multi-step refinement tree encoding:\n         \n> m = matrix(\"\",ncol=2,nrow=2)\n> m[1,1] = \"EUR|AFR|AMR\"\n> m[2,2] = \"EUR|AMR\"\nA 2 \u00d7 2 matrix is created representing a tree with a root node containing three populations and a child node containing two populations.\n5. Run the ancestry analysis:\n         \n> ethseq.Analysis(target.vcf = system.file(\"extdata\",\n\u00a0\u00a0\u00a0\u00a0\"Samples.HGDP.10000SNPs.vcf\", package=\"EthSEQ\"),\nout.dir = file.path(tempdir(),\"EthSEQ_Analysis/\"),\nmodel.available = \"Gencode.Exome\",\nverbose = TRUE,\nrefinement.analysis = m,\ncomposite.model.call.rate = 1,\nspace = \"3D\")\nThe same analysis can be performed using as target input: a GDS file (similar to Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], b steps) or a list of BAM files (similar to Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], c steps). Briefly, using the same procedure described in the Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], b steps, and Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], c steps, it is enough to add, in the \u201cRun the analysis\u201d step, the refinement.analysis parameter to perform the multi-step refinement procedure, as described above.This protocol provides complete information to generate a reference model given genotype data for a set of individuals already annotated for ancestry. This function takes as input a list of paths to VCF files to build the reference model. Optionally, the user can provide a path to a Browser Extensible Data (BED) file describing a set of genomic regions of interest to subset the VCF files. This is particularly useful when reference models for TS panels are to be created.\nNecessary Resources\nHardware\nA 64-bit computer with \u22658 GB RAM. Note that the RAM required for performing this step is proportional to the size of each input VCF file. In most cases, less than 8 GB are enough. The maximum RAM required for processing a file with approximately 250,000 variants and 1000 individuals will not exceed 4.5 GB when performing the file conversion from VCF to GDS format.\nSoftware\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001]\nInput files\nAs described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001]\nSample data\nInput data for the example analyses reported here are included and installed along with the EthSEQ package. The user can manually download and explore these data from https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata[href=https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata].\nOutput results of the example analyses reported here are available at https://github.com/cibiobcg/EthSEQ_Data/tree/master/example_outputs/[href=https://github.com/cibiobcg/EthSEQ_Data/example_outputs/]. The user can explore and compare the expected results available on the GitHub page with the results obtained by running the protocol.\nThe user should start by running step 1 to 3 as described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001] and then:\n4. Prepare the input data:\n         \n> vcf.files = c(system.file(\"extdata\",\"RefSample1.vcf\", package=\"EthSEQ\"),\nsystem.file(\"extdata\",\"RefSample2.vcf\", package=\"EthSEQ\"))\n> annot.samples = read.delim(system.file(\"extdata\", \"Annotations_Test_v3.txt\",\npackage=\"EthSEQ\"))\n5. Run the analysis:\n         \n> ethseq.RM(vcf.fn = vcf.files,\nannotations = annot.samples,\nout.dir = file.path(tempdir(),\"EthSEQ_Analysis/\"),\nmodel.name = \"Reference.Model\")\nThe function ethseq.RM takes as input the following parameters:\n         \nvcf.fn: Vector of paths to genotype files in VCF format;annotations: data.frame with mapping of all samples names, known ancestries and sex;\nout.dir (default = tempdir()): Path to output folder;\nmodel.name (default = \u201cReference.Model\u201d): Name of the output model;\nbed.fn (default = NA): Path to a BED file with a list of genomic regions of interest;\ncall.rate (default = 1): SNP call rate cutoff for inclusion in the final reference model;\ncores (default = 1): Number of parallel cores to be use in the generation of the reference model.\nOf note, the functions:\n         \n> system.file(\"extdata\",\"RefSample1.vcf\", package=\"EthSEQ\")\n> system.file(\"extdata\",\"RefSample2.vcf\", package=\"EthSEQ\")\n> system.file(\"extdata\", \"Annotations_Test_v3.txt\",package=\"EthSEQ\")\nretrieve the paths to the sample data files included in the EthSEQ package. Importantly, these files can be also manually downloaded from https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata[href=https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata] and provided to the ethseq.Analysis function specifying the corresponding file system paths.\nTable 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-tbl-0002] shows the header and the first 10 rows of the annotation data.frame. Three columns (sample, pop, sex) must be present and are used to perform the analysis. The sample column reports the sample name\u2014all the sample id's in the genotype files must be present in the sample column of the annotation data.frame. The pop column reports the known ancestry for each sample. The sex column reports the sex of each individual. If more columns are present in the data.frame, they will be ignored by EthSEQ and not included in the reference model.\nThe output of the analysis will be written to the out.dir folder. EthSEQ produces the file Reference.Model.gds that can be used as input reference model, as described in the Alternate Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0002]. See Guidelines for Understanding Results for details on all EthSEQ outputs and their interpretation.\nTable 2.\n                Example of the Annotation Table for Generating the Reference Model (annotations Parameter)a[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-tbl2-note-0001_72]\ntable:\n\ufeffSample,pop,sex\nHG00096,EUR,M\nHG00100,EUR,F\nHG00101,EUR,M\nHG00103,EUR,M\nHG00106,EUR,F\nHG00108,EUR,M\nHG00111,EUR,F\nHG00112,EUR,M\nHG00116,EUR,M\nHG00117,EUR,Ma First column represents individuals\u2019 names, the second column contains the associated known ancestry and column three contains the individuals\u2019 sex.This protocol provides complete information to generate the genotype calls of a set of SNPs positions from a BAM file using ASEQ. Here we describe also how to use the output VCF genotype data file as input to perform ancestry analysis as in Basic Protocols 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], a and c steps, Alternate Protocol and Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0003].\nNecessary Resources\nHardware\n64-bit computer with \u22658 GB RAM\nSoftware\nASEQ and wget. Although ASEQ can be run across different operating systems (e.g., Windows, MacOS and Linux), the code provided in this protocol is designed to run under Linux systems.\nInput files\nA BAM file containing aligned reads\nA VCF file containing a list of SNP positions. Of note, only positions with single reference and single alternative base are admitted in the file.\nSample data\nInput data are already included and installed along with the EthSEQ package. The user can manually download these data from https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata[href=https://github.com/cibiobcg/EthSEQ/tree/master/inst/extdata].\nOutput results are available from (https://github.com/cibiobcg/EthSEQ_Data/tree/master/example_outputs/[href=https://github.com/cibiobcg/EthSEQ_Data/example_outputs/]). The user can explore and compare the expected results on the GitHub page with the results obtained from the protocol.\n1. From the command line, download ASEQ:\n         \n$ wget https://github.com/cibiobcg/EthSEQ_Data/raw/master/ASEQ_binaries/linux64/ASEQ\n2. Add execute permission to ASEQ:\n         \n$ chmod u+x ASEQ\n3. Run the analysis:\n         \n$./ASEQ vcf=ModelPositions.vcf bam=HGDP00228.sub_GRCh38.bam mode=GENOTYPE threads=1 htperc=0.2 mbq=20 mrq=20 mdc=20 out=./\nASEQ takes as input the following parameters:\n         \nbam: path to the sequence alignment data in BAM format;\nvcf: path to the SNP positions in VCF format;\nout: Path to the folder where the output of the analysis is saved;\nmode (default = PILEUP): execution mode;\nthreads (default = 1): number of threads to be use for ASEQ computation;\nhtperc (default = 0.2): this value specifies the allelic fraction range [htperc,1-htperc] to call a SNP as heterozygous;\nmbq (default = 1): Minimum base quality used in the pileup;mrq (default = 1): Minimum read quality used in the pileup;\nmdc (default = 1): Minimum read count acceptable for genotype calculation.\nThis example relies on the data that can be downloaded from the command line using the following commands:\n         \n$ wget https://github.com/cibiobcg/EthSEQ_Data/raw/master/BAM/HGDP00228.sub_GRCh38.bam\n$ wget https://github.com/cibiobcg/EthSEQ_Data/raw/master/ModelPositions.vcf\nThe output of the analysis is written to the out folder. In this example, ASEQ produces the file HGDP00228.sub_GRCh38.genotype.vcf, containing the genotype calls for all SNPs position listed in the ModelPositions.vcf file. Having obtained a VCF file for each target individual of interest, the user can either run Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], c steps, moving the files into the expected EthSEQ folder (see Guidelines for Understanding Results for details), or aggregate all VCF files into a single VCF file and run Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0001], a steps, Alternate Protocol and Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.663#cpz1663-prot-0003].", "Step-by-step method details\nStep-by-step method details\nImaging and photolabeling\nTiming: 2 h\nThis section describes how to photolabel three concentric rings in the circular patch of epithelial cells. The same protocol can be used to label the outer layers of this patch, or researchers can customize the photopatterning script to label any region of interest. ROIs can be selected based on bright-field or nuclear stain fluorescence images.\nNote: The photoactivatable dyes used here are cell permeable and retained in the cytoplasm for several hours. The rationale of the photolabeling approach is that we first incubate cells with one photoactivatable dye, after which the cells in the outer population are photoactivated and labeled with a single color. When we subsequently incubate cells with the second photoactivatable dye, the first dye is still present in the cytoplasm of the cells. Therefore, when the middle population is illuminated, both dyes are photoactivated and these cells are labeled with both dyes.\nAspirate culture media from the glass-bottom dish and wash cells with 250\u00a0\u03bcL phenol-red free culture media.\nPipette 200\u00a0\u03bcL 40\u00a0\u03bcM PA-JF646 solution on the glass area of the dish. Ensure that the patch of cells is fully immersed in the solution.\nIncubate the cells for 20\u00a0min at 37\u00b0C.\nAspirate the PA-JF646 solution, wash cells with 250\u00a0\u03bcL phenol-red free culture media and add 250\u00a0\u03bcL phenol-red free culture media to the glass area of the dish.\nPlace the dish on the microscope stage and obtain a brightfield image of the patch of cells.\nCritical: It is essential to properly fix the position of the dish using stage clips, as any movement of the sample between the imaging and photolabeling steps will yield inaccurate results.Using the photopatterning code, obtain a mask for the DMD to selectively illuminate the outer ring of the patch.\nLoad the mask on the DMD and phototag the outer ring with a 450\u00a0nm laser for 150\u00a0s (\u223c100 J/cm2).\nAspirate media and pipette 200\u00a0\u03bcL 40\u00a0\u03bcM PA-JF549 solution on the glass area of the dish. Ensure that the patch of cells is fully immersed in the solution.\nIncubate the cells for 20\u00a0min at 37\u00b0C.\nAspirate the PA-JF549 solution, wash cells with 250\u00a0\u03bcL phenol-red free culture media and add 250\u00a0\u03bcL phenol-red free media to the glass area of the dish.\nPlace the dish on the microscope stage and obtain a brightfield image of the patch of cells.\nUsing the photopatterning code, obtain a mask for the DMD to selectively illuminate the middle ring of the patch.\nLoad the mask on the DMD and phototag the middle ring with a 450\u00a0nm laser for 150\u00a0s (\u223c100\u00a0J/cm2).\nObtain fluorescence images by exciting cells at approximately 549\u00a0nm and 646\u00a0nm (for PA-JF549 and PA-JF646, respectively) to assess the accuracy and efficiency of the photolabeling process (troubleshooting 2[href=https://www.wicell.org#sec7.3]).\nOptional: To assess the effect of the photolabeling process on the viability of cells, prepare an extra glass-bottom dish of cells and illuminate the cells with a 405\u00a0nm laser with the same laser intensity and exposure time that is used in a specific experimental set-up. Continue with the optional steps from the cell isolation section.\nPause point: If one is labeling and isolating cells from multiple glass-bottom dishes, photolabeled dishes can be stored in a tissue culture incubator until all dishes are labeled and ready for sorting.\nCell isolation\nTiming: 2 hThis section describes the process to isolate photolabeled cells and prepare them for scRNAseq. FACS sorting is used to separate the patch of cells in three populations: unlabeled cells (the center of the patch), cells labeled with only PA-JF646 (the outer ring) and cells labeled with both PA-JF549 and PA-JF646 (the middle ring).\nCritical: Keeping cells in HBSS for prolonged periods of time affects cell viability. If one is sorting multiple dishes in a single run, we recommend limiting the time that cells are suspended in HBSS by trypsinizing the next dish only when one is almost done with sorting the previous dish.\nAspirate media and wash cells with 2\u00a0mL DPBS.\nPipette 250\u00a0\u03bcL Trypsin-EDTA without phenol red on the glass area of the dish (where cells were seeded).\nIncubate until the cells are detached (\u223c20\u201330\u00a0min) at 37\u00b0C.\nCritical: From here on, use low retention pipette tips and tubes for handling the cells.\nDissociate the cells by carefully pipetting up and down and transfer cells to a 1.5\u00a0mL tube.\nRinse the glass area of the dish with 200\u00a0\u03bcL trypsin-EDTA without phenol red and transfer to the same tube.\nRinse the glass area of the dish with 200\u00a0\u03bcL HBSS with calcium and magnesium chloride (pH 7.0) and transfer to the same tube.\nGently pipette cell suspension up and down to further dissociate the cells.\nCentrifuge cells for 4\u00a0min at 4000\u00a0\u00d7\u00a0g.\nNote: The resulting pellet might be small and hard to see. Marking the outside of the tube before centrifuging helps to localize the pellet more easily.\nAspirate supernatant and thoroughly resuspend cells in 400\u00a0\u03bcL HBSS to obtain a single cell suspension. Directly place on ice.\nFACS sort the cells according to the machine\u2019s user manual (Figure\u00a02[href=https://www.wicell.org#fig2]; troubleshooting 3[href=https://www.wicell.org#sec7.5]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2864-Fig2.jpgFigure\u00a02. FACS gating strategy\n(A) Positive control (live cells) for DRAQ-7 live/dead staining.\n(B) Negative control (dead cells) for DRAQ-7 live/dead staining.\n(C) DRAQ-7 live/dead staining indicates that illuminating cells for 120\u00a0s with near-UV light does not alter cell viability.\n(D) Gating strategy to identify live, single cells.\n(E) Gating strategy to identify center, middle and outer populations (gated on live, single cells).\nSort live, single cells into the 384-well plates containing CEL-seq2 primers, RNA spike-ins and dinucleotide triphosphates (dNTPs) that are supplied by sequencing companies (i.e., Single Cell Discoveries in our study) or core facilities. Briefly centrifuge each plate before carefully removing the aluminum cover.\nSeal the plate with a new aluminum foil.\nCentrifuge for 1\u00a0min at 1000\u00a0\u00d7\u00a0g to ensure cells are at the bottom of the wells and immediately place 384-well plates on dry ice.\nPlates can be stored for up to 3\u00a0months at \u221280\u00b0C until shipment to the sequencing company or facility.\nCritical: For each well, write down to which population the isolated cell belongs. This information is essential for the analysis of scRNAseq data. For most FACS machines, this sorting report can be downloaded after the sort is complete.\nNote: We outlined the FACS gating strategy in Figure\u00a02[href=https://www.wicell.org#fig2]. From the population of live, single cells, we identify cells that are negative for PA-J646, which originate from the center population. From the population of cells that are positive for PA-JF646, we identify cells that are positive for PA-JF549 (the middle population) or negative for PA-JF549 (the outer population).Note: The number of cells sorted into each population depends on the experimental setup. For our experiments, we sorted each dish of cells into one 384-well plate (i.e., approximately 120\u2013150 cells per population). For statistical analysis, we recommend having approximately equal numbers of cells in the different populations.\nOptional: If assessing the effect of the photolabeling process on cell viability, first follow steps 15\u201322 and then continue with the steps outlined below:\nResuspend cells in 620\u00a0\u03bcL ice-cold HBSS buffer and add 3.1\u00a0\u03bcL 0.3\u00a0mM Draq7 viability dye.\nGently pipette to mix and incubate for 5\u00a0min at 37\u00b0C, while preventing exposure to light.\nFACS sort according to the user manual, live cells should be negative for Draq7 fluorescence (Figure\u00a02[href=https://www.wicell.org#fig2]C).\nSingle cell sequencing\nTiming: 1\u20133\u00a0weeks (depending on the facility used)\nThis section describes how to profile the isolated cells using single cell RNA sequencing.\nFor library preparation and scRNAseq, our laboratory uses the SORT-seq platform25[href=https://www.wicell.org#bib25] offered by Single Cell Discoveries (Utrecht, The Netherlands). This sequencing approach is based on the Cel-seq2 technology.26[href=https://www.wicell.org#bib26] We sequenced our cDNA libraries at 150,000 reads/cell on the Illumina NextSeq 500 platform. In principle, researchers could use any plate-based scRNAseq technology that offers sufficiently high read depth.", "Step-by-step method details\nStep-by-step method details\nVirus packaging\nTiming: 6\u00a0days\nIn this protocol, we apply a lentivirus transduction system to transfect the sgRNA library into A375 cells.\nCritical: Working with lentivirus must be in certified Class II biological safety cabinets. The equipment used in handling lentivirus should be cleaned with a 10% bleach solution and clearly labeled and processed. Researchers must wear personal protective equipment, including lab coats, gloves, surgical masks, and goggles.\nSeed HEK293T cells into 15-cm tissue culture dishes with 20\u00a0mL of media.\nProceed to the transfection step if the cell confluence reaches 70%\u201380% after 16\u00a0h of incubation.\nNote: For transfection of each 15-cm dish, add 22.5\u00a0\u03bcg library plasmid, 16.9\u00a0\u03bcg psPAX2 packaging plasmid, 6.75\u00a0\u03bcg pMD2.G envelope plasmid, and 184\u00a0\u03bcL PEI in 3\u00a0mL serum-free Opti-MEM.\nMix well and incubate for 20\u00a0min at 25\u00b0C. Add the mix dropwise gently to the plated cells and swirl to disperse the mixture evenly.\nIncubate the cells at 37\u00b0C, 5% CO2, for 6 h, and replace the media with the transfection mixture with 20\u00a0mL fresh media per plate.\nIncubate the cells at 37\u00b0C, 5% CO2 for 42 h.\nCollect the media from the culture and transfer to a 50\u00a0mL Falcon tube and store at 4\u00b0C.\nAdd 20\u00a0mL fresh media per plate and incubate at 37\u00b0C, 5% CO2 for 24 h.\nCollect the media from the culture and mix it with the media collected the day before. Filter the media through a 0.45\u00a0\u03bcm filter (PES) to remove the cells.\nConcentrate the virus with Lenti-X Concentrator according to the manufacturer\u2019s protocol (link: https://www.takarabio.com/products/gene-function/viral-transduction/lentivirus/lentivirus-concentration[href=https://www.takarabio.com/products/gene-function/viral-transduction/lentivirus/lentivirus-concentration]).Note: We typically collect the media twice to yield around 37.5\u00a0mL of virus-containing supernatant. Add 12.5\u00a0mL Lenti-X Concentrator, mix well, and keep the tubes at 4\u00b0C for 24 h. Then, centrifuge the tubes at 1,500\u00a0g for 45\u00a0min. Discard the supernatant and resuspend the virus with 3.75\u00a0mL DMEM to obtain the 10\u00a0\u00d7\u00a0concentrated virus.\nSplit the 10\u00a0\u00d7\u00a0concentrated virus into 15-mL centrifuge tubes.\nVirus may be stored at 4\u00b0C for a few days and should be frozen at \u221280\u00b0C for long-term storage.\nCritical: Do not freeze and thaw the virus repeatedly, severely reducing the virus infection efficiency (see \u201dtroubleshooting 2[href=https://www.wicell.org#troubleshooting]\u2033).\nPause point: Virus can be stored at \u221280\u00b0C for up to 6\u00a0months without substantial loss of infection efficiency.\nLentivirus infection and MOI testing\nTiming: 4\u00a0days\nMultiplicity of infection (MOI) must be determined for each new cell line. Ensure appropriate lentivirus infection that every single cell is infected with only one sgRNA. MOI of 0.3\u20130.4 is an optimal condition.\nSeed human melanoma A375 cells into two 6-well plates with 1.5\u00a0\u00d7\u00a0105 cells per well.\nNote: Before the experiment, the researcher should determine the doubling time of the cell line and test the minimal dosage of puromycin to kill all the cells after 48\u00a0h of puromycin treatment.\nTogether with 5\u00a0\u03bcg/mL polybrene, add the titered virus (10\u00a0\u00d7\u00a0concentrated lentivirus from Step 30) into the wells at volumes of 0\u00a0\u03bcL, 0\u00a0\u03bcL, 6.25\u00a0\u03bcL, 12.5\u00a0\u03bcL, 25\u00a0\u03bcL, 37.5\u00a0\u03bcL, 50\u00a0\u03bcL, 62.6\u00a0\u03bcL, 75\u00a0\u03bcL, 87.5\u00a0\u03bcL, 100\u00a0\u03bcL, and 125\u00a0\u03bcL.\nNote: We recommend two uninfected wells. One without puromycin is the negative control for measuring MOI. The other one with puromycin is the positive control.After incubating cells at 37\u00b0C, 5% CO2, for 24 h, change the media for all the wells.\nAdd puromycin into wells.\nMaintain the cell culture until all the cells in the well without infection are dead.\nNote: During the process, if the well without selection is confluent, split the cells into a new well and record the cell passaging ratio for MOI testing.\nCount the number of cells in the wells with or without puromycin and determine the MOI.\nNote: If the cells in the well as positive control are still alive after 48\u00a0h of puromycin selection, replace media with puromycin again and check after 24 h. If not enough cells are dying, repeat this experiment by increasing the concentration of puromycin.\nRecord the amount of virus used in Step 13 as Y.\nCRISPR-screen\nTiming: 3\u20134\u00a0weeks\nTo obtain reliable results from the CRISPR-Cas9 screening, over 400-fold (400 x) coverage of every sgRNA is required. For example, Liu Human CRISPR Knockout Library H2 contains 92871 sgRNAs; to achieve 400\u00a0\u00d7\u00a0coverage, 3.68\u00a0\u00d7\u00a0107 (92871\u00a0\u00d7\u00a0400) cells should be infected in one screen. Considering the Poisson distribution during infection, multiple surviving cells will have more than one sgRNA. Thus, the total number of A375 cells prepared for infection of MOI 0.35 is 1.247\u00a0\u00d7\u00a0108 (3.68\u00a0\u00d7\u00a0107 / (1-e\u22120.35)).\nMix the cells and virus into 15-cm culture plates:\ntable:files/protocols_protocol_3177_2.csv\nNote: The ratio of the surface area between the 15-cm plate and one well of the 6-well plate is 18. The number of 15-cm tissue culture plates used for infection is 46 (1.247\u00a0\u00d7\u00a0108 / (1.5\u00a0\u00d7\u00a0105\u00a0\u00d7\u00a018). We also recommend setting up puromycin-selected, uninfected control and non-selected, uninfected control at this step.\nMix gently with a shaker for 10\u00a0min at 25\u00b0C.Incubate the cells at 37\u00b0C, 5% CO2 for 24 h, change the media, and add puromycin.\nAfter 48\u00a0h of puromycin selection, passage the infected cells into the fresh medium. With the remaining cells, freeze two pellets (one for backup) for DNA extraction as the initial population (T0) at \u221280\u00b0C.\nNote: Make sure the coverage of the library is always at 400\u00a0\u00d7\u00a0coverage of the library. For example, Liu Human CRISPR Knockout Library H2 requires at least 3.68\u00a0\u00d7\u00a0107 cells to achieve 400\u00a0\u00d7\u00a0coverage. For the Human CRISPR Metabolic Gene Knockout Library, we recommend 1,000-fold coverage.\nContinue to passage cells, maintaining the required coverage of the library at each passage.\nNote: We typically passage A375 cells every two days to ensure the cells double at least twice between passages.\nOn day 14 (about 7 passages for A375 cells), collect the cell pellets as the final population (T14) and freeze the pellets at \u221280\u00b0C. At the same time, seed the cells for ferroptosis induction (Figure\u00a02[href=https://www.wicell.org#fig2]H).\nCritical: Harvesting enough cells to satisfy the minimum coverage (For Human CRISPR Knockout Library H1 or H2, 400\u00a0\u00d7\u00a0coverage. For Human CRISPR Metabolic Gene Knockout Library, 1,000\u00a0\u00d7\u00a0coverage) is critical to avoid low coverage of the mapped reads (see \u201dtroubleshooting 3[href=https://www.wicell.org#troubleshooting]\u2033). Thus, considering the rate of cell death, seed enough cells before the initiation of the treatment.\nTreat A375 cells with different ferroptosis-inducing conditions.\nHigh levels of ROS treatment: DMSO or 300\u00a0\u03bcM TBH for 12 h.\nGPX4 inhibitor RSL-3 treatment: DMSO or 0.75\u00a0\u03bcM RSL-3 for 5\u00a0days.\nSLC7A11 inhibitor IKE treatment: DMSO or 5\u00a0\u03bcM IKE for 7\u00a0days.\nCystine starvation (-Cys) treatment: culture cells with media containing 1\u00a0\u03bcM cystine for 5\u00a0days.Note: The treatment begins when the cells are seeded (purple line, Figure\u00a02[href=https://www.wicell.org#fig2]H). Glutamine, methionine, and cystine-deficient DMEM is supplemented with 4\u00a0mM glutamine, 200\u00a0\u03bcM methionine, 1\u00a0\u03bcM cystine, and 10% FBS is the media containing 1\u00a0\u03bcM cystine.\nNote: For long-term treatment (IKE, RSL-3, and cystine starvation treatment), check the conditions of the cells every day. Change the media to prevent the perturbing of dead cells according to the timeline in Figure\u00a02[href=https://www.wicell.org#fig2]H. We recommend at least two biological repeats for every treatment to obtain reliable results. We also recommend using T14 as the only control for all ferroptosis induction groups to save labor and time (see \u201dtroubleshooting 1[href=https://www.wicell.org#troubleshooting]\u2033).\nWhen the cells reach set time points, wash the cells with PBS and collect cells.\nNote: To eliminate the dead cells, we suggest centrifuging the cell pellets at low speed (500\u00a0g for 3\u00a0min).\nCount cells and collect the required coverage of cells. Freeze cell pellets in 50-mL centrifuge tubes at \u221280\u00b0C.\nPause point: The samples can be stored at \u221280\u00b0C for 6\u00a0months.\nPreparation for high-throughput sequencing\nTiming: 5\u20137\u00a0days\nTo analyze the sgRNA abundance present in each sample, genomic DNA (gDNA) is extracted from frozen cell pellets, and the sgRNA sequences are amplified by double-step PCR.\nThaw the collected samples and extract gDNA using the Wizard Genomic DNA purification Kit according to the manufacturer\u2019s instructions (link: https://www.promega.com/products/nucleic-acid-extraction/genomic-dna/wizard-genomic-dna-purification-kit[href=https://www.promega.com/products/nucleic-acid-extraction/genomic-dna/wizard-genomic-dna-purification-kit]).\nNote: During the procedures of genome DNA extraction, prolonging the centrifugation time is helpful. In theory, 1 million cells contain 6.6\u00a0\u03bcg genomic DNA. We typically obtain 220\u00a0\u03bcg genomic DNA from 3.68\u00a0\u00d7\u00a0107 cells.\nQuantitate the concentration of the extracted gDNA by NanoDrop.\nPerform the first step of the double-step sgRNA amplification PCR.Perform 96 separate 50\u00a0\u03bcL reactions with 1\u20134\u00a0\u03bcg genomic DNA in each reaction using Phusion Plus DNA Polymerase with the following reagents and reaction program.\ntable:files/protocols_protocol_3177_3.csv\ntable:files/protocols_protocol_3177_4.csv\nNote: To find the best PCR condition, perform PCR reactions using titered gDNA (0.5\u00a0\u03bcg, 1\u00a0\u03bcg, 2\u00a0\u03bcg, 3\u00a0\u03bcg, 4\u00a0\u03bcg, and 5\u00a0\u03bcg) with 30 cycles of amplification step. A bright band (212\u00a0bp for Liu Human CRISPR Knockout Library and 277\u00a0bp for Human CRISPR Metabolic Gene Knockout Library) between 200\u00a0bp and 300\u00a0bp should be obtained. Choose the condition with the brightest band for further analysis. Thus, the number of reactions is not strictly 96. The results of titered PCR testing determine the number of reactions. At least 200\u00a0\u03bcg genomic DNA extracted from each sample is required for adequate coverage at this step.\nCritical: P5 is a universal forward primer for both libraries we introduced. P7-lentiCrispr V1 is used for Human CRISPR Metabolic Gene Knockout Library and P7-lentiCrispr V2 is used for Liu Human CRISPR Knockout Library.\nCombine the resulting amplicons (around 4.8\u00a0mL) in a 50-mL centrifuge tube.\nAdd 480\u00a0\u03bcL 3M sodium acetate and 12\u00a0mL 100% ethanol. Then, put the tube into \u221280\u00b0C for 16 h.\nCritical: Avoid contact with ethanol with eyes and skin. Keep ethanol away from heat, sparks, and flame because ethanol is highly flammable.\nCentrifuge the ethanol precipitation mixture at 20,000\u00a0g for 30\u00a0min and resuspend the pellets with 400\u00a0\u03bcL Nuclease-Free Water.\nSeparate the concentrated PCR product using 1% agarose gel.\nCritical: Avoid contact with Ethidium Bromide (EB), because it can cause irritation to the skin, eyes, mouth, and upper respiratory tract. Personal protective equipment must be used when handling EB-stained gel.Extract the DNA using the QIAquick Gel Extraction Kit and resuspend the PCR product with 80\u00a0\u03bcL Nuclease-Free Water (link: https://www.qiagen.com/us/products/discovery-and-translational-research/dna-rna-purification/dna-purification/dna-clean-up/qiaquick-gel-extraction-kit[href=https://www.qiagen.com/us/products/discovery-and-translational-research/dna-rna-purification/dna-purification/dna-clean-up/qiaquick-gel-extraction-kit]).\nNote: This step aims to eliminate the interference of genomic DNA in the second PCR step. The PCR product is invisible in the gel. Load a positive control to ensure the gel is cut at the right place.\nSecond step of two-step sgRNA amplification PCR.\nPerform four separate 50\u00a0\u03bcL reactions with 10\u00a0\u03bcL first step PCR product in each reaction using barcoded primers with the following reagents and reaction program.\ntable:files/protocols_protocol_3177_5.csv\ntable:files/protocols_protocol_3177_6.csv\nCombine the PCR product in a 1.7\u00a0mL tube and purify with the QIAquick PCR purification Kit (link: https://www.qiagen.com/us/products/discovery-and-translational-research/dna-rna-purification/dna-purification/dna-clean-up/qiaquick-pcr-purification-kit[href=https://www.qiagen.com/us/products/discovery-and-translational-research/dna-rna-purification/dna-purification/dna-clean-up/qiaquick-pcr-purification-kit]).\nResuspend purified DNA with 20\u00a0\u03bcL Nuclease-Free Water.\nCheck the second step PCR product by loading 1\u00a0\u03bcL purified DNA by electrophoresis.\nNote: The second step PCR product should be a single band around 281\u00a0bp for Liu Human CRISPR Knockout Library and 346\u00a0bp for Human CRISPR Metabolic Gene Knockout Library.\nDetermine the DNA concentration by Qubit dsDNA HS Assay.\nCheck DNA quality using a Bioanalyzer system.\nPool all example DNA products together with equal amounts for high-throughput sequencing.\nNote: We typically send 10\u00a0\u03bcL of pool library at 2\u00a0nM for sequencing using Illumina NextSeq 550 system.\nSequencing was conducted by Illumina NextSeq 550 system using NextSeq 75 cycle High Output Kit V2.5 in Columbia University. Reads that passed QC were analyzed with FASTQ Generation (Version: 1.0.0).\nData analysis\nTiming: 5\u00a0daysMAGeCK (version 0.5.7), a commonly used pipeline, employs a sophisticated statistical model considering the inherent noise and variability in CRISPR screening data. This model-based approach helps to reduce false positives and increase the accuracy of the results. MAGeCK can also effectively distinguish true signals from background noise, making it suitable for analyzing large-scale screening datasets. For experiments involving two experimental conditions, we highly recommend using MAGeCK-RRA for the identification of essential targets from CRISPR knockout screens. MAGeCK-RRA allows researchers to assess the statistical significance of each observed change between the two states, providing valuable insights into the genes that play a crucial role in the biological processes under investigation. By comparing the gene-level effect sizes and conducting statistical tests, MAGeCK-RRA helps identify differentially essential genes, which are essential for understanding the underlying mechanisms and potential therapeutic targets in the context of the experimental conditions. Therefore, we utilize the MAGeCK pipeline to identify the essential targets.\nDownload and back up raw sequencing FASTQ files from BaseSpace of Illumina.\n> basemount BaseSpace\n>cd BaseSpace/Projectid/sampleid/Files; i=Sample_T14\n> cat \u2217_R1_00\u2217.fastq.gz\u00a0>\u00a0/data2/output/${i}_1.fastq.gz\nNote: The raw sequencing FASTQ files are automatically uploaded to our BaseSpace account, which provides 1 TB of storage. We use the BaseMount tool to efficiently move the reads from BaseSpace to our local server to access the files locally. This allows us to work with the data on our local infrastructure while releasing the storage of BaseSpace.\nDownload the Human CRISPR Metabolic Gene Knockout Library file targets 2,981 human genes with 29,790 guide RNAs (https://www.addgene.org/pooled-library/sabatini-human-crispr-metabolic-knockout/[href=https://www.addgene.org/pooled-library/sabatini-human-crispr-metabolic-knockout/]) as lentiCRISPR_v1_library.txt.Note: This library includes 499 negative control sgRNAs, which proves valuable for counting normalization across different time points. These negative control sgRNAs serve as a helpful baseline reference, enabling accurate comparisons and analyses of the experimental data. The library file (lentiCRISPR_v1_library.txt) consists of three essential columns: the sgRNA ID, the sgRNA sequence, and the gene it is designed to target.\nInstall MAGeCK version 0.5.7 (http://mageck.sourceforge.net[href=http://mageck.sourceforge.net]) through Anaconda platform with bioconda channel.\n> conda create -c bioconda -n mageck-vispr mageck\n> conda activate mageck-vispr\nNote: After installing Anaconda (https://docs.anaconda.com/free/anaconda/install[href=https://docs.anaconda.com/free/anaconda/install]), we recommend creating an isolated software environment for MAGeCK by executing.\nRun the MAGeCK count command to generate a count table and distribution of read counts for each sample (Figure\u00a03[href=https://www.wicell.org#fig3]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3177-Fig3.jpg\nFigure\u00a03. Quality control of the CRISPR library and potential targets identification\n(A) The frequency plots illustrate the distribution of log2-transformed read counts for sgRNAs in each sample.\n(B) A snapshot depicts the sequence structure of reads.\n(C) A violin plot displays the normalized counts of negative control sgRNAs in two conditions. The p-value is estimated using a two-sided Student\u2019s t-test to assess the statistical significance between the two conditions.\n(D) Pearson correlations of RRA score between two replicates. Each dot represents a gene from the CRISPR library.\n(E) A dot plot depicts the rank of genes based on the RRA score. VKORC1L1, positioned at rank 61, is prominently highlighted in red color.\n(F) A line plot presents the normalized counts change of sgRNA between two conditions. Each color corresponds to an individual sgRNA on VKORC1L1.\n> i=Sample_T14\n> mageck count -l lentiCRISPR_v1_library.txt -n ${i} --sample-label ${i} --fastq ${i}_1.fastq.gz --pdf-report\nNote: Since version 0.5.6, MAGeCK is now able to determine the trimming length and sgRNA length automatically. However, researchers can manually trim the reads under option --trim-5 22 (Figure\u00a03[href=https://www.wicell.org#fig3]B).Quality control is conducted using the generated count summary file, which provides a comprehensive overview of basic QC scores at the raw count level. These QC scores include map ratio, Gini index, and NegSelQC.\n> library(MAGeCKFlute)\n>countsummary<-read.table(\"sample_countsummary.txt\",header=T,sep=\"\u2216t\")\n> MapRatesView(countsummary)\n>IdentBarView(countsummary, x\u00a0= \"Label\", y\u00a0= \"GiniIndex\", ylab\u00a0= \"Gini index\", main\u00a0= \"Evenness of sgRNA reads\")\n>countsummary$Missed\u00a0= log10(countsummary$Zerocounts)\n>IdentBarView(countsummary, x\u00a0= \"Label\", y\u00a0= \"Missed\", fill\u00a0= \"#394E80\", ylab\u00a0= \"Log10 missed gRNAs\", main\u00a0= \"Missed sgRNAs\")\nNote: By analyzing these metrics, we can assess the overall quality and reliability of the data, ensuring that the subsequent analyses are based on accurate and robust information.\nCompare samples using the MAGeCK test subcommand to compare IKE treatment vs. T14 condition (Steps 20\u201325) based on the generated read count table.\n> t1=Sample_T14; t2=Sample_IKE_1; i=${t2}_${t1}\n> mageck test -k ${i}_count.txt -t ${t2} -c ${t1} --control-sgrna lentiCRISPR_v1_negative.list --normcounts-to-file --norm-method control -n ${i}_negcontrol\nNote: The list of negative control sgRNAs is extracted from the raw library, and we can examine the normalized counts of these negative control sgRNAs using a violin plot (Figure\u00a03[href=https://www.wicell.org#fig3]C). We typically compare T14 vs. T0 to obtain the effects of gene ablation, especially the common essential genes for A375 cells. Next, we further analyze the different treatments (IKE, RSL-3, TBH, or cystine starvation) vs. T14 to obtain the effects of gene ablation on ferroptosis induced under various conditions.\nTo compare MAGeCK RRA scores between replicates, we utilized the R script points_cor_sele.r (Figure\u00a03[href=https://www.wicell.org#fig3]D). The output generated is a point plot, which displays the coefficient index (R\u00a0= 0.84) and p-value (p\u00a0<\u00a02.2e-16), allowing for a visual comparison of the scores between the replicates.\n> awk -F \"\u2216t\" -v OFS=\"\u2216t\" 'NR==FNR{a[$1]=-log($3)/log(10)}NR>FNR{if($1 in a){print $1,-log($3)/log(10),a[$1]}}' IKE2.gene_summary.txt IKE1.gene_summary.txt | tail -n\u00a0+2\u00a0>\u00a0temp> Rscript \u223c/bin/perl/r-script/points_cor_sele.r temp 2,3 IKE1_RRA,IKE2_RRA 0,16 IKE_2rep_negcontrol\nNote: This analysis provides valuable insights into the consistency and reproducibility of the results across different experimental replicates, ensuring the reliability of the MAGeCK RRA scores.\nVisualize the intriguing target location on the dot plot (Figure\u00a03[href=https://www.wicell.org#fig3]E) and the count change of each sgRNA for the intriguing target between two conditions on the line plot (Figure\u00a03[href=https://www.wicell.org#fig3]F).\n> t1=Sample_T14; t2=Sample_IKE_1; i=${t2}_${t1}\n> Rscript \u223c/bin/perl/r-script/rra_dotplot_countchange.r ${i}_negcontrol VKORC1L1 T14,IKE ${i}_output\nNote: The files generated by the MAGeCK test subcommand have the prefix ${i}_negcontrol. In the output figures, we aim to highlight VKORC1L1. The conditions T14 and IKE represent the control and case conditions, respectively.\nThe maximum likelihood estimation (MLE) module of MAGeCK can improve the challenge of estimating gene effects across sgRNA screens. FDR and beta score of each gene were also estimated to detect the candidate hits using the MAGeCK-MLE algorithm under the method normalizing to the negative control sgRNAs.\n> t1= Sample_T14; t2= Sample_IKE_1; i=${t2}_${t1}\n> mageck mle --count-table ${i}_enrich_count.txt --design-matrix design.txt --norm-method control --control-sgrna lentiCRISPR_v1_negative.list -n ${i}_enrich --threads 40\nNote: Once we have identified potential hits from the RRA scores, we can conduct further confirmation using the beta score and FDR estimated by the MAGeCK-MLE algorithm.\nTarget selection and verification\nTiming: 3\u00a0weeks\nAfter target selection, validate the candidates by generating gene-specific knockouts using individual sgRNAs.\nDesign at least two highly efficient sgRNAs targeting the selected gene by CRISPOR tool.\nSynthesize sgRNAs by Synthego company.\nNote: This process usually takes one week.\nSeed A375 cells into 6-well plates with 1\u00a0\u00d7\u00a0105 cells per well.\nPerform the transfection after 16\u00a0h of incubation with the following reagents.Tube 1: mix 125\u00a0\u03bcL serum-free Opti-MEM with 1.25\u00a0\u03bcL TrueCut Cas9 Protein v2 (5\u00a0mg/mL), 50\u00a0nmol sgRNAs, and 12.5\u00a0\u03bcL Cas9 Plus Reagent.\nTube 2: mix 125\u00a0\u03bcL serum-free Opti-MEM with 7.5\u00a0\u03bcL Lipofectamine CRISPRMAX Reagent.\nMix the content from tube 1 and tube 2. After 15\u00a0min of incubation, add the mixture to the medium of the 6-well plate.\n72\u00a0h after transfection, collect sgRNAs-expressing pool A375 cells for Western blot analysis and passage cells for further experiments.\nNote: If the knockout efficiency is over 50% determined by Western blot, proceed to the next step.\nSeed sgRNAs expressing pool A375 cells to a 12-well plate with 4\u00a0\u00d7\u00a0104 cells per well.\nAfter 16\u00a0h of incubation, add the indicated drugs into the 12-well plate with 30\u00a0nM SYTOX green dye to stain dead cells.\nNote: Observe the differences in the cell morphology and green fluorescence signaling between the negative control and sgRNA-expressing cells upon ferroptotic stresses. If the results are consistent with the CRISPR-Cas9 screening results, proceed to the next step.\nDilute the cells to allow the formation of clones from single cells to obtain knockout cells of the gene for further analysis.", "Step-by-step method details\nStep-by-step method details\nOocyte collection\nTiming: [2\u20133 h]\nCattle (Bos taurus) oocytes can be collected from multiple sources depending on the hypothesis or the biological questions addressed in a specific study. Potential sources of oocytes are in\u00a0vivo aspiration of follicles using ultrasonography-guided ovum pick up (Bo et\u00a0al., 2019[href=https://www.wicell.org#bib5]) or ex\u00a0vivo aspiration of follicles from ovaries obtained from an abattoir (Tribulo et\u00a0al., 2019[href=https://www.wicell.org#bib34]). All oocytes used in this protocol were obtained from ex\u00a0vivo aspiration of follicles from ovaries obtained from an abattoir. Ovaries were obtained postmortem, and no animal was handled or euthanized for this study. Thus, this work was carried out in compliance with the Institutional Animal Care and Use Committee of Virginia Tech. We note that the user should follow the appropriate regulations when obtaining biological samples from vertebrates. Cumulus oocytes complexes (COCs) can be obtained from multiple sources.\nNote: This protocol starts with the COCs (Figure\u00a01[href=https://www.wicell.org#fig1]B) after their isolation and selection for further work. In a cumulus-oocyte complex, the oocyte will be identified as a large cell, often >100\u00a0\u03bcm in diameter, enclosed in a thick layer of glycoproteins, the zona pellucida (Gupta, 2018[href=https://www.wicell.org#bib15]; Hyttel et\u00a0al., 1986[href=https://www.wicell.org#bib17]; Wassarman and Litscher, 2018[href=https://www.wicell.org#bib36]). The cumulus cells are the small cells surrounding the zona pellucida (Figure\u00a01[href=https://www.wicell.org#fig1]B).\nNote: The procedures described for the handling of COCs and oocytes are carried out with the aid of a stereoscope.\nStripping of cumulus cells from oocytes.\nAdd three drops (50\u00a0\u03bcL) of Denuding solution on a 35mm dish and cover with oil.\nAdd three drops (50\u00a0\u03bcL) of Oocyte wash solution on a 35mm dish and cover with oil.\nTransfer the oocytes into one of the drops with Denuding solution and pipette (20 or 100\u00a0\u03bcL pipettor) to remove the cumulus cells.Using a stripper pipette (175\u00a0\u03bcm tip) aspirate the oocytes to the next drop of Denuding solution. Repeat the removal of the cumulus cells with the (20 or 100\u00a0\u03bcL pipettor) or the stripper pipette.\nRepeat the washing once again using the third drop of denuding solution. Aspirate as minimal volume as possible.\nUsing a stripper pipette (175\u00a0\u03bcm tip) aspirate the oocytes to the drop containing the Oocyte washing solution. Aspirate as minimal volume as possible.\nTransfer the oocytes to the next drop containing the Oocyte washing solution and repeat the procedure until no cumulus cells are visible under the stereoscope.\nUsing a stripper pipette (175\u00a0\u03bcm tip) aspirate one oocyte with minimal volume of washing solution and transfer into a 0.2\u00a0mL microcentrifuge tube.\nImmerse the tube immediately into liquid nitrogen (snap freezing). Preserve the material at -80\u00b0C until used for RNA extraction.\nAlternatively, if liquid nitrogen is not available, transfer the oocytes to tubes containing 5\u00a0\u03bcL of an RNA stabilization solution (Camacho-Sanchez et\u00a0al., 2013[href=https://www.wicell.org#bib7]). Preserve the material at -80\u00b0C until used for RNA extraction.\nPause point: Cells can be maintained at -80\u00b0C for long-term storage\nTotal RNA extraction from single oocytes\nTiming: [2\u20133 h]\nThis step involves the extraction of total RNA from single oocytes using Trizol reagent. The procedures described here have been adapted from the manufacturer\u2019s protocol to minimize the loss of total RNA obtained from single oocytes and embryos. A schematic of the procedure with pictures of critical steps is depicted in Figure\u00a02[href=https://www.wicell.org#fig2].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1108-Fig2.jpg\nFigure\u00a02. Schematic of the total RNA extraction from single oocytes\nNote: Pre-spin the Phasemaker Tubes for 30 sec at 12,000 \u00d7 g.\nNote: In our laboratory we work with up to 12 tubes containing single oocytes in a single batch.Critical: It is particularly important that the tubes containing the oocyte lysates do not thaw before the addition of the Trizol reagent to the tube.\nTransfer the 0.2\u00a0mL microcentrifuge tubes containing the single oocytes to a precooled PCR rack.\nBefore the solution containing the cell lysate thaws, add 150\u00a0\u03bcL of a monophasic solution of Trizol (Chomczynski and Sacchi, 1987[href=https://www.wicell.org#bib9], 2006[href=https://www.wicell.org#bib10]; Rio et\u00a0al., 2010[href=https://www.wicell.org#bib31]) to the 0.2\u00a0mL microcentrifuge tube. Add Trizol reagent to all tubes you are working with, then proceed to the next step.\nRemove the tube from the cold rack onto a rack at \u223c24\u00b0C.\nAssuming that a minimum volume (\u223c 1\u00a0\u03bcL) was added to the tube with the oocyte, add 10\u00a0\u03bcL of water to the solution, and mix the solution gently by pipetting up and down (\u223c5\u201310 times). Add water to all tubes you are working with, then proceed to the next step.\nTransfer the homogenate solution to into a 2\u00a0mL centrifuge tube containing a gel polymer capable of separating the aqueous phase from the organic phase (Murphy and Hellwig, 1996[href=https://www.wicell.org#bib26]).\nLet the solution stand at \u223c24\u00b0C for 5\u00a0min.\nAdd 30\u00a0\u03bcL of chloroform to the solution and mix the solution vigorously by shaking the tube for 15 s.\nLet the solution stand for 3\u00a0min at \u223c24\u00b0C.\nCentrifuge at 12,000 \u00d7 g for 5\u00a0min at 4\u00b0C.\nAdd 20\u00a0\u03bcL of chloroform to the solution and mix the solution vigorously by shaking the tube for 15 s,\nCentrifuge at 12,000 \u00d7 g for 5\u00a0min at 4\u00b0C.\nIn the meantime add 1\u03bcL (15\u00a0\u03bcg) of glycoblue to a new 0.2\u00a0mL microcentrifuge tube.\nRemove all aqueous solution from the phase maker tube and place it in the 0.2\u00a0mL microcentrifuge tube containing the glycoblue.Mix the aqueous solution with the glycoblue by pipetting very gently (5 times).\nNote: Do not to touch the gel with the pipette tip. If the gel is touched, dispense the aqueous solution onto the gel gently and use a new pipette tip.\nNote: This step can also be executed by transferring the aqueous solution to all tubes, followed by mixing of the solution with glycoblue, which can be done with a multi-channel pipette.\nAdd 100\u00a0\u03bcL of isopropanol to the solution and mix by gentle pipetting (5 times).\nNote: This step can also be executed by adding isopropanol to all tubes, followed by mixing of the solution with a multi-channel pipette.\nLet the tube stand at \u223c24\u00b0C for 10\u00a0min.\nNote: Extended precipitation may be used to increase the yield of total RNA. In this case store samples for 12\u201318h at -20\u00b0C and resume the protocol to pellet the RNA.\nCentrifuge at 15,000 \u00d7 g for 10\u00a0min at 4\u00b0C.\nNote: If the centrifuge rotor accommodates only 2\u00a0mL tubes. Add the 0.2\u00a0mL tubes into a 0.5\u00a0mL tube, which can be placed in an adaptor or add the 0.5\u00a0mL tube into a 2mL tube. Cut the lids of the 0.5 and 2\u00a0mL tubes to facilitate the handling.\nRemove the supernatant gently and discard. Do not disrupt the pellet.\nNote: It is easier to remove most of the liquid with a 200\u00a0\u03bcL pipette and remove the remainder with a 10\u00a0\u03bcL or 20\u00a0\u03bcL pipette, which allows a more precise removal of the liquid without touching the pellet.Note: If RNALater or equivalent high salt RNA stabilization solution was used there will a formation of a bubble-like blue precipitate (Camacho-Sanchez et\u00a0al., 2013[href=https://www.wicell.org#bib7]). Remove the isopropanol without disrupting this bubble. Add 150\u00a0\u03bcL of isopropanol 50% to the microcentrifuge tube, and repeat steps 21 and 22.\nAdd 150\u00a0\u03bcL of Ethanol 75% to the 0.2\u00a0mL tube.\nCentrifuge at 15,000 \u00d7 g for 2\u00a0min at 4\u00b0C.\nRemove the supernatant gently and discard. Do not disrupt the pellet.\nAdd 150\u00a0\u03bcL of Ethanol 75% to the 0.2\u00a0mL tube.\nPause point: The pelleted RNA can be stored long term in Ethanol 75% at -80\u00b0C without cause RNA degradation.\nCritical: Only continue the protocol if the reverse transcription and PCR amplification will be executed without interruption or storage of nucleic acids synthesized.\nNote: If the material was stored resume the protocol on step 27.\nCentrifuge at 15,000 \u00d7 g for 2\u00a0min at 4\u00b0C.\nRemove the supernatant gently and discard. Do not disrupt the pellet.\nNote: It is easier to remove most of the liquid with a 200\u00a0\u03bcL pipette and remove the remainder with a 10\u00a0\u03bcL or 20\u00a0\u03bcL pipette, which allows a more precise removal of the liquid. Remove ethanol 75% as much as possible without touching the pellet.\nCritical: Have the RNA elution mix ready before starting air drying the pellet.\nAir-dry the pellet, which takes about one minute if all the liquid is removed. Proceed immediately to the desired assay using the RNA.\nCritical: Proceed immediately to the reverse transcription using the RNA.\nCritical: Avoid extensive air drying because it will make the elution of the pellet very difficult. The pipetting required for the dissolution of a dry pellet will increase the breaking of the RNA strands.Note: Since the RNA used for sequencing is limited in quantity, assessment of quality prior to reverse transcription is not possible. Alternatively, assess the RNA quality of some samples that are collected and processed exclusively for quality control. Add 1\u00a0\u03bcL of water directly to the RNA pellet and wait for the pellet to dissolve in approximately 5\u201310\u00a0min. on ice. Proceed with the automated capillary electrophoresis.\nReverse transcription\nTiming: [2 h]\nThis protocol for synthesis of complementary DNA was adapted from the molecular crowding single-cell RNA barcoding sequencing (mcSCRB-seq) (Bagnoli et\u00a0al., 2018[href=https://www.wicell.org#bib1]) and the Smart-Seq2 (Picelli et\u00a0al., 2013[href=https://www.wicell.org#bib28], 2014[href=https://www.wicell.org#bib29]). The procedure described here rely on manual liquid handling and transfer, however the protocol can be adapted for robotic liquid handlers (Jaeger et\u00a0al., 2020[href=https://www.wicell.org#bib19]).\nCritical: Spin down all microtubes after thawing reagents (in the case of enzymes, spin down the tubes in a refrigerated centrifuge immediately before pipetting the necessary volume), before and after incubations.\nCritical: Use sterile and nuclease-free pipette tips containing a filter barrier. Always change pipette tips to avoid cross contamination.\nNote: Have all the programs set up on the thermocycler prior to starting the protocol.\nAdd 5\u00a0\u03bcL the RNA elution mix directly onto the pellet and allow the total RNA pellet to dissolve in this solution. Maintain the tubes on ice.\nNote: This step usually takes approximately 5\u00a0min. Do not speed up the elution by pipetting. Only proceed once the pellet disappears.\nUsing a thermocycler, incubate the solution at 72\u00b0C for 3\u00a0min.\nImmediately after the conclusion of the previous step, immerse the tubes on ice. Keep the tubes on ice for 3\u20135\u00a0min.\nWhile the tubes are on ice, prepare the Reverse transcription mix, keep tubes on ice.Add 5\u00a0\u03bcL of Reverse transcription mix to the solution containing RNA, mix gently by pipetting (\u223c5 times).\nUsing a thermocycler, incubate the solution at 42\u00b0C for 90\u00a0min.\nNote: Due to the presence of PEG 8000, the Reverse transcription mix is viscous. Start preparation of the reverse transcription mix by adding the buffer to help the dispensing of the PEG 8000. Mix this solution very gently. On step 34 mix the solution very gently.\nCleanup of the DNA:RNA hybrids\nTiming: [1 h]\nNote: Remove the magnetic beads from the refrigerator 30\u00a0minutes prior to using it. At the time of use, vortex the magnetic beads to mix the solution well. In our laboratory we prepare aliquots of the beads solution to prevent contamination of the bottle.\nNote: All procedures for DNA:RNA clean-up are carried out at \u223c24\u00b0C. Magnetic beads are magnetic nano spheres coated with a biopolymer exhibiting high affinity to nucleic acids (see review by (Berensmeier, 2006[href=https://www.wicell.org#bib2]))\nAdd 10\u00a0\u03bcL of magnetic beads solution to each tube. After the addition of magnetic beads to all tubes, mix the solution with gentle pipetting (\u223c 10 times). Let the tubes stand for 5\u00a0min.\nTransfer the tubes with magnetic beads to a magnetic rack. Let the tubes stand for 8\u00a0min.\nRemove the solution gently without disrupting the beads and discard the solution. Use a 100\u00a0\u03bcL pipette set at > 20\u00a0\u03bcL to remove the \u223c 20\u00a0\u03bcL of volume.\nWith the tubes still on the magnetic rack, gently add 200\u00a0\u03bcL of ethanol 80% to avoid disrupting the magnetic beads.\nWait for 30 s\nRemove the solution carefully to avoid disrupting the magnetic beads\nGently add 200\u00a0\u03bcL of ethanol 80% to avoid disrupting the magnetic beads.\nWait for 30 sRemove the solution carefully to avoid disrupting the magnetic beads\nRemove the tubes from the rack and let the tubes standing on a regular rack or the beads to dry.\nNote: Usually, it takes \u223c5\u00a0min for \u201ccracks\u201d to form on the beads, which indicates their dryness.\nAdd 9.5\u00a0\u03bcL of water to each tube, making sure that the water is deposited on the magnetic beads that are on the wall of the microcentrifuge tube.\nWait for \u223c2\u00a0min for the magnetic beads to begin absorbing water.\nMix the solution gently with a 10\u00a0\u03bcL pipette (\u223c 5 times).\nLet the tubes stand for 2\u00a0min.\nTransfer the tubes to the magnetic rack.\nLet the tubes stand for 5\u00a0min.\nVery gently, aspirate 9\u00a0\u03bcL of the solution containing the DNA:RNA hybrids into a new 0.2\u00a0mL tube.\nNote: Avoid aspirating beads. If beads are seen in the pipette tip, dispense the liquid gently, wait for 2\u00a0min and repeat the process.\nFull-length transcript polymerase chain reaction\nTiming: [2 h]\nPrepare a PCR reaction mix. Add 11\u00a0\u03bcL to each tube and mix by gentle pipetting (\u223c5 times).\nCarry out the amplification using the following cycles.\ntable:files/protocols_protocol_1108_5.csv\nNote: Ideally, the number of cycles should be determined by assaying the RNA from few oocytes at different cycle numbers to avoid over-amplification, which can be observed by the presence of peaks when assaying the amplified complementary DNA on the automated capillary electrophoresis.\nRemove the tubes from the thermocycler and proceed with the clean-up.\nAmplified DNA cleanup\nTiming: [1 h]\nNote: Remove the magnetic beads from the refrigerator 30\u00a0minutes prior to using it. At the time of use, vortex the magnetic beads to mix the solution well.\nNote: All procedures for DNA clean-up are carried out at \u223c24\u00b0CAdd 20\u00a0\u03bcL of magnetic beads solution to each tube. After the addition of magnetic beads to all tubes, mix the solution with gentle pipetting (\u223c 10 times). Let the tubes stand for 5\u00a0min.\nTransfer the tubes with magnetic beads to a magnetic rack. Let the tubes stand for 8\u00a0min.\nRemove the solution gently without disrupting the beads and discard the solution. Use a 100\u00a0\u03bcL pipette set at > 40\u00a0\u03bcL to remove the \u223c40\u00a0\u03bcL of volume.\nWith the tubes still on the magnetic rack, gently add 200\u00a0\u03bcL of ethanol 80% to avoid disrupting the magnetic beads.\nWait for 30 s\nRemove the solution carefully to avoid disrupting the magnetic beads\nGently add 200\u00a0\u03bcL of ethanol 80% to avoid disrupting the magnetic beads.\nWait for 30 s\nRemove the solution carefully to avoid disrupting the magnetic beads\nRemove the tubes from the rack and let the tubes stand on a regular rack for the beads to dry.\nNote: Usually, it takes \u223c5\u00a0min for \u201ccracks\u201d to form on the beads, which indicates their dryness.\nAdd 10.5\u00a0\u03bcL of Tris EDTA buffer to each tube, making sure that the buffer is deposited on the magnetic beads that are on the wall of the microcentrifuge tube.\nWait for \u223c2\u00a0min for the magnetic beads to begin absorbing water.\nMix the solution gently with a 10\u00a0\u03bcL pipette (\u223c 5 times).\nLet the tubes stand for 2\u00a0min.\nTransfer the tubes to the magnetic rack.\nLet the tubes stand for 5\u00a0min.\nVery gently, aspirate 10\u00a0\u03bcL of the solution containing the DNA:RNA hybrids into a new 0.2\u00a0mL tube.\nNote: Avoid aspirating beads. If beads are seen in the pipette tip, dispense the liquid gently, wait for 2\u00a0min and repeat the process.Pause point: DNA can be stored at -80\u00b0C for at least six months.\nAssess the yield of the amplified complementary DNA using fluorometry instrument and assess the DNA profile using capillary automated electrophoresis.\nPreparation of DNA library for high-throughput sequencing\nTiming: [1 h]\nThese steps will describe the production of DNA library for next generation sequencing based on bead-linked transposome technology (Bruinsma et\u00a0al., 2018[href=https://www.wicell.org#bib6]; Caruccio, 2011[href=https://www.wicell.org#bib8]) to produce fragments of DNA within a narrow range around 450 base pairs long.\nAssess the yield of the amplified complementary DNA using fluorometry instrument (e.g., Qubit) and assess the DNA profile using capillary automated electrophoresis (e.g., Bioanalyzer).\nPrepare a diluted sample of amplified cDNA at 1\u00a0ng in 30\u00a0\u03bcL of water.\nFollow the protocol described in the Nextera DNA flex Library preparation kit reference guide until the elution of the cleaned library.\nNote: Libraries are barcoded in one the specific steps of library construction. In this kit, during the PCR amplification. In our laboratory we use the dual barcode scheme.\nNote: Do not carry out the normalization steps for analysis of gene expression.\nNote: Carry out 12 cycles of PCR.\nAssess the yield of the amplified complementary DNA using fluorometry instrument (e.g., Qubit) and assess the DNA profile using capillary automated electrophoresis (e.g., Bioanalyzer). The automated electrophoresis will also be important for the estimation of average fragment length of the library\nAlternatives: The data presented in this protocol was prepared with the Nextera DNA flex Library preparation kit, however alternatives to this protocol exist. Since the production of the representative data, Illumina Inc. has released another kit for library production based on the transposon technology. It is also possible to quantify the library DNA using quantitative PCR reagents and the appropriate analytical procedures (Hawkins and Guest, 2018[href=https://www.wicell.org#bib16]).Pooling of libraries for high-throughput sequencing\nCalculate the molarity of each library in nanomolar using the standard equation:     (  c o n c e n t r a t i o n   ( n g / \u03bc l )  )    (  660  g  m o l    \u00d7  a v e r a g e  l i b r a r y  f r a g m e n t  l e n g t h   ( b p )  )     \u00d7  10 6   \nCombine samples maintaining equimolar quantities of material from each library.\nSubmit samples for sequencing according to the sequencing facility\u2019s guidelines.\nBasic bioinformatic procedures\nThe processing of the RNA-sequencing data often follows a basic flow of alignment of the reads to the reference genome and counting of the reads according to gene annotation, but there are many variations of this workflow (Conesa et\u00a0al., 2016[href=https://www.wicell.org#bib11]; Van den Berge et\u00a0al., 2019[href=https://www.wicell.org#bib3]). To produce the representative results, sequences were aligned with Hisat2 (Kim et\u00a0al., 2015[href=https://www.wicell.org#bib20], 2019[href=https://www.wicell.org#bib21]; Pertea et\u00a0al., 2016[href=https://www.wicell.org#bib27]), followed by filtering with samtools (Li et\u00a0al., 2009[href=https://www.wicell.org#bib23]) (remove unmapped reads, secondary alignments, failing of platform quality checks, PCR or optical duplicate) and removal of duplicates with the function \u2018bammarkduplicates\u2019 from biobambam (Tischler and Leonard, 2014[href=https://www.wicell.org#bib33]). Sorting and indexing were done with Picard (http://broadinstitute.github.io/picard/[href=http://broadinstitute.github.io/picard/]). Finally, fragments were counted with \u2018featurecounts\u2019 (Liao et\u00a0al., 2014[href=https://www.wicell.org#bib24]) using the Ensembl (Flicek et\u00a0al., 2014[href=https://www.wicell.org#bib14]; Kinsella et\u00a0al., 2011[href=https://www.wicell.org#bib22]) bovine annotation as a guide.\nThe code used to produce representative results is described below.\nPipeline for processing raw reads.\nAlign the reads to the reference genome.\n#!/bin/bash\n##Paths:\n-\npath_to_index= .../Bos_taurus.ARS-UCD1.2.99\npath_to_gtf=.../Bos_taurus.ARS-UCD1.2.98.gtf\nsplice_sites=.../hisat_splice.txt\npath_to_picard=.../bioinfo\nmerged_fastq=.../merged_fastq\nsample=70o\noutput_alignment= .../alignment/$sample\nmkdir $output_alignment\nread1=$merged_fastq/sample_51_R1_001.fastq.gz\nread2=$merged_fastq/sample_51_R2_001.fastq.gz\n/home/fbiase/bioinfo/hisat2-2.2.0/hisat2 -p 30 -k 1 -x $path_to\nindex -1 $read1 -2 $read2 --known-splicesite-infile $splice_sit\nes --no-mixed -S $output_alignment/$sample.alignment.sam --summary-file $output_alignment/$sample.summary.txt\nsample=72o\noutput_alignment= .../alignment/$sample\nmkdir $output_alignment\nread1=$merged_fastq/sample_55_R1_001.fastq.gz\nread2=$merged_fastq/sample_55_R2_001.fastq.gz\n/home/fbiase/bioinfo/hisat2-2.2.0/hisat2 -p 30 -k 1 -x $path_to\nindex -1 $read1 -2 $read2 --known-splicesit\ne-infile $splice_sites --no-mixed -S $output_alignment/$sample.alignment.sam --sum\nmary-file $output_alignment/$sample.summary.txt\nsample=87o\noutput_alignment= .../alignment/$sample\nmkdir $output_alignment\nread1=$merged_fastq/sample_67_R1_001.fastq.gz\nread2=$merged_fastq/sample_67_R2_001.fastq.gz\n/home/fbiase/bioinfo/hisat2-2.2.0/hisat2 -p 30 -k 1 -x $path_to\nindex -1 $read1 -2 $read2 --known-splicesite-infile $splice_sit\nes --no-mixed -S $output_alignment/$sample.alignment.sam --sum\nmary-file $output_alignment/$sample.summary.txt\nsample=89o\noutput_alignment= .../alignment/$sample\nmkdir $output_alignment\nread1=$merged_fastq/sample_71_R1_001.fastq.gz\nread2=$merged_fastq/sample_71_R2_001.fastq.gz\n/home/fbiase/bioinfo/hisat2-2.2.0/hisat2 -p 30 -k 1 -x $path_to\n_index -1 $read1 -2 $read2 --known-splicesite-infile $splice_sit\nes --no-mixed -S $output_alignment/$sample.alignment.sam --sum\nmary-file $output_alignment/$sample.summary.txt\nsample=91o\noutput_alignment= .../alignment/$sample\nmkdir $output_alignment\nread1=$merged_fastq/sample_75_R1_001.fastq.gz\nread2=$merged_fastq/sample_75_R2_001.fastq.gz\n/home/fbiase/bioinfo/hisat2-2.2.0/hisat2 -p 30 -k 1 -x $path_to\n_index -1 $read1 -2 $read2 --known-splicesite-infile $splice_sit\nes --no-mixed -S $output_alignment/$sample.alignment.sam --sum\nmary-file $output_alignment/$sample.summary.txt\nFilter low-quality alignments.\nFolder= .../alignment\nsamtools view -b -h -F 1796 $folder/70o/70o.alignment.sam -o $folder\n/70o/70o.alignment.filtered.bam\nsamtools view -b -h -F 1796 $folder/89o/89o.alignment.sam -o $folder\n/89o/89o.alignment.filtered.bam\nsamtools view -b -h -F 1796 $folder/87o/87o.alignment.sam -o $folder\n/87o/87o.alignment.filtered.bam\nsamtools view -b -h -F 1796 $folder/72o/72o.alignment.sam -o $folder\n/72o/72o.alignment.filtered.bam\nsamtools view -b -h -F 1796 $folder/91o/91o.alignment.sam -o $folder\n/91o/91o.alignment.filtered.bam\nSort alignments by coordinate.\njava -jar $path_to_picard/picard.jar SortSam INPUT=$folder\n/70o/70o.alignment.filtered.bam OUTPUT=$folder/70o/70o.align\nment.filtered.sorted.bam SORT_ORDER=coordinate\njava -jar $path_to_picard/picard.jar SortSam INPUT=$folder/\n89o/89o.alignment.filtered.bam OUTPUT=$folder/89o/89o.align\nment.filtered.sorted.bam SORT_ORDER=coordinate\njava -jar $path_to_picard/picard.jar SortSam INPUT=$folder/\n87o/87o.alignment.filtered.bam OUTPUT=$folder/87o/87o.align\nment.filtered.sorted.bam SORT_ORDER=coordinate\njava -jar $path_to_picard/picard.jar SortSam INPUT=$folder\n/72o/72o.alignment.filtered.bam OUTPUT=$folder/72o/72o.align\nment.filtered.sorted.bam SORT_ORDER=coordinate\njava -jar $path_to_picard/picard.jar SortSam INPUT=$folder\n/91o/91o.alignment.filtered.bam OUTPUT=$folder/91o/91o.align\nment.filtered.sorted.bam SORT_ORDER=coordinate\nIndex and create index file.\njava -jar $path_to_picard/picard.jar BuildBamIndex INPUT=$folder\n/70o/70o.alignment.filtered.sorted.bam\njava -jar $path_to_picard/picard.jar BuildBamIndex INPUT=$folder\n/89o/89o.alignment.filtered.sorted.bam\njava -jar $path_to_picard/picard.jar BuildBamIndex INPUT=$folder\n/87o/87o.alignment.filtered.sorted.bam\njava -jar $path_to_picard/picard.jar BuildBamIndex INPUT=$folder\n/72o/72o.alignment.filtered.sorted.bam\njava -jar $path_to_picard/picard.jar BuildBamIndex INPUT=$folder\n/91o/91o.alignment.filtered.sorted.bam\nRemove duplicates.\nbammarkduplicates I=$folder/70o/70o.alignment.filtered.sorted.bam\nO=$folder/70o/70o.alignment.filtered.sorted.undup.bam level=9\nmarkthreads=5 rmdup=1 index=1 dupindex=0 verbose=0\nbammarkduplicates I=$folder/89o/89o.alignment.filtered.sorted.bam\nO=$folder/89o/89o.alignment.filtered.sorted.undup.bam level=9\nmarkthreads=5 rmdup=1 index=1 dupindex=0 verbose=0\nbammarkduplicates I=$folder/87o/87o.alignment.filtered.sorted.bam\nO=$folder/87o/87o.alignment.filtered.sorted.undup.bam level=9\nmarkthreads=5 rmdup=1 index=1 dupindex=0 verbose=0\nbammarkduplicates I=$folder/72o/72o.alignment.filtered.sorted.bam\nO=$folder/72o/72o.alignment.filtered.sorted.undup.bam level=9\nmarkthreads=5 rmdup=1 index=1 dupindex=0 verbose=0\nbammarkduplicates I=$folder/91o/91o.alignment.filtered.sorted.bam\nO=$folder/91o/91o.alignment.filtered.sorted.undup.bam level=9\nmarkthreads=5 rmdup=1 index=1 dupindex=0 verbose=0Count fragments relative to genome annotation.\npath_to_gtf=.../Bos_taurus.ARS-UCD1.2.98.gtf\nfolder=.../alignment\nfolder1=.../counting\n/subread-2.0.0-Linux-x86_64/bin/featureCounts -s 0 -a $pa\nth_to_gtf -o $folder1/70o/70o.count -F 'GTF' -t 'exon' -g\n'gene_id' --ignoreDup -p -T 5 $folder/70o/70o.alignment\n.filtered.sorted.undup.bam\n/subread-2.0.0-Linux-x86_64/bin/featureCounts -s 0 -a $pa\nth_to_gtf -o $folder1/89o/89o.count -F 'GTF' -t 'exon' -g\n'gene_id' --ignoreDup -p -T 5 $folder/89o/89o.alignment\n.filtered.sorted.undup.bam\n/subread-2.0.0-Linux-x86_64/bin/featureCounts -s 0 -a $pa\nth_to_gtf -o $folder1/87o/87o.count -F 'GTF' -t 'exon' -g\n'gene_id' --ignoreDup -p -T 5 $folder/87o/87o.alignment\n.filtered.sorted.undup.bam\n/subread-2.0.0-Linux-x86_64/bin/featureCounts -s 0 -a $pa\nth_to_gtf -o $folder1/72o/72o.count -F 'GTF' -t 'exon' -g\n'gene_id' --ignoreDup -p -T 5 $folder/72o/72o.alignment\n.filtered.sorted.undup.bam\n/subread-2.0.0-Linux-x86_64/bin/featureCounts -s 0 -a $pa\nth_to_gtf -o $folder1/91o/91o.count -F 'GTF' -t 'exon' -g\n'gene_id' --ignoreDup -p -T 5 $folder/91o/91o.alignment\n.filtered.sorted.undup.bam\nIn R software, combine the counts from all files into one matrix.\nfiles<-list.files(\"/mnt/storage/lab_folder/oocyte_project/oocyte_\nCC_BCB/counting_JOVE_paper\", recursive=T, pattern=\".count\", full.\nnames\u00a0= TRUE)\nfiles<-files[grep(\"summary\", files, invert\u00a0= TRUE)]\nlength(files)\ncount_data<-data.frame(matrix(nrow=27607))\nfor (n in 1:length(files)) {\n\u00a0\u00a0count<-read.delim(files[n], header=TRUE, sep= \"\\t\", stringsAsFa\nctors\u00a0= FALSE, comment.char= \"#\")\n\u00a0\u00a0count<-count[,c(1,7)]\n\u00a0\u00a0count_data<-cbind(count_data,count)\n}\nrownames(count_data)<-count_data[,2]\ncount_data<-count_data[,seq(from\u00a0= 3, to\u00a0= 11, by\u00a0= 2)]\ncolnames(count_data)<- paste(\"oocyte\" , substr(colnames(count_dat\na), 80, 81), sep=\"\")\nSubset the data to retain genes with > 50 reads\ncount_data_a<-count_data[rowSums(count_data)>50,]\nannotation.ensembl.symbol<-read.delim(\"/resources/2020_05_29\n_annotation.ensembl.symbol.txt.bz2\", header=TRUE, sep= \"\\t\",\nrow.names=1, stringsAsFactors\u00a0= FALSE)\ncount_data_annotated<-merge(count_data_a,annotation.ensembl.\nsymbol, by.x=\"row.names\", by.y=\"ensembl_gene_id\", all.x=TRUE,\nall.y=FALSE)\ncount_data_annotated<-count_data_annota\nted[count_data_annotated$gene_biotype %in% c('protein_coding', 'lncRNA','pseudoge\nne'),]\ncount_data_annotated_length<-count_data_annotated$transcript_length\nLoad the necessary libraries\nlibPaths(\"/usr/lib/R/site-library\")\nlibrary('dplyr', lib.loc=\"/usr/lib/R/site-library\")\nlibrary('ggplot2', lib.loc=\"/usr/lib/R/site-library\")\nlibrary('edgeR', lib.loc=\"/usr/lib/R/site-library\")\nlibrary(\"cowplot\", lib.loc=\"/usr/lib/R/site-library\")\nlibrary(\"GGally\",lib.loc=\"/usr/lib/R/site-library\")\nlibrary(\"DESeq2\",lib.loc=\"/usr/lib/R/site-library\")\nCalculate fragments per kilobase and counts per million\noocyte_fpkm<-rpkm(count_data_annotated[,c(2:6)], count_data_annotated_length)\noocyte_cpm<-edgeR::cpm(count_data_annotated[,c(2:6)])\nCode to generate Figure\u00a04[href=https://www.wicell.org#fig4].\ntable_N_genes_oocyte<-data.frame()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for (i in seq(0.1,1,0.1)){\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for (j in seq(1,5,1)){\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0oocyte_fpkm_a<-oocyte_fpkm[rowSums(oocyte_fp\nkm >= i) >= j,]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dim(oocyte_fpkm_a)[1]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0table_N_genes_oocyte<-rbind(table_N_genes_oo\ncyte, data.frame(\"treshold\"=i,\"n_samples\"=j, \"n_genes\"=di\nm(oocyte_fpkm_a)[1]))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\nfont_size<-12\nplot2<-ggplot(table_N_genes_oocyte, aes(treshold, n_genes,\ncolor\u00a0= n_samples))\u00a0+\ngeom_point()\u00a0+\nscale_x_continuous(\"FPKM\",breaks\u00a0= seq(0,1,0.1))+\nscale_y_continuous(\"Number of genes \\n equal or above FPK\nM threshold\", breaks\u00a0= seq(11000,15000,500), limits=c(110\n00,15000))+\nscale_color_continuous(name\u00a0= \"Sample (N)\",breaks\u00a0= seq(1,\n5,1) ,high\u00a0= \"#132B43\", low\u00a0= \"#56B1F7\")+guides(colour\u00a0= guide_legend(reverse=T))+\ntheme_bw(base_size\u00a0= font_size)+\ntheme(panel.grid.major\u00a0= element_blank(),\n\u00a0\u00a0\u00a0\u00a0#panel.grid.minor\u00a0= element_blank(),\n\u00a0\u00a0\u00a0\u00a0panel.background\u00a0= element_blank(),\n\u00a0\u00a0\u00a0\u00a0axis.title=element_text(color=\"black\"),\n\u00a0\u00a0\u00a0\u00a0axis.text=element_text(color=\"black\"),\n\u00a0\u00a0\u00a0\u00a0legend.position\u00a0= c(0.1, 0.25),\n\u00a0\u00a0\u00a0\u00a0legend.key.size\u00a0= unit(.1, \"cm\"),\n\u00a0\u00a0\u00a0\u00a0legend.text\u00a0= element_text(size=10),\n\u00a0\u00a0\u00a0\u00a0legend.title\u00a0= element_text(size=10))\ntable_N_genes_oocyte<-data.frame()\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for (i in seq(0.1,1,0.1)){\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0for (j in seq(1,5,1)){\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0oocyte_cpm_a<-oocyte_cpm[rowSums(oocyte_cpm\n>= i) >= j,]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0dim(oocyte_cpm_a)[1]\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0table_N_genes_oocyte<-rbind(table_N_genes_oo\ncyte, data.frame(\"treshold\"=i,\"n_samples\"=j, \"n_genes\"=di\nm(oocyte_cpm_a)[1]))\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0}\nplot3<-ggplot(table_N_genes_oocyte, aes(treshold, n_genes,\ncolor\u00a0= n_samples))\u00a0+\ngeom_point()\u00a0+\nscale_x_continuous(\"CPM\",breaks\u00a0= seq(0,1,0.1))+\nscale_y_continuous(\"Number of genes \\n equal or above CPM\nthreshold\", breaks\u00a0= seq(12000,15000,500), limits=c(1200\n0,15000))+\nscale_color_continuous(name\u00a0= \"Sample (N)\",breaks\u00a0= seq(1,\n5,1) ,high\u00a0= \"#132B43\", low\u00a0= \"#56B1F7\")+\nguides(colour\u00a0= guide_legend(reverse=T))+\ntheme_bw(base_size\u00a0= font_size)+\ntheme(panel.grid.major\u00a0= element_blank(),\n\u00a0\u00a0\u00a0\u00a0#panel.grid.minor\u00a0= element_blank(),\n\u00a0\u00a0\u00a0\u00a0panel.background\u00a0= element_blank(),\n\u00a0\u00a0\u00a0\u00a0axis.title=element_text(color=\"black\"),\n\u00a0\u00a0\u00a0\u00a0axis.text=element_text(color=\"black\"),\n\u00a0\u00a0\u00a0\u00a0legend.position\u00a0= c(0.1, 0.25),\n\u00a0\u00a0\u00a0\u00a0legend.key.size\u00a0= unit(.1, \"cm\"),\n\u00a0\u00a0\u00a0\u00a0legend.text\u00a0= element_text(size=10),\n\u00a0\u00a0\u00a0\u00a0legend.title\u00a0= element_text(size=10))\nplot_grid(plot2,plot3, nrow=2)\nTransform the counts using the rlog approach.\noocyte_count_b<-count_data_annotated[,c(2:6)]\ncolnames(oocyte_count_b)<-c(\"oocyte_A\", \"oocyte_B\" ,\"oocyte_C\" ,\"oocyte_D\", \"o\nocyte_E\")\ndeseq2_oocyte<-DESeqDataSetFromMatrix(oocyte_count_b, colData=DataFrame(c(\"ooc\nyte_A\", \"oocyte_B\" ,\"oocyte_C\" ,\"oocyte_D\", \"oocyte_E\")), design\u00a0= \u223c1 )\ndeseq2_oocyte_RLOG<-rlog(deseq2_oocyte, blind=TRUE)\nCode to generate Figure\u00a05[href=https://www.wicell.org#fig5]\nggpairs(as.data.frame(assay(deseq2_oocyte_RLOG)), xlab\u00a0= \"Regularized Log2 tra\nnsformed counts\",ylab\u00a0= \"Regularized Log2 transformed counts\", diag=list(conti\nnuous='blank'), lower\u00a0= list(continuous\u00a0= wrap(\"smooth\", alpha\u00a0= 0.3, size=0.\n5)))", "Step-by-step method details\nStep-by-step method details\nWe first describe how to acquire videos with MIAS. We then demonstrate the analysis workflow utilizing SoAL with a series of graphical user interfaces and scripts (Figure\u00a04[href=https://www.wicell.org#fig4], right part).\nExperiment\nTiming: 2\u20134\u00a0days for daily experiments. 18\u201324\u00a0h for starvation. 0.5\u20131.5\u00a0h for fly loading and adaptation. 1\u00a0h for video acquisition\nCarry out the experiments in the incubator at 22\u00b0C and 50% humidity, within 0\u20134\u00a0h after the beginning of the day cycle.\nNote: We have provided a 10\u00a0min example video acquired during the experiment. The example video is included in our tutorial data (https://doi.org/10.6084/m9.figshare.19711738[href=https://doi.org/10.6084/m9.figshare.19711738], containing an example video along with all the analysis results after \u201cPose estimation\u201d). If you want to skip the experiment and practice the pose estimation (refer to step-by-step method details[href=https://www.wicell.org#step-by-step-method-details] \u201cPose estimation\u201d), download this tutorial data and perform the pose estimation (refer to step-by-step method details[href=https://www.wicell.org#step-by-step-method-details] step 7).\nNote: We combined single-housed males with group-housed females to enhance courtship, as single-housing promotes male courtship while group-housing suppresses female receptivity.\nFemale starvation.\nNote: Females need to be starved 18\u201324\u00a0h before the beginning of the experiment.\nShift the group-housed flies (refer to before you begin[href=https://www.wicell.org#before-you-begin] step 9) into an empty vial with a sheet of wet paper.\nPut the empty vial back into the rearing incubator.\nFly loading.\nSwitch the incubator on and set the temperature to 22\u00b0C and the humidity to 50%.\nAssemble the chambers.\nRemove the chamber walls and top plates in Figure\u00a02[href=https://www.wicell.org#fig2].\nHeat the fly food (refer to before you begin[href=https://www.wicell.org#before-you-begin] step 3) by microwave until melted.\nPour the food into the holes at the center of each chamber on the bottom plate (Figure\u00a01[href=https://www.wicell.org#fig1]B) with a pipette.Stack up the chamber walls and top plates with both the top and left edges aligned (Figures\u00a01[href=https://www.wicell.org#fig1]C and 2[href=https://www.wicell.org#fig2]).\nInsert the four separators into the slits by rows.\nIntroduce males into the parts with food (Figure\u00a01[href=https://www.wicell.org#fig1]B, the hole below the separation line) guided by a soft tube. Cover the holes on the top plate with the cover plates immediately after the fly is guided into the chambers.\nNote: To make an introducing tube, cut the two ends of a 2\u00a0mL pipette, and make sure that one end fits the microcentrifuge tube and the other end fits the introducing hole. Quickly open the lid of the microcentrifuge tube when the fly stays away from the lid, while aligning the end of the tube with the microcentrifuge tube. After the fly moves into the soft tube, pinch the big end to prevent escaping. Place the small end above the introducing hole and wait until the fly moves into the chamber.\nNote: Anesthetization affects the neural activity of the flies that may not recover immediately after waking up. We therefore recommend not to anesthetize the males to maintain the best condition of the animals at the cost of experiment difficulty.\nAnesthetize the group-housed flies by ice.\nPick the females with tweezers and gently moved them into the parts without food (Figure\u00a01[href=https://www.wicell.org#fig1]B, the introducing hole above the separation line), thus preventing them from eating before the experiment.\nCover the holes on the top plate with the cover plates.\nAdaptation.\nLeave the flies in the chamber for 20\u201330\u00a0min for recovery and adaptation.\nClosing chamber.\nJust before the video acquisition, remove the separators and slide the top plate up to cover the introducing holes (Figure\u00a01[href=https://www.wicell.org#fig1]D).\nRemove the cover plates to clear the view.Note: This operation should finish within 2\u00a0min to reduce the interaction time before video acquisition.\nCritical: When sliding the top plate, take care not to crush the flies with the introducing hole. When there are flies in the introducing hole, wait a short time until all the flies leave the holes.\nVideo acquisition.\nRun the video acquisition software (e.g., MIAS_FLIR.exe for FLIR cameras). This program shows a black console (Figure\u00a03[href=https://www.wicell.org#fig3]) and two preview windows.\nInspect the preview window to ensure the field of view is fine.\nPress \u201cc\u201d and \u201cEnter\u201d in the console to start the recording.\nPress \u201cq\u201d and \u201cEnter\u201d in the console to stop the recording and quit MIAS.\nNote: In our example, the video is recorded with a resolution of 1280 by 1024 pixels and a fixed acquisition rate of 66.6 frames/s.\nNote: The videos acquired are stored in the folders named \u201cvideo\u201d under MIAS directory, with the folder structure as follows:\n- MIAS\n\u00a0\u00a0\u00a0\u00a0config.json\n\u00a0\u00a0\u00a0\u00a0MIAS_All.exe\n\u00a0\u00a0\u00a0\u00a0MIAS_FLIR.exe\n\u00a0\u00a0\u00a0\u00a0[Other files]\n\u00a0\u00a0\u00a0\u00a0-video\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-20200703_141812_1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_1.avi\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_1.log\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0-20200703_141812_2\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2.avi\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2.log\nFor each camera, MIAS generates two files (\u201cxxx.avi\u201d is the recorded video. \u201cxxx.log\u201d stores the timestamps of each frame) in the same folder. The name (e.g., \u201c20200703_141812_2\u201d) is composed of date, time, and the camera name, separated by \u201c_\u201d.\nChamber cleaning.\nAfter each experiment, anesthetize the flies by ice or CO2 and discard them. Rinse the surface of the supporters and all the parts of the chambers with deionized water.\nPose estimation\nTiming: hours to days\nTiming: For keypoint detection (step 13), about 2\u00a0h for the example video on the Basic Hardware Configuration\nThe following steps use the tutorial data to demonstrate the analysis pipeline. The analysis result includes the time series of 5 keypoint positions.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1923-Fig6.jpg\nFigure\u00a06. The top-down approach for keypoints detection(A and B) Segment a local region around each fly, and centralize by transforming the segmented image to ensure that the fly body appears vertically at the center of a 64 by 48 pixels image (B).\n(C) Detect the 5 keypoints of a fly.\n(D) Transform the keypoints back to the original coordinate.\nNote: To perform pose estimation, we used a top-down approach, applying ROI extraction, body detection, and keypoints prediction on the video frames in parallel (Figure\u00a06[href=https://www.wicell.org#fig6]).\nDownload tutorial data.\nDownload and unzip the tutorial data (https://doi.org/10.6084/m9.figshare.19711729[href=https://doi.org/10.6084/m9.figshare.19711729]).\nNote: The folder\u00a0\u201cvideo\u201d\u00a0under the SoAL code directory corresponds to the \u201cvideo\u201d folder under the MIAS directory (refer to step-by-step method details[href=https://www.wicell.org#step-by-step-method-details] step 5).\nNote: As a reference, another package of tutorial data (https://doi.org/10.6084/m9.figshare.19711738)[href=https://doi.org/10.6084/m9.figshare.19711738)] already contain configuration files and analysis results. The files will be replaced, after performing the following steps.\nConfiguration (for steps 8\u201312 see demonstration in Methods video S1[href=https://www.wicell.org#mmc1]).\nExecute the following command to call the configuration UI.\n>python SoAL_PreProcess.py \u2216\nvideo/20200703_141812_2/20200703_141812_2.avi\nNote: The command line parameter \u201cvideo/20200703_141812_2/20200703_141812_2.avi\u201d is the path of the video file to be configured (the symbol \u201c\u2216\u201d is a line continuation, do not input this symbol and the following line break). This configuration shows 3 dialog boxes in series (described in the following steps 9\u201311) to set the parameters for keypoint detection and behavior annotation.\nSetting the scale.\nThe first dialog shows a random sampled frame from the video for setting the scale, defined as the number of pixels occupying a 1-mm length.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1923-Fig7.jpg\nFigure\u00a07. UI for setting the scale factor(A) This UI shows the image of a frame with an auto-detected chamber diameter (blue line). Optionally, modify the textboxes below the image to change the experiment date (\u201cExpDate\u201d), female eclosion date (\u201cFemaleDoB\u201d), and temperature (\u201cTemperature\u201d). The textbox \u201cDiameter (mm)\u201d is for specifying the real length of the chamber diameter. \u201cScale (px/mm)\u201d is updated automatically when the diameter and the real length are both specified. Click button \u201cRandom\u201d to change a frame to show.\n(B and C) If the detected diameter is incorrect, label two points on the image by clicking to indicate a diameter. First, click the start point (B, red point). Second, click the end point (C).\nNote: The scale factor is calculated by automatic detection of the chamber diameter (Figure\u00a07[href=https://www.wicell.org#fig7]A blue line). Alternatively, the chamber diameter can also be specified manually (Figures\u00a07[href=https://www.wicell.org#fig7]B and 7C).\nInput the real diameter (in mm) of the chamber in the textbox \u201cDiameter (mm)\u201d. The textbox \u201cScale (px/mm)\u201d then shows the calculated scale factor.\nClick \u201cConfirm\u201d to proceed to the next step.\nROI extraction.\nNow another dialog shows an image superimposed with 16 blue circles indicating the auto-detected chamber regions (Figure\u00a08[href=https://www.wicell.org#fig8]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1923-Fig8.jpg\nFigure\u00a08. UI for ROI extraction\n(A) This UI shows the image of a frame with auto-detected ROIs (blue circles).\n(B) Deselect ROIs as needed by clicking in the circle (red circle, the third one of the fourth row, which includes a dead fly). Click again to re-include the circle as a ROI.(C) Input fly information for each ROI in the textbox \u201cFlyInfo\u201d. Specify all ROIs according to the order of top to bottom and left to right (skipping the excluded ROIs with red circles). The text for one ROI is composed of a group name and an age number, separate by \u201c_\u201d. The adjacent ROIs with the same group and age can be merged and represented by adding \u201c\u2217N\u201d (N is the number of the same ROIs). Different ROIs are separated by \u201c,\u201d. For example, the text \u201cCtrl_9\u22177,Exp_8,Exp_9\u22177\u201d (textbox \u201cFlyInfo\u201d) means the first 7 males are from \u201cCtrl\u201d group and are 9\u00a0d old, the eighth male is from \u201cExp\u201d group and are 8\u00a0d old, and the last 7 males are from \u201cExp\u201d group and are 9\u00a0d old. The red one (the third one of the fourth row) is excluded.\nOptional: If the detection is incorrect, change the parameters for circle detection (the textbox \u201cCircleParam\u201d) or use the manual mode (see problems 10[href=https://www.wicell.org#sec7.19] and 14[href=https://www.wicell.org#sec7.27] for the details).\nSome of the regions should not be analyzed due to the unsuccessful preparation. Click in any circle to exclude the region from being ROI (the circle will change to red, Figure\u00a08[href=https://www.wicell.org#fig8]B).\nClick \u201cConfirm\u201d to proceed to the next step.\nBackground subtraction.\nWait several seconds of computing until the appearance of the next dialog box.\nNote: The program computes the background image by averaging 100 frames that are uniformly sampled in time throughout the video.\nFly segmentation.\nNote: The program segments the flies in the images by detecting connected regions in binarized images.\nThe dialog shows the binarized ROI regions (Figure\u00a09[href=https://www.wicell.org#fig9], 15 ROIs are shown because one ROI is excluded).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1923-Fig9.jpg\nFigure\u00a09. UI for fly segmentation(A\u2013C) The UI shows the images of 15 extracted ROIs. The histogram under the images is the gray scale histogram of current image. Slide the bottom bar to change the binarization threshold (the value is shown on the right, \u201c150\u201d). Switch the radio buttons at the left bottom to change the viewing mode: original (A), background-subtracted (B), and binary (C). The default viewing mode is binary (C).\nSlide the bar at the bottom to change the binarization threshold.\nCritical: A rule of thumb for choosing a good threshold is to make sure that the segmented fly body as previewed in Figure\u00a09[href=https://www.wicell.org#fig9]C is complete and does not include the wings.\nOptional: For different videos with the same illumination condition, the same threshold can be used (the default value of the threshold can be changed in the source file \u201cSoAL_Constants.py\u201d named \u201cDEFAULT_GRAY_THRESHOLD\u201d).\nOptional: Change the preview mode by clicking the buttons at the bottom (Figure\u00a09[href=https://www.wicell.org#fig9]).\nClick \u201cConfirm\u201d to finish the configuration.\nNote: The configuration results are stored in two files, a text file \u201cxxx_config.json\u201d and a background image \u201cxxx.bmp\u201d (\u201cxxx\u201d is the video name), both in the same directory as the video file.\nRunning keypoint detection (Methods video S2[href=https://www.wicell.org#mmc2]).\nChoosing the model. Modify the configuration item \u201cMODEL_FILE\u201d (under the item \u201cTEST\u201d) in \u201chrnet/fly_w32.yaml\u201d to specify the model to use for keypoint detection (same as the step \u201cbefore you begin[href=https://www.wicell.org#before-you-begin], step 12b\u201d). By default, the provided model \u201chrnet_w32_SDPD-15k.pth\u201d is used.\nExecute the script to automatically analyze all the videos in the directory.\n>python SoAL_KptDetect.py all\nThe program will print the processing progress. Wait until the message \u201c[Main]: all finished\u201d is printed. Close the console window to quit.Note: The directory is specified by the item \u201cVIDEO_TODO_DIR\u201d in \u201cSoAL_Constants.py\u201d, and the default path is \u201cvideo/\u201d. The videos are automatically processed in parallel based on the hardware configuration.\nNote: To determine whether a video has been successfully processed, inspect the text file \u201c.state\u201d in each video folder to see if \u201cfinish\u201d is shown.\nNote: Processing speed is about 150 centralized images per second on the Basic Hardware Configuration, and 400 centralized images per second on the Advanced Hardware Configuration with at least two videos processing simultaneously.\nOptional: Before running keypoint detection, edit the following items in the file \u201cSoAL_Constants.py\u201d to modify some parameters.\nMODEL_CONFIG\u00a0= \"hrnet/fly_w32.yaml\"\nVIDEO_TODO_DIR\u00a0= \"video/\"\nMAX_TASK\u00a0= 2\nBATCH_SIZE\u00a0= 60\nMODEL_SHAPE\u00a0= 64, 48\nFLY_NUM\u00a0= 2\n\u201cMODEL_CONFIG\u201d provides the path of the model configuration file. \u201cVIDEO_TODO_DIR\u201d is the parent folder of the video to be analyzed. \u201cMAX_TASK\u201d is the number of videos processed in parallel (limited by the clock speed of the CPU and the RAM), which is 2 with the Basic Hardware Configuration. \u201cBATCH_SIZE\u201d is the number of centralized images processed in a batch on the GPU (limited by the VRAM capacity of the GPU), which is 60 with the Basic Hardware Configuration and 200 with the Advanced Hardware Configuration. \u201cMODEL_SHAPE\u201d defines the size (width and height, in pixels) of the centralized image as input to the HRNet model. \u201cFLY_NUM\u201d defines the number of flies in one ROI, this number is 2 in cases of male-female courtship.\nInspecting keypoints (Methods video S3[href=https://www.wicell.org#mmc3]).\nNote: Here is an example of part of the folder structure after the keypoint detection:\n- video\n\u00a0\u00a0\u00a0\u00a0- 20200703_141812_2\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2.avi\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2.bmp\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2_config.json\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- 0\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2_0_kpt.csv\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2_0_config.json\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- 1\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2_1_kpt.csv\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2_1_config.json\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0- 2\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2_2_kpt.csv\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a020200703_141812_2_2_config.jsonThe digital folders include the keypoint information files \u201cxxx_kpt.csv\u201d of each ROI (the last number in the name indicates the ROI number). The file \u201cxxx_config.json\u201d contains the configuration information of this ROI.\nInspect the keypoint information \u201cxxx_kpt.csv\u201d by executing the following script (Figure\u00a010[href=https://www.wicell.org#fig10]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1923-Fig10.jpg\nFigure\u00a010. UI for Inspecting detected keypoints\nThis UI shows 4 diagrams of a frame: the image of the ROI, the keypoints and heading directions (triangles, two flies are color-coded in blue and red), and two centralized flies with the keypoints. Move the slide bar to change the showing frame. Press \u201cLeft arrow\u201d or \u201cRight arrow\u201d to navigate to the next and previous frame respectively. Directly input the frame number and press \u201cEnter\u201d to navigate to the requested frame.\n>python tools/SoAL_ViewKptUI.py \u2216\nvideo/20200703_141812_2/0/20200703_141812_2_0_kpt.csv\nNote: The command line parameter \u201cvideo/20200703_141812_2/0/20200703_141812_2_0_kpt.csv\u201d is the keypoint information file to inspect. This UI shows the image of the specific ROI of the current frame, the 5 keypoints of each fly, and two centralized flies superimposed with the keypoints and the skeletons.\nNavigate to desired frames by the bottom slide bar.\nNote: The \u201cSoAL_ViewKptUI\u201d command can be executed alongside \u201cSoAL_KptDetect\u201d simultaneously. This is useful for inspecting the partial results quickly to find the problems. If the results are not satisfactory, you can stop the keypoint detection and make adjustments before running the \u201cSoAL_KptDetect.py\u201d command again.Note: The table file \u201cxxx_kpt.csv\u201d contains the following columns. \u201cframe\u201d is the frame number. The same frame number will appear twice for the two flies. \u201creg_n\u201d is the number of regions detected. This number is generally 2 for social behavior, but it becomes 1 when the two flies overlap. \u201carea\u201d is the area of one region. \u201cpos:x\u201d and \u201cpos:y\u201d is the position of the region center in the coordinate of the ROI (origin point on the upper left corner). \u201cori\u201d is the orientation angle. \u201ce_maj\u201d and \u201ce_min\u201d is the major and minor axis of the fitted ellipse. \u201cpoint:xs\u201d and \u201cpoint:ys\u201d are the horizontal and vertical positions of the 5 keypoints.\nBehavior annotation\nTiming: Several minutes depending on the specific case\nThe following step assigns the correct identities to the detected keypoints and annotates behaviors of each fly. The results include the motion parameters and the behavior bouts.\nRun identity assignment and behavior annotation (Methods video S4[href=https://www.wicell.org#mmc4]).\nRun the following script to assign identities and annotate behaviors of one ROI:\n>python SoAL_ID_Anno.py \u2216\nvideo/20200703_141812_2/0/20200703_141812_2_0_kpt.csv\nNote: The command line parameter \u201cvideo/20200703_141812_2/0/20200703_141812_2_0_kpt.csv\u201d is the keypoint information file to analyze. To analyze an entire folder, pass the video folder as the parameter.\n>python SoAL_ID_Anno.py video/20200703_141812_2\nNote: The annotation process loads the keypoint information \u201cxxx_kpt.csv\u201d. The motion parameters for each frame are then calculated. Next, identities are assigned according to the wing extension angle. Finally, we detect and annotate specific behavior bouts, such as copulation, wing extension, crabwalking, and circling.\nOptional: To enable anti-distortion (refer to before you begin[href=https://www.wicell.org#before-you-begin] step 6), modify the value of \u201cNEED_UNDISTORT\u201d in the file \u201cSoAL_ID_Anno.py\u201d. In this way, executing \u201cSoAL_ID_Anno.py\u201d command also loads the camera calibration information file.\n    Your browser does not support HTML5 video.Methods video S5. Preparing the dataset for training network model, related to \u201cbefore you begin 11\"\n    Your browser does not support HTML5 video.\n  \nMethods video S6. Training and testing the network model, related to \u201cbefore you begin 12\"", "Step-by-step method details\nStep-by-step method details\nRibodepletion\nTiming: 2 h\nNote: It is necessary to reduce the excess of ribosomal reads in sequencing experiments. The step described below removes ribosomal RNA (rRNA) from total RNA preparations. We used the rRNA NEBNext\u00ae rRNA Depletion Kit from New England Biolabs.\nCritical: To avoid unintended degradation of RNA samples, perform all procedures in a clean area, using RNase-free reagents, RNase-free tubes, and spraying all pipettes with RNase-way reagent.\nAnneal rRNA depletion probes.\nPrepare 1\u00a0\u03bcg of total RNA in 12\u00a0\u03bcL of nuclease-free H2O. Add the following components:\ntable:files/protocols_protocol_2162_4.csv\nPlace samples in a Thermocycler and run the following program with the lid set at 105\u00b0C:\ntable:files/protocols_protocol_2162_5.csv\nSpin down the samples and place them on ice.\nPrepare the RNase H master mix.\nMix the following reagents:\ntable:files/protocols_protocol_2162_6.csv\nAdd 5\u00a0\u03bcL of the above mix to the RNA samples. Mix by pipetting up and down.\nPlace samples in a thermocycler (with lid at 40\u00b0C) and incubate at 37\u00b0C for 30\u00a0min.\nSpin down the samples in a tabletop centrifuge and place them on ice.\nPrepare the DNAse I master mix:\nMix the following reagents:\ntable:files/protocols_protocol_2162_7.csv\nAdd 30\u00a0\u03bcL of the above and mix by pipetting up and down.\nPlace the samples in a thermocycler (with lid at 40\u00b0C) and incubate at 37\u00b0C for 30\u00a0min.\nSpin down samples in a tabletop centrifuge and place on ice.\nPurify ribodepleted RNA.\nAdd 110\u00a0\u03bcL (2.2\u00d7) of resuspended Agencourt RNAclean XP beads to the RNA samples.\nMix well by pipetting up and down.\nIncubate for 15\u00a0min on ice.\nSpin, place on a magnetic rack for 5\u00a0min.\nRemove supernatant.\nAdd 200\u00a0\u03bcL 80% ethanol.\nIncubate at 22\u00b0C\u201325\u00b0C for 30\u00a0s and remove supernatant.\nRepeat for a total of 2 washing steps.With lid opened, air dry the beads for 5\u00a0min at 22\u00b0C\u201325\u00b0C.\nElute RNA into 10\u00a0\u03bcL nuclease-free water.\nIncubate for 2\u00a0min at 22\u00b0C\u201325\u00b0C.\nPlace tubes on a magnetic rack to separate.\nRemove 10\u00a0\u03bcL of the supernatant.\nTransfer it to a clean nuclease-free PCR tube.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2162-Fig1.jpg\nFigure\u00a01. RNA integrity and efficiency of ribodepletion\n(A and B) Bioanalyzer profile of total RNA pre- and post-ribodepletion.\nPause point: Ribodepleted RNA can be stored at \u221280\u00b0C for prolonged periods of time. However, we moved to the next step immediately.\nNote: rRNA ribodepletion efficiency can be verified through RT-qPCR or by running 1\u00a0\u03bcL of sample in a bioanalyzer using the Agilent RNA 6000 kit (Figure\u00a01[href=https://www.wicell.org#fig1]). Approximately 100\u00a0ng ribodepleted RNA is expected.\nCritical: Sodium borohydride treatment\nTiming: 2 h\nNote: This step reduces ac4C to tetrahydro-ac4C without affecting unmodified cytidine (Figure\u00a02[href=https://www.wicell.org#fig2]). It is the most critical step of the procedure.\nTreat RNA with NaBH4.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2162-Fig2.jpg\nFigure\u00a02. Reduction of ac4C\nNaBH4 reduces ac4C to tetrahydro-ac4C (top). Reduced ac4C less efficiently base pairs with guanosine and can interact with adenosine instead (bottom).\nPlace 10\u00a0\u03bcL of ribodepleted RNA (\u223c50\u2013100\u00a0ng) in a 0.2\u00a0mL RNase-free tube.\nAdd 10\u00a0\u03bcL of 200\u00a0mM NaBH4. Mix tubes by finger-flicking.\nIncubate for 1\u00a0h at 55\u00b0C in the dark.\nCritical: NaBH4 releases heat and gas, generating bubbles within the solution. Since bubbles are generated, to avoid increased pressure inside the tubes, open the lids every 10\u00a0min and quickly spin down the tubes.\nNote: NaBH4 is an alkaline solution and induces RNA fragmentation to \u223c100 nt at 55\u00b0C (Figure\u00a03[href=https://www.wicell.org#fig3]).Optional: Reduction of ac4C with NaBH4 may also be performed at 37\u00b0C. However, an extra step of RNA fragmentation needs to be included when using temperatures lower than 55\u00b0C. While not performed in this protocol, in case further RNA fragmentation is needed, we use the NEBNext Magnesium RNA Fragmentation Module following the manufacturer\u2019s instructions[href=https://www.neb.com/protocols/0001/01/01/nebnext-magnesium-rna-fragmentation-module-protocol-e6150].\nNeutralize the reaction by adding 10\u00a0\u03bcL of 200\u00a0mM HCl.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2162-Fig3.jpg\nFigure\u00a03. RNA integrity after NaBH4 treatment\n(A) Agarose gel electrophoresis of total RNA samples treated with NaBH4 or left untreated.\n(B) Bioanalyzer profile of total RNA treated with 100\u00a0mM NaBH4 for 1\u00a0h at 55\u00b0C.\nPurify RNA.\nTransfer the solutions to new 1.5\u00a0mL RNase-free Tubes.\nAdd 370\u00a0\u03bcL of nuclease-free H2O.\nAdd 3\u00a0\u03bcL of 5\u00a0mg/mL linear acrylamide.\nMix well by pipetting up and down.\nAdd 40\u00a0\u03bcL of 3\u00a0M Sodium Acetate pH 5.5.\nMix well by inverting the tube several times.\nAdd 1.1\u00a0mL of 100% ethanol. Mix well.\nIncubate at \u221280\u00b0C for at least 1 h.\nPause point: samples can be stored at \u221280\u00b0C for 16\u201320 h.\nCentrifuge at 16,100\u00a0\u00d7\u00a0g for 15\u00a0min at 4\u00b0C in a refrigerated microcentrifuge.\nDiscard the supernatant.\nAdd 500\u00a0\u03bcL of 70% ethanol. Mix vigorously.\nCentrifuge at 16,100\u00a0\u00d7\u00a0g for 5\u00a0min at 4\u00b0C in a refrigerated microcentrifuge.\nDiscard the supernatant.\nRemove all traces of alcohol.\nAir dry the pellet for 5\u00a0min at 22\u00b0C\u201325\u00b0C with the lid opened.\nResuspend pellets in 6\u00a0\u03bcL of nuclease-free H2O.\nNote: To avoid further freeze and thaw of the NaBH4-treated RNA, we recommend moving directly to the next step.\nLibrary preparation\nTiming: 4 hNote: This step performs cDNA synthesis from NaBH4-treated RNA, followed by adapter ligation and PCR amplification. We used the NEBNext\u00ae UltraII\u2122 Directional RNA Library Prep Kit for library preparation.\nFirst-strand cDNA synthesis.\nNote: After ribodepletion, NaBH4 treatment and all isolation steps, we obtained \u223c10\u201340\u00a0ng of RNA material. NEB recommends using 1\u00a0ng\u2013100\u00a0ng of ribodepleted RNA for library preparation. We used 10\u00a0ng of RNA (Table\u00a01[href=https://www.wicell.org#tbl1]).\nTo each sample of 5\u00a0\u03bcL NaBH4-treated RNA, add 1\u00a0\u03bcL of 50\u00a0\u03bcM NEBNext Random primers. (Provided with Kit).\nIncubate the sample at 65\u00b0C for 5\u00a0min, with a heated lid set at 105\u00b0C.\nHold at 4\u00b0C.\nTo each sample (6\u00a0\u03bcL), add the following components and mix by gentle pipetting:\ntable:files/protocols_protocol_2162_8.csv\nIncubate samples (20\u00a0\u03bcL total volume) in a preheated thermal cycler (with the heated lid set at 105\u00b0C) as follows:\ntable:files/protocols_protocol_2162_9.csv\nNote: The temperature is elevated compared to the recommended by NEB. We have observed that elevating the temperature of reverse transcriptases increases C>T conversion upon NaBH4 treatment. See the \u201ctroubleshooting[href=https://www.wicell.org#troubleshooting]\u201d section for additional comments.\nAlternatives: Several reverse transcriptases can be used, including TGIRT, Superscript III, and AMV.2[href=https://www.wicell.org#bib2] An adapter ligation to the 3\u2032 end of RNAs can be performed, followed by reverse transcription using a cDNA primer specific to the adapter. However, we have observed that adapter ligation is inefficient in NaBH4-treated RNA resulting in very poor library yield.\nSecond strand cDNA synthesis.\nAdd the following reagents to the first strand synthesis reactions:\ntable:files/protocols_protocol_2162_10.csv\nMix thoroughly by gentle pipetting.\nIncubate samples (60\u00a0\u03bcL total volume) in a thermal cycler for 1\u00a0h at 16\u00b0C, with a heated lid set at \u2264 40\u00b0C.\nPurify cDNA.\ntable:files/protocols_protocol_2162_11.csv\nAdd 144\u00a0\u03bcL (1.8\u00d7) of resuspended AMPure XP Beads to the second strand synthesis reaction.\nMix well by pipetting up and down.Incubate for 5\u00a0min at 22\u00b0C\u201325\u00b0C.\nSpin and place the tube on a magnetic rack for 5\u00a0min.\nRemove supernatant.\nAdd 200\u00a0\u03bcL of 80% ethanol to the tube.\nIncubate at 22\u00b0C\u201325\u00b0C for 30 s, and then remove supernatant.\nRepeat for a total of 2 washing steps.\nAir dry the beads for 5\u00a0min with lids open.\nRemove the tube from the magnet.\nElute the DNA target from the beads into 53\u00a0\u03bcL 0.1\u00d7 TE buffer.\nMix well by pipetting up and down.\nIncubate for 2\u00a0min at 22\u00b0C\u201325\u00b0C.\nPlace the tube in the magnetic rack until the solution is clear.\nRemove 50\u00a0\u03bcL of the supernatant.\nTransfer it to a clean nuclease-free PCR tube.\nEnd Prep of cDNA Library.\nMix the following components in a sterile nuclease-free tube:\ntable:files/protocols_protocol_2162_12.csv\nMix by pipetting up and down.\nIncubate samples (60\u00a0\u03bcL total volume) in a thermal cycler (with the heated lid set at 75\u00b0C) as follows: 30\u00a0min at 20\u00b0C. 30\u00a0min at 65\u00b0C. Hold at 4\u00b0C.\nPerform Adaptor Ligation.\nNote: Dilute the NEBNext Adaptor for Illumina by mixing 1\u00a0\u03bcL of stock adapter (provided with Kit) with 9\u00a0\u03bcL with adaptor dilution buffer (provided with Kit)).\nAdd the following components directly to the End Prep Reaction. Caution: Do not pre-mix the components to prevent adaptor-dimer formation.\ntable:files/protocols_protocol_2162_13.csv\nMix samples (93.5\u00a0\u03bcL total volume) by pipetting up and down.\nQuick spin to collect all liquid from the sides of the tube.\nIncubate 15\u00a0min at 20\u00b0C in a Thermomixer.\nAdd 3\u00a0\u03bcL USER Enzyme (provided with the kit) to the ligation mixture resulting in a total volume of 96.5\u00a0\u03bcL.\nMix well by pipetting up and down.\nIncubate at 37\u00b0C for 15\u00a0min with the lid set to higher than 45\u00b0C.\nPurify adapter-ligated cDNA.\nAdd 96.3\u00a0\u03bcL (1\u00d7) AMPure XP Beads.Mix well by pipetting up and down.\nIncubate for 10\u00a0min at 22\u00b0C\u201325\u00b0C.\nQuickly spin, and place on a magnetic rack for 5\u00a0min.\nDiscard the supernatant.\nAdd 200\u00a0\u03bcL of freshly prepared 80% ethanol to the tube.\nIncubate at 22\u00b0C\u201325\u00b0C for 30 s.\nRemove supernatant.\nRepeat for a total of 2 washing steps.\nSpin the tube and put the tube back in the magnetic rack.\nCompletely remove the residual ethanol and air dry beads for 5\u00a0min.\nElute DNA with 17\u00a0\u03bcL 0.1\u00d7 TE buffer.\nIncubate for 2\u00a0min at 22\u00b0C\u201325\u00b0C.\nWithout disturbing the bead pellet, transfer 15\u00a0\u03bcL of the supernatant to a clean PCR tube.\nPCR enrichment.\nTo the adapter-ligated cDNA (15\u00a0\u03bcL total volume), add the following components and mix by gentle pipetting:\ntable:files/protocols_protocol_2162_14.csv\nSet the thermocycler with the following steps:\ntable:files/protocols_protocol_2162_15.csv\nPurify libraries.\nTake samples out of the thermocycler and add 50\u00a0\u03bcL (1\u00d7) of resuspended AMPure XP Beads.\nMix well by pipetting up and down.\nIncubate for 5\u00a0min at 22\u00b0C\u201325\u00b0C.\nSpin and place the tube on a magnetic rack for 5\u00a0min.\nDiscard the supernatant.\nAdd 200\u00a0\u03bcL of freshly prepared 80% ethanol.\nIncubate at 22\u00b0C\u201325\u00b0C for 30 s.\nRemove supernatant.\nRepeat for a total of 2 washing steps.\nSpin the tubes and put back into the magnetic rack.\nCompletely remove the residual ethanol and air dry beads for 5\u00a0min.\nElute DNA target from the beads with 23\u00a0\u03bcL 0.1 TE buffer.\nIncubate for 2\u00a0min at 22\u00b0C\u201325\u00b0C.\nWithout disturbing the bead pellet, transfer 21\u00a0\u03bcL of the supernatant to a clean PCR tube.\nPause point: Libraries can be stored at \u221220\u00b0C indefinitely.\nQuality control step: Bioanalyzer of constructed libraries\nTiming: 2 h\nNote: The integrity, purity, and size distribution of DNA libraries should be checked in a Bioanalyzer or TapeStation.Usually, 1\u00a0\u03bcL of sample is used for quality control using the Bioanalyzer and Agilent High Sensitivity DNA Kit.\nFollow the manufacturer\u2019s suggestions[href=https://www.agilent.com/cs/library/usermanuals/Public/G2938-90321_SensitivityDNA_KG_EN.pdf]. Representative results for libraries made from NaBH4-treated RNA used in our study are provided in Figure\u00a04[href=https://www.wicell.org#fig4].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2162-Fig4.jpg\nFigure\u00a04. DNA library integrity\nBioanalyzer profiles of DNA libraries from HeLa WT and NAT10\u2212/\u2212 cells.\nQuality control step: Checking the efficiency of C>T conversion by PCR\nTiming: 2\u00a0days\nNote: This step is required to estimate the efficiency of C>T in a conserved ac4C site in 18S rRNA. A pair of primers surrounding position 1842 in 18S rRNA is used to amplify a region that contains an ac4C site at 100% stoichiometry. The residual amount of rRNA in the libraries is enough to perform this quality control. Following PCR, amplicons are analyzed by Sanger sequencing. We typically observed \u223c50% C>T efficiency in position 1842 (Figure\u00a05[href=https://www.wicell.org#fig5]). While the PCR and amplicon purification takes \u223c2 h, sending the samples for Sanger sequencing and analyzing the data can take up to two days.\nTo 0.5\u00a0\u03bcL of libraries, add the following components and mix by gentle pipetting:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2162-Fig5.jpg\nFigure\u00a05. Sanger sequencing of 18S rRNA helix 45 after NaBH4 treatment\ntable:files/protocols_protocol_2162_16.csv\nSet the thermocycler with the following steps:\ntable:files/protocols_protocol_2162_17.csv\nPurify 18S rRNA amplicons.\nTake samples out of the thermocycler and add 45\u00a0\u03bcL (1.8\u00d7) of resuspended AMPure XP Beads.\nMix well by pipetting up and down.\nIncubate for 5\u00a0min at 22\u00b0C\u201325\u00b0C.\nSpin and place the tube on a magnetic rack for 5\u00a0min.\nDiscard the supernatant.\nAdd 200\u00a0\u03bcL of freshly prepared 80% ethanol.\nIncubate at 22\u00b0C\u201325\u00b0C for 30 s.\nRemove supernatant.\nRepeat for a total of 2 washing steps.\nSpin the tubes and put back into the magnetic rack.Completely remove the residual ethanol and air dry beads for 5\u00a0min.\nElute DNA target from the beads with 12\u00a0\u03bcL 0.1 TE buffer.\nIncubate for 2\u00a0min at 22\u00b0C\u201325\u00b0C.\nWithout disturbing the bead pellet, transfer 11\u00a0\u03bcL of the supernatant to a clean PCR tube and store at \u221220\u00b0C.\nSend samples for Sanger sequencing using the 18S rRNA helix 45 Primer F (10\u00a0\u03bcM)\u00a0\u2013 5\u2032\u00a0CGCTACTACCGATTGGATGG 3\u2032\nAlign and verify C>T conversion at the ac4C site in position 1842 (Figure\u00a05[href=https://www.wicell.org#fig5]).\nLibrary sequencing\nTiming: 2\u20136\u00a0days\nInstrument run time (HiSeq 2500): 40\u00a0h (rapid run mode) to 6\u00a0days (high output mode).\nThis step generates sequencing reads for the constructed library and should be performed by a sequencing facility. General guidance on run parameters includes:\nSequence on an Illumina HiSeq 2500 or comparable four-channel instrument (see Note) for 126\u2013150 cycles in paired-end mode (For 126\u2013150\u00a0bp paired-end reads).\nNote: Illumina\u2019s two-channel instruments have different baseline error profiles than their four-channel instruments. This protocol describes data produced with four-channel chemistry. It is critical that data that are compared are produced on the same instrument to ensure mismatch differences detected are not due to differences in chemistry.\nRead processing and alignments\nTiming: 1.5\u00a0days\nObtain raw reads from the sequencing facility.\nFor each sample, perform adapter trimming with cutadapt (v1.16).4[href=https://www.wicell.org#bib4]\nConsult the documentation (https://cutadapt.readthedocs.io/en/stable/[href=https://cutadapt.readthedocs.io/en/stable/]) to customize parameters for your dataset. For example:\ncutadapt -f fastq --match-read-wildcards --times 1 -e 0.1 -O 1 \\\n--quality-cutoff 6 -m 18 -a GATCGGAAGAGCACA -g ACGCTCTTCCGATCT \\\n-A AGATCGGAAGAGCGT -G GTGCTCTTCCGATC -o Sample1_R1_cutadapt.fastq.gz \\\n-p Sample1_R2_cutadapt.fastq.gz Sample1_R1_001.fastq.gz \\\nSample1_R2_001.fastq.gz\u00a0> Sample1.adapterTrim.metrics\nPrepare a genomic reference for mapping.\nYou can obtain genomic sequence and gene annotation from sources such as UCSC (http://hgdownload.soe.ucsc.edu/downloads.html#human[href=http://hgdownload.soe.ucsc.edu/downloads.html]) or igenomes (https://support.illumina.com/sequencing/sequencing_software/igenome.html[href=https://support.illumina.com/sequencing/sequencing_software/igenome.html]).\nFormat the reference for mapping as in this example:STAR --runMode genomeGenerate --runThreadN 8\u00a0\u2013genomeDir indexes/hg19 \\\n\u00a0\u00a0--genomeFastaFiles ref.fa --sjdbGTFfile genes.gtf --sjdbOverhang 100\nPerform the alignment using STAR (v 2.5.4.a).6[href=https://www.wicell.org#bib6]\nConsult the documentation (https://github.com/alexdobin/STAR[href=https://github.com/alexdobin/STAR]) to customize parameters for your dataset. For example:\nSTAR --runMode alignReads --runThreadN 8 --genomeDir /data/indexes/STAR/hg19 \\\n\u00a0\u00a0--genomeLoad LoadAndRemove \\\n\u00a0\u00a0--readFilesIn Sample1_R1_cutadapt.fastq.gz Sample1_R2_cutadapt.fastq.gz \\\n\u00a0\u00a0--outFilterMultimapNmax 10 --clip3pNbases 6 --clip5pNbases 6 \\\n\u00a0\u00a0--outFilterMultimapScoreRange 1 --outFileNamePrefix Sample1_genome.bam \\\n\u00a0\u00a0--outSAMattributes All --readFilesCommand zcat --outStd BAM_Unsorted \\\n\u00a0\u00a0--outSAMtype BAM Unsorted --outFilterMismatchNmax 5 --outFilterType BySJout \\\n\u00a0\u00a0--outReadsUnmapped Fastx --outFilterScoreMin 10 --outSAMattrRGline ID:foo \\\n\u00a0\u00a0--alignEndsType Local\nSort and index the alignments, merge alignments as desired.\nsamtools sort -T sort_scratch -o Sample1_genome.sorted.bam Sample1_genome.bam \\\n\u00a0\u00a0&& samtools index Sample1_genome.sorted.bam\nsamtools merge -f WT_NaBH4.bam Sample1_genome.sorted.bam Sample2_genome.sorted.bam \\\n\u00a0\u00a0&& samtools sort -T sort_scratch -o WT_NaBH4.sorted.bam WT_NaBH4.bam \\\n\u00a0\u00a0&& samtools index WT_NaBH4.sorted.bam\nVariant analysis\nTiming: 2.5\u00a0days\nNote: This step will produce a pileup, or summary of coverage and base calls by position, across samples. Conversion to interpretable read counts is performed by the mpileup2readcounts script, which enforces additional quality criteria on alignments. Before beginning, install this script by following the instructions at (https://github.com/IARCbioinfo/mpileup2readcounts[href=https://github.com/IARCbioinfo/mpileup2readcounts]). Place the executable in your working directory or a directory in your $PATH.\nRun the mpileup command and pipe to the mpileup2readcounts script.\nThis example runs this on three samples, called\u00a0\u2013 wildtype (WT) NaBH4 treated, KO (NAT10\u2212/\u2212) NaBH4 treated, and WT Untreated:\nsamtools mpileup -A -R -Q20 -C0 -d 100000 --ff UNMAP,SECONDARY,QCFAIL,DUP \\\n\u00a0\u00a0-f /data/indexes/STAR/hg19/ref.fa WT.BH4.bam KO.BH4.bam WT.Ctrl.bam \\\n\u00a0\u00a0| sed 's/ / \u2217 \u2217/g' | \\\n\u00a0\u00a0mpileup2readcounts 0 -5 true 0 0\u00a0>\u00a0mpileup_output.txt;\nOptional: To reduce downstream compute time, you may restrict output to positions with a minimum depth, with an additional pipe, as this enforces a depth of 10 in each sample:\nsamtools mpileup -A -R -Q20 -C0 -d 100000 --ff UNMAP,SECONDARY,QCFAIL,DUP \\\n\u00a0\u00a0-f /data/indexes/STAR/hg19/ref.fa WT.BH4.bam KO.BH4.bam WT.Ctrl.bam \\| sed 's/ / \u2217 \u2217/g' | \\\n\u00a0\u00a0mpileup2readcounts 0 -5 true 0 0 | \\\n\u00a0\u00a0awk '$4\u00a0>= 10\u00a0&& $15\u00a0>= 10\u00a0&& $26\u00a0>= 10'\u00a0>\u00a0mpileup_output.txt;\nParse the output to produce tidier results for comparing mismatch rates.\nThis parsing script is available at Github: https://github.com/dsturg/RedaCT-Seq[href=https://github.com/dsturg/RedaCT-Seq].\nUsage: redact_parse_script.pl [starting file] [number of samples].\nredact_parse_script.pl mpileup_output.txt 3\u00a0>\u00a0mpileup_output_parsed.txt\nQuantification and statistical analysis\nTiming: 2 h\nNote: Following the generation of base calling summaries via pileup, the next step is to load and process these data in the R environment. An example workflow with sample data is provided at Github: https://github.com/dsturg/RedaCT-Seq[href=https://github.com/dsturg/RedaCT-Seq]. The timing estimate above reflects computational run time along with consideration of diagnostic plots within the workflow. The workflow consists of 4 major steps, described below:\nCalculation of mismatch rates at each queried position, for each sample. For each mismatch relative to the reference genome, the mismatch rate is calculated as:\nMismatchRate\u00a0= MismatchCounts / depth\nProjection of genomic coordinates into transcript coordinates. Candidate converted sites are projected onto reference transcripts using functions in the Genomation, GenomicFeatures, and Rtracklayer packages.7[href=https://www.wicell.org#bib7],8[href=https://www.wicell.org#bib8],9[href=https://www.wicell.org#bib9]\ntxmapped <- mapToTranscripts(bed, exon_by_tx,ignore.strand=FALSE)\nQC and determination of candidate modified sites.\nBefore statistical testing, screening of sites is performed to ensure specificity of transcript assignment, absence of polymorphism, and mismatch rate above sequencing error:\nMismatch rate in untreated control\u00a0<\u00a01%.\nMapping to a single reference transcript.\nAbsence of multiple mismatch types at the same position.\nMismatch rate elevated relative to untreated sample.\nStatistical testing, thresholding, and exploratory plots.Note: Statistical testing is performed on mismatch and reference base calls between NaBH4 treated WT and NAT10\u2212/\u2212 samples. To perform this test, 2\u00a0\u00d7\u00a02 matrices are constructed for each relevant site, using the data: Mismatched base counts (WT), Reference base counts (WT), Mismatched base counts (NAT10\u2212/\u2212), Reference base counts (NAT10\u2212/\u2212). Fisher\u2019s Exact Tests are performed in R as:\npvalue <- fisher.test(matrix)$p.value\nFinal selection of sites uses criteria on magnitude of difference of mismatch rate (as measured by fold change), in addition to the p-value.", "Step-by-step method details\nStep-by-step method details\nPart 1: Single cell RNA seq analysis\nTiming: 1 h (for step 1 to step 9)\nIn this section, we describe essential steps to analyze scRNA-seq datasets.\nLoad datasets using Seurat package troubleshooting 4[href=https://www.wicell.org#sec5.7].\nlibrary(dplyr)\nlibrary(Seurat)\nlibrary(monocle3)\nlibrary(SeuratWrappers)\n# for plotting\nlibrary(ggplot2)\nlibrary(patchwork)\nset.seed(1234)\naPSM.matrix <- Read10X(data.dir\u00a0=\"./aPSM_scRNA/filtered_feature_bc_matrix/\")\nCreate Seurat object.\naPSM <- CreateSeuratObject(counts = aPSM.matrix, min.cells = 3, min.features = 200, project = \u201c aPSM\u201d)\nNote: Options for min.cell and min.features are selected as default values from Seurat tutorials (https://satijalab.org/seurat/articles/pbmc3k_tutorial.html[href=https://satijalab.org/seurat/articles/pbmc3k_tutorial.html]). File names should be barcodes.tsv.gz, features.tsv.gz, and matrix.mtx.gz. For Window OS, \u201c.\\\\filtered_feature_bc_matrix\\\\ can be used.\nSelect cells for the analysis through quality control (QC) (Figure\u00a02[href=https://www.wicell.org#fig2]A). Troubleshooting 5[href=https://www.wicell.org#sec5.9].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2694-Fig2.jpg\nFigure\u00a02. scRNA-seq data quality control\n(A) Violin plots of scRNA-seq data of aPSM scRNA-seq data. mRNA counts (nCount_RNA), number of detected genes(nFeature_RNA), mitochondria gene percentage (percent.mt).\n(B) Elbowplot of aPSM scRNA-seq data describing the standard deviations of the principal components (PC).\naPSM[[\"percent.mt\"]] <- PercentageFeatureSet(aPSM, pattern\u00a0= \"\u02c6mt-\")\n\u00a0\u00a0VlnPlot(aPSM, features\u00a0= c(\"nFeature_RNA\", \"nCount_RNA\", \"percent.mt\"), ncol\u00a0= 3)\nplot1\u00a0<- FeatureScatter(aPSM, feature1\u00a0= \"nCount_RNA\", feature2\u00a0= \"percent.mt\")\nplot2\u00a0<- FeatureScatter(aPSM, feature1\u00a0= \"nCount_RNA\", feature2\u00a0= \"nFeature_RNA\")\nplot1+plot2\naPSM <- subset(aPSM, subset\u00a0= nFeature_RNA\u00a0>\u00a00\u00a0& nFeature_RNA\u00a0<\u00a08000\u00a0& percent.mt\u00a0<\u00a020)\nPreprocess data and select features for the analysis.\naPSM <- NormalizeData(object\u00a0= aPSM, normalization.method\u00a0= \"LogNormalize\", scale.factor\u00a0= 1e4)\naPSM <- FindVariableFeatures(aPSM, selection.method\u00a0= \"vst\", nfeatures\u00a0= 2000)\naPSM_top10\u00a0<- head(VariableFeatures(aPSM), 10)\nplot1\u00a0<- VariableFeaturePlot(aPSM)\nplot2\u00a0<- LabelPoints(plot\u00a0= plot1, points\u00a0= aPSM_top10, repel\u00a0= TRUE)\nplot1+plot2\naPSM.all.genes <- rownames(aPSM)\naPSM <- ScaleData(aPSM, features\u00a0= aPSM.all.genes)\nFilter cell cycle genes.\nconvertHumanGeneList <- function(x){\n\u00a0\u00a0require(\"biomaRt\")\n\u00a0\u00a0human <- useMart(\"ensembl\", dataset\u00a0= \"hsapiens_gene_ensembl\" , host\u00a0= \"https://dec2021.archive.ensembl.org/\")\n\u00a0\u00a0mouse <- useMart(\"ensembl\", dataset\u00a0= \"mmusculus_gene_ensembl\" ,host\u00a0= \"https://dec2021.archive.ensembl.org/\")tmp <- getLDS(attributes\u00a0= c(\"hgnc_symbol\"), filters\u00a0= \"hgnc_symbol\", values\u00a0= x , mart\u00a0= human, attributesL\u00a0= c(\"mgi_symbol\"), martL\u00a0= mouse, uniqueRows=TRUE)\n\u00a0\u00a0mousex <- unique(tmp[,2])\n\u00a0\u00a0return(mousex)}\ns.genes <- convertHumanGeneList(cc.genes.updated.2019$s.genes)\ng2m.genes <- convertHumanGeneList(cc.genes.updated.2019$g2m.genes)\ncell_cycle <- t(read.csv(file=\"aPSM_scRNA/cell_cycle.txt\",header=F))[,1]\nfiltered_genes <- c(s.genes,cell_cycle)\nFilter cell cycle genes, reduce dimensions and establish dataset dimensionality (Figure\u00a02[href=https://www.wicell.org#fig2]B).\naPSM <- RunPCA(object\u00a0= aPSM, features\u00a0= VariableFeatures(object\u00a0= aPSM), verbose\u00a0= FALSE)\naPSM <- CellCycleScoring(aPSM, s.features\u00a0= filtered_genes, g2m.features\u00a0= g2m.genes, set.ident\u00a0= TRUE)\naPSM <- ScaleData(aPSM, vars.to.regress\u00a0= c(\"S.Score\", \"G2M.Score\"), features\u00a0= rownames(aPSM))\naPSM <- JackStraw(aPSM, num.replicate\u00a0= 100)\naPSM <- ScoreJackStraw(aPSM, dims\u00a0= 1:20)\nElbowPlot(object\u00a0= aPSM,ndims\u00a0=50)\nCluster and visualize cells (Figure\u00a03[href=https://www.wicell.org#fig3]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2694-Fig3.jpg\nFigure\u00a03. Visualization of aPSM scRNA-seq data\n(A) UMAP plot of aPSM scRNA-seq data.\n(B) Pseudotime plot of aPSM scRNA-seq data. The heatmap represents units of progress, with 1 located at the root of the trajectory.\naPSM <- FindNeighbors(object\u00a0= aPSM, dims\u00a0= 1:30)\naPSM <- FindClusters(object\u00a0= aPSM, resolution\u00a0= 0.25)\naPSM <- RunTSNE(object\u00a0= aPSM, dims\u00a0= 1:30)\naPSM <- RunUMAP(object\u00a0= aPSM, dims\u00a0= 1:30)\nDimPlot(object=aPSM,reduction='umap',label=T)+labs(title\u00a0= \" aPSM\")\nsave(aPSM,file=\"aPSM_scRNA.RData\")\nAnalyze unique features of each cluster.\naPSM.markers <- FindAllMarkers(aPSM, only.pos\u00a0= TRUE, min.pct\u00a0= 0.25, logfc.threshold\u00a0= 0.25)\naPSM.markers_table <- aPSM.markers %>%group_by(cluster) %>% slice_max(n\u00a0= 20, order_by\u00a0= avg_log2FC)\nNote: Options for min.pct and logfc.threshold are selected as default values from Seurat tutorials (https://satijalab.org/seurat/articles/pbmc3k_tutorial.html[href=https://satijalab.org/seurat/articles/pbmc3k_tutorial.html]).\nVisualize clusters pseudotime (Figure\u00a03[href=https://www.wicell.org#fig3]B).\nDefaultAssay(aPSM) <- \"RNA\"\naPSM_cds <- as.cell_data_set(aPSM)\naPSM_cds <- cluster_cells(aPSM_cds,reduction=\"UMAP\",k\u00a0= 30,resolution\u00a0= 0.00012)\naPSM_cds <- learn_graph(aPSM_cds, close_loop\u00a0= F,use_partition\u00a0= T,learn_graph_control\u00a0=list(minimal_branch_len=5))\nplot_cells(aPSM_cds, label_groups_by_cluster\u00a0= T, label_leaves\u00a0= F, label_branch_points\u00a0= T,graph_label_size\u00a0= 3)\naPSM.min.umap <- which.min(unlist(FetchData(aPSM, \"UMAP_2\")))\naPSM.min.umap <- colnames(aPSM)[aPSM.min.umap]\naPSM_cds <- order_cells(aPSM_cds, root_cells\u00a0= aPSM.min.umap)\nplot_cells(aPSM_cds, color_cells_by\u00a0= \"pseudotime\", label_cell_groups\u00a0=T, label_leaves\u00a0= F, label_branch_points\u00a0= F,show_trajectory_graph\u00a0= T,graph_label_size\u00a0= 3,label_groups_by_cluster\u00a0= T)Part 1: Single cell ATAC seq analysis\nTiming: 1 h (for step 10 to step 17)\nIn this section, we describe steps to evaluate chromatin accessibility using scATAC-seq.\nLoad datasets using Signac package.\nlibrary(Signac)\nlibrary(Seurat)\nlibrary(GenomeInfoDb)\nlibrary(EnsDb.Mmusculus.v79)\nlibrary(patchwork)\nset.seed(1234)\naPSM.counts <- Read10X_h5(\"./aPSM_scATAC/ filtered_peak_bc_matrix.h5\")\naPSM_meta <- read.table(\"./aPSM_scATAC/singlecell.csv.gz\", sep\u00a0= \",\", header\u00a0= TRUE, row.names\u00a0= 1)\naPSM_chrom_assay <- CreateChromatinAssay(\n\u00a0\u00a0counts\u00a0= aPSM.counts,\n\u00a0\u00a0sep\u00a0= c(\":\",\"-\"),\n\u00a0\u00a0genome\u00a0= 'mm10',\n\u00a0\u00a0fragments\u00a0= './aPSM_scATAC/filtered_feature_bc_matrix/fragments.tsv.gz', min.cells\u00a0= 3, min.features\u00a0= 100)\nNote: Options for min.cell and min.features are selected as default values from Signac tutorials (https://stuartlab.org/signac/articles/pbmc_vignette.html[href=https://stuartlab.org/signac/articles/pbmc_vignette.html]).\nCreate Seurat object.\naPSM_atac <- CreateSeuratObject(\ncounts\u00a0= aPSM_chrom_assay,\nassay\u00a0= 'aPSM_peaks',\nproject\u00a0= 'aPSM_atac',\nmeta.data\u00a0= aPSM_meta[colnames(aPSM_chrom_assay),])\nAdd annotation information.\nannotations <- GetGRangesFromEnsDb(ensdb\u00a0= EnsDb.Mmusculus.v79)\n# change to UCSC style since the data was mapped to mm10\nseqlevelsStyle(annotations) <- 'UCSC'\ngenome(annotations) <- \"mm10\"\n# add the gene information to the object\nAnnotation(aPSM_atac) <- annotations\nSelect cells for analysis through QC (Figure\u00a04[href=https://www.wicell.org#fig4]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2694-Fig4.jpg\nFigure\u00a04. Quality control and visualization of integration between scATAC and scRNA-seq data\n(A) Features distribution of aPSM scATAC-seq data.\n(B) UMAP plot of aPSM scATAC-seq datasets.\n(C) Features distribution between na\u00efve and instructed scRNA-seq datasets.\n(D) UMAP plots before and after Harmony integration.\naPSM_atac <- NucleosomeSignal(object\u00a0= aPSM_atac)\n# compute TSS enrichment score per cell\naPSM_atac <- TSSEnrichment(object\u00a0= aPSM_atac, fast\u00a0= FALSE)\naPSM_atac$pct_reads_in_peaks <- aPSM_atac$peak_region_fragments / aPSM_atac$passed_filters \u2217 100\naPSM_atac$blacklist_ratio <- aPSM_atac$blacklist_region_fragments / aPSM_atac$peak_region_fragments\nVlnPlot(\n\u00a0\u00a0object\u00a0= aPSM_atac,\n\u00a0\u00a0features\u00a0= c('pct_reads_in_peaks', 'peak_region_fragments',\n\u00a0\u00a0'TSS.enrichment', 'blacklist_ratio', 'nucleosome_signal'),pt.size\u00a0= 0.1, ncol\u00a0= 5)\nPreprocess data and select features for the analysis.\nFeatureScatter(aPSM_atac, feature1\u00a0= \"peak_region_fragments\", feature2\u00a0= \"nCount_aPSM_peaks\")\naPSM_atac <- subset(\n\u00a0\u00a0x\u00a0= aPSM_atac,\n\u00a0\u00a0subset\u00a0= peak_region_fragments\u00a0>\u00a02586\u00a0&\n\u00a0\u00a0\u00a0\u00a0peak_region_fragments\u00a0<\u00a020000\u00a0& pct_reads_in_peaks\u00a0>\u00a015\u00a0& blacklist_ratio\u00a0<\u00a00.05)\nncol(aPSM_atac)\nVlnPlot(\n\u00a0\u00a0object\u00a0= aPSM_atac,\nfeatures\u00a0= c('nucleosome_signal','peak_region_fragments'),pt.size\u00a0= 0.1)\u00a0+ NoLegend()FeatureScatter(aPSM_atac, feature1\u00a0= \"peak_region_fragments\", feature2\u00a0= \"nCount_aPSM_peaks\")\nReduce dimensions.\naPSM_atac <- RunTFIDF(aPSM_atac)\naPSM_atac <- FindTopFeatures(aPSM_atac, min.cutoff\u00a0= 'q0')\naPSM_atac <- RunSVD(\n\u00a0\u00a0object\u00a0= aPSM_atac, assay\u00a0= 'aPSM_peaks',\n\u00a0\u00a0reduction.key\u00a0= 'LSI_', reduction.name\u00a0= 'lsi')\nCluster and visualize cells (Figure\u00a04[href=https://www.wicell.org#fig4]B).\nlibrary(ggplot2)\naPSM_atac <- RunUMAP(object\u00a0= aPSM_atac, reduction\u00a0= 'lsi', dims\u00a0= 1:40)\naPSM_atac <- RunTSNE(object\u00a0= aPSM_atac, reduction\u00a0= 'lsi', dims\u00a0= 1:40)\naPSM_atac <- FindNeighbors(object\u00a0= aPSM_atac, reduction\u00a0= 'lsi', dims\u00a0= 1:40)\naPSM_atac <- FindClusters(object\u00a0= aPSM_atac, verbose\u00a0= FALSE,resolution=0.25)\nDimPlot(object\u00a0= aPSM_atac, label\u00a0= F,reduction\u00a0= 'umap')\u00a0+labs(title\u00a0= \" aPSM scATAC\")\nCalculate gene activities and add them to Seurat object.\naPSM_gene.activities <- GeneActivity(aPSM_atac)\nsave(aPSM_gene.activities,file=\"aPSM_atac_gene.activities.RData\")\n# add the gene activity matrix to the Seurat object as a new assay and normalize it\naPSM_atac[['RNA']] <- CreateAssayObject(counts\u00a0= aPSM_gene.activities)\naPSM_atac <- NormalizeData(\n\u00a0\u00a0object\u00a0= aPSM_atac, assay\u00a0= 'RNA', normalization.method\u00a0= 'LogNormalize',scale.factor\u00a0= median(aPSM_atac$nCount_RNA) )\nsave(aPSM_atac,file=\"aPSM_scATAC.RData\")\nPart 1: Integrated data analysis\nTiming: 1 h (for step 18 to step 19)\nIn this section, we describe steps to integrate and analyze data from different platforms. Users can infer the changes of gene expressions during time points or relations between gene expressions and chromatin accessibility during cellular differentiation.\nIntegrate single cell RNA seq datasets.\nPrepare R library for the integration.\nlibrary(dplyr)\nlibrary(Seurat)\nlibrary(harmony)\nlibrary(data.table)\nlibrary(parallel)\nset.seed(1234)\n# Set number of cores to use\nNCORES\u00a0= 1\nmeta <- fread(\"naive_instructed_esc.csv\")\nLoad datasets.\ndata_dir <- list(\"./naive_scRNA/\",\"./instructed_scRNA/\")\nmat.list <- list()\nsoupx.used <- list()\nfor(i in 1:length(data_dir)){\n\u00a0\u00a0mat.list[[i]] <- Read10X(data.dir\u00a0= paste0(data_dir[i], 'filtered_feature_bc_matrix'))\n\u00a0\u00a0soupx.used[[i]] <- F}\ncat(sum(unlist(lapply(mat.list, ncol))),\"cells (total) loaded...\\n\")\nsample_num<-min(ncol(mat.list[[1]]),ncol(mat.list[[2]]))\nsel.id<-sample(colnames(mat.list[[2]]), size=sample_num, replace=FALSE)\nmat.list[[2]]<-mat.list[[2]][,sel.id]\nNote: Files should be in filtered_feature_bc_matrix folder, and file names should be barcodes.tsv.gz, features.tsv.gz, and matrix.mtx.gz.\nCreate Seurat objects.\nseu.list <- list()\nseu.list <- mclapply(\n\u00a0\u00a0mat.list,\n\u00a0\u00a0FUN\u00a0= function(mat){\n\u00a0\u00a0\u00a0\u00a0return(CreateSeuratObject(\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0counts\u00a0= mat, min.features\u00a0= 200, min.cells\u00a0= 3,project\u00a0= 'naive_instructed_data'))}, mc.cores\u00a0= NCORES)\nfor(i in 1:length(seu.list)){\n\u00a0\u00a0cat(' ------------------------------------\\n',\n\u00a0\u00a0\u00a0\u00a0'--- Processing dataset number ', i, '-\\n',\n\u00a0\u00a0\u00a0\u00a0'------------------------------------\\n')\n\u00a0\u00a0# Add meta data\n\u00a0\u00a0for(md in colnames(meta)){\n\u00a0\u00a0\u00a0\u00a0seu.list[[i]][[md]] <- meta[[md]][i]\n\u00a0\u00a0}\n\u00a0\u00a0# add %MT\n\u00a0\u00a0seu.list[[i]][[\"percent.mt\"]] <- PercentageFeatureSet(seu.list[[i]], pattern\u00a0= \"mt-\")\n\u00a0\u00a0# Filter out low quality cells according to the metrics defined above\n\u00a0\u00a0seu.list[[i]] <- subset(seu.list[[i]],\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0subset\u00a0= nFeature_RNA\u00a0>\u00a01600\u00a0& nFeature_RNA\u00a0<\u00a08000\u00a0& percent.mt\u00a0<\u00a020)\n\u00a0\u00a0# Only mito and floor filtering; trying to find doublets\n}\ncat((sum(unlist(lapply(mat.list, ncol)))-sum(unlist(lapply(seu.list, ncol)))),\"cells (total) removed...\\n\")\nPreprocess Seurat objects.\nseuPreProcess <- function(seu, assay='RNA', n.pcs=30, res=0.25){\n\u00a0\u00a0pca.name\u00a0= paste0('pca_', assay)\n\u00a0\u00a0pca.key\u00a0= paste0(pca.name,'_')\n\u00a0\u00a0umap.name\u00a0= paste0('umap_', assay)\n\u00a0\u00a0seu\u00a0= NormalizeData(\n\u00a0\u00a0\u00a0\u00a0seu\n\u00a0\u00a0) %>% FindVariableFeatures(\n\u00a0\u00a0\u00a0\u00a0assay\u00a0= assay,\n\u00a0\u00a0\u00a0\u00a0selection.method\u00a0= \"vst\",\n\u00a0\u00a0\u00a0\u00a0nfeatures\u00a0= 2000,\n\u00a0\u00a0\u00a0\u00a0verbose\u00a0= F\n\u00a0\u00a0) %>% ScaleData(\n\u00a0\u00a0\u00a0\u00a0assay\u00a0= assay\n\u00a0\u00a0) %>% RunPCA(\n\u00a0\u00a0\u00a0\u00a0assay\u00a0= assay,\n\u00a0\u00a0\u00a0\u00a0reduction.name\u00a0= pca.name,\n\u00a0\u00a0\u00a0\u00a0reduction.key\u00a0= pca.key,\n\u00a0\u00a0\u00a0\u00a0verbose\u00a0= F,\n\u00a0\u00a0\u00a0\u00a0npcs\u00a0= n.pcs\n)\nn.pcs.use\u00a0=n.pcs\n\u00a0\u00a0# FindNeighbors %>% RunUMAP, FindClusters\n\u00a0\u00a0seu <- FindNeighbors(\n\u00a0\u00a0\u00a0\u00a0seu,\n\u00a0\u00a0\u00a0\u00a0reduction\u00a0= pca.name,\n\u00a0\u00a0\u00a0\u00a0dims\u00a0= 1:n.pcs.use,\n\u00a0\u00a0\u00a0\u00a0force.recalc\u00a0= TRUE,\n\u00a0\u00a0\u00a0\u00a0verbose\u00a0= FALSE\n\u00a0\u00a0) %>% RunUMAP(\n\u00a0\u00a0\u00a0\u00a0reduction\u00a0= pca.name,\n\u00a0\u00a0\u00a0\u00a0dims\u00a0= 1:n.pcs.use,\n\u00a0\u00a0\u00a0\u00a0reduction.name=umap.name\n\u00a0\u00a0)\n\u00a0\u00a0seu@reductions[[umap.name]]@misc$n.pcs.used <- n.pcs.use\n\u00a0\u00a0seu <- FindClusters(object\u00a0= seu,resolution\u00a0= res)\n\u00a0\u00a0seu[[paste0('RNA_res.',res)]] <- as.numeric(seu@active.ident)\n\u00a0\u00a0return(seu)\n}\n# preprocess each dataset individually\nseu.list <- lapply(seu.list, seuPreProcess)\nMerge datasets (Figure\u00a04[href=https://www.wicell.org#fig4]C).\ntmp.list <- list()\nfor(i in 1:length(seu.list)){\n\u00a0\u00a0DefaultAssay(seu.list[[i]]) <- \"RNA\"\n\u00a0\u00a0tmp.list[[i]] <- DietSeurat(seu.list[[i]], assays\u00a0= \"RNA\")\n}\n# merge tmp count matrices\n\u00a0\u00a0scMuscle.pref.seurat <- merge(\n\u00a0\u00a0\u00a0\u00a0tmp.list[[1]],\n\u00a0\u00a0\u00a0\u00a0y\u00a0= tmp.list[[2]]\n)\nVlnPlot(\n\u00a0\u00a0scMuscle.pref.seurat,\n\u00a0\u00a0features\u00a0= c(\n\u00a0\u00a0\u00a0\u00a0'nCount_RNA',\n\u00a0\u00a0\u00a0\u00a0'nFeature_RNA',\n\u00a0\u00a0\u00a0\u00a0'percent.mt'\n\u00a0\u00a0),\n\u00a0\u00a0group.by\u00a0= 'source',\n\u00a0\u00a0pt.size\u00a0= 0\n)\nPreprocess merged data.\n# Seurat preprocessing on merged data ----\nDefaultAssay(scMuscle.pref.seurat) <- 'RNA'\nscMuscle.pref.seurat <-\n\u00a0\u00a0NormalizeData(\n\u00a0\u00a0\u00a0\u00a0scMuscle.pref.seurat, assay\u00a0= 'RNA'\n\u00a0\u00a0) %>% FindVariableFeatures(\n\u00a0\u00a0\u00a0\u00a0selection.method\u00a0= 'vst',\n\u00a0\u00a0\u00a0\u00a0nfeatures\u00a0= 2000,verbose\u00a0= TRUE\n\u00a0\u00a0) %>% ScaleData(\n\u00a0\u00a0\u00a0\u00a0assay\u00a0= 'RNA',\n\u00a0\u00a0\u00a0\u00a0verbose\u00a0= TRUE\n\u00a0\u00a0) %>% RunPCA(\n\u00a0\u00a0\u00a0\u00a0assay\u00a0= 'RNA',reduction.name\u00a0= 'pca_RNA',\n\u00a0\u00a0\u00a0\u00a0reduction.key\u00a0= 'pca_RNA_',\n\u00a0\u00a0\u00a0\u00a0verbose\u00a0= TRUE,\n\u00a0\u00a0\u00a0\u00a0npcs\u00a0= 50\n\u00a0\u00a0)\nElbowPlot(scMuscle.pref.seurat, reduction\u00a0= 'pca_RNA', ndims\u00a0= 50)\nFind clusters for individual datasets.\nn.pcs\u00a0= 30\nscMuscle.pref.seurat <-\n\u00a0\u00a0RunUMAP(\n\u00a0\u00a0\u00a0\u00a0scMuscle.pref.seurat, reduction\u00a0= 'pca_RNA',\n\u00a0\u00a0\u00a0\u00a0dims\u00a0= 1:n.pcs, reduction.name='umap_RNA'\n\u00a0\u00a0) %>% FindNeighbors(\n\u00a0\u00a0\u00a0\u00a0reduction\u00a0= 'pca_RNA',\n\u00a0\u00a0\u00a0\u00a0dims\u00a0= 1:n.pcs,\n\u00a0\u00a0\u00a0\u00a0force.recalc\u00a0= TRUE,\n\u00a0\u00a0\u00a0\u00a0verbose\u00a0= F\n\u00a0\u00a0)\nscMuscle.pref.seurat <- FindClusters(object\u00a0= scMuscle.pref.seurat, resolution\u00a0= 0.25)\nscMuscle.pref.seurat[['RNA_res.0.25']] <- as.numeric(scMuscle.pref.seurat@active.ident)\nIntegrate datasets using Harmony package.\nscMuscle.pref.seurat <-\n\u00a0\u00a0scMuscle.pref.seurat %>% RunHarmony(\n\u00a0\u00a0\u00a0\u00a0group.by.vars=c('sample'), reduction='pca_RNA',\n\u00a0\u00a0\u00a0\u00a0assay='RNA',plot_convergence\u00a0= TRUE,verbose=TRUE)\nscMuscle.pref.seurat <-\n\u00a0\u00a0scMuscle.pref.seurat %>% RunUMAP(\n\u00a0\u00a0\u00a0\u00a0reduction\u00a0= 'harmony', dims\u00a0= 1:n.pcs,\n\u00a0\u00a0\u00a0\u00a0reduction.name='umap_harmony')\nscMuscle.pref.seurat@reductions$umap_harmony@misc$n.pcs.used <- n.pcs\nscMuscle.pref.seurat <-\n\u00a0\u00a0scMuscle.pref.seurat %>% FindNeighbors(\n\u00a0\u00a0\u00a0\u00a0reduction\u00a0= 'harmony',dims\u00a0= 1:n.pcs,\n\u00a0\u00a0\u00a0\u00a0graph.name\u00a0= 'harmony_snn',force.recalc\u00a0= TRUE,\n\u00a0\u00a0\u00a0\u00a0verbose\u00a0= FALSE)\nscMuscle.pref.seurat <- FindClusters(\n\u00a0\u00a0object\u00a0= scMuscle.pref.seurat,resolution\u00a0= 1.0,\n\u00a0\u00a0graph.name='harmony_snn')\nscMuscle.pref.seurat[['harmony_res.1.0']] <- as.numeric(scMuscle.pref.seurat@active.ident)\nscMuscle.pref.seurat <- FindClusters(\n\u00a0\u00a0object\u00a0= scMuscle.pref.seurat,\n\u00a0\u00a0resolution\u00a0= 2.0, graph.name='harmony_snn')\nscMuscle.pref.seurat[['harmony_res.2.0']] <- as.numeric(scMuscle.pref.seurat@active.ident)\nValidate integrated results (Figure\u00a04[href=https://www.wicell.org#fig4]D).\nlibrary(cowplot)\nlibrary(ggplot2)\np1<-DimPlot(object\u00a0= scMuscle.pref.seurat, reduction\u00a0= \"umap_RNA\", pt.size\u00a0= .1, group.by\u00a0= \"sample\")+labs(title\u00a0= \"Merged by Seurat\")\np2<-DimPlot(object\u00a0= scMuscle.pref.seurat, reduction\u00a0= \"umap_harmony\", pt.size\u00a0= .1, group.by\u00a0= \"sample\")+labs(title\u00a0= \"Merged by Seurat with Harmony\")\np1+p2\nsave(scMuscle.pref.seurat,file=\"naive_instructed_scRNA_ESCs.RData\")\nIntegrate scATAC-seq dataset with scRNA-seq dataset.\nPrepare R library for integration.\nlibrary(Signac)\nlibrary(Seurat)\nlibrary(GenomeInfoDb)\nlibrary(EnsDb.Mmusculus.v79)\nlibrary(patchwork)\nlibrary(ggplot2)\nset.seed(1234)\nLoad datasets.\nload(\"aPSM_scRNA.RData\")\nload(\"aPSM_scATAC.RData\")\nInfer relations between scRNA-seq and scATAC-seq.\nDefaultAssay(aPSM_atac) <- 'RNA'\nncol(aPSM_atac)\ntransfer.anchors <- FindTransferAnchors(\n\u00a0\u00a0reference\u00a0= aPSM, query\u00a0= aPSM_atac, k.anchor\u00a0= 20,\n\u00a0\u00a0k.filter\u00a0= 200, reduction\u00a0= 'cca', dims\u00a0= 1:30)\npredicted.labels <- TransferData(\n\u00a0\u00a0anchorset\u00a0= transfer.anchors,\n\u00a0\u00a0refdata\u00a0= aPSM$seurat_clusters,\n\u00a0\u00a0weight.reduction\u00a0= aPSM_atac[['lsi']],\n\u00a0\u00a0dims\u00a0= 2:30)\nsave(transfer.anchors,file=\"transfer.anchors_aPSM_atac.RData\")\naPSM_atac <- AddMetaData(object\u00a0=aPSM_atac, metadata\u00a0= predicted.labels)\nsave(aPSM_atac,file=\"aPSM_atac_meta.RData\")\nVisualize the clusters of the integrated datasets.\nDimPlot(object\u00a0= aPSM_atac, label\u00a0= F,reduction\u00a0= 'umap',group.by\u00a0='predicted.id' )\u00a0+labs(title\u00a0= \" aPSM scATAC\")\nDimPlot(object\u00a0= aPSM, label\u00a0= F,reduction\u00a0= 'umap')\u00a0+labs(title\u00a0= \" aPSM\")\nPart 2: Multiomics analysisTiming: 1 h (for step 20 and step 21)\nIn this section, we describe major steps on how to perform multimodal analysis.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2694-Fig5.jpg\nFigure\u00a05. Multiomics data quality control\n(A) snRNA and snATAC QC plot before removing low quality cells.\n(B) snRNA and snATAC QC plot after removing low quality cells.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2694-Fig6.jpg\nFigure\u00a06. Characterization and annotation of cell states\n(A) UMAP visualization of the clustering based on snRNA-seq, snATAC-seq, and WNN analysis before cell state annotation.\n(B) Pseudotime single cell trajectory plot. The heatmap represents units of progress, with 1 located at the root of the trajectory.\n(C) Cell states derived from pseudotime trajectory inference. State 2 and state 4 are marked NA (not assigned), since they may represent transitioning states and could not be unambiguously assigned to a specific developmental stage.\n(D) UMAP visualization of cell states after annotated clustering.\nData preprocessing.\nLoad the libraries and setup working directory.\nlibrary(Seurat)\nlibrary(Signac)\nlibrary(patchwork)\nlibrary(monocle3)\nlibrary(SeuratWrappers)\nlibrary(EnsDb.Mmusculus.v79)\nlibrary(GenomeInfoDb)\nlibrary(ggplot2)\nlibrary(dplyr)\nset.Seed(1234)\nsetwd(getwd())\nLoad snRNA and snATAC data and create Seurat object.\nStar.data <- Read10x(data.dir\u00a0= \" ./multiomics/filtered_feature_bc_matrix/\u201d)\n# Extract RNA and ATAC data\nrna_counts <- Star.data$`Gene Expression`\natac_counts <- Star.data$Peaks\n# Create Seurat object containing snRNA data\nStar <- CreateSeuratObject(counts\u00a0= rna_counts, project\u00a0= \"Star\", min.cells=5, min.features\u00a0= 100, assay\u00a0= \"RNA\")\nCritical: HIFLR_snRNA_barcodes.tsv.gz, HIFLR_snRNA_features.tsv.gz, and HIFLR_snRNA_matrix.mtx.gz are the files generated by CellRanger-arc v2.0.0. Files should be kept in the same folder, named as filtered_feature_bc_matrix.\nLoad snATAC-seq fragments files.\ngrange.counts <- StringToGRanges(rownames(atac_counts), sep\u00a0= c(\":\", \"-\"))\ngrange.use <- seqnames(grange.counts) %in% standardChromosomes(grange.counts)\natac_counts <- atac_counts[as.vector(grange.use), ]\nAdd annotation.\nannotation <- GetGRangesFromEnsDb(ensdb\u00a0= EnsDb.Mmusculus.v79)\nseqlevelsStyle(annotation) <- \"UCSC\"\ngenome(annotation) <- \"mm10\"\nCreate snATAC assay.\nfragpath <- \" ./multiomics/filtered_feature_bc_matrix/fragments.tsv.gz\"\nStar[[\"ATAC\"]] <- CreateChromatinAssay(counts\u00a0= atac_counts, sep\u00a0= c(\":\", \"-\"), genome\u00a0= 'mm10', fragments\u00a0= fragpath, min.cells\u00a0= 10, annotation\u00a0= annotation)\nDownsize the dataset.\nset.seed(111)Star <- subset(x\u00a0= Star, downsample\u00a0= 6000)\nsave(Star, file=\"Star_ds6k.RData\")\nload(\"./Star_ds6k.RData\")\nCritical: To load the snATAC-seq fragments file properly, fragments.tsv.gz.tbi file is required to be in the same folder.\nCritical: Use only peaks in standard chromosomes and set sequence level style as UCSC.\nQuality control:\nCalculate percentage of mitochondrial genes in snRNA-seq.\nCompute both TSS enrichment score and nucleosome signal metrics in Signac for snATAC-seq (Figure\u00a05[href=https://www.wicell.org#fig5]A).\nDefaultAssay(Star) <- \"RNA\"\nStar[[\"percent.mt\"]] <- PercentageFeatureSet(Star, pattern\u00a0= \"\u02c6mt-\")\nVlnPlot(Star, features\u00a0= c(\"nCount_RNA\", \"nFeature_RNA\", \"percent.mt\"), ncol\u00a0= 3, log\u00a0= TRUE, pt.size\u00a0= 0)\u00a0+ NoLegend()\nDefaultAssay(Star) <- \"ATAC\"\nStar <- NucleosomeSignal(Star)\nStar <- TSSEnrichment(object=Star, fast=FALSE)\nVlnPlot(Star, features\u00a0= c(\"nCount_ATAC\", \"nFeature_ATAC\", \"TSS.enrichment\", \"nucleosome_signal\"), ncol\u00a0= 4, log\u00a0= TRUE, pt.size\u00a0= 0)\u00a0+ NoLegend()\nNote: Low-quality cells refer to potential damaged cells, empty droplets, cell doublets, or multiplets.\nRemove low quality cells (Figure\u00a05[href=https://www.wicell.org#fig5]B).\nStar <- subset(x\u00a0= Star,\n\u00a0\u00a0subset\u00a0= nCount_RNA\u00a0<\u00a0100000\u00a0&\n\u00a0\u00a0nCount_RNA\u00a0>\u00a01200\u00a0&\n\u00a0\u00a0nCount_ATAC\u00a0<\u00a01e5\u00a0&\n\u00a0\u00a0nCount_ATAC\u00a0>\u00a01e2\u00a0&\n\u00a0\u00a0nucleosome_signal\u00a0<\u00a02.5\u00a0&\n\u00a0\u00a0TSS.enrichment\u00a0>\u00a03\u00a0&\n\u00a0\u00a0Percent.mt\u00a0<\u00a010)\nsaveRDS(Star, file=\"Star.RData\")\nVlnPlot(Star, features\u00a0= c(\"nCount_RNA\", \"nFeature_RNA\", \"percent.mt\"), ncol\u00a0= 3, log\u00a0= TRUE, pt.size\u00a0= 0)\u00a0+ NoLegend()\nVlnPlot(Star, features\u00a0= c(\"nCount_ATAC\", \"nFeature_ATAC\", \"TSS.enrichment\", \"nucleosome_signal\"), ncol\u00a0= 4, log\u00a0= TRUE, pt.size\u00a0= 0)\u00a0+ NoLegend()\nCritical: The filtering criteria are dataset specific. Chose a cutoff to avoid losing unique cell populations or to include noisy cells.\nWNN analysis.\nPerform normalization and dimensional reduction of snRNA-seq and snATAC-seq assays independently and individually.\n# snRNA analysis\nDefaultAssay(Star) <- \"RNA\"\nStar <- SCTransform(Star, verbose\u00a0= FALSE) %>% RunPCA() %>% RunUMAP(dims\u00a0= 1:30, reduction.name\u00a0= 'umap', reduction.key\u00a0= 'UMAP_')\n# snATAC analysis\nDefaultAssay(Star) <- \"ATAC\"\nStar <- RunTFIDF(Star)\nStar <- FindTopFeatures(Star, min.cutoff\u00a0= 'q0')\nStar <- RunSVD(Star)\nStar <- RunUMAP(Star, reduction\u00a0= 'lsi', dims\u00a0= 2:30,reduction.name\u00a0= \"umap.atac\", reduction.key\u00a0= \"atacUMAP_\")\nNote: In snATAC-seq assay, the first dimension is typically correlated with sequencing depth rather than biological variation. It is thus excluded in the UMAP computing.\nLearn cell-specific modality weights and construct a WNN graph.\nStar <- FindMultiModalNeighbors(Star, reduction.list\u00a0= list(\"pca\", \"lsi\"),dims.list\u00a0= list(1:30, 2:30))\nStar <- RunUMAP(Star, nn.name\u00a0= \"weighted.nn\",\n\u00a0\u00a0reduction.name\u00a0= \"umap.wnn\", reduction.key\u00a0=\"wnnUMAP_\")\nStar <- FindClusters(Star, graph.name\u00a0= \"wsnn\",\n\u00a0\u00a0resolution\u00a0= 0.8, algorithm\u00a0= 3, verbose\u00a0= FALSE)\nVisualize the clusters. (Figure\u00a06[href=https://www.wicell.org#fig6]A).\np1\u00a0<- DimPlot(Star, reduction\u00a0= \"umap\", group.by\u00a0= \"seurat_clusters\", label\u00a0= TRUE, label.size\u00a0= 8, repel\u00a0= TRUE)\u00a0+ ggtitle(\"RNA\")\np2\u00a0<- DimPlot(Star, reduction\u00a0= \"umap.atac\",group.by\u00a0= \"seurat_clusters\", label\u00a0= TRUE, label.size\u00a0= 8, repel\u00a0= TRUE)\u00a0+ ggtitle(\"ATAC\")\np3\u00a0<- DimPlot(Star, reduction\u00a0= \"umap.wnn\", group.by\u00a0= \"seurat_clusters\", label\u00a0= TRUE, label.size\u00a0= 8, repel\u00a0= TRUE)\u00a0+ ggtitle(\"WNN\")\np1+p2+p3\u00a0&NoLegend()\nsnRNA-seq analysis: Characterization and annotation of cell states are achieved by identifying marker genes via differential expression analysis in both pseudotemporal ordering identified clusters and WNN clusters. Cell types are defined using known gene markers. For example, Myod1, Myog, and Myf5 are myogenic markers and Ascl1, Neurod4, and Nhlh1 are neurogenic markers. Pax7 drives both myogenesis and neurogenesis. Meis1 and Pbx1 are anterior presomitic mesoderm (aPSM) markers. As an example, here we analyze myogenic genes Myod1 and Myog.\nFind markers.\nDefaultAssay(Star) <- \"RNA\"\nStar.rna.markers <- FindAllMarkers(Star, assay\u00a0= \"RNA\", only.pos\u00a0= TRUE, min.pct\u00a0= 0.25, logfc.threshold\u00a0= 0.25)\nStar.rna.markers %>%\n\u00a0\u00a0group_by(cluster) %>%\n\u00a0\u00a0top_n(n\u00a0= 2, wt\u00a0= avg_log2FC)\nAdd cell states annotations.\nStar <- RenameIdents(Star, '10'\u00a0= 'cell_5','11'\u00a0= 'cell_2')\nStar <- RenameIdents(Star, '5'\u00a0= 'cell_4','6'\u00a0= 'cell_1','7'\u00a0= 'cell_1','8'\u00a0= 'cell_3','9'\u00a0= 'cell_5')\nStar <- RenameIdents(Star, '0'\u00a0= 'cell_1','1'\u00a0= 'cell_2','2'\u00a0= 'cell_1','3'\u00a0= 'cell_1','4'\u00a0= 'cell_3')\nStar$celltype <- Idents(Star)Critical: Cell states can be assigned with known markers. Writing the Star.rna.markers into a file and studying the markers potentially used to annotate the clusters would be recommended.\nVisualize the cell states (Figure\u00a06[href=https://www.wicell.org#fig6]D).\np1\u00a0<- DimPlot(Star, reduction\u00a0= \"umap\", group.by\u00a0= \" celltype\", label\u00a0= FALSE, label.size\u00a0= 8, repel\u00a0= TRUE)\u00a0+ ggtitle(\"RNA\")\np2\u00a0<- DimPlot(Star, reduction\u00a0= \"umap.atac\",group.by\u00a0= \" celltype\", label\u00a0= FALSE, label.size\u00a0= 8, repel\u00a0= TRUE)\u00a0+ ggtitle(\"ATAC\")\np3\u00a0<- DimPlot(Star, reduction\u00a0= \"umap.wnn\", group.by\u00a0= \" celltype\", label\u00a0= FALSE, label.size\u00a0= 8, repel\u00a0= TRUE)\u00a0+ ggtitle(\"WNN\")\np1+p2+p3\nsnATAC-seq analysis.\nLoad libraries.\nlibrary(chromVAR)\nlibrary(motifmatchr)\nlibrary(JASPAR2020)\nlibrary(TFBSTools)\nlibrary(BSgenome.Mmusculus.UCSC.mm10)\nFind snATAC markers.\nDefaultAssay(Star) <- \"ATAC\"\nStar.atac.markers <- FindAllMarkers(Star, assay\u00a0= \"ATAC\", only.pos\u00a0= TRUE, min.pct\u00a0= 0.25, logfc.threshold\u00a0= 0.25)\nStar.atac.markers %>%\n\u00a0\u00a0group_by(cluster) %>%\n\u00a0\u00a0top_n(n\u00a0= 2, wt\u00a0= avg_log2FC)\nAdd motif information.\npwm_set <- getMatrixSet(x\u00a0= JASPAR2020, opts\u00a0= list(collection\u00a0= \"CORE\", tax_group\u00a0= 'vertebrates', all_versions\u00a0= FALSE))\nStar <- AddMotifs(\n\u00a0\u00a0object\u00a0= Star,\n\u00a0\u00a0genome\u00a0= BSgenome.Mmusculus.UCSC.mm10,\n\u00a0\u00a0pfm\u00a0= pwm_set,\n\u00a0\u00a0assay=\"ATAC\")\nComputer motif activities.\nStar <- RunChromVAR(\n\u00a0\u00a0object\u00a0= Star,\n\u00a0\u00a0genome\u00a0= BSgenome.Mmusculus.UCSC.mm10)\nPart 2: Data visualization\nTiming: 1 h (for step 22)\nIn this section, we describe steps to do data visualization.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2694-Fig7.jpg\nFigure\u00a07. Visualization of myogenic cells\n(A) Individual and paired-plots expression of Myod1 and Myog in cell states derived from pseudotime trajectory inference.\n(B) Myod1 and Myog footprinting profile in aPSM, neurogenic and myogenenic clusters.\nPseudotime analysis:\nConvert Seurat object to Monocle object.\nDefaultAssay(Star) <- \"RNA\"\nset.seed(22)\ncds <- SeuratWrappers::as.cell_data_set(Star, assay\u00a0= \"RNA\", reduction\u00a0= \"umap\", group.by\u00a0= \"celltype\")\ncds@rowRanges@elementMetadata@listData[[\"gene_short_name\"]] <- rownames(Star[[\"RNA\"]])\nCreate CDS object.\ncds <- preprocess_cds(cds, method\u00a0= \"PCA\")\ncds <- reduce_dimension(cds, preprocess_method\u00a0= \"PCA\",umap.n_neighbors= 14L, reduction_method\u00a0= \"UMAP\")\ncds <- cluster_cells(cds, reduction_method\u00a0= \"UMAP\")\ncds <- learn_graph(cds, use_partition\u00a0= FALSE, close_loop\u00a0= FALSE)Set the root with Seurat clusters 0 and order cells.\ncell_ids <- colnames(cds)[Star$seurat_clusters\u00a0== \"0\"]\nclosest_vertex <- cds@principal_graph_aux[[\"UMAP\"]]$pr_graph_cell_proj_closest_vertex\nclosest_vertex <- as.matrix(closest_vertex[colnames(cds), ])\nclosest_vertex <- closest_vertex[cell_ids, ]\nclosest_vertex <- as.numeric(names(which.max(table(closest_vertex))))\nmst <- principal_graph(cds)$UMAP\nroot_pr_nodes <- igraph::V(mst)$name[closest_vertex]\nrowData(cds)$gene_name <- rownames(cds)\nrowData(cds)$gene_short_name <- rowData(cds)$gene_name\ncds <- order_cells(cds, root_pr_nodes\u00a0= root_pr_nodes)\nVisualize trajectory plot (Figure\u00a06[href=https://www.wicell.org#fig6]B).\nplot_cells(cds, color_cells_by\u00a0= \"pseudotime\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0label_cell_groups\u00a0=T, label_leaves\u00a0= F,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0label_branch_points\u00a0= F,show_trajectory_graph\u00a0= T,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0graph_label_size\u00a0= 3, label_groups_by_cluster\u00a0= T)\nVisualize cell states derived from trajectory inference (Figure\u00a06[href=https://www.wicell.org#fig6]C).\nplot_cells(cds, color_cells_by\u00a0= \"cluster\", cell_size\u00a0= 1,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0label_cell_groups\u00a0= TRUE, group_label_size\u00a0= 4,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0show_trajectory_graph\u00a0= FALSE,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0label_branch_points\u00a0= FALSE,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0label_roots\u00a0= FALSE,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0label_leaves\u00a0= FALSE)\nVisualize paired-plots expression of Myod1 and Myog (Figure\u00a07[href=https://www.wicell.org#fig7]A).\nStar.seur <- as.Seurat(cds, assay\u00a0= NULL, clusters\u00a0= \"UMAP\")\nStar.seur <- AddMetaData(Star.seur,metadata= cds@principal_graph_aux$UMAP$pseudotime,\ncol.name\u00a0= \"monocle3_pseudotime\")\nFeaturePlot(Star.seur,features\u00a0= c(\"Myod1\",\"Myog\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0reduction\u00a0=\"UMAP\",combine\u00a0= T,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0blend\u00a0= TRUE, blend.threshold\u00a0= 0.0,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0min.cutoff\u00a0= 0,max.cutoff\u00a0= 6)\nVisualize Footprinting plots (Figure\u00a07[href=https://www.wicell.org#fig7]B).\nStar_135\u00a0<- subset(x\u00a0= Star, idents\u00a0= c(\"cell_1\", \"cell_3\", \"cell_5\"), invert\u00a0= FALSE)\nDefaultAssay(Star_135) <- \"ATAC\"\nStar_135\u00a0<- Footprint(\n\u00a0\u00a0object\u00a0= Star_135,\n\u00a0\u00a0motif.name\u00a0= c(\"MYOG\", \"MYOD1\"),\n\u00a0\u00a0genome\u00a0= BSgenome.Mmusculus.UCSC.mm10)\nPlotFootprint(Star_135, features\u00a0= c(\"MYOD1\"))\u00a0+ patchwork::plot_layout(ncol\u00a0= 1)\nPlotFootprint(Star_135, features\u00a0= c(\"MYOG\"))\u00a0+ patchwork::plot_layout(ncol\u00a0= 1)\nNote: Cell_1 is aPSM cells, Cell_3 is a neurogenic cluster, and Cell_5 is a myogenic cluster.", "Step-by-step method details\nStep-by-step method details\nSwitching to custom model generation\nTiming: \u00a0<1\u00a0min (for step 1)\nSiliFish provides the models generated by our previous study,1[href=https://www.wicell.org#bib1] which are referred to as predefined models throughout this protocol. These models are accessible through the Single Coil, Double Coil, and Beat and Glide radio buttons at the top of the main window (Figure\u00a01[href=https://www.wicell.org#fig1]). These predefined models have very limited customization. To showcase the full features of SiliFish, the steps of generation of fully customizable Custom Models is explained in this protocol. The following step shows how to switch to custom model creation mode.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig1.jpg\nFigure\u00a01. Switching to custom model generation\nThe software can be switched between predefined models (Single coil, double coil, and beat and glide) and custom model creation using the radio buttons on the top of the window.\nClick on the Custom radio button at the top to switch the software to Custom Model mode (Figure\u00a01[href=https://www.wicell.org#fig1]).\nNote: Switching between predefined and custom models will not cause you to lose data. However, clicking the Clear Model link will. Caution: Currently, there is no undo in SiliFish.\nNote: There are customizable versions of the predefined models that are saved as JSON files4[href=https://www.wicell.org#bib4]\u00a0and are located in the DataSet 1.0[href=https://github.com/Bui-lab/SiliFish/tree/main/DataSet%201.0] folder on GitHub. (JSON is a commonly used human-readable text-based file format). In the program, you can click on the Load Model link and do the following steps on an already created model rather than creating from scratch, which will expedite the learning curve.\nSetting general parameters\nTiming: \u00a0<5\u00a0min\nThe following section describes the general parameters related to the physical dimensions of the body and the spinal cord the user can define.\nThe General tab contains information on the size and structure of the model animal studied (Figure\u00a02[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig2.jpgFigure\u00a02. Setting general parameters\nThe General tab contains information about the model (like name and description) and the size of the model animal.\nDefine Model Name and Description.\nDefine body size.\nNote: There is no specific unit of measure (UoM) used within SiliFish in terms of length. The user can select any unit that is appropriate but needs to use the same UoM across the software for any length and speed measures. It is recommended to consider both the dimensions of the animal, the minimum distance between two cells, and the conduction velocity of the connections to determine the appropriate UoM. As many of the conductance velocities in the literature are reported as m/s and SiliFish uses ms as the time unit, mm would be an appropriate starting point.\nNote: The data types used in the code allow a minimum of 15 digits precision with an approximate range of\u00a0\u00b15.0\u00a0\u00d7\u00a010\u2212324 to\u00a0\u00b11.7\u00a0\u00d7\u00a010308. Given appropriate UoM will prevent exceeding these limits.\nNote: Depending on how the spatial distributions are defined in Creating Cell Pools and Connections, the spinal cord and the body can be considered an elliptic cylinder or a rectangular prism.\nNote: The representation of the spinal cord and the body as an elliptic cylinder or a rectangular prism is an oversimplification intended to keep data entry relatively easy in the user interface. A more anatomically correct distribution can be modeled by entering absolute coordinate values in Creating Cell Pools and Connections.\nDorsal-Ventral: Enter the value of the size of the fish body in the dorsal-ventral axis.\nMedial-Lateral: Enter the value of the size of the fish body from midline to the most lateral point.\nDefine the spinal cord size.\nDorsal-Ventral: Enter the value of the size of the spinal cord in the dorsal-ventral axis.Medial-Lateral: Enter the value of the size of the spinal cord from midline to the most lateral point.\nRostro-Caudal: Enter the length of the spinal cord.\nBody Position: Enter the distance of the spinal cord from the ventral region of the body.\nSetting dynamic parameters\nTiming: \u00a0<5\u00a0min\nThe following section explains how to update the values that will define the dynamics of the simulation, how cells will behave to a certain stimulus, and the timing of the propagation of electrical activities from one cell to another.\nThe Dynamic tab includes parameters relevant to the cellular dynamics of the cells or the model in general, like conduction velocity and reversal potentials.\nConduction Velocity: Enter the default conduction velocity down the axon of a neuron.\nNote: The conduction velocity is used to calculate the time it takes for a change in membrane potential at the cell body to propagate without attenuation to the presynaptic terminal. The unit of measure is the UoM used to define the length values in Setting General Parameters, divided by time (ms). As explained in Creating Cell Pools and Connections, this value can be overridden in specific neuronal groups.\nE_ach, E_glu, E_gly, and E_gaba: Enter the reversal potentials of acetylcholine, glutamate, glycine, and GABA, respectively.\nNote: As explained in Creating Cell Pools and Connections, these values can be overridden in specific neuronal groups. The UoM of reversal potentials is assumed to be mV.\nCreating cell pools and connections\nTiming: Depends on the complexity of the model\nThe following section explains how to define cell populations and connections between specific cells.SiliFish allows the definition of different neuronal and muscle cell groups, called cell pools, with different intrinsic properties, distribution patterns, and activity timelines. Furthermore, each cell pool can have electrical (gap) and/or synaptic (chemical) connections to themselves or other cell pools. The Cell Pools\u00a0& Connections tab provides the user interface to create these cell pools and connections.\nDefining cell pools and connections are the most crucial part of the model definition.\nCreate a cell pool: By right-clicking on the mouse on the Cell Pools panel and selecting Add Item menu, open the Cell Pool edit form (Figures\u00a03[href=https://www.wicell.org#fig3] and 4[href=https://www.wicell.org#fig4]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig3.jpg\nFigure\u00a03. Creating cell pools\u00a0& connections\nThe Cell Pools\u00a0& Connections tab lists all the defined cell pools and the projections between them. By right-clicking the mouse, you can access different menu items to create, edit, delete the cell pools, or change how they are displayed in the list. See also Figures\u00a04[href=https://www.wicell.org#fig4] and 9[href=https://www.wicell.org#fig9].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig4.jpg\nFigure\u00a04. Editing a cell pool\nCell Pool edit form is where the user can define the cells\u2019 intrinsic properties, spatial distributions, and activation timelines. See also Figures\u00a05[href=https://www.wicell.org#fig5], 6[href=https://www.wicell.org#fig6], and 7[href=https://www.wicell.org#fig7].\nEnter a unique group name for the cell pool.\nOptional: Enter the description field.\nSelect whether the cell type is a neuron or a muscle cell.\nOptional: If the neuron is selected as the cell type, you can set the neuronal class (glutamatergic, GABAergic, etc.). This selection will allow the default reversal potentials to be used while creating the synapses.\n# of cells: Define the number of cells within a cell pool.\nNote: Number of cells can be defined as a total number for the model or as a number per somite.2D Model Column is a multiplier of the y-axis used in creating a 2D plot, as explained in expected outcomes[href=https://www.wicell.org#expected-outcomes].\nNote: 2D Model Column value is only for display purposes and does not affect the placement of the cells.\nSelect the color that will be used to represent these cells in 2D and 3D models, as well as plots.\nNote: Using different colors for interacting cells will make it easier to understand the visual results. For example, if a cell is receiving multiple incoming currents from different cells, the plot displaying the incoming currents will show each current with the color of the source cell pool. The cell pool based coloring will allow reading the plots quickly.\nSet whether the cell is active or not.\nNote: It is possible to deactivate a cell pool without removing it from the model to run some quick tests.\nEnter the spatial distribution of the cells to create more realistic models (Figures\u00a05[href=https://www.wicell.org#fig5] and 6[href=https://www.wicell.org#fig6]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig5.jpg\nFigure\u00a05. Types of distributions\nMany values in SiliFish can be defined as a distribution (Uniform, Gaussian, Bimodal, Equally Spaced) or as a specific number.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig6.jpg\nFigure\u00a06. The Y-Z distribution modes\n(A and B) The Y and Z axes distributions can be defined as (A)\u00a0a linear distribution or (B)\u00a0an angular distribution. The panels A and B are two different modes of the form based on the Linear/Angular selection.\nNote: Make sure you use the same UoM for distances and spatial distributions throughout the data entry.\nNote: As there are many options in creating spatial distributions, it is recommended that the 3D model is frequently checked, as explained in the expected outcomes[href=https://www.wicell.org#expected-outcomes], to make sure the settings are entered properly.Note: If the spatial distribution you are trying to achieve cannot be defined with the available options, you can divide the distribution into subparts and create multiple cell pools with the same intrinsic properties and connectivity patterns.\nNote: For the spatial placement of the cells, different distributions can be selected to determine the cell\u2019s coordinates in each axis (Figure\u00a05[href=https://www.wicell.org#fig5], shown for the x-axis). When constant distribution is selected, all the cells in the cell pool are assigned the same value in the given axis. When uniform distribution is selected, the cells are placed randomly using uniform distribution. You can define the minimum and maximum values the specific axis can take. When equally spaced distribution is selected, the cells are placed at the same intervals within the given minimum and maximum range. When Gaussian distribution is selected, the cells will be placed with a normal distribution, with the given mean and standard deviation values. The cells can also be placed in a bimodal distribution with the given mean and standard deviation values.\nOptional: If constant or equally spaced distributions are selected, you can also define a noise parameter. This optional noise value will be used as a standard deviation for a Gaussian distribution of mean one, which will, in turn, be a multiplier to the value generated by the distribution.\nX-Axis Distribution: Enter the rostro-caudal distribution pattern of the cells in the cell pool.\nNote: Using the Absolute/Percentage selection, you can select whether the x-values will be absolute coordinates or will be calculated by the maximum length of the axis, which is the spinal cord\u2019s rostral-caudal length defined in Setting General Parameters.\nY-Z Distribution: Enter the medial-lateral (y-axis) and dorsal-ventral (z-axis) distribution pattern of the cells in the cell pool.Note: The spatial distributions of the cells in the dorsal-ventral and medial-lateral axes can be defined in two ways: Linear or Angular (Figure\u00a06[href=https://www.wicell.org#fig6]). The linear distributions within the Y-Axis (Medial-Lateral) and Z-Axis (Dorsal-Ventral) are defined similarly to the X-Axis distribution explained above. If the percentage option is selected, the neurons will be placed within the spinal cord, whereas the muscle cells will be placed within the body. You can define the coverage\u2019s angle and radius range for angular distributions. The angle is 0\u00b0 in the most dorsal region and 180\u00b0 for the most ventral region. The distribution is defined only for half of the body. Using the sagittal position dropdown, you can define whether the cells exist on the left, right, or both sides.\nNote: In the current version of SiliFish GUI, only spinal neurons are considered and will be placed within the spinal cord. However, it is possible to simulate peripheral neurons by giving absolute coordinates rather than percentages.\nNote: SiliFish GUI allows the definition of only one medial-lateral or dorsal-ventral range of the body. However, the spinal cord of zebrafish is known to vary in size along the rostro-caudal axis. With this non-uniform size distribution in mind, SiliFish does not check whether a coordinate is within the cylindrical or prismatic boundaries defined by the medial-lateral or dorsal-ventral ranges that is inputted by the user. For example, even if the user defines absolute y and x-axis values outside of this medial-lateral or dorsal-ventral range, SiliFish would run without any issues. Thus, the user defined medial-lateral and dorsal-ventral ranges are used in combination with the Percentage option to facilitate assignment of coordinates based on some user-defined reference ranges.\nYou can define the intrinsic properties of the cells within the pool through the Dynamics tab.Note: Throughout SiliFish, specific UoMs are used for consistency: All membrane potentials (V)\u00a0are in millivolts (mV), current (I)\u00a0is in picoAmperes (pA), resistance (R) is in gigaohms (G\u03a9), capacitance (C) is in picoFarads (pF).\nIf the conduction velocity of a specific cell pool is different from the rest of the model, it can be overridden here.\nNote: The conduction velocity can be a specific number or defined as a distribution. If not entered, the conduction velocity set at the Setting Dynamic Parameters will be used.\nEnter the Izhikevich values (for neurons only) to define the membrane potential dynamics of the cell1[href=https://www.wicell.org#bib1],5[href=https://www.wicell.org#bib5] (Figure\u00a07[href=https://www.wicell.org#fig7]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig7.jpg\nFigure\u00a07. Setting dynamical properties of neurons\nSiliFish uses Izhikevich simple neuronal model for neurons. The parameters can be entered in the Dynamics tab on the Cell Pool edit form. All parameters can be defined as a specific number or a distribution to add some randomization to the model.\n  C   d V   d t   = k \u2217  (  V \u2212  V r   )  \u2217  (  V \u2212  V t   )  \u2212 u + I  \n     d u   d t   = a \u2217  (  b \u2217 ( V \u2212  V r   )  \u2212 u  )  \n  i f  V >   V max  \u2192 V = c ; u = u + d  \nNote: The Izhikevich model shows how the membrane potential (V)\u00a0and the feedback current (u)\u00a0change across a certain incoming current I:V is the membrane potential that changes throughout the simulation. u is the recovery variable that allows the relaxation of the cell after an action potential. The UoM of u is pA. The initial values of V and u can be set through the grid (Figure\u00a07[href=https://www.wicell.org#fig7]). Parameters a and b represent the time scale and the sensitivity of u, respectively. They are unitless within the formalism used by the Izhikevich model. The membrane potential V resets to value c (in mV) after a spike. Similarly, recovery variable u is incremented by the value d. Vmax represents the maximum membrane potential at the time of a spike, and Vr is the resting membrane potential. Vt is the threshold potential for a spike. All membrane potentials are in mV. k is an approximation of the subthreshold region of the fast component of the I-V relationship of the neuron. Finally, Cm is the membrane capacitance (pF).\nLeaky Integrator values (for muscle cells only): Enter the resistance and capacitance values of the muscle cells.\nNote: SiliFish uses the leaky-integrator model for muscle cells, where R and C are the resistance and the capacitance of the muscle cells, respectively. The UoM of resistance and capacitance are G\u03a9 and pF, respectively.\nNote: It is possible to see how a neuron behaves to a specific stimulus or calculate its rheobase value using the Test Dynamics tool (Figure\u00a08[href=https://www.wicell.org#fig8]). This feature is beneficial in generating cell populations that mimic the firing behavior observed in other studies in the literature.Optional: It is possible to define a timeline with one or more start and end times through the Timeline tab. The cells will be silenced outside these time windows. If no timeline is entered, the cells will be active throughout the run. This option allows users to investigate the effects of silencing specific populations of cells on circuit and motor activity.\nNote: For complex models, deactivating cells can also help visualize them in 2D and 3D models. Inactive cells and cell pools are not displayed in the 2D and 3D models. Temporary deactivation can help complex models to be more manageable.\nNote: Creating the cell pools with the right parameters is crucial in generating a successful model. As there are many parameters that can be set in creating a cell pool, it can be very time-consuming. The Create Copy menu item accessible through the mouse right-click on the Cell Pools\u00a0& Connections tab (Figure\u00a03[href=https://www.wicell.org#fig3]) allows for creating a copy of an existing cell pool, which helps create similar cell populations. Duplicating a cell pool using Create Copy option will also create corresponding duplicates of its connections.\nNote: If different models share cell pools with similar characteristics, you can save a cell pool using the Save Pool link (Figure\u00a04[href=https://www.wicell.org#fig4]) in one model and load it back in another.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig8.jpg\nFigure\u00a08. Test dynamics form\nTest Dynamics form allows the user to calculate the rheobase of a neuron and test how it behaves to a specific stimulus.\nAfter defining the cell pools, define the projections from one cell pool to another by mouse right-clicking and selecting Add Item on the Connections list (Figure\u00a03[href=https://www.wicell.org#fig3]). A window to edit the projection properties will open (Figure\u00a09[href=https://www.wicell.org#fig9]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig9.jpg\nFigure\u00a09. Defining projectionsThe user can define multiple features of a connection from one cell pool to another: the connection type and weight, the intrinsic properties, the reach information, and the activation timeline.\nSelect the Source Pool and Target Pool.\nNote: In the case of a gap junction, which cell pool is defined as the source pool or the target pool is not important.\nAxon Reach: Enter whether the connections are ipsilateral, contralateral, or bilateral.\nDepending on source and target pool cell types, select whether the connection type is a gap junction, a synapse, or a neuromuscular junction.\nDefine how the distance between two cells will be calculated.\nNote: There are two distance modes you can select from: Euclidean and Manhattan. In Euclidean mode, the distance between the two points is calculated as the shortest path between the two. In Manhattan distance, the sum of x, y, and z distances is used.\nNote: Manhattan distance calculation may be helpful, for example, in the case of a neuron projecting first in the dorsal-ventral axis and then contralaterally.\nTo add some randomization, you can define the probability of the presence of the connection between two cells.\nEnter the maximum conductance value (in nS) for the connection using the weight field.\nSiliFish will automatically populate the name of the connection. You can override this value if you prefer to use another naming convention.\nNote: If you change the name of a connection, SiliFish will stop automatic naming for this connection.\nOptional: You can enter a description of the details of the cell pool.\nEnter the limits of the reach of the connection.Note: In the case of the model consisting of somites, you can define whether the connection can be within the same or to a different somite. The minimum and maximum reach fields define the possible extent of the connection. The ascending and descending reach fields define the maximum extent in the spinal axis (the x-coordinate).\nOptional: The time it takes for a potential change from one cell to another is calculated as\u00a0distance/conduction velocity. If the model requires a delay on top of this calculation, you can define it here. You can also enter a fixed duration, in which case the distance, the conduction velocity, and the delay values will be obsolete. All durations are assumed to be in ms.\nIf the connection is a chemical synapse, the time course of the current is a sum of two exponentials. Enter the following synapse parameters: drop and rise decay times (\u03c4d and \u03c4r), the threshold membrane potential (Threshold V), and the reversal potential (E rev).\nOptional: It is possible to define a timeline with one or more start and end times. The connections will be inactive in the times outside these time windows. The connections will be active throughout the run if no timeline is entered.\nOptional: You can deactivate a connection by unchecking the Active check box on the connection edit form. Deactivation can help test different scenarios without removing the connections from the model.\nDefining stimuli\nTiming: \u00a0<5\u00a0min\nThe following section explains how electric stimuli can be applied to cells in the model.\nIf the model requires an external stimulation to be applied, you can define them by clicking the Stimuli tab, right-clicking to open the pop-up menu, and selecting Add Item (Figure\u00a010[href=https://www.wicell.org#fig10]). The stimulus edit form will open (Figure\u00a011[href=https://www.wicell.org#fig11]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig10.jpg\nFigure\u00a010. Defining stimuliThe Stimuli tab lists all the defined stimuli. You can access different menu items to create, edit, or delete a stimulus by right-clicking the mouse. See also Figure\u00a011[href=https://www.wicell.org#fig11].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig11.jpg\nFigure\u00a011. Stimulus edit form\nThe stimulus edit form is where the user can define a stimulus\u2019s amount, timing, and target.\nYou can define stimuli that will be applied to one or multiple cells or the whole cell population within a cell pool.\nEnter the stimulus type (step, ramp, or Gaussian) and corresponding values: stimulus value for the step stimulus, the start and finish values for the ramp stimulus, and the mean and the standard deviation values for the Gaussian stimulus.\nEnter the cells to which the stimulus will be applied.\nNote: A stimulus can be applied to the desired cells located in all somites, a single somite, or a range of somites (e.g., 2\u20137). Similarly, within a somite, the stimulus can be applied to all cells, a single cell, or a range of cells.\nNote: The UoM of all stimuli and currents are in picoamperes (pA).\nOptional: It is possible to define a timeline with one or more start and end times. The stimulus will be zero in the times outside these time windows. Otherwise, the stimulus will be active throughout the run if no timeline is entered.\nOptional: You can deactivate a stimulus by unchecking the Active check box on the stimulus edit form. This can help test different scenarios without removing the stimulus from the model.\nSetting simulation parameters\nTiming: \u00a0<1\u00a0min\nThe following section explains how to set the length of the simulations.In the Simulation Parameters section, enter how long the model will be run (Time End\u00a0\u2013 in ms) and how much time will be used to wait for the initial conditions to subside (Skip\u00a0\u2013 in ms) (Figure\u00a012[href=https://www.wicell.org#fig12]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig12.jpg\nFigure\u00a012. Setting simulation parameters\nYou can enter the simulation duration, the time that will be skipped for initial conditions to subside, the unit of time used in calculations, and how many times the simulation will be run.\nNote: The simulation results within the first period that is skipped will not be visible in any of the plots or animations.\nNote: To decide the right amount of time to skip, you can run the model for a short amount of time without any stimulus and look at the plots to see when the model comes to an equilibrium state.\nOptional: By default, SiliFish uses 0.1\u00a0ms as the unit of time in running the iterations in the model. If the model requires a faster or slower response time for the cells, you can set the \u0394t parameter.\nOptional: It is possible to run a simulation multiple times to collect data for statistical analysis. If certain parameters are probabilistically assigned, the parameter values will be reassigned in each run. To run a simulation multiple times, you need to check the Multiple check box and enter the number of times you want to run the simulation for. The cellular information of each run and the episode information will be saved as JSON and CSV files in the output folder.\nNote: Depending on the complexity of the model and the duration, multiple runs may take a long time. Make sure the model is running to your expectation before working on statistical analysis.Note: When the simulation is run multiple times, you can only see the plots of the last run.\nSave the model as a human-readable JSON file using the Save Model button for future use.\nNote: Some changes may be easier done on the model JSON file than using the SiliFish UI. If you want to change all of the noise standard deviation values of the X-coordinates, for example, a text editor's find/replace feature will be faster than opening up every cell group and updating its X-distribution parameters. Instead, you can save the model as a JSON file, open the file with a text editor of your choice, edit it, and reload it from SiliFish (Figure\u00a013[href=https://www.wicell.org#fig13]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig13.jpg\nFigure\u00a013. An alternative way to edit models\nYou can save the model templates as a JSON file for transferability and easy editing.\nSetting kinematic parameters\nTiming: \u00a0<5\u00a0min\nAfter the model is run, you can visualize the behavior of the fish by converting the muscle cells\u2019 membrane potentials to muscle contractions. How to update the parameters of this visualization is explained below.\nThe parameters of muscle membrane potential to muscle contraction conversion can be set by clicking the Kinematics tab.\nThe force generated by the contraction of the muscle cells in each somite depends on the activity of the motoneuron innervating that somite. The details of this conversion can be found in our previous model.1[href=https://www.wicell.org#bib1] In summary, the membrane potential of the muscle cells in each somite is converted into oscillation angle using the following formula.\n   \u03b8 i \u2033  + 2 \u03b6   \u03c9 0   \u03b8 i \u2032  +  \u03c9 0 2   \u03b8 i  = \u03b4   (   V  r i g h t  m u s c l e   \u2212  V  l e f t  m u s c l e    )Enter the kinematic parameters: the damping coefficient (\u03b6), the natural oscillation frequency (\u03c90), and the conversion coefficient (\u03b4).\nOptional: The Alpha and Beta values are used to calculate the conversion coefficient by the\u00a0\u03b4\u00a0= \u03b1\u00a0+ \u00df\u2217R formula, where R is the average of the resistance of the muscle cells within a somite. If the calculated value is 0, the Conversion Coefficient entered by the user will be used.\nNote: Entering zero to alpha and beta values will make sure the entered conversion coefficient is used for every somite.\nEnter the Boundary and Delay values that will be used to detect tail beat episodes.\nNote: If the tip of the tail moves in the y-axis at least the Boundary amount, it will be considered the start of an episode. If the dislocation from the central axis is less than the Boundary amount on the left and right for the duration of Delay, it will be considered a rest between episodes.\nNote: Unlike other parameters, kinematic parameters can be modified after running the\u00a0model. The kinematic parameters are used only to generate the body movement animation using the run results; these parameters do not affect the neuronal activity of the cells.\nGenerating animation\nTiming: \u00a0<5\u00a0min\nHow to generate the swimming animation using the parameters set in Setting Kinematic Parameters is explained below.\nEnter the Animation Start and Animation End time in milliseconds (Figure\u00a014[href=https://www.wicell.org#fig14]). By default, these values are set to 0 and the simulation end time respectively.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2307-Fig14.jpg\nFigure\u00a014. Setting animation parameters\nThe animation start and finish times and the time resolution (\u0394t) can be set by the user.\nEnter \u0394t, the unit of time, for the animation. Using an animation \u0394t greater than simulation \u0394t will create a smaller animation file.Click the Animate button. The animation will be generated as an HTML file and displayed on the window.", "Step-by-step method details\nStep-by-step method details\nInstall TARSII and CARSII\nTiming: 2\u00a0min\nNote: The timing calculated for each step of TARSII and CARSII is based on a computer with 8 Gb memory and 1 core. A minimal of 8 Gb memory and 1 core is required. TARSII and CARSII are functional on Linux and Mac OSX systems.\nThe TARSII/CARSII packages are available through the following public repository:\nhttps://doi.org/10.5281/zenodo.5484230[href=https://doi.org/10.5281/zenodo.5484230]\nTo activate TARSII and CARSII, you may follow the command lines below:\ncd TARSII/CARSII_folder\nchmod u+x \u2217\ncd ./bin\nchmod u+x \u2217\nPredict germline DMRs using TARSII\nConsidering germline DMRs are composed of both hypomethylated and hypermethylated alleles, those regions should have partial DNA methylation and should enrich for both hypomethylated and hypermethylated reads (Figure\u00a02[href=https://www.wicell.org#fig2]A). Because DMRs are believed to be generally conserved across different tissues (Babak et\u00a0al., 2015[href=https://www.wicell.org#bib1]), identifying common candidate DMRs across different tissues can help reduce FDR. These are the basis for imprinting prediction by TARSII.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1502-Fig2.jpg\nFigure\u00a02. Model for prediction of putative germline DMRs by TARSII\n(A) Schematic model showing allelic DNA methylation for a typical maternal imprinted DMR.\n(B) Schematic models showing the strategies for identifying PMDs from the genome and for identifying candidate DMRs from the PMDs in step 2.\n(C) Schematic model showing the strategy for integrated analysis of candidate DMRs in all tissues in step 4.\n(D) Schematic model showing the strategy for categorizing the parental origin of the predicted imprinted DMRs in step 7.\nIdentify candidate DMRs from individual DNA methylome by TARSII\nTiming: 2 hIn this step, candidate DMRs in each tissue are selected out by two analyses. First, genomic regions containing consecutive CpG sites (>= 10) with 5mC level ranging from 0.3 to 0.7 are selected out as partially methylated domains (PMDs) (Figure\u00a02[href=https://www.wicell.org#fig2]B). Then, the PMDs enriched for both hypomethylated reads (5mC <= 0.2, reads percentage >= 30%) and hypermethylated reads (5mC >= 0.8, reads percentage >= 30%) are identified as candidate DMRs (Figure\u00a02[href=https://www.wicell.org#fig2]B).\nCandidate DMRs of individual somatic tissue methylome are identified following the command:\n# Take cortex methylome data as an example\nTARSII_step1_DMR_identify.sh -x cortex_5mC.wig -s\ncortex_picard_deduplicated.sam -o cortex\n# Run this script for DNA methylome of each somatic tissue\nNote: The input files and output file name are mandatory; other parameters have been optimized but can be adjusted according to user-specific requests.\nParameters should be provided:\n-x\u00a0\u00a0\u00a0\u00a0wig file presenting DNA methylation levels at base resolution\n-s\u00a0\u00a0\u00a0\u00a0sorted sam file with duplicates removed\n-o\u00a0\u00a0\u00a0\u00a0output file name\nParameters available to be adjusted through [options]:\n-n\u00a0\u00a0\u00a0\u00a0minimal CpG number required to be included in a PMD. Default: 10 (>= 1)\n-m\u00a0\u00a0\u00a0\u00a0minimal methylation level for the CpG sites in a PMD. Default: 0.3 (ranges from 0 to 1)\n-M\u00a0\u00a0\u00a0\u00a0maximal methylation level for the CpG sites in a PMD. Default: 0.7 (ranges from 0 to 1)\n-r\u00a0\u00a0\u00a0\u00a0minimal CpG number required in a single read. Default: 3 (>= 1)\n-l\u00a0\u00a0\u00a0\u00a0minimal number of reads required to be aligned to a PMD. Default: 30 (>=1)\n-b\u00a0\u00a0\u00a0\u00a0bin number to categorize methylation levels of the reads in a PMD. Default: 5 (>= 2)\nNote: -b 5 means to categorize the reads into 5 groups with methylation levels range from 0.0\u20130.2, 0.2\u20130.4, 0.4\u20130.6, 0.6\u20130.8, and 0.8\u20131.0. Option -c/-C is applied to the first/last bin-c\u00a0\u00a0\u00a0\u00a0minimal percentages of hypomethylated reads versus total reads for a candidate DMR. Default: 0.3 (30%) (ranges from 0 to 1)\n-C\u00a0\u00a0\u00a0\u00a0minimal percentages of hypermethylated reads versus total reads for a candidate DMR. Default: 0.3 (30%) (ranges from 0 to 1)\nFollowing completion of step 2, users will get 3 output files in working directory that include (troubleshooting problem 1[href=https://www.wicell.org#sec6.1]):\nA bed file containing all PMDs identified from the input DNA methylome.\nA tab separated file containing all PMDs with additional information as percentage of hypomethylated reads, percentage of hypermethylated reads and total reads number in each bin.\nA bed file containing all candidate DMRs identified from the input DNA methylome for processing in the next step.\nIdentify common candidate DMRs from different somatic tissues\nTiming: 5\u00a0min\nIn this step, the overlapping candidate DMRs from all tissues are merged as one (Figure\u00a02[href=https://www.wicell.org#fig2]C). Then, if the candidate DMRs from at least 5 different tissues show overlap with the merged candidate DMRs, those candidate DMRs are defined as putative imprinted DMRs identified by TARSII (Figure\u00a02[href=https://www.wicell.org#fig2]C).\nThe putative imprinted DMRs are identified through integrated analysis of candidate DMRs in different somatic tissues following the command:\nTARSII_step2_DMR_integration.sh -f \u2018cerebellum_DMR_candidate.bed\ncortex_DMR_candidate.bed heart_DMR_candidate.bed\nintestine_DMR_candidate.bed kidney_DMR_candidate.bed\nliver_DMR_candidate.bed\u2019 -o human\nNote: Users should include all the bed files of candidate DMRs and an output file name to run this script. The minimal number of tissues required for a putative imprinted DMR to be identified could be adjusted with the parameter -n. However, users should be cautious that reducing the cutoff will increase false discovery rate, while increasing the cutoff may increase the false negative rate.\nParameters should be provided:\n-f\u00a0\u00a0\u00a0\u00a0all bed files containing candidate DMRs identified from step 1\nNote: File names should be located within \u2018 \u2019 symbol\n-o\u00a0\u00a0\u00a0\u00a0output file nameParameters available to be adjusted through [options]:\n-n\u00a0\u00a0\u00a0\u00a0minimal number of tissues for a putative imprinted DMR to be commonly identified in\u00a0(>=\u00a01)\nFollowing completion of step 4, users will get 1 bed file presenting the putative imprinted DMRs in working directory.\nCategorize parental origin of the putative imprinted DMRs predicted by TARSII\nTiming: 10\u00a0min\nSince the prediction of TARSII is independent of SNPs, the parental origin of these imprinted DMRs cannot be directly inferred from somatic tissue methylomes by TARSII. Nevertheless, considering the allelic DNA methylation of germline DMRs is inherited from gametes, the differentially methylated regions between sperm and oocyte, as well as between androgenetic and parthenogenetic early embryos, can help infer the parental origin of the predicted imprinted DMRs without SNP information.\nIn this step, paternal and maternal DMRs in sperm/oocyte or uniparental early embryos are identified. Based on those parental DMRs, TARSII predicted putative imprinted DMRs can be categorized into maternal germline DMRs, paternal germline DMRs and somatic DMRs (Figure\u00a02[href=https://www.wicell.org#fig2]D).\nWig files showing DNA methylation levels at base resolution from sperm/oocyte or uniparental early embryos are required, as shown in Figure\u00a01[href=https://www.wicell.org#fig1]B. To prepare for these files, please refer to \u201cbefore you begin[href=https://www.wicell.org#before-you-begin]\u201d and our previous study (Chu et\u00a0al., 2021[href=https://www.wicell.org#bib4]).\nParental origin of the putative imprinted DMRs predicted by TARSII can be categorized following the command:\nTARSII_step3_germline_DMR.sh -p androgenetic_5mC.wig -m\nparthenogenetic_5mC.wig -b human_putative_imprinted_DMR.bed -o human\nNote: The input files and output file name are mandatory; other parameters have been optimized but can be adjusted according to user-specific requests.\nParameters should be provided:\n-p\u00a0\u00a0\u00a0\u00a0wig file presenting DNA methylation levels at base resolution of sperm/androgenetic embryos\n-m\u00a0\u00a0\u00a0\u00a0wig file presenting DNA methylation levels at base resolution of oocyte/parthenogenetic embryos\n-b\u00a0\u00a0\u00a0\u00a0bed file of the imprinted DMRs predicted by TARSII (generated from step 4)-o\u00a0\u00a0\u00a0\u00a0output file name\nParameters available to be adjusted through [options]:\n-d\u00a0\u00a0\u00a0\u00a0minimal cutoff to define a differentially methylated CpG site for a DMR in gemmates. Default: 0.5 (ranges from 0 to 1)\n-n\u00a0\u00a0\u00a0\u00a0minimal CpG number required to be included in a DMR in gemmates. Default: 10. (>= 1)\n-c\u00a0\u00a0\u00a0\u00a0maximal paternal methylation level in a maternal DMR in gemmates. Default: 0.15 (ranges from 0 to 1)\n-C\u00a0\u00a0\u00a0\u00a0maximal maternal methylation level in a paternal DMR in gemmates. Default: 0.30 (ranges from 0 to 1)\nFollowing completion of step 7, users will get 5 files in working directory. File 3, 4, 5 (below) are the final results generated by TARSII (troubleshooting[href=https://www.wicell.org#troubleshooting] problems 2[href=https://www.wicell.org#sec6.3] and 3[href=https://www.wicell.org#sec6.5]).\nA bed file contains paternal DMRs in gemmates with average methylation levels of maternal and paternal alleles.\nA bed file contains maternal DMRs in gemmates with average methylation levels of paternal and maternal alleles\nA bed file contains maternal germline DMRs predicted by TARSII.\nA bed file contains paternal germline DMRs predicted by TARSII.\nA bed file contains somatic DMRs predicted by TARSII.\nPredict germline DMRs using CARSII\nTARSII predicts the germline DMRs through integrated analyses of DNA methylomes from different somatic tissues. However, in some case, identification of tissue-specific imprinting is required. To help predict germline DMRs in a single tissue independent of SNPs, we developed another computational tool, CARSII.Different from the genome-wide identification of germline DMRs in TARSII, only CpG islands (genomic regions with high CpG density) are included in the analysis of CARSII. This is mainly because: 1) DNA methylation in CpG islands is generally under rigid regulation of multiple transcription factors and epigenetic regulators (Deaton and Bird, 2011[href=https://www.wicell.org#bib6]). Thus, the DNA methylation in CpG islands is more stable across different cells compared to that of a random region in genome; 2) the majority of germline DMRs in mammals, such as mouse and human, overlap with GpG islands. In contrast, CpG islands only occupy a small portion of the genome (Chu et\u00a0al., 2021[href=https://www.wicell.org#bib4]). Thus, the chances for a germline DMR to be identified from a CpG island are much higher than that from a random region in the genome.\nNotably, the definition of CpG islands may vary according to different standards used. Nevertheless, we recommend using the CpG islands defined in the UCSC Genome Browser database, which could be found in the following link:\nhttps://hgdownload.soe.ucsc.edu/downloads.html[href=https://hgdownload.soe.ucsc.edu/downloads.html]\nIdentify candidate differentially methylated CpG-islands (DMCs) from a single DNA methylome by CARSII\nTiming: 6 h\nThe biological basis of CARSII is similar to TARSII, which assumes germline DMRs are enriched for both hypomethylated and hypermethylated alleles (Figure\u00a02[href=https://www.wicell.org#fig2]A). However, due to heterogeneity of the cells in a tissue, methylation inconsistency in part of the CpG islands and experimental variations caused by batch effects, the false discovery rate (FDR) of CARSII is relatively higher than that of TARSII. Thus, to help reduce the FDR in CARSII, we designed additional steps to remove germline DMRs that may be resulted from random effect or methylation inconsistency.In this step, CpG islands that enriched for both hypomethylated reads (5mC <= 0.2, reads percentage >= 30%) and hypermethylated reads (5mC >= 0.8, reads percentage >= 30%) are first selected as candidate DMCs (Figure\u00a03[href=https://www.wicell.org#fig3]A). Then, random test is performed to calculate the FDR for each candidate DMC and those with FDR\u00a0< 0.05 were removed (Figure\u00a03[href=https://www.wicell.org#fig3]B). Finally, the methylation consistency in each candidate DMC is determined and candidate DMCs with inconsistent DNA methylation along the CpG island are removed (Figure\u00a03[href=https://www.wicell.org#fig3]C).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1502-Fig3.jpg\nFigure\u00a03. Model for prediction of putative germline DMCs by CARSII\n(A\u2013C) Schematic models showing the strategies for quantifying methylated reads (A), random testing reads distribution (B) and analyzing methylation consistency (C) in candidate DMCs.\nCandidate DMCs from DNA methylome of a certain somatic tissue are identified by CARSII following the command:\n# Take cortex methylome data as an example\nCARSII_step1_DMC_identify.sh -g human_CpG_island.bed -x cortex_5mC.wig\n-s cortex_picard_deduplicated.sam -o cortex\nNote: The input files and output file names are mandatory; other parameters have been optimized but can be adjusted according to user-specific requests:\nParameters should be provided:\n-g\u00a0\u00a0\u00a0\u00a0bed file of all the CpG islands\n-x\u00a0\u00a0\u00a0\u00a0wig file presenting DNA methylation levels at base resolution\n-s\u00a0\u00a0\u00a0\u00a0sorted sam file with duplicates removed\n-o\u00a0\u00a0\u00a0\u00a0output file name\nParameters available to be adjusted through [options]:\n-r\u00a0\u00a0\u00a0\u00a0minimal CpG number required in a single read. Default: 3 (>= 1)\n-l\u00a0\u00a0\u00a0\u00a0minimal number of reads required to be aligned to a CpG island. Default: 20 (>= 1)\n-b\u00a0\u00a0\u00a0\u00a0bin number to categorize methylation levels of the reads in a CpG island. Default: 5 (>= 2)\nNote: -b 5 means to categorize the reads into 5 groups with methylation levels range from 0.0\u20130.2, 0.2\u20130.4, 0.4\u20130.6, 0.6\u20130.8, and 0.8\u20131.0. Option -c/-C is applied to the first/last bin-c\u00a0\u00a0\u00a0\u00a0minimal percentages of hypomethylated reads versus total reads for a candidate DMC. Default: 0.3 (30%) (ranges from 0 to 1)\n-C\u00a0\u00a0\u00a0\u00a0minimal percentages of hypermethylated reads versus total reads for a candidate DMC. Default: 0.3 (30%) (ranges from 0 to 1)\n-p\u00a0\u00a0\u00a0\u00a0maximal false discovery rate for a candidate DMC. Default: 0.05 (<= 0.05)\n-t\u00a0\u00a0\u00a0\u00a0test times for calculation of false discovery rate. Default: 10000 (>=1)\n-d\u00a0\u00a0\u00a0\u00a0maximal methylation differences allowed within a candidate DMC. Default: 0.2 (ranges from 0 to 1)\nCritical: We do not recommend using CARSII to directly predict non-germline/somatic DMRs because no allelic DNA methylation in gametes could be used to confirm the imprinting status for a somatic DMR.\nFollowing completion of step 9, the user will get 2 files in working directory (troubleshooting[href=https://www.wicell.org#troubleshooting] problem 1[href=https://www.wicell.org#sec6.1]):\nA tab separated file containing all CpG islands with the percentage of hypomethylated reads, percentage of hypermethylated reads, total reads number in a CpG island and reads number in each bin.\nA bed file containing predicted candidate DMCs by CARSII. The candidate DMCs in this file are viewed as the putative imprinted DMCs predicated by CARSII.\nCategorize parental origin of the putative imprinted DMCs predicted by CARSII\nTiming: 10\u00a0min\nSimilar to TARSII, in this step, paternal and maternal-specific methylated CpG-islands in gemmates are identified by analyzing DNA methylomes in sperm/oocyte or uniparental early embryos (Figure\u00a02[href=https://www.wicell.org#fig2]D). Then, based on those parental-specific methylated CpG-islands, putative imprinted DMCs are categorized into maternal germline DMCs, paternal germline DMCs and somatic DMCs (Figure\u00a02[href=https://www.wicell.org#fig2]D).\nParental origin of imprinted DMCs predicted by CARSII can be identified following the\u00a0command:\n# Take cortex methylome data as an example\nCARSII_step2_germline_DMR.sh -p androgenetic_5mC.wig -m\nparthenogenetic_5mC.wig -b cortex_putative_imprinted_DMC.bed -o cortexNote: The input files and output file names are mandatory; other parameters have been optimized but can be adjusted according to user-specific requests.\nParameters should be provided:\n-p\u00a0\u00a0\u00a0\u00a0wig file presenting DNA methylation levels at base resolution of sperm/androgenetic embryos\n-m\u00a0\u00a0\u00a0\u00a0wig file presenting DNA methylation levels at base resolution of oocyte/parthenogenetic embryos\n-b\u00a0\u00a0\u00a0\u00a0bed file of putative imprinted DMCs generated from step 1 script\n-o\u00a0\u00a0\u00a0\u00a0output file name\nParameters available to be adjusted through [options]:\n-d\u00a0\u00a0\u00a0\u00a0minimal cutoff to define a differentially methylated CpG site for a DMC in gemmates. Default: 0.5 (ranges from 0 to 1)\n-c\u00a0\u00a0\u00a0\u00a0maximal paternal methylation in a maternal DMC in gemmates. Default: 0.15 (ranges from 0 to\u00a01)\n-C\u00a0\u00a0\u00a0\u00a0maximal maternal methylation in a paternal DMC in gemmates. Default: 0.30 (ranges from 0 to 1)\nFollowing completion of step 11, the user will get 3 files in working directory (troubleshooting[href=https://www.wicell.org#troubleshooting] problems 2[href=https://www.wicell.org#sec6.3] and 4[href=https://www.wicell.org#sec6.7]):\nA bed file containing maternal germline DMCs predicted by CARSII\nA bed file containing paternal germline DMCs predicted by CARSII.\nA bed file containing somatic DMCs predicted by CARSII.\nNote: As noted above, we do not recommend applying CARSII to predict somatic DMCs. Nevertheless, if the user does use this approach to predict somatic DMCs, validation by allelic methylation analysis is suggested before moving forward (see the following part).\nAnalysis of allelic DNA methylation for the putative imprinted DMRs with CGmapToolsAlthough the putative germline DMRs predicted by TARSII and CARSII is relatively accurate (Chu et\u00a0al., 2021[href=https://www.wicell.org#bib4]), certain level of false discovery rate is unavoidable without the information of the allelic DNA methylation. Therefore, we introduce CGmapTools (Guo et\u00a0al., 2018[href=https://www.wicell.org#bib7]) to help easily and quickly validate the imprinted DMRs predicted by TARSII/CARSII. CGmapTools is capable of identifying SNPs directly from DNA methylomes and calculating allelic methylation levels associated with those SNPs.\nBy combining TARSII/CARSII and CGmapTools, putative imprinted DMRs in the whole genome can be identified first using TARSII/CARSII independent of SNPs. Then, allelic methylation of the putative imprinted DMRs can be calculated by CGmapTools with only a few SNPs located within those DMRs. In this way, instead of collecting large number of DNA methylomes from many different individuals and SNPs from their parents\u2019 genomes, only a few DNA methylomes are sufficient for accurate de novo identification of imprinted regions in outbred mammals.\nHere, we only provide an integrated analysis of CGmaptools and TARSII/CARSII. For a complete and detailed instructions on CGmapTools, please refer to the published study (Guo et\u00a0al., 2018[href=https://www.wicell.org#bib7]) and the link below:\nhttps://cgmaptools.github.io/[href=https://cgmaptools.github.io/]\nIdentify SNPs from DNA methylome using CGmapTools\nTiming: 1\u00a0day\nIn this step, SNPs in certain DNA methylome are extracted using CGmapTools (Figure\u00a04[href=https://www.wicell.org#fig4]A). In TARSII, we can extract SNPs from every DNA methylome that used for analysis.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1502-Fig4.jpg\nFigure\u00a04. Validation of predicted germline DMRs by CGmapTools\n(A) schematic models showing allelic methylation analysis in predicted germline DMRs by CGmapTools. Red arrows indicate SNPs.\n(B) A list of human putative germline DMRs predicted by TARSII. Red color indicates the predicated germline DMRs are associated with SNPs identified from somatic tissue methylomes by CGmapTools.(C) Bar plots showing allelic DNA methylation of TARSII predicted human germline DMRs. The allelic methylation surrounding each SNP located inside the predicted germline DMRs is calculated by CGmapTools. Stars indicate significance of the allelic methylation differences. \u2217: p-value\u00a0< 1.0E-3, \u2217\u2217:\u00a0p-value\u00a0< 1.0E-4, \u2217\u2217\u2217: p-value\u00a0< 1.0E-5. N.S. not statistically significant.\nSNPs from DNA methylome can be extracted following the commands:\n# Take cortex methylome data as an example\ncgmaptools convert bam2cpmap -b cortex_picard_deduplicated.bam -g\nhuman_genome.fa -o cortex\n# Then:\ncgmaptools snv -i cortex.ATCGmap.gz -m bayes -v cortex_SNP.vcf -o\ncortex_SNP.snv --bayes-dynamicP\nExtracting SNPs located within the putative imprinted DMRs predicted by TARSII/CARSII using the perl script (Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1502-Mmc1.zip]) and command:\n# Take cortex methylome data as an example\nExtract_SNPs_from_DMRs.pl cortex_SNP.vcf cortex_SNP.snv\nhuman_putative_imprinted_DMR.bed\nThis script generates a selected vcf file containing SNPs located within the putative imprinted DMRs as listed in the input bed file.\nNote: CGmapTools perform allelic analysis but cannot distinguish parental origin. To infer the parental origin of non-germline/somatic DMRs, SNPs identified from parental genomes are still needed.\nCalculate allelic DNA methylation using CGmapTools\nTiming: 5\u00a0min\nAllelic DNA methylation is calculated by CGmapTools following the command:\n# Take cortex methylome data as an example\ncgmaptools asm -r human_genome.fa -b cortex_picard_deduplicated.bam -l\ncortex_SNP_DMRs.vcf > cortex_SNP_DMRs.asm\nNote: The asm file contains information on allelic DNA methylation value, p-value and false discovery rate for each SNP in the analyzed DNA methylome. A TRUE or FALSE judgement is provided as an inference to users about whether the allelic methylation associated with a particular SNP is significantly different or not (troubleshooting[href=https://www.wicell.org#troubleshooting] problem 5[href=https://www.wicell.org#sec6.9]).Critical: Since the length and range of imprinted DMRs predicted by TARSII/CARSII can vary when compared to the real imprinted DMRs, some SNPs located within the predicted imprinted DMRs may actually located outside of the real imprinted DMRs. Thus, to check the accurate location of individual SNP using genome browser tools on UCSC genome browser or integrative genomic viewer (IGV) is highly recommended. In general, informative SNPs should locate within a clear partially methylated domain. On the other hand, it would be more solid if several SNPs within certain DMR are all exhibit significant allelic DNA methylation differences.\nAs an example, we predicted the germline DMRs by TARSII using human somatic tissue methylomes as indicated in \u201cbefore you begin[href=https://www.wicell.org#before-you-begin]\u201d. In total, 29 of germline DMRs were identified, most of which are maternal germline DMRs except H19 (Figure\u00a04[href=https://www.wicell.org#fig4]B). 22 of the 29 germline DMRs are associated with at least 1 SNP identified by CGmapTools based on the human somatic tissue methylomes (Figure\u00a04[href=https://www.wicell.org#fig4]B). All of those SNP-associated germline DMRs exhibit clear allele-specific DNA methylation (Figure\u00a04[href=https://www.wicell.org#fig4]C), confirming the high accuracy of our approaches.", "Step-by-step method details\nStep-by-step method details\nStep 1: Cell nuclei purification\nTiming: 1.5 h\nThis step allows for maximized binding of antibodies to nuclear factors and will result in cleaner CUT&RUN signal compared to protocol using whole cells\nCell collection and swelling\nCollect 2\u2217106\u20132\u2217107 cells of interest by either scraping or centrifugation. We successfully used this protocol on SNU398, K562, and NB4 cell lines.\nResuspend in 5\u00a0mL of cold PBS and spin at 2,500\u00a0rpm at 4\u00b0C for 15\u00a0min\nMeasure the packed cell volume (PCV) of the cell pellet and add 5\u00d7 PCV of 1\u00d7 Buffer A (10\u00d7 Buffer A contains 100\u00a0mM HEPES, 15\u00a0mM MgCl2, 100\u00a0mM KCl, adjust pH to 7.9 with 10M KOH)\nVortex on high for 2s\nIncubate on ice for 20\u00a0min to swell the cells\nNuclei collection\nSpin cell pellet at 2,500\u00a0rpm for 10\u00a0min\nMeasure PCV again (should be slightly larger), add 2\u00d7 PCV (original) of 1\u00d7 Buffer A containing fresh protease inhibitor (Sigma complete protease inhibitor cocktail, 11697498001), 1\u00a0mM PMSF, 1\u00a0mM benzamidine, and 1\u00a0mM DTT; keep on ice\nDounce with a 1\u00a0mL, prechilled glass douncer to break up the cell pellet, 7 strokes; alternatively, pass the cell lysate through a 25G needle 10\u00d7\nIncubate on ice for 10\u00a0min, transfer to 1.5\u00a0mL tube\nSpin down at 6,000\u00a0rpm at 4\u00b0C for 20\u00a0min\nTake out supernatant (cytoplasmic portion) for testing if needed; keep the pellet containing the nuclei\nEither flash freeze in liquid N2 or move directly to the CUT&RUN experiments\nNote: Each CUT&RUN reaction requires 2\u2217106 equivalent of cell nuclei.\nStep 2: Antibody binding\nTiming: 1.5\u00a0h to Overnight (16\u201318 h)Resuspend nuclei in 1\u00a0mL room temperature (20\u00b0C\u201325\u00b0C) Wash Buffer (20\u00a0mM HEPES pH7.5, 150\u00a0mM NaCl, 0.5\u00a0mM Spermidine) supplemented with fresh protease inhibitor\nPrepare Concanavalin A beads (Bangs Laboratories BP531)\nTake out enough beads for 10\u00a0\u03bcL/condition, can process together in one 1.5\u00a0mL low-binding microcentrifuge tube until protocol step 2.4.g\nAdd 1.5\u00a0mL Binding Buffer (20\u00a0mM HEPES pH7.9, 10\u00a0mM KCl, 1\u00a0mM CaCl2, 1\u00a0mM MnCl2)\nPlace in a magnetic stand to clear for between 30\u00a0s to 2\u00a0min\nRemove from the stand and add 1.5\u00a0mL Binding Buffer, mix by inversion\n5s spin at 500\u00a0rpm\nPut the tube back to the stand to clear\nRe-suspend with 1:1 volume of Binding Buffer (i.e., 10\u00a0\u03bcL for 10\u00a0\u03bcL of beads) and aliquot into corresponding CUT&RUN low-binding tubes\nWhile gently vortexing prepared Concanavalin A beads, add the nuclei in wash buffer from protocol step 2.3\nRotate at room temperature (20\u00b0C\u201325\u00b0C) for 10\u00a0min\nPlace on magnetic stand to clear (may take up to 1\u00a0min)\nTake tubes off of the stand, add 50\u00a0\u03bcL of Antibody Buffer (Wash Buffer supplemented with fresh protease inhibitors, 2\u00a0mM EDTA, and 0.02%\u20130.1% Digitonin, exact concentration as determined in Preparation Two)\nTo each condition, add the appropriate amount of antibody or corresponding normal isotype IgG control (typically 2.5\u00a0\u03bcg of each, but exact concentration should be determined experimentally)\nIncubate 1\u00a0h to overnight (16\u201318 h) with rocking at 4\u00b0C\nCritical: If you are running CUT&RUN for the first time, include a histone mark antibody for the quality control step.\nStep 3: ProteinA-Mnase binding and Mnase digestion\nTiming: 2 h\nQuick spin the tubes at 500rom and place on magnetic stand to clear\nAdd 1\u00a0mL of Dig-Wash buffer (Wash Buffer with Digitonin, important: NO EDTA)Mix by inversion, place on magnetic stand to clear\nTo each tube, add 50\u00a0\u03bcL of Dig-Wash containing 700\u00a0ng/mL of proteinA-MNase (concentration as determined in Preparation One, step 4)\nRotate for 1\u00a0h at 4\u00b0C\nQuick spin the tubes at 500\u00a0rpm and place on magnetic stand to clear\nAdd 1\u00a0mL of Dig-Wash buffer\nRepeat protocol steps 1, 3.16, and 3.17 for a total of 2 washes\nAdd 100\u00a0\u03bcL of Dig-Wash buffer along the sides to dislodge the beads\nPlace tubes in heat block sitting on wet ice (should be around 0\u00b0C)\nWhile gently shaking each tube, add 2\u00a0\u03bcL of 100\u00a0mM CaCl2\nIncubate for 10\u201330\u00a0min (start with 30\u00a0min, if too long, then decrease the incubation, see Quality Control in step 5.47)\nAdd 100\u00a0\u03bcL of 2\u00d7 Stop Buffer (0.34M NaCl, 20\u00a0mM EDTA, 4\u00a0mM EGTA, 0.02% Digitonin, 50\u00a0\u03bcg/mL RnaseA, 50\u00a0\u03bcg/mL glycogen, and 20 pg/mL heterologous spike-in purified DNA from yeast or bacteria)\nGently vortex\nIncubate at 37\u00b0C for 10\u00a0min to release fragmented DNA\nCentrifuge at 16,000 g for 5\u00a0min at 4\u00b0C\nPlace on magnetic stand to clear\nSave clear supernatant containing DNA to a new tube\nStep 4: DNA purification\nTiming: 2.5\u00a0h to Overnight (16\u201318 h)\nTo each tube (\u223c200\u00a0\u03bcL total), add 2\u00a0\u03bcL of 10% SDS (final concentration 0.1%), 2.5\u00a0\u03bcL of proteinase K (20\u00a0mg/mL)\nMix by inverting and incubate for 10\u00a0min at 70\u00b0C\nAdd 300\u00a0\u03bcL of phenol:chloroform:isopropanol (25:24:1) to each tube\nVortex for 2s\nTransfer mixture to a phase-lock tube (Qiagen MaXtract, prepared by pre-spinning at 13,000\u00a0rpm for 30\u00a0s to settle the resin)\nCentrifuge at 16,000 g for 5\u00a0min at room temperature (20\u00b0C\u201325\u00b0C)\nTake out the top aqueous layer and transfer to a tubeAdd 2\u00a0\u03bcL of glycogen (2\u00a0mg/mL, Thermo catalog number R0561)\nAdd 750\u00a0\u03bcL of cold 100% ethanol\nMix by inversion, incubate for at least an hour (up to overnight (16\u201318 h) at \u221220\u00b0C\nCentrifuge at 16,000 g for 10\u00a0min at 4\u00b0C\nPour off liquid and dry on a piece of tissue\nRinse the pellet once with 950\u00a0\u03bcL of 100% ethanol\nCentrifuge at 16,000 g for 1\u00a0min at 4\u00b0C\nPour off the liquid and drain on tissue\nAir dry for 3\u00a0min\nDissolve the pellet in 25\u00a0\u03bcL of 1\u00a0mM Tris-HCl (pH8) and 0.1\u00a0mM EDTA (i.e., 0.1\u00d7 TE)\nStep 5: Quality control\nTiming: 1 h\nQuantify 1\u00a0\u03bcL of purified DNA by Qubit HS DNA kit (ThermoFisher Q32851) following manufacturer\u2019s instructions\nExample amounts of DNA recovered\nHistone mark: 15\u201320\u00a0ng/\u03bcL\nTF: 10\u201315\u00a0ng/\u03bcL\nIgG isotype control: <10\u00a0ng/\u03bcL\nRun control histone mark CUT&RUN DNA on the 2100 Bioanalyzer system (Agilent)\nTranscription factor CUT&RUN DNA are typically around 50\u2013150\u00a0bp and may not show up on bioanalyzer, proceed to library amplification and size-selection\nNote: Transcription factor CUT&RUN DNA may not show up on the bioanalyzer electropherogram if run prior to amplification. This is due to the low concentration of DNA present. To asses protocol efficacy, it is recommended to run, in parallel, a control CUT&RUN with an antibody against a histone mark.\nCritical: If running a control histone mark CUT&RUN, should see mono-, di-, and tri-nucleosomes in the Bioanalyzer traces (Figure\u00a01[href=https://www.wicell.org#fig1]).\nTroubleshooting 2[href=https://www.wicell.org#sec5.3]\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/944-Fig1.jpg\nFigure\u00a01. Expected Bioanalyzer electropherogram for CUT&RUN with an antibody against an histone mark\nWhen running a histone mark CUT&RUN in parallel to your transcription factor of interest as quality control, one should expect to see small peaks representing mono-, di-, and tri-nucleosomes, which indicate successful proteinA-Mnase digestion.\nTroubleshooting 3[href=https://www.wicell.org#sec5.5]Step 6: Library construction (with NEBNext ultra II DNA library Prep, NEB 7103, for transcription factor CUT&RUN DNA\nTiming: 3 h\nNote: For histone mark CUT&RUN library preparation, see Skene et al., 2018[href=https://www.wicell.org#bib20].\nNote: This step is based on the protocol (https://dx.doi.org/10.17504/protocols.io.bagaibse[href=https://dx.doi.org/10.17504/protocols.io.bagaibse]) and publication (Liu et\u00a0al., 2018[href=https://www.wicell.org#bib11])\nDilute CUT&RUN DNA (6\u00a0ng) into 25\u00a0\u03bcL with water, add 1.5\u00a0\u03bcL NEBNext Ultra II End Prep enzyme mix and 3.5\u00a0\u03bcL of NEBNext Ultra II End Prep reaction buffer (30\u00a0\u03bcL total)\nMix well and place in thermocycler, with heated lid set to >60\u00b0C: 30\u00a0min at 20\u00b0C, 60\u00a0min at 50\u00b0C, hold at 4\u00b0C\nDilute the adapters supplied by the NEBNext kit from the original 15 uM to 3 uM\nCombine End repair mix from protocol step 6.50 with 15\u00a0\u03bcL of Ligation mater mix, 0.5\u00a0\u03bcL enhancer, 1.25\u00a0\u03bcL diluted adapter (\u223c47\u00a0\u03bcL total)\nIncubate for 15\u00a0min at 20\u00b0C in a thermocycler with no heated lid\nAdd 1.5\u00a0\u03bcL of USER enzyme to the mix\nMix well and incubate for 15\u00a0min at 37\u00b0C with heated lid set to >47\u00b0C\nVortex Ampure beads (Beckman A63880) and bring up to room temperature (20\u00b0C\u201325\u00b0C) for at least 30\u00a0min\nAdd 80\u00a0\u03bcL of beads (\u223c1.75\u00d7 volume) to the DNA mix from protocol step 6.55\nPipette 10 times to mix, incubate for 5\u00a0min at room temperature (20\u00b0C\u201325\u00b0C)\nPlace on magnetic stand for less than 5\u00a0min or until clear\nRemove and discard the supernatant while the tube in the on the stand, leave the beads\nWhile on the stand, wash twice with 200\u00a0\u03bcL of freshly made 80% ethanol, each time with incubation for 30\u00a0s at room temperature (20\u00b0C\u201325\u00b0C)\nRemove all trance of ethanol with a p10 pipette tipAir-dry the beads up to 5\u00a0min (don\u2019t over dry)\nRemove the tubes from the stand and add 14\u00a0\u03bcL of 0.1\u00d7 TE (same as protocol step 6.45)\nMix well or gently vortex\nIncubate for 2\u00a0min at room temperature (20\u00b0C\u201325\u00b0C)\nPlace on the magnetic stand for 5\u00a0min\nRemove and save 13\u00a0\u03bcL of supernatant containing DNA\nPause point: Can store at \u221220\u00b0C before PCR amplification\nSet up PCR by combining 13\u00a0\u03bcL of DNA from protocol step 6.68, 15\u00a0\u03bcL NEBNext Ultra II Q5 Master Mix, 1\u00a0\u03bcL Index primer, and 1\u00a0\u03bcL Universal PCR primer (30\u00a0\u03bcL total)\nRun PCR:\nCycle 1: 98\u00b0C for 30 s\nCycle 2: 57\u00b0C for 10 s, repeat 12 times total (Low Tm for transcription factor libraries)\nCycle 3: 65\u00b0C for 5\u00a0min\nHold at 4\u00b0C\nVortex Ampure beads and bring up to room temperature (20\u00b0C\u201325\u00b0C) for at least 30\u00a0min\nRemoving DNA products of >350\u00a0bp: add 24\u00a0\u03bcL (0.8\u00d7) Ampure beads to the PCR reaction and mix well, incubate for 5\u00a0min at room temperature (20\u00b0C\u201325\u00b0C), place the tubes on the magnetic stand for 5\u00a0min or until clear, carefully transfer the supernatant containing small DNA into a new tub3e\nKeep DNA products of <150\u00a0bp: add 12\u00a0\u03bcL (1.2\u00d7) of re-suspended Ampure beads to the supernatant (so the effective bead to DNA ratio is 2\u00d7) from protocol step 6.72, mix 12 times, incubate for 5\u00a0min at room temperature (20\u00b0C\u201325\u00b0C), place the tubes on the magnetic stand for 5\u00a0min or until clear, remove and discard the supernatant\nWash twice with 200\u00a0\u03bcL of freshly made 80% ethanol, incubate for 30\u00a0s each time at room temperature (20\u00b0C\u201325\u00b0C)\nAir-dry the beads for up to 5\u00a0min, do not over dryRemove the beads from the magnetic stand and elute the DNA from the beads by adding 15\u00a0\u03bcL of 0.1\u00d7 TE\nMix well and incubate for 2\u00a0min at room temperature (20\u00b0C\u201325\u00b0C)\nPlace the tubes back on the magnetic stand for 5\u00a0min or until clear\nTransfer 13\u00a0\u03bcL of supernatant containing the library into a new PCR tube\nCheck size distribution on the 2100 Bioanalyzer system (Figure\u00a02[href=https://www.wicell.org#fig2])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/944-Fig2.jpg\nFigure\u00a02. Expected Bioanalyzer electropherograms following library construction for nuclear DNA\nFollowing library amplification, the bioanalyzer electropherograms should show sharp peaks at the size of the fragmented DNA with adapters as shown. Transcription factor CUT&RUN samples tend to show a smaller peak if following the described protocol\nPause point: Store the libraries at \u221220\u00b0C until sequencing\nCritical: Do not discard supernatant from protocol step 6.72 which contain the desired DNA library, discard the Ampure beads from this step which contain larger DNA fragments.\nStep 7: Sequencing with MiniSeq (illumina): 4 h\nUp to 24 barcoded quantified libraries can be mixed at equal molar ratio\n(Optional) Remove PCR dimers with Pippin prep size selection according to the manufacturer\u2019s instructions\nPerform paired-end sequencing (2\u00a0\u00d7 42\u00a0bp) on MiniSeq (Illumina) 5 million reads per library are sufficient\nStep 8: Data analysis with CnRAP \u2013 time \u2013 hours to a couple of days depending on computer hardware, sequencing depth, and the number of samples to be processed\nThe bioinformatics pipeline written for this analysis consists of three essential scripts and three optional scripts. Each of the essential scripts performs a required function in processing the sequencing files in order to call the Cut&Run peaks while the optional scripts perform the peak annotation and motif enrichment analysis once the peaks are called. Briefly, the scripts perform the following functions:Script 01 \u2013 Read quality trimming and alignment to reference genomes\nScript 02 \u2013 Generation of bedgraph files and normalization\nScript 03 \u2013 Peak calling using SEACR\nScript 04 \u2013 (Optional) Annotate called peaks using ChIPSeeker\nScript 05 \u2013 (Optional) File preparation for HOMER motif enrichment analysis\nScript 06 \u2013 (Optional) File preparation for MEME motif enrichment analysis.\nIn this section, we will outline the function and purpose of each script, as well as outline how to call and use said script. Additional details can also be found on the GitHub page.\nFor those wishing to follow this analysis, the dataset analyzed in our original publication (Kong et\u00a0al., 2021[href=https://www.wicell.org#bib8]) is referenced in the Data Availability section. Users can download the raw and processed data from GEO and compare their results with the published findings.\nNote: Before beginning, it is important to note that for each sample sequenced, there should exist a pair of de-multiplexed fastq files corresponding to both read directions. These typically\u00a0contain R1/R2 in the filename to denote read direction. Both files are needed per sample.\nNote: Ensure that you have followed preparation 3 in the \u201cBefore You Begin\u201d section before proceeding. Next, the configured conda environment must be activated prior to attempting to run any of the following commands. See the GitHub repository for additional details.The first step in running the analysis pipeline is to run CnRAP script 01 on each sample separately. Note, each sample will consist of 2 read files, R1 and R2, corresponding to the reads in both directions. This script takes care of performing all the necessary quality trimming steps required on the sequenced reads and alignment to the reference genomes. Firstly, trimmomatic is run to remove poor quality bases from both the start and the end of the reads while also removing any potential adapter sequences that might be found. Following this, the kseq_trimming tool is run to remove any additional barcode sequences. Once the reads are cleaned up, alignment to the reference genomes is performed using BWA followed by Stampy. In this protocol, reads are aligned to both the human (hg) and saccharomyces cerevisiae (sacCer) genomes. Alignment to hg is because the cell line used (SNU-398) is a human cell line, while alignment to the sacCer genome enables utilizing the spike-in DNA for normalizing. BWA and Stamy were chosen for alignment as they were found to result in improved alignment performance when benchmarked (Thankaswamy-Kosalai et\u00a0al., 2017[href=https://www.wicell.org#bib21]). Following alignment, unmapped reads are removed; bam files are sorted, indexed and alignment statistics are calculated.\nTo run script 01, use the following command.\n> python3 01_cut_n_run_pairedReads_filter_align.py <sample_name> <read1_fq_gz> <read2_fq_gz> <num_cores> <aligned_folder>\nWherein:\n> <sample_name> defines the prefix corresponding this sample;\n> <read1_fq_gz> is the full path location of the read1 fastq file;\n> <read2_fq_gz> is the full path location of the read2 fastq file;\n> <num_cores> defines the number of processor cores to use for processing;\n> <aligned_folder> is the folder where all output will be saved.Following the above python3 call, a bash script will be generated 01_cut_n_run_pairedReads_filter_align.sh which can simply be run in the terminal to perform the required steps for this stage of analysis.\nOptional: Following genome alignment, some users may wish to assess the degree of PCR duplication present in their data. Marking and removal of PCR duplicates can be performed by Picard tools as well. Such an analysis can be performed using the following code. For additional information on interpreting the output of Picard, refer to the official documentation on the Broad website.\nOptional: Following the generation of the aligned bam files, users can optionally run the tool \u201cplotFingerprint\u201d on the sample and IgG control bam files to check whether they see greater enrichment as expected in their sample. An example of such a figure can be found in Figure\u00a03[href=https://www.wicell.org#fig3], wherein users can see a separation between the IgG and sample curves.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/944-Fig3.jpg\nFigure\u00a03. Example Enrichment fingerPrint plot following genome alignment\nFollowing alignment to the human genome, users can optionally generate a global enrichment fingerPrint plot to assess the extent of enrichment observed in their sample over control. The sample curve (blue) should be closer to the bottom right corner than the control IgG curve (orange). The greater the separation between the curves the greater enrichment observed, which will likely result in more peaks being called at the peak calling step.\nOptional: Following genome alignment, some users may wish to assess library complexity before proceeding. For this, Picard tools is an ideal and commonly accepted toolkit to do so. Such an analysis can be performed using the following code. For additional information on interpreting the output of Picard, refer to the official documentation on the Broad website.\n> java -jar picard.jar EstimateLibraryComplexity I=input.bam O=estimated_library_complexity_metrics.txt> java -jar picard.jar MarkDuplicates I=input.bam O=marked_duplicates.bam M=marked_duplicates_metrics.txt\nThis command will simply mark the duplicate reads and not remove them. If you wish to remove duplicate reads, the MarkDuplicates command has two flags that can use utilized REMOVE_DUPLICATES and REMOVE_SEQUENCING_DUPLICATES. For full details on how to incorporate these flags into the afore mentioned command, refer to the official documentation on the Broad website.\nNote: Be sure to set the same output <aligned_folder> for all samples as subsequent scripts will process all samples contained in given folders. Only script 01 is to be run separately per sample.\nTroubleshooting 4[href=https://www.wicell.org#sec5.7]\nTroubleshooting 5[href=https://www.wicell.org#sec5.9]\nNext, CnRAP script 02 needs to be run on the output folder of script 01 which is supposed to contain the aligned bam files of all samples processed with script 01. Script 02 first takes the aligned bam files for each sample and converts them to bedgraph files. Next, the hg bedgraph files are normlized to the sacCer spike-in controls. For this, a normalization factor is calculated for each hg-sacCer file pair which is calculated as follows\n  n o r m a l i z a t i o n _ f a c t o r =    10,000,000     m a p p e d _ r e a d s _ p e r _ s a c C e r _ g e n o m e   2    \nThis normalization ensures that peak heights are adjusted correctly prior to peak calling in the next step.\nTo run script 02, use the following command.\n> python3 02_cut_n_run_bamToBed_normalize_SEACRPrepv1.py <aligned_bams_folder> <normalized_beds_folder> <chrom_sizes_txt>\nWherein:\n> <aligned_bams_folder> defines where the aligned bams, the output of script 01, are saved;\n> <normalized_beds_folder> is the folder where the normalized bed files will be saved in preparation for running SEACR, the peak calling algorithm for Cut&Run;> <chrom_sizes_txt> is a text file defining the base-pair size of each chromosome. This is required when converting the aligned bam files to bedGraph files. This file can be downloaded from UCSC directly.\nFollowing the above python3 call, a bash script will be generated 02_cut_n_run_bamToBed_normalize_SEACRPrepv1.sh which can simply be run in the terminal to perform the required steps for this stage of analysis.\nNext, run CnRAP script 03 is required to be run over the normalized bedgraph files. This script proceeds to first make bigWig coverage files for all normalized bedgraph files (for viewing in IGV or on UCSC) and then calls peaks using SEACR in both \u201cstringent\u201d and \u201crelaxed\u201d modes.\nTo run script 03, use the following command\n> python3 03_cut_n_run_SEACR_peakProcess_v1beds.py <seacr_location> <normalized_beds_folder> <output_folder> <chrom_sizes_txt>\nWherein:\n> <seacr_location> defines where the Cut&Run peak caller SEACR is saved;\n> <normalized_beds_folder> is the folder where the normalized bed files have been saved;\n> <output_folder>is the folder where the called peak files will be saved;\n> <chrom_sizes_txt> is a text file defining the base-pair size of each chromosome. This is required for bedGraph file manipulation.\nFollowing the above python3 call, a bash script will be generated 03_cut_n_run_SEACR_peakProcess_v1beds.sh which can simply be run in the terminal to perform the required steps for this stage of analysis.Once peaks are called, users will likely want to annotate them although this is not essential. As such, script 04, is considered as an optional script to be run. If users wish to annotate their called peaks, script 04 can be utilized which will annotate called peaks using the R package, ChIPSeeker. In addition to annotating the called peaks, ChIPSeeker will also generate summary plots which may be of relevance to users. Script 04 will only annotate using ChIPSeeker. Alternatively, users may wish to annotate their called peaks using the HOMER annotatePeaks function. No script is provided as part of CnRAP to use HOMER though. For details on how to annotate your peaks using HOMER, refer to the official documentation.\nOptional: Once peaks are called, users may wish to perform motif enrichment analysis to investigate the presence of characteristic motifs. For this, users can use script 5 and/or script 6. Script 05 script, processes the called peaks and generates the required script to run motif enrichment analysis using the HOMER toolkit. Once generated, the corresponding bash script can be run in the terminal to perform said analysis. This script is optional as, depending on the experimental design, motif enrichment analysis may not be required. It is evident from user forums however that some users prefer to use the MEME-Suite of tools in preference to HOMER for motif discovery. As such, we have also provided script 06 which will prepare the required files for performing motif enrichment analysis using the MEME-Suite. The choice for which toolkit to use will be up to the users and their experiment. For the results generated using this protocol, both HOMER and MEME results were compared to ensure reproducibility in the called motif.", "Step-by-step method details\nStep-by-step method details\nQuality control analysis of germline data\nTiming: Approximately 1\u20132\u00a0weeks. Dependent on server capabilities\nThis section describes quality control (QC) assessment of the TCGA Affymetrix Genome-Wide SNP 6.0 germline genotyping data (Figure\u00a01[href=https://www.wicell.org#fig1]) using PLINK to generate a high-quality set of SNPs for all whitelisted TCGA samples (i.e., a list of platform-specific samples verified to be appropriate for use). See key resources table[href=https://www.wicell.org#key-resources-table].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1887-Fig1.jpg\nFigure\u00a01. Schematic overview of genotype quality control workflow\nStepwise description of pre-processing steps taken to generate clean quality-controlled germline genotyping data for stranding and imputation. The protocol requires specific calculations to be performed (yellow), and steps to filter SNPs (green) or individuals (blue).\n(A) Histogram of X chromosome homozygosity estimate (XHE) inbreeding F coefficient. F coeff thresholds at 0.2 and 0.8 are shown.\n(B) Heterozygosity rate vs. log10 of the proportion of missing genotypes per ancestry group. Thresholds for the proportion of missing genotypes at log10(0.05) and mean heterozygosity\u00a0+/- 3\u2217standard deviations per ancestry group are shown.\n(C) Empirical cumulative distribution function of HWE log10 p-value for the European ancestry group. HWE threshold at p=10-6 is shown.\n(D) Histogram of log10 MAF. MAF threshold at log10(0.005) is shown.\nReview other resources for suitable QC steps based on the study design (Anderson, 2011[href=https://www.wicell.org#bib3]; Anderson et\u00a0al., 2010[href=https://www.wicell.org#bib4]; Aron and Choudhury, 2015[href=https://www.wicell.org#bib6]).\nNote: Original QC steps were performed in PLINK version 1.9. QC analysis requires a high-performance compute cluster.\nNote: To skip this step, download the controlled access \u201cQC Unimputed Genotyping Data\u201d generated from (Sayaman et\u00a0al., 2021[href=https://www.wicell.org#bib20]), as described in step 11 of the \u201cprepare germline genetic variation dataset[href=https://www.wicell.org#sec1.6]\u201d section of this protocol OR proceed with the QC protocol steps provided below.\nNote: Scripts used in this section are available at: https://github.com/rwsayaman/TCGA_PanCancer_Genotyping_Imputation[href=https://github.com/rwsayaman/TCGA_PanCancer_Genotyping_Imputation].Map birdseed genotyping file names to corresponding TCGA aliquot barcode using the download annotation JSON file from GDC TCGA Legacy archive.\nVerify sample list for inclusion in the analysis and filter out samples which are not represented in the whitelist, and which do not pass the analyte code filter.\nCross-reference sample set with whitelisted germline samples from GDC PanCanAtlas Publications page (https://gdc.cancer.gov/about-data/publications/pancanatlas[href=https://gdc.cancer.gov/about-data/publications/pancanatlas]). Non-whitelisted samples have since been flagged for withdrawal in the various TCGA projects.\nDownload the Merged Sample Quality Annotations file (merged_sample_quality_annotations.tsv[href=http://api.gdc.cancer.gov/data/1a7d7be8-675d-4e60-a105-19d4121bdebf]).\nTo select whitelisted samples, filter for samples with \u201cplatform\u201d column set to \u201cGenome_Wide_SNP_6\u201d and the \u201cDo not use\u201d\u2019 column set to \u201cFALSE\u201d.\nBased on established TCGA barcode identifiers, ensure all whitelisted samples have Analyte code \u201cD\u201d (DNA). Exclude samples with other Analyte codes.\nNote: The final TCGA whitelisted samples used in this analysis are available from (Sayaman et\u00a0al., 2021[href=https://www.wicell.org#bib20]), Table\u00a0S1. The GDC Genome Wide SNP 6.0 platform whitelisted files included samples with TCGA analyte barcode identifiers annotated \u201cD\u201d (DNA) or \u201cG\u201d (Whole Genome Amplification). Samples with analyte barcode identifier \u201cG\u201d were excluded from our analysis.\nLoad and concatenate individual whitelisted genotyping birdseed files using custom scripts, selecting SNPs with call confidence values \u2264 0.1. Annotate variants and generate PLINK files.\nTo take advantage of parallel processing, concatenate and filter birdseed text files in batches.\nRead each birdseed text file as a tab delimited table with 906,600 SNPs as rows and three columns containing the following information: (See page 1, http://tools.thermofisher.com/content/sfs/brochures/genome_wide_snp6_sample_dataset_readme.pdf[href=http://tools.thermofisher.com/content/sfs/brochures/genome_wide_snp6_sample_dataset_readme.pdf]).\nComposite Element REF: the probeset ID.\nCall: the genotype call with values of {-1, 0, 1, 2} corresponding to {NoCall, AA, AB, BB}.\nConfidence: the call confidence with values ranging from [0,1] with lower values corresponding to greater confidence.Pre-filter to exclude SNPs with lower call confidence and set the \u201cCall\u201d value to NA for SNPs with \u201cConfidence\u201d\u00a0>\u00a00.1 prior to concatenation.\nIteratively concatenate each call column, generating a table with SNPs as rows, samples as columns, and call values as elements of the matrix.\nCheck that probeset IDs match prior to concatenating a genotyping call; if not, exclude and log the mismatched birdseed file.\nUsing a custom script, convert batch concatenated birdseed files into PLINK standard input transposed text format files.\nUsing the Affymetrix SNP Array 6.0 (release 35) annotation file, convert concatenated data into PLINK transposed text genotype tables (.tped) with allele calls (See .tped file format specification: https://www.cog-genomics.org/plink2/formats#tped[href=https://www.cog-genomics.org/plink2/formats#tped]).\nCreate corresponding PLINK sample information files (.tfam) (See .tfam file format specification: https://www.cog-genomics.org/plink2/formats#tfam[href=https://www.cog-genomics.org/plink2/formats#tfam]).\nImport whitelisted germline data into PLINK for QC. Convert PLINK standard input transposed text files (--tfile) to standard input binary files (--bfile). https://www.cog-genomics.org/plink2/input[href=https://www.cog-genomics.org/plink2/input].\nImport the tfile set (--tfile) into PLINK and create a bfile set (--make-bed --out) that generates corresponding PLINK binary biallelic genotype tables (.bed), PLINK extended MAP files (.bim) and PLINK sample information files (.fam). See file format specifications: https://www.cog-genomics.org/plink2/formats#bed[href=https://www.cog-genomics.org/plink2/formats#bed]. https://www.cog-genomics.org/plink2/formats#bim[href=https://www.cog-genomics.org/plink2/formats#bim]. https://www.cog-genomics.org/plink2/formats#fam[href=https://www.cog-genomics.org/plink2/formats#fam].\nImpute the genotyping sex associated with each sample by calculating the X chromosome homozygosity estimate (XHE): https://www.cog-genomics.org/plink/1.9/basic_stats#check_sex[href=https://www.cog-genomics.org/plink/1.9/basic_stats#check_sex].\nNote: To minimize loss of TCGA samples when no self-reported sex is available and sex information is needed as a covariate in the analysis, sex can be imputed based on the XHE (F or inbreeding coefficient).\nSplit off the X chromosome\u2019s pseudo-autosomal region (--split-x) which is treated by PLINK as a separated XY chromosome. Indicate the proper build code.\nPerform LD pruning (--indep-pairphrase).\nRun check sex (--check-sex) which compares reported sex assignments with those imputed from X chromosome F coefficients.\nPlot a histogram of the XHE F coefficients (F coeff). See Figure\u00a01[href=https://www.wicell.org#fig1]A.A very tight distribution of F coeff around 1 is expected for males, and a more spread distribution of F coeff centered around zero is expected for females.\nIn PLINK, F estimates\u00a0<\u00a00.2 are by default assigned female and F estimates\u00a0>\u00a00.8 assigned male. However, when (i)\u00a0is observed and there is a clear gap between the two distributions, F coeff thresholds can be loosened and adjusted to correspond to the empirical gap. See \u201c--check-sex\u201d implementation and notes on TCGA sex assignment below.\nImpute sex (--impute-sex) based on the XHE F coefficient.\nCurate imputed sex assignments as needed and update sex assignments (--update-sex).\nNote: Not all TCGA samples have self-reported sex information and we imputed sex based XHE. However, we found cases where self-reported and imputed sex were discordant; sex assignments were curated depending on whether F coefficients fall within the expected range (F coeff\u00a0<\u00a00.2 for females and\u00a0>\u00a00.8 for males) or F coefficients fall out of the expected\u00a0range (F coeff\u00a0>\u00a00.2 and\u00a0<\u00a00.8) (see troubleshooting[href=https://www.wicell.org#troubleshooting] section, Problem 4). These imputed/curated sex assignments for TCGA germline samples are available in Table\u00a0S1 from (Sayaman et\u00a0al., 2021[href=https://www.wicell.org#bib20]).\nExclude SNPs and individuals with greater than 5% missingness.\nFilter variants (--geno) to include only SNPs with 95% genotyping rate (5% missing).\nFilter samples (--mind) to exclude individuals with more than 5% missing genotypes.\nCalculate heterozygosity within each ancestry cluster, and filter samples with excess heterozygosity. https://www.cog-genomics.org/plink/1.9/basic_stats#ibc[href=https://www.cog-genomics.org/plink/1.9/basic_stats#ibc].\nCalculate heterozygosity (--het) vs. missingness (--missing) rates.\nUsing downloaded UCSF ancestry assignments, calculate heterozygosity means and standard deviations within each of the European (EUR), African (AFR), East Asian (EAS) and Admixed American (AMR) ancestry clusters.\nPlot the log10 proportion of missing genotypes against heterozygosity rates with mean\u00a0+/-3\u2217SD for each ancestry cluster for QC. See Figure\u00a01[href=https://www.wicell.org#fig1]B.Flag samples with heterozygosity >3\u2217SD above the mean for each ancestry cluster; remove individuals as part of 8b sample filtering.\nNote: Samples with low heterozygosity are expected for certain ancestry groups and are not removed.\nNote: Not all TCGA samples have self-reported race and ethnicity data. Initial ancestry cluster assignments can be calculated based on principal component analysis (PCA) of germline data (--pca). In (Sayaman et\u00a0al., 2021[href=https://www.wicell.org#bib20]) initial ancestry calls were made based on Partition Around Medoids (PAM) clustering with k=4 using the first 3 principal components as described in (Sayaman et\u00a0al., 2021[href=https://www.wicell.org#bib20]), (Carrot-Zhang et\u00a0al., 2020[href=https://www.wicell.org#bib7]).\nSelect a representative sample for each individual with more than one sample. Conduct final filtering steps for all autosomal SNPs across the set of unique individuals.\nRestrict to autosomal chromosomes by excluding all unplaced and non-autosomal SNPs (--autosome).\nCreate a final list of samples to include in the study (--keep).\nExclude samples flagged in 7d for excess heterozygosity. ii. For individuals with more than one sample, preferentially select blood-derived normal samples; for those with more than one blood-derived sample, retain the samples with higher call rates.\nNote: All individuals and selected representative sample aliquots from TCGA germline data are listed in Table\u00a0S1 from (Sayaman et\u00a0al., 2021[href=https://www.wicell.org#bib20]).\nCalculate Hardy-Weinberg Equilibrium (HWE) within the largest ancestry cluster (EUR ancestry cluster). https://www.cog-genomics.org/plink/1.9/basic_stats#hardy[href=https://www.cog-genomics.org/plink/1.9/basic_stats#hardy].\nSubset for samples in EUR ancestry cluster. Calculate HWE (--hardy) across autosomal chromosomes.\nPlot the -log10 HWE p-value distribution for QC. See Figure\u00a01[href=https://www.wicell.org#fig1]C.\nExclude SNPs (--exclude) that deviate from the expectation under HWE (p\u00a0<\u00a01\u00a0\u00d7\u00a010-6) within the EUR ancestry cluster with the exception of SNPs previously associated with any cancer as reported in the GWAS catalog (p\u00a0<\u00a05\u00a0\u00d7\u00a010-8) (Rashkin et\u00a0al., 2020[href=https://www.wicell.org#bib18]) since they may deviate from HWE in cancer patients.Calculate Minor allele frequency (MAF) and exclude SNPs with MAF less than 0.5%. https://www.cog-genomics.org/plink/1.9/filter#maf[href=https://www.cog-genomics.org/plink/1.9/filter#maf].\nCalculate SNP MAFs (--freq).\nPlot the MAF cumulative distribution and histogram of -log10 MAF for QC. See Figure\u00a01[href=https://www.wicell.org#fig1]D.\nFilter out SNPs (--maf) with MAF\u00a0<\u00a00.005.\nRemove duplicate SNPs with identical genomic first position.\nUsing a custom script, find SNPs with duplicate genomic first positions in the .bim file or alternatively identify SNPs sharing the same bp coordinate and allele codes in PLINK (--list-duplicate-vars).\nFilter out duplicate SNPs (--exclude).\nNote: The final QC\u2019d list of sample (.fam) and SNP (.bim) files are available as part of the \u201cQuality-controlled unimputed genotyping data plink files - QC_Unimputed_plink.zip\u201d file under the \u201cQC Unimputed Genotyping Data\u201d sub-section of \u201cTCGA QC HRC Imputed Genotyping Data used by the AIM AWG (from Sayaman et\u00a0al.)\u201d section of the \u201cSupplemental Data Files\u201d: https://gdc.cancer.gov/about-data/publications/CCG-AIM-2020[href=https://gdc.cancer.gov/about-data/publications/CCG-AIM-2020].\nStranding\nTiming: Approximately <1\u00a0day. Dependent on server capabilities\nThis section describes the stranding of the QC\u2019ed genotyping data to the Haplotype Reference Consortium (HRC) prior to imputation.\nNote: To skip this step, download the controlled access \u201cHRC Stranded Genotyping Data\u201d generated from (Sayaman et\u00a0al., 2021[href=https://www.wicell.org#bib20]), as described in step 11 of the \u201cprepare germline genetic variation dataset[href=https://www.wicell.org#sec1.6]\u201d section of this protocol OR proceed with the Stranding protocol steps provided below.\nNote: Scripts used in this section are available at: https://github.com/rwsayaman/TCGA_PanCancer_Genotyping_Imputation[href=https://github.com/rwsayaman/TCGA_PanCancer_Genotyping_Imputation].\nPrior to stranding, identify and remove all palindromic SNPs (A/T or G/C) (--extract).\nPerform stranding to the Haplotype Reference Consortium using the McCarthy Group tools (https://www.well.ox.ac.uk/\u223cwrayner/tools/[href=https://www.well.ox.ac.uk/%7Ewrayner/tools/]; see section \u201cHRC or 1000G Imputation preparation and checking\u201d).\nDownload and unzip the tab delimited HRC reference file (version v1.1 HRC.r1-1.GRCh37.wgs.mac5.sites.tab or current version) from the Haplotype Reference Consortium (http://www.haplotype-reference-consortium.org/site[href=http://www.haplotype-reference-consortium.org/site]).Perform stranding of the quality-controlled genotyping file against the HRC reference panel using the high performance cluster version of the script (HRC-1000G-check-bim-v4.2.13-NoReadKey.zip[href=https://www.well.ox.ac.uk/~wrayner/tools/HRC-1000G-check-bim-v4.2.13-NoReadKey.zip]), which compares genotyping alleles to the corresponding SNP alleles from HRC.\nProvide the .bim file, the calculated allele frequencies (--freq) and the reference panel as inputs (See \u201cUsage with HRC reference panel\u201d).\nNote: The McCarthy Group tools (https://www.well.ox.ac.uk/\u223cwrayner/tools/[href=https://www.well.ox.ac.uk/%7Ewrayner/tools/]) stranding script removes SNPs with differing alleles, SNPs with\u00a0>\u00a00.2 allele frequency difference, and SNPs not in the reference panel. The McCarthy Group stranding script would also remove A/T & G/C palindromic SNPs with MAF\u00a0>\u00a00.4, however we chose to remove all palindromic SNPs in the preceding step to remove ambiguity.\nGenotype imputation\nTiming: Approximately 1\u00a0week. Dependent on imputation server availability\nThis section describes generation of Haplotype Reference Consortium (HRC) imputed genotyping files from the stranded and QC\u2019ed data (Figure\u00a02[href=https://www.wicell.org#fig2]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1887-Fig2.jpg\nFigure\u00a02. Expected distributions of imputation R2 and MAF values\n(A) Schematic of the number of SNPs (i)\u00a0originally downloaded, (ii) after QC, (iii) after imputation, and (iv) after imputation QC.\n(B) Hexagonal heatmap of 2d bin counts of the number SNPs post-imputation, showing the distribution of SNP HRC Imputation R2 (x-axis) against the log10 Minor Allele Frequency (MAF) values across all autosomal chromosomes (y-axis). (c) Table\u00a0showing the number and percent of SNPs below and above the suggested threshold levels of R2 \u2265 0.5 and MAF \u2265 0.005.\nNote: To skip this step, download the controlled access \u201cHRC Imputed Genotyping Data\u201d generated from (Sayaman et\u00a0al., 2021[href=https://www.wicell.org#bib20]) as described in step 11 of the \u201cprepare germline genetic variation dataset[href=https://www.wicell.org#sec1.6]\u201d section of this protocol OR proceed with phasing and imputation protocol steps provided below.\nNote: Scripts used in this section are available at: https://github.com/rwsayaman/TCGA_PanCancer_Genotyping_Imputation[href=https://github.com/rwsayaman/TCGA_PanCancer_Genotyping_Imputation] https://github.com/rwsayaman/TCGA_PanCancer_Immune_Genetics[href=https://github.com/rwsayaman/TCGA_PanCancer_Immune_Genetics].Perform phasing and imputation using the Haplotype Reference Consortium (HRC) (Loh et al., 2016[href=https://www.wicell.org#bib14]; McCarthy et\u00a0al., 2016[href=https://www.wicell.org#bib16]).\nTo reduce the run time, divide the HRC stranded PLINK file into 22 files corresponding to individual autosomal chromosomes, recode to VCF files and compress as .vcf.gz files.\nConduct phasing and imputation using a standard pipeline on the Michigan Imputation Server (MIS).\nPerform phasing using Eagle (version v2.3 or current version) on the variant call file (VCF) (Loh et al., 2016[href=https://www.wicell.org#bib14]). By default, Eagle restricts analysis to bi-allelic variants that exist in both the target and reference data.\nRun Minimac3 (Das et\u00a0al., 2016[href=https://www.wicell.org#bib10]) for imputation. For each of the 22 VCF files, the MIS breaks the dataset into non-overlapping chunks prior to imputation. For HRC imputation, select the HRC reference panel (version r1.1.2016 or current version) using mixed population for QC.\nDownload the HRC imputed germline files for each chromosome (\u201cchr\u2217.zip) from the MIS.\nUnzip each file using the provided password.\nEach unzipped folder contains 3 files:\n.dose.vcf.gz - imputed genotypes with dosage information.\n.dose.vcf.gz.tbi - index file of the .vcf.gz file.\n.info.gz file - information for each variant including quality and frequency (For Minimac3 info file, see: https://genome.sph.umich.edu/wiki/Minimac3_Info_File[href=https://genome.sph.umich.edu/wiki/Minimac3_Info_File]).\nFilter to exclude SNPs with imputation R2\u00a0<\u00a00.5 using bcftools, see Figure\u00a02[href=https://www.wicell.org#fig2]B. The imputation R2 is the estimated value of the squared correlation between imputed genotypes and true, unobserved genotypes.\nFilter \u201cchr\u2217.dose.vcf.gz\u201d files for R2 \u2265 0.5 and index. Generate filtered \u201cchr\u2217.rsq0.5.dose.vcf.gz\u201d and \u201cchr\u2217.rsq0.5.dose.vcf.gz.tbi \u201cfiles.\nGenerate new filtered \u201cchr\u2217.info.rsq0.5.gz\u201d files.\nConvert VCF files to PLINK files. Filter to exclude SNPs with MAF\u00a0<\u00a00.005, see Figure\u00a02[href=https://www.wicell.org#fig2]B.\nConvert VCF \u201cchr\u2217.rsq0.5.dose.vcf.gz\u201d files to PLINK \u201ctcga_imputed_hrc1.1_rsq0.5_chr\u2217.bed\u201d files (--double-id --vcf).\nFilter out SNPs (--maf) with MAF\u00a0<\u00a00.005 in PLINK.Note: If you plan to analyze only a subset of the samples, recalculate the MAF in PLINK (--freq) for the population of interest. Filter SNPs based on the recalculated frequency.\nDetermination of ancestry-associated SNPs\nTiming: 2 h\nThis section describes the association analysis between inferred genetic ancestry and SNP genotypes using the logistic regression implementation in the Hail framework.\nLoad imputed genotype data into the Hail framework.\nImport multi-sample .vcf files for each chromosome into Hail to create a \u2018matrix table\u2019 object.\nLoad sample metadata (described above) and annotate matrix table object using the hail \u2018annotate_cols\u2019 function.\nPerform sample quality control analysis using the hail \u2018sample_qc\u2019 function by filtering out samples that do not meet the following criteria:\nSample call rate, the proportion of non-missing or filtered genotype calls \u2265 95%.\nNon-admixed samples according to the consensus ancestry call annotation.\nPerform variant quality control analysis by filtering out SNPs that:\nDeviate from Hardy-Weinberg Equilibrium (HWE test p\u00a0<\u00a01\u00a0\u00d7\u00a010-6).\nGlobal allele frequency\u00a0<\u00a01%.\nFor each comparison (EUR-AFR and EUR-EAS), test the association between genetic ancestry and SNP genotypes using logistic regression (Hail function \u2018logistic_regression_rows\u2019).\nbinary response variable: ancestry (encoding: 0 EUR and 1 AFR/EAS).\nexplanatory variable: number of alternate alleles per sample (Ref: 0, Homozygous:1, Homozygous Alternative: 2).\ncovariates: biological sex, age.\nDetecting ancestry-associated quantitative expression trait loci\nTiming: 2 h\nLastly, we integrate the SNP genotype associations with ancestry and cancer-specific eQTLs to determine the extent to which germline genetic variation explains differential expression between ancestries.\nExtract significant cancer-specific eQTLs from the PancanQTL database.\nKeep eGene-eSNP pairs for which a given eGene is in the set of genes with significant ancestry-association expression.\nPerform table joins between the filtered eQTL results table and SNP genotype associations by SNP identity (dbSNP identifier).Note: The underlying datasets may be derived from different versions of dbSNP. Alternatively, you can consider joining tables by the chromosome name, genomic position, reference allele and alternate allele, assuming that both datasets are derived from the same version of the reference genome.\nFor each gene with demonstrated ancestry-differential expression, determine whether it has at least one ancestry-associated eSNP.\nCalculate summary statistics and visualize representative loci by cancer type as shown in Figure\u00a06 of (Carrot-Zhang et\u00a0al., 2020[href=https://www.wicell.org#bib7]).", "We developed compare_genomes, a transferable and extendible comparative genomics workflow for eukaryotic species built using the Nextflow framework and Conda package management system. It provides a wieldy pipeline to test for non-random evolutionary patterns that can be mapped to evolutionary processes to help identify the molecular basis of specific biological properties of the species analyzed. Additionally, it provides a template that other comparative genomics pipelines can use for improved reproducibility. It is also worth mentioning that other comparative genomics analysis methods exist that are not included in this workflow. These include syntenic block detection, as well as population genetics tools such as demographic history estimation to infer divergence times between populations or species.\nA detailed user manual describing how to install, set up, and run the workflow is presented in the README page of the compare_genomes project repository (https://github.com/jeffersonfparil/compare_genomes[href=https://github.com/jeffersonfparil/compare_genomes]). We included a tutorial analyzing four Arabidopsis species. Here, we detail the steps needed to run this example.\nNecessary Resources\nTo run this workflow using the provided example dataset: A minimum of one CPU core, one gigabyte (GB) of random-access memory (RAM), and 100 GB of free storage (our tests using the example dataset revealed that leveraging 32 CPU cores operating at base clock speeds of 2 GHz and using 50 GB of RAM enabled the completion of the analysis within 6 hr)\nDownload and run the example\n1. Download the compare_genomes repository:\n         \ngit clone https://github.com/jeffersonfparil/compare_genomes.git\n2. Install Conda. For more information, please follow the official Conda installation guide (https://conda.io/projects/conda/en/latest/user-guide/install/index.html[href=https://conda.io/projects/conda/en/latest/user-guide/install/index.html]). Installation on Linux can be achieved via:\n         \nwget\u2003\u2003\u2003 https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\nsh./Miniconda3-latest-Linux-x86_64.sh\n3. Import and activate the compare_genomes Conda environment:\n         \nconda\u2003\u2003\u2003 env\u2003\u2003\u2003 create\u2003\u2003\u2003 -n\u2003\u2003\u2003 compare_genomes\u2003\u2003\u2003 --file compare_genomes/compare_genomes.yml\nconda activate compare_genomes4. Edit the configuration file, i.e., compare_genomes/config/params.config, by replacing line number 2, dir = \u2018/data/TEST\u2019, with the absolute path on the user's computer that will be used as the output directory for the workflow:\n         \ncd compare_genomes\nnano config/params.config\nReplace \u201cdir='/data/TEST'\u201d with your path.\n5. Run the example:\n         \nchmod +x run.sh\ntime ./run.sh\nEdit configuration files\n6. To set up the workflow for a new analysis, edit the seven configuration files located in compare_genomes/config/.\n         \na.\u2018urls.txt\u2019: list of web links or absolute paths to the genome sequences, genome annotations, coding DNA sequences, and amino acid sequences for at least three species to be included in the analyses. It is formatted as a headerless, two-columned, comma-separated file. Column 1 contains the filenames of the genome sequence, genome annotation, coding DNA sequence, and amino acid sequences (species names and extension names should be the consistent across these files, e.g., \u2018.fna\u2019 for the genomes, \u2018.gff\u2019 for the annotations, \u2018.cds\u2019 for the coding DNA sequences, and \u2018.faa\u2019 for the amino acid sequences). Column 2 contains the URL (uniform resource locator) of the zipped (\u2018.gz\u2019 or \u2018.zip\u2019) or unzipped files for download. Alternatively, this can be the absolute path to the pre-downloaded zipped or unzipped files on a local computer.\nb.\u2018dates.txt\u2019: list of pairwise divergence times between species. This information can be found at http://timetree.org[href=http://timetree.org]. Divergence times between all pairs of species are not required. Pick at least two pairs of species divergence times, ideally including the outgroup species. This file is formatted as a headerless, two-columned, tab-delimited file. Column 1 contains the pair of species separated by a comma with the same names used in the \u2018urls.txt\u2019. Column 2 contains the time in million years, e.g., \u2018-160\u2019 for 160 million years ago.c.\u2018comparisons_4DTv.txt\u2019: list of species and pairs of species to be included in the estimation of transversion rates among four-fold-degenerate sites (4DTv). This statistic is used to set the molecular clock, with more mutations at the third codon position meaning more divergence time between a pair of sequences. By default, 4DTv is estimated using genes present with two copies. Edit line 34 of \u2018compare_genomes/modules/assess_WGD.nf\u2019 to include genes with more than two copies. This is formatted as a headerless, one-columned file. Column 1 contains the species and/or pairs of species names, which should match the names in \u2018urls.txt\u2019, and species pairs should be written as, for example, \u201cZea_mays X Oryza_sativa\u201d.\nd.\u2018venn_species_max_5.txt\u2019: list of at most five species to be plotted in the Venn diagram comparing the differences and commonalities of gene families between species. It is currently not possible to fit more than five species in the Venn diagram because of the limitations of the plotting package used. This is formatted as a headerless, one-columned file. Column 1 contains the species names matching those in \u2018urls.txt\u2019.\ne.\u2018genes.txt\u2019: links to the gene sequences to be tested for significant expansion/contraction and for nonsynonymous/synonymous mutation (Ka/Ks) rates between pairs of sequences within and among species. It is formatted as a headerless, three-columned, comma-separated file. Column 1 contains phenotype names or some identifier (noncritical information). Column 2 contains the species names from which the gene sequence was derived and can be a species not included in \u2018urls.txt\u2019 (noncritical information). Column 3 contains the URL of the genes to be downloaded and analyzed.\nf.\u2018params.config\u2019: configuration file listing the parameter values for the specific analyses to be conducted.\n                  \n\u2018dir\u2019: output directory.\n\u2018species_of_interest\u2019: a single species of interest, which should match one of the species listed in \u2018urls.txt\u2019.\u2018species_of_interest_panther_HMM_for_gene_names_url\u2019: URL to the Panther HMM database to extract gene names from, preferably from the species used for the gene ontology (GO) term enrichment analysis. See the current release list at http://data.pantherdb.org/ftp/sequence_classifications/current_release/PANTHER_Sequence_Classification_files/[href=http://data.pantherdb.org/ftp/sequence_classifications/current_release/PANTHER_Sequence_Classification_files/].\n\u2018urls\u2019: location of \u2018urls.txt\u2019.\n\u2018dates\u2019: location of \u2018dates.txt\u2019.\n\u2018comparisons_4DTv\u2019: location of \u2018comparisons_4DTv.txt\u2019.\n\u2018venn_species_max_5\u2019: location of \u2018venn_species_max_5.txt\u2019.\n\u2018genes\u2019: location of \u2018genes.txt\u2019.\n\u2018cafe5_n_gamma_cats\u2019: number of the Gamma values (parameter of the substitution model) to use for the assessment of significant gene family expansion and contraction using CAFE5. If this is equal to 1, then we use the substitution model without the gamma function.\n\u2018cafe5_pvalue\u2019: significance threshold for the gene family expansion and contraction test.\n\u2018go_term_enrich_genome_id\u2019: genome ID for the species specified in \u2018species_of_interest_panther_HMM_for_gene_names_url\u2019 or some closely related species. Find the appropriate taxon ID at https://pantherdb.org/services/oai/pantherdb/supportedgenomes[href=https://pantherdb.org/services/oai/pantherdb/supportedgenomes].\n\u2018go_term_enrich_annotation_id\u2019: code for the gene ontology level to be used, e.g., \u201cGO:0008150\u201d for \"Biological Process\". See the list of GO codes at https://pantherdb.org/services/oai/pantherdb/supportedannotdatasets[href=https://pantherdb.org/services/oai/pantherdb/supportedannotdatasets].\n\u2018go_term_enrich_test\u2019: GO term enrichment test, which can be set to either \u201cFISHER\u201d (Fisher's exact test) or \u201cBINOMIAL\u201d (binomial distribution test).\n\u2018go_term_enrich_correction\u2019: multiple testing correction which can be set to \u201cNONE\u201d, \u201cFDR\u201d (false-discovery rate), or \u201cBONFERRONI\u201d (Bonferroni correction).\n\u2018go_term_enrich_ngenes_per_test\u2019: number of randomly sampled genes to include in each GO term enrichment analysis.\n\u2018go_term_enrich_ntests\u2019: number of GO term enrichment test replications to perform.\ng.\u2018process.config\u2019: configuration file setting the computing resource allocation. Assign the number of \u2018cpus\u2019 and \u2018memory\u2019 capacity to use for low- and high-resource tasks with \u2018LOW_MEM_LOW_CPU\u2019 and \u2018HIGH_MEM_HIGH_CPU\u2019, respectively.\nWorkflow of compare_genomes\n7. The compare_genomes workflow consists of nine analysis steps under the default setup (Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-fig-0001], left).a.Download the user-defined genome datasets: genome sequences (fasta, .fna), annotations (general feature format, .gff), coding DNA sequences or CDS (.cds), protein sequences (fasta, .faa), protein-coding gene models (probabilistic protein model format, .hmm), corresponding gene ontology terms (.txt), and protein sequences of specific genes of interest (.faa).\nb.Identify orthogroups using OrthoFinder (Emms & Kelly, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-bib-0004]) and gene families for each orthogroup using HMMER3 (Mistry et\u00a0al., 2013[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-bib-0008]) and Panther HMMs (protein-coding gene family models; Thomas et\u00a0al., 2022[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-bib-0015]). An orthogroup is a set of genes descended from a single gene from the last common ancestor of all the species included in the analysis.\nc.Infer phylogenetic trees for each orthogroup using IQ-TREE 2 (Minh et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-bib-0007]) based on CDS alignments generated by MACSE (Ranwez et\u00a0al., 2011[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-bib-0013]) and the most likely nucleotide substitution model inferred by ModelFinder (Kalyaanamoorthy et\u00a0al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-bib-0005]).\nd.Infer the rate of sequence divergence based on transversion rates among four-fold-degenerate sites (4DTv) in single-copy genes between pairs of species using the custom Julia script for this purpose that ships with compare_genomes. 4DTv is a proxy for time, where the accumulation of transversion mutations at the degenerate (neutral) site is proportional to the amount of time passed.\ne.Identify whole-genome duplication events using 4DTv computed from multi-copy gene families. If the paralogs within a genome show an accumulation of 4DTv that is greater than 0, it is likely that a genome duplication or polyploidization occurred from which the multiple paralogs are derived.\nf.Test for significant gene family expansion or contraction across genomes using CAFE (version 5; De Bie et\u00a0al., 2006[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-bib-0002]). Gene family expansion and contraction are calculated relative to the gene family count in the ancestral species given the phylogenetic tree from IQ-TREE 2. Expanded and contracted gene families can be indicative of adaptation.g.Analyze gene ontology (GO) term enrichment for significantly expanded gene families using the Panther GO API (Mi et\u00a0al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-bib-0006]). To test the significantly contracted gene families, replace all instances of the term \u2018expanded\u2019 with \u2018contracted\u2019 in lines 47-59 of \u2018compare_genomes/modules/GO_enrichment.nf\u2019.\nh.Visualize a summary of the results (i.e., see Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-fig-0001], right, for a sample output). This generates the summary output of the whole-genome-level analysis.\ni.This optional step is available for testing hypotheses involving specific genes: analyze user-defined genes of interest, i.e., gene family expansion/contraction analyses with CAFE, and estimate non-synonymous to synonymous nucleotide substitution rates (Ka/Ks) using KaKs_Calculator 2.0 (Wang et\u00a0al., 2009[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.876#cpz1876-bib-0017]) and custom R script.\nCompare_genomes was implemented using Nextflow to easily integrate other Linux-based bioinformatics analysis steps. Analysis steps can be easily added or modified, for example by adding a GO term enrichment analysis for significantly contracted gene families, or substituting MACSE for another multiple sequence alignment tool.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/87506be9-3fb0-4a29-b99e-e34dd1c8a3fa/cpz1876-fig-0001-m.jpg</p>\nFigure 1\nLeft, the steps performed by the compare_genomes comparative genomics workflow. Right, a sample output plot generated by the compare_genomes workflow using four Arabidopsis species, with three parts indicated by letters (see Understanding Results for description).", "Step-by-step method details\nStep-by-step method details\nOption 1. PNN disassembly by repeated KXA-induced anesthesia\nTiming: 1\u20133\u00a0weeks\nHere, we describe first how to prepare a ready-to-use KXA solution, followed by performing the intraperitoneal injection, and finally we outline how to check the level of anesthesia and how to maintain the animals during the anesthetized state.\nThe drug dosages used for mice are: (1) ketamine 100\u00a0mg/kg; (2) xylazine 10\u00a0mg/kg; (3) acepromazine 3\u00a0mg/kg.\nNote: The effect of PNN loss also occurs with just ketamine and xylazine. On the other hand, acepromazine reduces the death rate and improves the surgical plane (Arras et al., 2001[href=https://www.wicell.org#bib4]).\nCritical: Ketamine at this dosage must be combined with xylazine to prevent uncontrolled muscular contraction.\nPrepare the ketamine-xylazine-acepromazine (KXA) solution.\nCombine 2\u00a0mL of ketamine, 1\u00a0mL of xylazine, and 0.6\u00a0mL of acepromazine in 3\u00a0mL of physiological saline solution containing 0.9% (w/v) NaCl.\nVortex for 30 s.\nThe KXA solution should look yellow without any precipitates and can be stored for up to 2\u00a0weeks at 4\u00b0C. Troubleshooting 1[href=https://www.wicell.org#sec7.1]\nWeigh the mice on a fine scale.\nCalculate the injection dosage for each mouse. A mouse receives 33\u00a0\u03bcL/10\u00a0g weight e.g., a mouse weighing 30\u00a0g receives 99\u00a0\u03bcL of the KXA solution (Harkness and Wagner, 1995[href=https://www.wicell.org#bib9]).\nCritical: The drug dosage must be adjusted for each mouse. Even if mice are the same sex or come from the same litter, there may be fluctuations in the body weight.\nPrepare a 1\u00a0mL syringe equipped with a 26G needle with the appropriate amount of KXA solution.\nInject intraperitoneally. A schematic of the repeated KXA treatment protocol can be found in Figure\u00a01[href=https://www.wicell.org#fig1]A)\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1234-Fig1.jpg\nFigure\u00a01. PNN disassembly by repeated KXA-induced anesthesia(A) Experimental timeline for repeated KXA application including the expected PNN loss and recovery rate after KXA exposure. KXA, ketamine-xylazine-acepromazine. PNN, perineuronal net. S1, primary somatosensory cortex. V1, primary visual cortex.\n(B) Mouse receiving injection. Left, restraining position for intraperitoneal (i.p.) injection. Arrow, position of the tail between the ring and the pinky of the operator. Right, the injection procedure, in which the needle is inserted with a 45\u00b0 angle into the lower quadrant of the abdomen.\nGrab the mouse at the neck and turn it around (Figure\u00a01[href=https://www.wicell.org#fig1]B).\nInsert the needle at 45\u00b0 into the peritoneal cavity (Figure\u00a01[href=https://www.wicell.org#fig1]B).\nCritical: Do not inject on the linea alba. This might cause lesions to the bladder.\nCritical: Make sure that the solution does not leak out of the mouse.\nNote: After the injection, no solution should accumulate and be visible under the skin.\nTurn the mouse back, head down, and gently release it back into its home cage.\nNote: Tilting the mouse head down helps push the internal organs towards the head, which prevents damage from the injection.\nCritical: The injection must be performed intraperitoneally with the needle inserted directly into the peritoneal cavity. In our experience and with the anesthetic dosage used in this protocol, misplacement will cause inefficient anesthesia induction. As a note, it has been shown that ketamine anesthesia can be achieved with subcutaneous administration in mice with double the concentration used here (Levin-Arama et al., 2016[href=https://www.wicell.org#bib14]).\nMonitor the state of anesthesia.\nAfter the injection, the mouse will undergo a hyperexcitation phase for the first 1\u20132\u00a0min marked by increased running activity. Then, the mouse will slowly become unconscious, and the anesthesia will begin. It might take 5\u201310\u00a0min to reach the induction of the surgical plane, which will last for approximately 40\u00a0min. Troubleshooting 2[href=https://www.wicell.org#sec7.3]Apply eye ointment to prevent dehydration of the sclera.\nConfirm the anesthesia by checking the following parameters (Tsukamoto et al., 2015[href=https://www.wicell.org#bib21]):\nAbsence of the toe pinch reflex around 10\u00a0min after induction.\nDecrease in respiratory frequency.\nNo responses to noxious stimuli.\nFlaccid paralysis.\nAbsence of whisker movement.\nCritical: To prevent hypothermia, the mouse should be kept at 37\u00b0C with a heating pad or a red lamp for the entire time (induction, deep anesthesia and recovery phase).\nNote: Regularly check temperature with a thermometer and adjust the heating device accordingly.\nAfter complete recovery, return the mouse to the animal facility with food and water ad libitum.\nNote: If many animals are housed in the same cage, make sure that all animals are awake, as they otherwise become aggressive to the ones that are still waking up.\nNote: To avoid potential impact of circadian rhythm on brain activity (Hor et al., 2019[href=https://www.wicell.org#bib10]), we always injected the animals at the same time (in our case 9 a.m.) and collected the tissue 4\u00a0h after the last treatment.\nRepeat steps 1\u20137 every 2\u20133\u00a0days at least two times (up to 6 times) to reach the desired level of reduction in the number of PNN-coated cells (Figure\u00a01[href=https://www.wicell.org#fig1]A). Expected results for 6\u00d7 KXA treatment can be found in Figures 2[href=https://www.wicell.org#fig2]A and 2B.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1234-Fig2.jpg\nFigure\u00a02. Expected results for PNN loss and recovery with repeated KXA-induced anesthesia\n(A) Wisteria floribunda agglutinin (WFA)-labeled parasagittal brain sections after 6\u00d7 saline (left), and 6\u00d7 KXA treatment (right). Scale bar: 1000\u00a0\u03bcm and 800\u00a0\u03bcm, respectively. Color intensity reflects PNN density with blue and white low and high density, respectively. CB, cerebellum. CP, caudato putamen. FR, frontal cortex. IC, inferior colliculus. M1, primary motor cortex. PIR, piriform cortex. S1, primary somatosensory cortex. V1, primary visual cortex.(B and C) PNN density in S1. Absolute number of PNN-coated cells\u00a0\u00b1 SEM after repeated KXA treatment (B) and after 3\u00d7 KXA withdrawal (C). Triangle, female. Circle, male. Linear regression model with selected post-hoc comparison. \u2217\u2217p\u00a0< 0.01. \u2217\u2217\u2217p\u00a0< 0.001, nsp > 0.05. SEM: standard error of the mean.\nNote: We typically performed our analyses 4\u00a0h after the last injection.\nNote: 20% of PNN recovers within 3\u00a0days after the last 3\u00d7 KXA injection and reaches full coverage by 14\u00a0days for S1 (Figure\u00a02[href=https://www.wicell.org#fig2]C) and V1 of C57BL/6J mice (Venturino et al., 2021[href=https://www.wicell.org#bib22]).\nNote: PNN recovery kinetics may differ between the mouse model and strain background.\nNote: We observed a significant reduction in the number of PNNs in S1 and V1 after repeated KXA anesthesia (Figure\u00a02[href=https://www.wicell.org#fig2]A). The effect of this treatment may not be the same in all brain regions because ketamine does not act equally throughout the brain (Alda et\u00a0al., 1994[href=https://www.wicell.org#bib3]). For example, we found PNN removal in the hippocampus only after 6\u00d7 KXA (Venturino et al., 2021[href=https://www.wicell.org#bib22]).\nOption 2. PNN disassembly by 60-Hz light entrainment\nThis section describes first how to build and program a 60\u00a0Hz light stimulation device, and then how to verify the stimulation frequency. Steps 9\u201317 outline the building of the hardware, steps 18\u201324 the software.\nPart 1. Build light stimulation device for PNN disassembly with 60\u00a0Hz\nTiming: up to 1 h\nAn overview of the circuit and the components needed for the assembly of the light stimulation device can be found in Figures 3[href=https://www.wicell.org#fig3]A and 3B.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1234-Fig3.jpg\nFigure\u00a03. Construction of the light stimulation device\n(A) Schematic of the circuit designed with the software Fritzing.\n(B) Overview of the necessary tools.(C\u2013I) Step-by-step constructions of building the electric circuit. (C) Transistor is inserted on the breadboard. White arrow, ground pin. (D) Electric connection built between the transistor ground (blue arrow) and the breadboard ground (orange arrow). (E) The resistor is connected with the left transistor pin (blue arrow) to the breadboard ground (orange arrow). (F) The orange cable connects the Arduino ground port (orange arrow) to the breadboard ground pin (blue arrow). (G) The yellow cable connects the Arduino digital port 5 (orange arrow) to the left transistor pin (blue arrow). (H) The red LED voltage cable is connected to the Arduino Vin port (red arrow). The grey jumper wire to the transistor exit pin (blue arrow). (I) Soldering the red voltage wire from the LED strip to a jumper wire.\n(J) Screenshot of Arduino IDE software user-interface for the board programming.\n(K and L) Code \u201ccheck\u201d and \u201cupload\u201d Arduino IDE icons, respectively.\n(M) Oscilloscope reading of the Arduino digital port 5 output showing 60\u00a0Hz square wave function.\n(N) Adjust a mouse cage to the center of the stimulation box for equal illumination.\nStick the LED strip horizontally along the inner walls of a black plastic box (60\u00a0\u00d7 40\u00a0\u00d7 32\u00a0cm, 62 l volume). The LED strip goes round the box in 6 layers with 4\u00a0cm space in between.\nNote: Add the LED stripes in a way from top to bottom to allow equal illumination.\nThe circuit is built as outlined in Figure\u00a03[href=https://www.wicell.org#fig3]A.\nTake the breadboard and insert the transistor. The ground pin of the transistor is on the right (Figure\u00a03[href=https://www.wicell.org#fig3]C).\nNote: The ground pin of the transistor is often indicated on the chip with the letter \u201cG\u201d.\nConnect the ground of the breadboard (blue symbol \u201c\u2212\u201d) with the ground of the transistor (Figure\u00a03[href=https://www.wicell.org#fig3]D).Put a 200 \u03a9 resistor between the ground of the breadboard and the left pin of the transistor (Figure\u00a03[href=https://www.wicell.org#fig3]E).\nNote: A resistor is critical for the functionality of the transistor.\nConnect one ground port of the Arduino board (indicated with the letters GND) to the ground of the breadboard (blue symbol \u201c\u2212\u201d) (Figure\u00a03[href=https://www.wicell.org#fig3]F).\nConnect the digital port 5 of the Arduino board to the left pin of the transistor (Figure\u00a03[href=https://www.wicell.org#fig3]G).\nNote: The resistor should be in front of the cable that goes to the left pin of the transistor.\nNote: The number of the Arduino digital port depends on the programming code. We used digital port 5 in our code to send the signal to the LEDs.\nConnect the voltage wires from the LED strip as follows:\nthe red cable is connected to the Arduino board at the Vin (voltage in) port, and\nthe black cable to the middle pin of the transistor (Figure\u00a03[href=https://www.wicell.org#fig3]H).\nNote: If the cable is a fine-stranded conductor, solder a jumper wire on the cable to allow a stable connection with the transistor on the breadboard (Figure\u00a03[href=https://www.wicell.org#fig3]I).\nCritical: Insulate the solder joint with electrical tape.\nThe system is connected to a 12\u00a0V power supply via the jack plug of the Arduino.\nDownload and install the Arduino IDE for PC or Mac from this link: https://www.arduino.cc/en/main/software[href=https://www.arduino.cc/en/main/software]\nConnect the Arduino board to the laptop with a USB cable type A/B.\nOpen the Arduino IDE (Figure\u00a03[href=https://www.wicell.org#fig3]J) and select the Arduino board in Tools > Board > Arduino/Genuino UNO.\nNote: Ensure that the correct Arduino board is selected.\nCopy and paste the following codes:\nfor the 60\u00a0Hz flickering in the Arduino IDE text editor:\nint ledPin\u00a0= 5;//digital input from the board\nvoid setup ()\n{\n\u00a0\u00a0pinMode(ledPin, OUTPUT);\n}\nvoid loop()\n{digitalWrite(ledPin, HIGH);//light on, no fading\n\u00a0\u00a0delay(8.3);//delay in ms\n\u00a0\u00a0digitalWrite(ledPin, LOW);//light off, no fading\n\u00a0\u00a0delay(8.3);//delay in ms\n}\nfor the control with the same light intensity without 60\u00a0Hz flickering.\nint ledPin\u00a0= 5;//digital input from the board\nvoid setup ()\n{\n\u00a0\u00a0pinMode(ledPin, OUTPUT);\n}\nvoid loop()\n{\n\u00a0\u00a0digitalWrite(ledPin, HIGH);//keep light on\n}\nNote: In this script, we use digital port 5 to send the signal to the LEDs. If another digital port is used, the script has to be modified accordingly.\nVerify that the script is functional by clicking on the \u201ccheck\u201d icon (Figure\u00a03[href=https://www.wicell.org#fig3]K).\nNote: If the script doesn\u2019t work the Arduino IDE will show a red error message reporting in which line of the code the problem is. Troubleshooting 3[href=https://www.wicell.org#sec7.5]\nSelect the output port for uploading the code in Tools > Port. In our example, the port is COM5.\nNote: Ensure that the port number corresponds to your Arduino.\nUpload the code from step 21 by clicking on the \u201cupload\u201d icon (Figure\u00a03[href=https://www.wicell.org#fig3]L).\nNote: The Arduino board memorizes previously uploaded code. Each time the board is powered up it will execute the code automatically.\nConfirm with an oscilloscope that the frequency (60\u00a0Hz) and the shape (square wave) of the stimulation is correct (Figure\u00a03[href=https://www.wicell.org#fig3]M).\nTake a mouse cage, adjust it to the center of the stimulation box, and mark the position with a tape (Figure\u00a03[href=https://www.wicell.org#fig3]N).\nPart 2. PNN disassembly with 60\u00a0Hz light entrainment\nTiming: 5\u00a0days\nThis section outlines the light stimulation protocol for inducing PNN disassembly with 60\u00a0Hz light entrainment with the previously built device. An overview of the 60\u00a0Hz light stimulation protocol and the expected results can be found in Figure\u00a04[href=https://www.wicell.org#fig4].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1234-Fig4.jpg\nFigure\u00a04. PNN disassembly by 60-Hz light entrainment with expected PNN loss(A) Experimental timeline for the 60\u00a0Hz light entrainment. V1, primary visual cortex.\n(B) Wisteria floribunda agglutinin (WFA)-labeled coronal brain sections of mice treated for 5\u00a0days with light control (no flickering, left) or 60\u00a0Hz (right). HP, hippocampus. Scale bar, 200\u00a0\u03bcm. Zoom-in, 50\u00a0\u03bcm.\n(C) Percentage of change in mean PNN-coated cells\u00a0\u00b1 SEM in V1. Triangle, females. Circles, males. Two-sample t test. \u2217\u2217\u2217p\u00a0< 0.001. SEM, standard error of the mean.\nAcclimatize animals after they have arrived from the animal facility for 1 h.\nNote: To avoid potential impact of circadian rhythm on brain activity (Hor et al., 2019[href=https://www.wicell.org#bib10]), we always stimulated the animals at the same time (in our case from 9 to 11 a.m.) and collected the tissue four hours after the last stimulation.\nRemove all enrichment from the cage before stimulation to prevent the mouse from hiding.\nNote: The animals should always have unlimited access to food and water.\nPosition the animal cage in the marked position in the center of the box.\nNote: Up to 4 animals can be simultaneously stimulated.\nStart the Arduino software as described in the previous section in steps 21\u201324 and select the stimulation protocol for either 60\u00a0Hz or control non-flickering light but with the same light intensity.\nExpose the mice to light stimulation for 2\u00a0h per day for 5 consecutive days (Figure\u00a04[href=https://www.wicell.org#fig4]A).\nNote: During the stimulation the box lid was closed.\nNote: This test paradigm causes approximately 40% PNN reduction in V1 (Figures 4[href=https://www.wicell.org#fig4]B and 4C). We have no insights into whether prolonged treatment will further reduce PNN and how fast the PNN recovers. According to the recovery rate after 3\u00d7 KXA administration, the PNN recovers by 20% within 3\u00a0days after KXA withdrawal (Figure\u00a02[href=https://www.wicell.org#fig2]C).Note: We typically perfused the animal 4 hours after the last stimulation and performed image analysis. For detailed protocols see Venturino et al. (2021)[href=https://www.wicell.org#bib22]. Alternatively, this protocol could be explored for in vivo electrophysiological recordings in awake mice, behavioral studies, and in vivo imaging experiments to name a few.\nNote: The stimulation device generates electrical noise, therefore further modifications, such as isolating the electrical connections with copper mesh, are required to enable in-vivo electrophysiology during the 60\u00a0Hz light stimulation. Troubleshooting 4[href=https://www.wicell.org#sec7.7]", "Step-by-step method details\nStep-by-step method details\nExtraction of cell nuclei\nTiming: 1\u20132 h\n    \nThis step describes the procedure for preparation of the nuclei suspension. Steps 1 and 2 are used for\n      preparation of cell nuclei from embryos, while steps 3 and 4 are used for seedlings.\nApproximately 200 embryos at the late-cotyledon stage were collected with a 1\u00a0mL injection syringe under a\n        dissecting microscope at 25\u00b0C. The collected embryos were placed on a 3.5\u00a0cm diameter round dish supplemented\n        with B5 agar media (Figure\u00a01[href=https://www.wicell.org#fig1]A). The B5 agar will keep the embryos alive. The dissected\n        embryos were transferred into a 12\u00a0cm long square dish with 500\u00a0\u03bcL pre-chilled lysis buffer on ice.\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/418-Fig1.jpg\nFigure\u00a01. Generation of nuclei suspension\n(A) The Arabidopsis embryos at the late-cotyledon stage were collected on the B5 agar.\n(B) The nuclei lysis suspension of embryos after filtering.\n(C) The 3-day-old Arabidopsis seedlings were collected on the B5 agar.\n(D) The nuclei lysis suspension of seedlings after filtering.\nFinely chop embryos with a double-sided blade to release nuclei on ice in fume hood.\nNote: The embryos naturally settle at the bottom of the lysis buffer, so we can\n      collect the materials together and repeatedly cut them. Due to the small volume of embryos, please cut them\n      repeatedly to release more nucleus until the color of the lysis buffer is yellow-green (Figure\u00a01[href=https://www.wicell.org#fig1]B), which should take approximately 10\u00a0min. 200 embryos should provide a sufficient\n      number of nuclei.\n    \nApproximately 3-day-old 150 - 200 seedlings grown on B5 agar were collected with forceps at RT. The seedlings\n        were transferred to a 12\u00a0cm long square dish with 500\u00a0\u03bcL pre-chilled lysis buffer on ice.\nFinely chop seedlings with a double-sided blade to release nuclei on ice in fume hood.Note: For seedlings, 5\u00a0min of chopping is enough. The color of the lysis buffer\n      will be bright green after chopping.\n    \nFilter the slurry through a 40\u00a0\u03bcm filter into 15\u00a0mL collection tubes on ice. Rinse the petri dish with 1.5\u00a0mL\n        lysis buffer and filter into the same 15\u00a0mL collection tube. The final nuclei suspension is about 2\u00a0mL in volume\n        and the color should appear yellow-green.\nCritical: For acquiring more intact nuclei, we recommend extensively chopping\n      plant materials in lysis buffer rather than grinding them in liquid nitrogen which leads to more tissue fragments\n      that will disturb FACS.\n    \nFACS of cell nuclei\nTiming: 1\u20135 h\n    \nThis step describes the procedure for sorting nuclei by FACS.\nThe nuclei suspensions are stained with DAPI for flow cytometric analysis. Set up an unstained control without\n        DAPI.\nNote: It is not necessary to estimate the density of nuclei suspension in this\n      step because we will sort \u223c50,000 nuclei per replicate to ensure repeatability. The amount of nuclei suspension\n      required for control samples is flexible. The purpose of control is to determine the appropriate voltage needed to\n      exclude the interference of other particles.\n    \nAdd 2\u00a0\u03bcL of 1\u00a0mg/mL DAPI to the nuclei suspension in a sample tube, flick gently to mix.\nSelect parameters including FSC-A (forward scatter-A, particle size), SSC-A (side scatter-A, internal\n        complexity), DAPI-A (intensity of DAPI staining), and DAPI-W (voltage pulse width).\nUse the unstained control and stained samples to set appropriate photomultiplier tube (PMT) voltages and\n        adjust compensations on a BD FACSAria II cell analyzer.\nCreate gates for the samples according to the position of 2C or 4C nuclei (Figure\u00a02[href=https://www.wicell.org#fig2]).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/418-Fig2.jpg\nFigure\u00a02. Gate set for FACS\n(A) FACS histogram of embryo nuclei suspension. The DAPI-A-2C and 4C represent cell ploidy with 2 timesand 4 times, respectively. Only DAPI-A-2C cell nuclei was sorted.\n(B) FACS histogram of seedling nuclei suspension. The DAPI-A-2C, -4C, and -8C represent cell ploidy with\n              2 times, 4 times, and 8 times, respectively. All DAPI-A-2C, -4C, and -8C cell nuclei were sorted.\nAdd 500\u00a0\u03bcL of lysis buffer to a 10\u00a0mL BD collect tube and collect a total of 50,000 events per sample (each\n        sample per tube). The nozzle size, psi, and sorting speed of FACS are 70\u00a0\u03bcm, 70, and 1, respectively.\nNote: 200 embryos and 150\u2013200 3-day-old seedlings are enough material for getting\n      50,000 nuclei.\n    \nCentrifuge at 1,000\u00a0\u00d7 g for 10\u00a0min at 4\u00b0C.\nNote: Since we cannot see the pellet at the bottom of the tube after\n      centrifugation, please leave \u223c50\u00a0\u03bcL of liquid to avoid nuclei loss when discarding the supernatant.\n    \nPipette 1\u00a0mL of pre-chilled wash buffer into the centrifuge tube.\nCentrifuge at 1,000\u00a0\u00d7 g for 10\u00a0min at 4\u00b0C and discard the supernatant as much as possible.\nNote: Immature embryo cells at late-cotyledon developmental stage rarely divide.\n      DAPI-A detected nuclei almost with 2C from FACS (Figure\u00a02[href=https://www.wicell.org#fig2]A). The nuclei were sorted into lysis\n      buffer as plant nuclei do not remain intact in PBS buffer that is used for FACS.\n    \nATAC-seq library constructing\nTiming: 5\u20138 h\n    \nThis step describes the procedure for generation of ATAC-seq library.\nWe use the TruePrep DNA Library Prep Kit V2 for Illumina including 5\u00d7 TTBL and TTE Mix V50 (Vazyme, TD50102; see\n      manual at http://www.vazymebiotech.com/products_detail/productId=70.html[href=http://www.vazymebiotech.com/products_detail/productId=70.html])\n      to construct ATAC-seq libraries. We use the MinElute Reaction Cleanup Kit including Buffer PB, PE and EB (Qiagen)\n      to purify transposed DNA.\nPrepare the transposition master mix in a 200\u00a0\u03bcL Lobind tube (Eppendorf) on ice. Add reagents in the order\n        shown below.\ntable:files/protocols_protocol_418_3.csvNote: The volume of nuclei suspension after step 14 is variable. Please sure that\n      the remaining nuclear suspension is less than 34.5\u00a0\u03bcL since the total reaction volume for transposition is 50\u00a0\u03bcL.\n      It is not necessary to optimize reaction conditions (e.g., transposition time) when working with different initial\n      volumes or numbers of nuclei.\n    \nDispense master mix into a PCR tube containing nuclear suspension. Pipette to mix.\nLoad the tube into a thermal cycler pre-warmed to 37\u00b0C and incubate for 30\u00a0min with occasional gentle mixing\n        to keep the nuclei in suspension.\nAdd 3\u20135 volumes of Buffer PB to 1 volume of the PCR sample.\nTransfer the mixture to a spin column that is situated in a provided 2\u00a0mL collection tube.\nCentrifuge at 13,523\u00a0\u00d7 g for 1\u00a0min at 4\u00b0C.\nDiscard the flow-through and add 750\u00a0\u03bcL of Buffer PE to the spin column.\nCentrifuge at 13,523\u00a0\u00d7 g for 1\u00a0min at 4\u00b0C.\nDiscard the flow-through and centrifuge the column for an additional 2\u00a0min.\nPlace column in a clean 1.5\u00a0mL DNA Lobind microcentrifuge tube and add 11\u00a0\u03bcL of buffer EB to the center of the\n        column for 2\u00a0min.\nCentrifuge at 13,523\u00a0\u00d7 g for 1\u00a0min at 4\u00b0C.\nPause Point: Samples can be safely stored overnight at 4\u00b0C or \u221220\u00b0C.\n    \nWe use the TruePrep DNA Index Kit V2 for Illumina sequencing (Vazyme) and NEBNext high-fidelity PCR mix to\n      amplify transposed DNA.\nPrepare the PCR amplification mix in a 0.2\u00a0mL PCR tube on ice as follows:\ntable:files/protocols_protocol_418_4.csv\nIncubate the reaction in a thermo cycler with the following PCR program.\ntable:files/protocols_protocol_418_5.csv\nTo determine the number of additional PCR cycles needed to adequately amplify the DNA library, prepare the\n        quantitative PCR (qPCR) Library Amplification Mix as follows:\ntable:files/protocols_protocol_418_6.csvIncubate the reaction in a qPCR thermocycler with the following PCR program.\ntable:files/protocols_protocol_418_7.csv\nTo determine the optimal number of cycles needed to amplify the remaining 45\u00a0\u03bcL of each library, view the\n        linear fluorescence versus cycle number plot on the qPCR machine once the reaction is finished. The cycle number\n        at which the fluorescence for a given reaction is at 1/3 of its maximum is the number of additional cycles (N)\n        that each library requires for adequate amplification (Figure\u00a03[href=https://www.wicell.org#fig3]).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/418-Fig3.jpg\nFigure\u00a03. Schematic diagram of qPCR\nThe maximum of fluorescence is 2,000. As such, one-third of the maximum is ~666, and the number of PCR\n              cycles should be 9 (red line).\nIncubate the remaining reaction in a thermocycler with the following PCR program.\ntable:files/protocols_protocol_418_8.csv\nAdd 67.5\u00a0\u03bcL (1.5\u00d7 of sample volume) resuspended Ampure XP beads to the PCR reaction. Mix well by pipetting up\n        and down at least 10 times. Alternatively, samples can be mixed by vortexing for 3\u20135 s.\nNote: If Ampure XP beads are stored at 4\u00b0C, please pre-warm to 25\u00b0C for at least\n      30\u00a0min and vortex before use.\n    \nIncubate samples on the bench for at least 5\u00a0min at 25\u00b0C.\nPlace the tube on an appropriate magnetic stand to separate the beads from the supernatant. Quickly centrifuge\n        the sample to collect the liquid from the sides of the tube before placing it on the magnetic stand if\n        necessary.\nAfter 5\u00a0min, carefully remove and discard the supernatant. Be careful not to disturb the beads that contain\n        bound DNA.\nAdd 200\u00a0\u03bcL of freshly prepared 80% ethanol to the tube while in the magnetic stand.\nIncubate at 25\u00b0C for 30 s, and carefully remove and discard the supernatant.\nRepeat steps 34 and 35 once for a total of two washes.Note: Be sure to remove all visible liquid after the second wash. Briefly\n      centrifuge the tube on a benchtop centrifuge if necessary.\n    \nAir-dry the beads for up to 5\u00a0min while the tube is on the magnetic stand with the lid open.\nNote: Do not over-dry the beads. This may result in a lower recovery of DNA. Elute\n      the samples when the beads are still dark-brown and glossy looking, but when all visible liquid has evaporated.\n      When the beads turn lighter brown and start to crack, it shows beads are too dry.\n    \nRemove the tube from the magnetic stand. Elute the DNA from the beads by adding 30\u00a0\u03bcL EB buffer provided by\n        Qiagen.\nMix well by pipetting up and down 10 times, or on a vortex mixer. Incubate for at least 2\u00a0min at room\n        temperature. Quickly spin the sample to collect the liquid from the sides of the tube if necessary.\nPlace the tube on the magnetic stand for 5\u00a0min. Transfer 28\u00a0\u03bcL to a new 1.5\u00a0mL tube. Libraries can be stored\n        at \u221220\u00b0C. The concentration of the library can be checked by NanoDrop.\nATAC-seq library quality assessment\nThis step describes the procedure for assessment of ATAC-seq library.\nWe pipette 20\u00a0\u03bcL of library samples and send them to Novogene for library quality determination and sequencing\n      (Figure\u00a04[href=https://www.wicell.org#fig4]). The library quality platform: NGS3K; Sequencing platform: HiseqPE150, paired-end.\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/418-Fig4.jpg\nFigure\u00a04. Assessment of ATAC-seq library\n(A\u2013D) The purified ATAC-seq DNA libraries of 4 samples on NGS3K platform. Em, immature embryo; G3, 3-day-old\n          seedlings grown on the B5 agar plate.\nNote: If we started with 50,000 sorted nuclei, 20\u00a0\u03bcL of library sample is enough\n      for quality assessment and sequencing. Most sequencing companies perform library quality analysis before\n      sequencing.", "Step-by-step method details\nStep-by-step method details\nCreating an ethogram and choosing interactions\nTiming: 4\u201310\u00a0weeks\nArabian babblers have a diverse set of behaviors and social interactions that have been studied for the last fifty years (Zahavi, 1991[href=https://www.wicell.org#bib34]). We recorded almost all known interactions, and classify them into six interaction types: allopreening, aggression, allofeeding, playing, proximal foraging and scrounging. These interaction types included more than 99% percent of all recorded interactions. More information about the interaction types is in our corresponding paper (Dragi\u0107 et\u00a0al., 2021[href=https://www.wicell.org#bib10]). During the creation of the protocol, we used previous studies and tried several options before forming the final ethogram. Here are a few universal steps for deciding which interactions to collect:\nDefine interactions as directed or undirected.\nNote: Directed interactions have an actor and a receiver in each interaction. For example, in aggression, there is an actor (the \u201cattacker\u201d) and a receiver (the \u201cvictim\u201d).\nNote: Directed networks are more informative than undirected ones and can always be transformed into undirected networks.\nWe used the gambit of the group method to record associations in groups as undirected edges (Franks et\u00a0al., 2010[href=https://www.wicell.org#bib15]).\nDefine interactions precisely to avoid sampling errors.\nTest for correlations between different networks.\nNote: See the script \u201cTesting correlation between matrices using QAP\u201d in Data S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2126-Mmc1.pdf] for more information.\nNote: If networks are correlated, the interactions probably have a similar biological function, or one of them is directly caused by the other. The correlation and interchangeability between networks can be tested using matrix correlation tests (Farine, 2017[href=https://www.wicell.org#bib11]) or permutations (Farine and Whitehead, 2015[href=https://www.wicell.org#bib12]; van der Marel et\u00a0al., 2021[href=https://www.wicell.org#bib32]).Note: Testing the correlation of matrices can sometimes give inconclusive results, especially if the study species\u2019 animals live in small groups. For example, in our study allofeeding and dominance display had a strong correlation in some groups (Table\u00a01[href=https://www.wicell.org#tbl1]). However, the two interactions had a different biological functions which we could see in changes in interaction patterns (Figure\u00a03[href=https://www.wicell.org#fig3]). In addition, the two interactions were clearly defined in previous studies (Zahavi, 1991[href=https://www.wicell.org#bib34]) and easy to distinguish in the field. The similarities probably came from the unidirectional nature of both interactions, where the actor is always a higher-ranked individual.\nOmit rare interactions from the analysis. Sparse graphs can be noisy and produce inaccurate results (Farine and Whitehead, 2015[href=https://www.wicell.org#bib12]).\ntable:files/protocols_protocol_2126_1.csv\nValues in the table present P values calculated using Quadratic Assignment Problem (QAP). If one of the adjacency matrices is empty (no interactions) or almost empty (< 90%) the correlation was not calculated (NA).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2126-Fig2.jpg\nFigure\u00a02. Cybertracker application windows\nThe top row contains \u201cSTART\u201d, \u201cSTOP\u201d, and \u201cPAUSE\u201d commands, which were used to mark the beginning and the end of semi-focal observation. In case of losing sight of the focal individual, the observation was paused. The individuals were listed in the following window.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2126-Fig3.jpg\nFigure\u00a03. Seasonal variation in aggression and allofeeding\nDominance display and allofeeding social networks of the same group. The thickness of edges represents the number of interactions between individuals. Arrows represent direction of interactions. While in some cases these two interaction types can have similar interaction patterns, they have different functions, and therefore their frequency can vary between seasons.Note: While recording as many interactions as possible could be beneficial, animal social networks give the best results with clearly defined and abundant interaction types. For example, an interaction called morning dance (Zahavi, 1991[href=https://www.wicell.org#bib34]) occurred only in some groups. Since it could not be joined to any other interaction type, morning dance was excluded from the analysis.\nSemi-focal observations\nTiming: 6\u201312\u00a0months\nTraditionally, behavioral observations follow two types of data collection: focal observations and all occurrences of behavior in the group (Altmann, 1974[href=https://www.wicell.org#bib1]). However, groups of Arabian babblers are mostly compact, and it is possible to observe and record the interactions of 4\u20138 individuals at any moment. If we recorded only interactions of only one individual we would \u201close\u201d the majority of observed behavior. Altmann (1974)[href=https://www.wicell.org#bib1] suggested that predetermined subgroups of individuals can be observed, but babbler movement was hard to predict and hence it was difficult to define a predetermined group. Our solution was semi-focal observations, where the subgroup was determined by the distance to the focal individual. During the semi-focal observations, we recorded all interactions involving the focal individual as well as interactions involving all individuals in a \u223c10 meters radius.\nDetermine duration of sampling time.\nNote: Depending on the species, focal observations can last from a few minutes (Coleman and Wilson, 1998[href=https://www.wicell.org#bib7]; Manson, 1999[href=https://www.wicell.org#bib21]; Stahl et\u00a0al., 2001[href=https://www.wicell.org#bib31]) up to 14\u00a0h for focal observation of groups (Ruckstuhl, 1998[href=https://www.wicell.org#bib26]).\nNote: Our sampling time was ten minutes. There was no exact reasoning for ten minutes focal observations, but it allowed us to observe each individual in an average group once before the temperatures become too high and babblers reduced their activity.Note: We would pause the observations if we would lose the focal individual from sight or if any of the individuals would interact with the observer. The observation would be continued once we find the individual or all individuals go back to their natural behavior.\nDetermine the number of focal observations.\nNote: The number of focal observations depends on the abundance of interactions and their distribution. There are several methods to test the quality of sample size, including bootstrapping and jackknifing, which can be used once the data are collected (Farine and Whitehead, 2015[href=https://www.wicell.org#bib12]; Lusseau et\u00a0al., 2008[href=https://www.wicell.org#bib20]; Wey et\u00a0al., 2008[href=https://www.wicell.org#bib33]).\nNote: It is important to note that there is still no consensus about the right sample size and testing method.\nWe used a technique as in jackknifing, where we measured the difference between normalized strengths of individuals (Figure\u00a04[href=https://www.wicell.org#fig4]). Since we observed all individuals equally, in our study strength was defined as the total number of interactions an individual had during the observation period.\nNote: See the \u201cTesting semi-focal observations\u201d script in Data S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2126-Mmc2.pdf] for more details.\nFor each number of focal rounds, calculate the mean strength for each individual and normalized it.\nCompare the normalized values between focal rounds.\nNote: For example, we compared individuals\u2019 normalized strength after just one round to normalized strength after two rounds and so on.\nThe preferred number of focal rounds is the one after which changes in normalized mean strength approach zero.\nNote: In our study, changes in mean strength approached zero after ten focal rounds for all interaction types (Figure\u00a04[href=https://www.wicell.org#fig4]).\nPrepare the group for the focal observations by following them every day, 3\u20137\u00a0days before the beginning of observations.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2126-Fig4.jpg\nFigure\u00a04. Estimating gained information on social interactions after each focal sessionDuring each focal session, we observed individuals ten times. After each round of focal observations, we calculated the mean strength, normalized it, and compared it to the previous focal round. Each boxplot represents the change in mean strength between consecutive focal rounds. Changes in normalized mean strength approach zero after ten rounds for all six interaction types, suggesting that ten focal observations are sufficient to estimate the strength of each association in this case.\nThe goals of the preparation period are to learn individuals\u2019 names, test the habituation of individuals, and for animals to get used to everyday human presence.\nThe habituation is considered sufficient if you can approach them to a minimal observation distance (3\u20136 meters) without disturbing them.\nNote: The minimal observation distance depends on the species.\nOnce all individuals are habituated start implementing the standardized feeding protocol, 1\u20132\u00a0days before observations begin.\nFeeding protocol.\nFeed animals in the morning, before they begin foraging.\nRandomly distribute small pieces of food.\nNote: We fed Arabian babblers with 2\u20133 mealworms (Tenebrio molitor) and \u223c5 grams of bread.\nStart observations once individuals finish the feeding.\nCritical: We fed individuals only before the observations.\nNote: This combination of food as a reward in the morning and consistent following before observations gave us the best results. The level of habituations stayed steady over a long period and animals did not interact with observers after the feeding period.\nNote: Other feeding protocols, as well as observations without supplemented food, did not provide consistent behavior during observations.\nPrepare the order of focal observations a day before.\nObserve each individual once before moving to the next focal round.\nNote: For example, if all individuals in the group were observed six times and individual A was observed five times, we would observe individual A.Equally distribute observations of an individual during different periods of the observation time.\nNote: For example, in our study individuals had a similar number of observations in the first, the second, and the third hour of observation time.\nIf individuals had the same number and distribution of observations the focal individual was selected randomly.\nData collection tools\nFor recording multiple interaction types it is preferable to customize an application which will allow fast and accurate documentation.\nThe application should contain the list of individuals and the list of interactions.\nNote: We used the customizable application Cybertracker (https://www.cybertracker.org/[href=https://www.cybertracker.org/]) (Figure\u00a02[href=https://www.wicell.org#fig2]).\nIf the application does not have a built-in time measuring option, use a timer on a watch or mobile phone.\nNote: In case you record the time using a watch, the first window should contain commands for starting, pausing, and ending the focal observation (Figure\u00a02[href=https://www.wicell.org#fig2]).\nFor directed interactions create two identical windows containing the list of individuals: one for the actor and one for the receiver of interaction.", "Step-by-step method details\nStep-by-step method details\nStep-1: Install SPrime\nTiming: 1\u00a0min\nThe SPrime software has been included in the SPrime pipeline download. The latest version of the software can be downloaded from the github page: https://github.com/browning-lab/sprime[href=https://github.com/browning-lab/sprime]. Place the software file \u201csprime.jar\u201d in the \u201ctools\u201d folder. Check that it is working and print out information on the parameters by running \u201cjava -jar sprime.jar\u201d (Figure\u00a01[href=https://www.wicell.org#fig1]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/669-Fig1.jpg\nFigure\u00a01. Screenshot showing the contents of the tools folder and sprime.jar\u2019s help message\nStep-2: Prepare input data for the SPrime analysis\nTiming: 5 h\nSPrime takes the genotype data and recombination map as required inputs, along with a file specifying the outgroup samples. The example genotype data are downloaded from the 1000 Genomes Project. In this protocol we will use an East Asian population, the CHB (Han Chinese in Beijing, n=103), as the target group, and an African population, YRI (Yoruba in Ibadan, n=108), as the outgroup. We need to extract the samples of both the target group and the outgroup for each chromosome, and filter to remove all variants that are not bi-allelic SNPs as follows:\npfile=../download/1000genome/integrated_call_samples_v3.20130502.ALL.panel\ngrep -E \"(YRI|CHB)\" ${pfile} | cut -f1\u00a0> sample.txt\ngrep YRI ${pfile} | cut -f1\u00a0> outgroup.txt\necho -n \"\" > vcf.file.list\nfor chr in {1..22}; do\nvcf=../download/1000genome/ALL.chr${chr}.phase3_shapeit2_mvncall_integrated_v5a.20130502.genotypes.vcf.gz\noutvcf=../tmp/chr${chr}.vcf.gz\necho ${outvcf} >> vcf.file.list\nbcftools view --samples-file sample.txt ${vcf} | bcftools view -c1 -m2 -M2 -v snps | bcftools annotate -x INFO,\u02c6FORMAT/GT -Oz > ${outvcf}\ndone\nExtracting samples and filtering SNPs for all chromosomes takes about 5 h. The maximum memory in use is 13.5Mb.We concatenate all autosomes into one file, because SPrime needs whole genome data to estimate key parameters. Although we will analyze the chromosomes one by one in order to parallelize computation, SPrime will obtain information about relative mutation rates from the whole autosome.bcftools concat --file-list vcf.file.list --naive --output-type z --output all.auto.vcf.gz\nStep-3: Run SPrime to detect introgressed variants\nTiming: 1 h\nWe use the HapMap combined LD map as the input recombination map in this example, following the analysis in (Browning et\u00a0al., 2018[href=https://www.wicell.org#bib2]). The recombination map must be in plink format (https://www.cog-genomics.org/plink/1.9/formats#map[href=https://www.cog-genomics.org/plink/1.9/formats#map]) and have the same genome build version and chromosome identifiers as the genotype data. Since the phase 3 1000 Genomes Project data uses GRCh37 coordinates, we use a build 37 map here.map=../download/plinkmap/plink.all.GRCh37.map\nSPrime requires specification of the genotype data \u201cgt=[file]\u201d, the outgroup sample list \u201coutgroup=[file]\u201d, the recombination map \u201cmap=[file]\u201d, and the output prefix \u201cout=[string]\u201d. One can also specify the chromosome using \u201cchrom=[chrom]\u201d, or the target region using \u201cchrom= [chrom]:[start]-[end]\u201d. Here we parallelize the analysis by chromosome, so we use the \u201cchrom=\u201d parameter.\noutgroup=../step2/outgroup.txt\ngt=../step2/all.auto.vcf.gz\nfor chr in {1..22}; do\nmap=../download/plinkmap/plink.all.GRCh37.map\nout=chb.yri.${chr}\njava -jar ../tools/sprime.jar gt=${gt} outgroup=${outgroup} map=${map} out=${out} chrom=${chr}\ndone\nIt takes approximately 1\u00a0h to run SPrime on all autosomes. The maximum memory in use for one chromosome is 4.6GB.\nStep-4: Calculate match rates to a known archaic genome\nTiming: 1.5 hSPrime is able to detect archaic introgression without knowing the archaic genome by utilizing a purported non-admixed population as an outgroup. For Neanderthal or Denisovan introgression, a West African population is typically used as the outgroup, for example the YRI from the 1000 Genomes Project. If a relevant archaic genome has been sequenced, one can map the detected variants to the archaic genome to confirm the source of introgression. We use the genome of the Altai Denisovan and the genome of the Vindija Neanderthal to represent two different sources of archaic introgression. The archaic genomes are in VCF format and the mask files are in BED (Browser Extensible Data) format.\nFor each variant detected by SPrime, map it to the archaic genome, resulting in \u201cmatch\u201d, \u201cmismatch\u201d, or \u201cnotcomp\u201d to the archaic genome. The three states mean the detected variant is present in the archaic genome, is not present in the archaic genome, or is not comparable because genotype quality in the archaic genome is low for that locus. To complete this step, we have an C script named \u201cmap_arch\u201d, which adds the match status for each variant as an additional column to SPrime\u2019s output. In the following code, we add match status to the Neanderthal genome and match status to the Denisovan genome.\nmaparch=\"../tools/map_arch_genome/map_arch\"\nfor chr in {1..22}; do\nscript=o.script.${chr}.sh\n#map variants to the Neanderthal genome\nbedfile=\"../download/archaic_genome/RecalledVindija/chr${chr}_mask.bed.gz\"\narchaicfile=\"../download/archaic_genome/RecalledVindija/chr${chr}_mq25_mapab100.vcf.gz\"\nreftag=\"AltaiNean\"\nscorefile=\"../step3/chb.yri.${chr}.score\"\noutmscore=\"out.chr${chr}.mscore\";\ntmpprefix=../tmp/${RANDOM}\necho \"\n#! /bin/bash\n${maparch} --kp --sep '\\t' --tag ${reftag} --mskbed ${bedfile} --vcf ${archaicfile} --score ${scorefile} > ${tmpprefix}.tmp1.${chr}.mscore\n\" >${script}\n#map variants to the Denisovan genome\nbedfile=\"../download/archaic_genome/RecalledDenisova/chr${chr}_mask.bed.gz\"\narchaicfile=\"../download/archaic_genome/RecalledDenisova/chr${chr}_mq25_mapab100.vcf.gz\"\nreftag=\"AltaiDeni\"\necho \"\n${maparch} \u2013kp \u2013sep '\\t' \u2013tag ${reftag} \u2013mskbed ${bedfile} \u2013vcf ${archaicfile} --score ${tmpprefix}.tmp1.${chr}.mscore > ${tmpprefix}.tmp2.${chr}.mscore\nmv ${tmpprefix}.tmp2.${chr}.mscore ${outmscore}\nrm ${tmpprefix}.tmp\u2217.${chr}.mscore\nrm ${script}\n\" >> ${script}\nsh ${script}\ndoneThe mismatch analysis takes 7\u00a0mins for chromosome 2 and 81\u00a0mins for all autosomes. The maximum memory in use for one chromosome is 10GB.\nStep-5: Find multiple sources of archaic introgression\nTiming: 1\u00a0min\nThis is an optional step for those who are interested in population history. Once we know the match status of each detected variant to the archaic genome, we are able to calculate the match rate for each reported introgression segment. The match rate for a segment is the number of matching positions divided by the sum of matching and mis-matching positions (the match rate is undefined if all the SPrime variants in the segment are not comparable to the archaic genome). If a segment has high match rate to a particular archaic genome, this segment probably shares close ancestry with that archaic genome. By calculating the match rate to different archaic genomes, we may find evidence of different sources of introgression as in Figure\u00a02[href=https://www.wicell.org#fig2] (Browning et\u00a0al., 2018[href=https://www.wicell.org#bib2]). The commands to generate this figure are:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/669-Fig2.jpg\nFigure\u00a02. Contour plot of CHB results showing three sources of archaic admixture\nThe peak in the upper left represents ancestry from a group that is closely related to the Altai Denisovan, the peak in the middle left represents ancestry from a group that is distantly related to the Altai Denisovan, the peak on the lower right represents ancestry from Neanderthals, and the peak in the lower left represents other segments which may be false positive introgression calls or introgression from another source.\n# calculate match rate for each introgressed segment\n## Rscript ../tools/score_summary.r [directory with annotated score files from step 4] [output filename]\nRscript ../tools/score_summary.r ../step4 match.summary.txt\n# contour plot show different waves of archaic introgression\n## Rscript ../tools/plot_contour.r [input summary file] [prefix of plot output]Rscript ../tools/plot_contour.r match.summary.txt chb.contour", "Step-by-step method details\nStep-by-step method details\nThe overall flowchart of this protocol is shown in Figure\u00a01[href=https://www.wicell.org#fig1].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/453-Fig1.jpg\nFigure\u00a01. The overall flowchart of this protocol\nThis protocol is designed for general usage of efficient recombinant protein purification with GFP fusion and ribonucleoprotein assembly for interaction analysis using the MST assay coupled with GaMD simulations. GFP, green fluorescent protein; MST, MicroScale Thermophoresis; GaMD, Gaussian Accelerated Molecular Dynamics.\nBacterial transformation \u2013 day 1\nTiming: 1.5 h\nTake agar plates with appropriate antibiotics (Ampicillin: 100\u00a0\u03bcg/mL) from the 4\u00b0C fridges and warm them up to room temperature (20\u00b0C\u201325 \u00b0C).\nTake out the BL21(DE3) competent cells from the \u221280 \u00b0C freezer and thaw the cells on ice (approximately 20\u201325\u00a0min).\nMix 1\u20135\u00a0\u03bcL (concentration around 150\u00a0ng/\u03bcL) of the 2GFP-T_M2-1 plasmid encoding the RSV M2-1 (Figure\u00a02[href=https://www.wicell.org#fig2]) into 20\u201350\u00a0\u03bcL of competent cells in a microcentrifuge tube. Gently mix the cells and the plasmids and put them on ice for 30\u00a0min.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/453-Fig2.jpg\nFigure\u00a02. Expression of the GFP-fused protein\n(A) The illustration of the construct that is used for the expression of the RSV M2-1 with the 6\u2217His and GFP tag.\n(B) The protein expression and protein gel analysis of the GFP labeled protein. The green color indicates the successful expression of the GFP-fused protein in the cell culture without running a gel.\nHeat shock each transformation tube into a 42\u00b0C water bath for 40 s.\nPlace the tube on ice for 2\u00a0min.\nAdd 250\u20131,000\u00a0\u03bcL LB or SOC media (without antibiotics) to the microcentrifuge tube and grow in the shaking incubator for 45\u00a0min at 37\u00b0C.\nTake 50\u00a0\u03bcL on each plate and incubate all plates at 37\u00b0C overnight (\u223c16 h).\nProtein expression test and bacterial glycerol stock \u2013 day 2\nTiming: 1\u00a0dayAdd 5\u00a0mL liquid LB media to a tube and add the appropriate antibiotic to the correct concentration (Ampicillin: 100\u00a0\u03bcg/mL).\nUse a sterile pipette tip, and select a single colony from the LB agar plate.\nDrop the pipette tip into the tube. Loosely cover the tube with the cap, and make sure it is not airtight.\nIncubate bacterial culture at 37\u00b0C in the shaking incubator.\nCheck the OD600 to measure the density of the culture until the OD600 value reaches 0.5.\nAdd 500\u00a0\u03bcL of the cell culture to 500\u00a0\u03bcL of 50% glycerol in a 2\u00a0mL screw-cap tube. Freeze the glycerol stock tube at \u221280\u00b0C.\nTake 500\u00a0\u03bcL of the cell culture to a new microcentrifuge tube, and label it as \u201c-\u201d and store in a 4 \u00b0C fridge.\nInduce the rest of the cell culture with the IPTG with a final concentration of 0.5\u00a0mM. Adjust the temperature to 16\u00b0C and incubate overnight (\u223c16 h).\nTake 500\u00a0\u03bcL of the cell culture to a new microcentrifuge tube, label it as \u201c+\u201d. Then analyze the sample before inducing (\u201c-\u201d) and after inducing (\u201c+\u201d) by SDS-PAGE gel (Figure\u00a02[href=https://www.wicell.org#fig2]).\nPause Point: The glycerol stock can be stored at \u221280\u00b0C for up to 2\u20133 years.\nScale up the cell culture \u2013 day 3\nTiming: 1\u00a0day\nInoculate the glycerol stock of M2-1 with GFP tag into 500\u00a0mL LB media with antibiotics (Ampicillin: 100\u00a0\u03bcg/mL). Put the flask in the shaker at 220\u00a0rpm at 37\u00b0C overnight (\u223c16 h).\nInoculate 10\u00a0mL of the overnight cell culture into 1\u00a0L of LB media with antibiotics (Ampicillin: 100\u00a0\u03bcg/mL) and grow the cell culture at 37\u00b0C until the OD600 is between 0.6 and 0.8.Cooldown the cell media to 16 \u00b0C for 1 h. Then induce the cells with IPTG at a final concentration of 0.5\u00a0mM at 16 \u00b0C overnight (\u223c16 h).\nHarvest the cells by centrifugation at 4,557\u00a0\u00d7 g for 20\u00a0min at 4\u00b0C.\nPurification of His-GFP tagged M2-1 using affinity column \u2013 day 4\nTiming: 3 h\nResuspend the cell pellets with the lysis buffer (20\u00a0mL lysis buffer per 1\u00a0L cell culture).\nLyse the cell using the Misonix Sonicator 3000. Process time: 15\u00a0min; time on: 3 s, time off: 3 s, amplitude: 30%.\nCentrifuge the cell lysate at 42,625\u00a0\u00d7 g for 40\u00a0min at 4\u00b0C, and keep the supernatant.\nPrepare the gravity-based column with an appropriate amount of cobalt resin. Allow the storage buffer to drain.\nWash the beads with 2 CVs DI H2O.\nEquilibrate the beads with 2 CVs of lysis buffer, and allow the buffer to drain.\nAdd the supernatant sample to the column, mix, and incubate with mechanical rotation for 60\u00a0min at 4\u00b0C. Remember to save 50\u00a0\u03bcL supernatant for analysis by SDS-PAGE gel.\nCollect the sample lysate by gravity flow and save 50\u00a0\u03bcL flow-through samples for analysis by SDS-PAGE gel.\nWash the beads with 5 CVs of the wash buffer. Collect all the flow-through and save 50\u00a0\u03bcL samples for SDS-PAGE gel.\nRepeat the wash step with the high salt wash buffer. Collect all the flow-through, and save 50\u00a0\u03bcL samples for SDS-PAGE gel.\nAdd 3 CVs of elution buffer. Collect all the flow-through and save 50\u00a0\u03bcL samples for SDS-PAGE gel.\nDetect the flow-through samples from each step with SDS-PAGE gel. Pool together the samples containing target protein M2-1.Note: (For steps 28\u201331) After loading the lysate to the column, collect all the flow-through of each step, including washing and elution. Take 5\u223c10\u00a0\u03bcL from each flow-through sample for SDS-PAGE gel analysis.\nTEV cleavage of the His-GFP tag \u2013 day 5\nTiming: 16 h\nMake the fresh dialysis buffer. Ensure that in the dialysis buffer, the target protein is stable and soluble. Evaluate the compatibility of the dialysis buffer by mixing the purified proteins with the buffer (check whether white aggregates form).\nMix the TEV protease with the protease: target protein at a ratio of 1:100 (w/w), and seal the sample in the dialysis bag.\nDialyze against the dialysis buffer at 4\u00b0C overnight (\u223c16 h).\nAnalyze by the SDS-PAGE gel to check the TEV cleavage result.\nCritical: The TEV protease has activity in the pH range of 6\u20139. At pH lower than 5, the TEV protease is inactive.\nProtein purification with heparin column and size exclusion chromatography - day 6\nTiming: 1 h\nDilute the TEV cleavage target sample with heparin wash buffer A. Make sure that the final concentration of NaCl is lower than 150\u00a0mM.\nEquilibrate the column with 5\u201310 CVs of heparin wash buffer A.\nApply the sample to the heparin column.\nElute with 10\u201320 CVs using a step gradient from 5%\u2013100% heparin wash buffer B buffer (monitored by UV absorption at A280 and A260).\nProtein purification with size exclusion chromatography (SEC) - day 6\nTiming: 2 h\nTest the buffer compatibility with the protein sample. Mix 10\u00a0\u03bcL gel filtration buffer with 10\u00a0\u03bcL protein sample, and centrifuge with 12,000\u00a0\u00d7 g for 15\u00a0min. Check the bottom of the microcentrifuge tube to check if there are any precipitations.\nIf there is no precipitation, do the next step.If precipitation occurs, optimize the buffer components to make them compatible with the protein (i.e., change the pH, increase the concentration of salt).\nNote: We typically repeat step 41 by mixing a small volume (10\u00a0\u03bcL) of protein sample with an equal volume (10\u00a0\u03bcL) of different buffer solutions to identify a suitable buffer for the further purification step.\nUse Superdex 200 Increase 10/300 GL. Equilibrate the column with 1.5 CV of the Gel Filtration buffer (50\u00a0mM HEPES pH7.4 200\u00a0mM NaCl, 5% glycerol).\nCentrifuge the sample with 12,000\u00a0\u00d7 g for 15\u00a0min. Collect the supernatant and inject it into the column.\nRun the SEC program.\nTurn on the UV monitor at the wavelengths of 260\u00a0nm, 280\u00a0nm, and 488\u00a0nm (which can monitor the residual GFP tag).\nFlow rate: 0.5\u00a0mL/min.\nElute the column with 1.2 CV Gel Filtration buffer (50\u00a0mM HEPES pH 7.4, 200\u00a0mM NaCl, 5% glycerol).\nSet up the 96-deep-well plate for collecting the flow-through with 0.5\u00a0mL for each well.\nCollect the fractions from the peak and run the SDS-PAGE gel to check the sample purity.\nCritical: The sample should be tested to see whether it is compatible with the gel filtration buffer as described in step 41.\nOptional: Several alternative assessment methods could provide more in-depth tests, such as (1) Prometheus (NanoTemper) monitors the intrinsic fluorescence signal of proteins as a measure of their folding states (https://nanotempertech.com/prometheus/[href=https://nanotempertech.com/prometheus/]) and (2) the solubility and stability screen of the sample using the crystallographic hanging or sitting drops methods. An example of a screen kit can be found here: https://hamptonresearch.com/product-Solubility-Stability-Screen-620.html[href=https://hamptonresearch.com/product-Solubility-Stability-Screen-620.html].\nCritical: The sample should be fully dissolved. Centrifuge or filter to remove the precipitations before loading it to the column.\nMicroScale Thermophoresis (MST) assay \u2013 day 7\nTiming: 3\u20134 hThe instrument we used is Monolith NT.115 Blue/Red. The NT.115 instrument has two detectors:\ntable:files/protocols_protocol_453_12.csv\nNote: Before performing the MST assays, it is crucial to determine the specific equipment model to be used. The specific NT.115 model will determine which fluorophore label to use. Here, the model we used is NT.115 Blue/Red. Because we use the GFP as the fusion tag, and we noticed that even after the TEV cleavage, it has a trace amount of GFP left in the solution. To avoid the unwanted fluorescent signal of the trace mount free GFP in the sample, we limit the fluorophore labeling choices to cy5, NT-647(RED), or Alexa647. We chose to use the cy5 labeled RNA and operated in the red mode in this protocol but not the blue mode.\nNote: Sample preparation: The MST experiments are set up with one fluorescently labeled molecule, which is called the target. The target will be at a fixed concentration and mixed with various concentrations of the other non-fluorescent molecule called the ligand. Reaction buffers in which the target and ligand should be well behaved. The addition of 0.05% Tween 20 or other detergent is usually required to prevent sticking to the capillaries.\nPretest: The Pretest examines the fluorescence intensity, adsorption on the capillaries, variation, and sample aggregation of the fluorescent molecule.\nPlan Your Experiment\nName the Target (the fluorescent molecule): Cy5_SH13\nEnter the concentration of the stock solution of Target: 40\u00a0nM\nDescribe the assay buffer: 50\u00a0mM HEPES pH7.4, 200\u00a0mM NaCl, 0.05% Tween 20\nChoose Capillary: Monolith NT.115 capillary\nSystem settings, Excitation power: Auto-detect, MST Power: Medium\nTemperature Control: 22 \u00b0C\nInstructions\nFollow the on-screen instructions to prepare the samples Cy5_SH13\nFill 2 capillaries, and load the capillary tray, put the higher concentration in position1Insert the capillaries tray into the instrument, and click the button \u201cStart Measurement\u201d\nResults\nExamine the capillary scans for fluorescence intensity, adsorption, and variation\nExamine the MST trace for signs of aggregation\nClick the \u201cReview\u201d buttons for more details\nNote: Check the Fluorescence signal (800\u20131,000 counts should be good for the assays). Make sure there are no aggregates that can be visualized.\nBinding Affinity Experiment: Setting up the concentration of the fluorescent-labeled target sample (e.g., Cy5 labeled single-strand RNA SH13), the ligand sample (e.g., M2-1 protein), the assay buffer, and the system settings, which including Excitation Power (Auto-detect) and MST Power (Medium).\nPlan your experiment\nName the Target (the fluorescent molecule): Cy5_SH13\nEnter the concentration of the stock solution of Target: 40\u00a0nM\nDescribe the assay buffer: 50\u00a0mM HEPES pH7.4, 200\u00a0mM NaCl, 0.05% Tween 20\nChoose Capillary: Monolith NT.115 capillary\nSystem settings, Excitation power: Auto-detect, MST Power: Medium\nName the Ligand: M2-1\nEnter the concentration of the stock solution of Ligand: 100\u00a0\u03bcM\nTemperature Control: 22 \u00b0C\nInstruments\nFollow the on-screen instructions to prepare your samples\nPrepare a serial dilution of the diluted ligand using the assay buffer\nMix the Target to each tube of ligand by pipetting\nFill the capillaries and load the capillary tray, put the higher concentration in position 1\nInsert the capillaries tray into the instrument, and click the button \u201cStart Measurement\u201d\nExamples\nPrepare the PCR tubes and label them from #1 to #16.\nPrepare 25\u00a0\u03bcL of the M2-1 at 2\u00d7 concentration (e.g., for a final concentration of 1\u00a0mM, prepare the sample at a concentration of 2\u00a0mM)Note: At the first time of the test, the suitable concentration range of the ratio of M2-1: RNA is unknown. The range of the molar ratio for M2-1:RNA can be initially set up to 0.05\u201320. If too few or too many dose-response points are in a bound or unbound state, consider adjusting the M2-1 and RNA concentrations accordingly.\nAdd 10\u00a0\u03bcL of PBS-T into the PCR tubes #2\u201316.\nAdd 20\u00a0\u03bcL of M2-1 into PCR tube #1.\nGradient dilution from the PCR tube from #1 to #16. Transfer 10\u00a0\u03bcL of the sample M2-1 from tube #1 to tube #2 with low retention pipette tips and mix by pipetting up-and-down multiple times. Make sure no bubbles and transfer 10\u00a0\u03bcL to tube #3, and mix. Repeat the procedure for PCR tube #4-#16. Discard the extra 10\u00a0\u03bcL from the last tube #16\nAdd 10\u00a0\u03bcL of Cy5 labeled RNA to each tube (#1-#16) and mix by pipetting.\nPut the capillaries to each PCR tube, load the capillaries, and measure the samples. The recommended instrument setting is 40% LED/excitation power and 40% MST power (Medium setting).\nNote: It is recommended that MST is performed at \u201clow,\u201d \u201cmedium,\u201d or \u201chigh\u201d settings. The lowest setting that produces the expected thermophoresis signals should be chosen as the default setting for a specific sample.\nResults\nExamine the capillary scans for fluorescence intensity, adsorption, and variation\nExamine the MST trace for signs of aggregation\nClick the \u201cReview\u201d buttons for more details\nData analysis\nStart Affinity Analysis software.\nLoad raw data.\nChoose Fluorescence intensity, No fluorescence variation, No absorption, and No aggregates, No ligand-induced photobleaching rate changes. Make the signal/noise ratio is large enough to conclude binding.\nThe Kd can be determined using the Kd fit with the default setting.Generate Full Report will create a PDF report.\nUse the export menu to export the curves images.\nNote: Usually, at the first test, the concentration of M2-1 is 20 folds higher than the cy5-labeled RNA. Check the does response. If too few dose-response points are in a bound or unbound state, consider adjusting the M2-1 concentration range.\nGaMD simulations\nTiming: \u223c8\u201310\u00a0days\nGaMD simulations were performed using the GPU version of AMBER18 (Case et\u00a0al., 2018[href=https://www.wicell.org#bib4]; Miao et\u00a0al., 2015[href=https://www.wicell.org#bib15]) (Figure\u00a09[href=https://www.wicell.org#fig9]). The simulation speed depends on various factors, including the system size, GPU power, etc. For the SH7 RNA binding to the M2-1 protein system with 187,013 atoms, the simulation speed obtained was \u223c35\u201340\u00a0ns/day on NVIDIA RTX 2080 GPU. It took about a week to complete 300\u00a0ns simulations.\nSystem equilibration for GaMD production simulations\nRunning conventional molecular dynamics (cMD) using AMBER\nThe first step is to run energy minimization to relax the system and eliminate any steric clashes in the system using files downloaded from CHARMM-GUI.\nNext would be to run the equilibration for a minimum of 1\u00a0ns timescale to bring the system to an equilibrium using the default parameters provided by CHARMM-GUI.\nFinally, run cMD for at least 10\u00a0ns timescale to further equilibrate the system.\nRunning short cMD and GaMD equilibration using AMBER\nA template GaMD equilibration input file as provided here http://miao.compbio.ku.edu/GaMD/tutorial.html[href=http://miao.compbio.ku.edu/GaMD/tutorial.html] was used to run short cMD of 4\u00a0ns and GaMD equilibration of 40\u00a0ns with 2\u00a0fs timestep using GaMD implemented in GPU version of AMBER18 (Case et\u00a0al., 2018[href=https://www.wicell.org#bib4]).\nThe parameters were set as the following:\nnstlim\u00a0= 22000000,\nirest\u00a0= 0,\nntx\u00a0= 1,\nigamd\u00a0= 3, iE\u00a0= 1, irest_gamd\u00a0= 0,\nntcmd\u00a0= 2000000, nteb\u00a0= 20000000, ntave\u00a0= 400000,\nntcmdprep\u00a0= 800000, ntebprep\u00a0= 800000,sigma0P\u00a0= 6.0, sigma0D\u00a0= 6.0,\nRunning GaMD production\nStart GaMD production simulation using a template input file as provided here http://miao.compbio.ku.edu/GaMD/tutorial.html[href=http://miao.compbio.ku.edu/GaMD/tutorial.html] in GPU version of AMBER18 (Case et\u00a0al., 2018[href=https://www.wicell.org#bib4]). The parameters were set as the following:\nnstlim\u00a0= 25000000,\nirest\u00a0= 0,\nntx\u00a0= 1,\nigamd\u00a0= 3, iE\u00a0= 1, irest_gamd\u00a0= 1,\nntcmd\u00a0= 0, nteb\u00a0= 0, ntave\u00a0= 400000,\nntcmdprep\u00a0= 0, ntebprep\u00a0= 0,\nsigma0P\u00a0= 6.0, sigma0D\u00a0= 6.0,\nRepeat running jobs using the following input file until end of GaMD production simulation with parameters as follows:\nnstlim\u00a0= 25000000,\nirest\u00a0= 1,\nntx\u00a0= 5,\nigamd\u00a0= 3, iE\u00a0= 1, irest_gamd\u00a0= 1,\nntcmd\u00a0= 0, nteb\u00a0= 0, ntave\u00a0= 400000,\nntcmdprep\u00a0= 0, ntebprep\u00a0= 0,\nsigma0P\u00a0= 6.0, sigma0D\u00a0= 6.0,", "Step-by-step method details\nStep-by-step method details\n      Before identification and investigation of time-course HVGs and their\n      dynamic expression profiles, the scRNA-seq data must be pre-processed and\n      quality controlled, which are summarized briefly here, and are described\n      in details through GitHub (https://github.com/vclabsysbio/scRNAseq_DVtimecourse[href=https://github.com/vclabsysbio/scRNAseq_DVtimecourse]). Note that these steps have also been comprehensively described\n      elsewhere.16[href=https://www.wicell.org#bib16],17[href=https://www.wicell.org#bib17],18[href=https://www.wicell.org#bib18]\nQC and filtering single-cell RNA-seq data\nTiming: \u223c 1\u20132 h/sample (for steps 1 to\n      3)\n    \n      The quality control and filtering scRNA-seq data were performed separately\n      on individual samples, before data integration and cell type annotation\n      (Figure\u00a01[href=https://www.wicell.org#fig1]). The QC stage comprises correcting of\n      ambient RNAs, exclusion of dead and low quality cells and potential\n      doublets, in prior to the downstream bioinformatic analyses. Raw and\n      filtered expression matrices generated by the\n      cellranger count function are required as the inputs of this step -\n      see key resources table[href=https://www.wicell.org#key-resources-table].\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2780-Fig1.jpg\n          Figure\u00a01. Overview of bioinformatic pipeline of scRNA-seq data\n          analysis described in this protocol\n        \nHVGs, highly variable genes.\nNote:\nFigure\u00a01[href=https://www.wicell.org#fig1] illustrates the overview of the bioinformatic\n      pipeline described in this protocol.\n    \nNote: R packages and codes required for\n      the QC and filtering step are provided in Github URL:\n      https://github.com/vclabsysbio/scRNAseq_DVtimecourse[href=https://github.com/vclabsysbio/scRNAseq_DVtimecourse]. Complete datasets used for this step are deposited in Mendeley Data:\n      https://data.mendeley.com/datasets/6ry3x7r8hf/3[href=https://data.mendeley.com/datasets/6ry3x7r8hf/3].\n    \nCorrect ambient RNAs using SoupX.11[href=https://www.wicell.org#bib11]\nNote: Check for the expression levels of\n      ambient RNAs in your datasets. If an excessive amount is observed, users\n      can apply several ambient RNA removal tools such as SoupX11[href=https://www.wicell.org#bib11]\n      to correct their expression levels. For more details of predicting and\n      correcting the expression values of ambient RNAs, please refer to the\n      SoupX tutorial.11[href=https://www.wicell.org#bib11]\n        Exclude the cells expressing excessive mitochondrial genes, in this\n        example, more than 10% in the total transcript counts.\n      \nNote: Check the distribution of percent\n      mitochondrial genes (percent.mt) to determine suitable cut-offs in your\n      scRNA-seq datasets. For details, please refer to the Seurat toolkit.2[href=https://www.wicell.org#bib2]Predict and discard doublets using doubletFinder.12[href=https://www.wicell.org#bib12]\nNote: For droplet-based scRNA-seq, we\n      recommend excluding \u201cdoublets\u201d and \u201cmultiplets\u201d. For more details about\n      the optimal cut-offs in each parameter, please refer to doubletFinder.12[href=https://www.wicell.org#bib12]\n      Data normalization, integration, and cell type annotation\n    \nTiming: \u223c 2\u00a0h (for steps 4 to 7)\n    \n      After filtering out low quality cells and correcting the expression values\n      of ambient RNAs, the data are used as inputs for normalization,\n      integration, and cell type annotation. Seurat objects of individual\n      samples after QC and filtering step are used as the inputs of this step.\n    \n        Run the standard preprocessing workflow for each individual sample.\n        \n            Import and create a list of individual Seurat objects after the QC\n            and filtering step.\n          \n            Run the SCTransform function, a regularized negative\n            binomial regression.19[href=https://www.wicell.org#bib19]\n            Apply the first 30 principal components (PCs) for cell clustering.\n          \n            Identify cell clusters by the shared nearest neighbor (SNN) method\n            using the Louvain algorithm with multilevel refinement.\n          \n        Integrate all the samples. We used the following settings in our\n        examples here: three thousand (3,000) features, a Louvain algorithm with\n        multi-level refinement for clustering and the resolution of three. Other\n        parameters are as default.\n      \n        Normalize the transcript counts of each cell using the\n        NormalizeData() function.\n      \n        Identify major immune cell types using known canonical marker genes (as\n        described in\n        Table\u00a0S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2780-Mmc1.pdf]).\n      \nNote: For cell type annotation, in our\n      examples we first labeled different clusters of T\u00a0cells with the same name\n      in order to group them together as a single \u201cT cells\u201d cluster, which was\n      subsequently re-clustered into subpopulations.\n    \nNote: High heterogeneity of PBMCs,\n      especially at the subpopulation levels, might be difficult to identify\n      using the known marker genes (Table\u00a0S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2780-Mmc1.pdf]). Alternatively, reference-based for cell type annotation such as\n      SingleR,20[href=https://www.wicell.org#bib20] Azimuth,21[href=https://www.wicell.org#bib21] and\n      ScType22[href=https://www.wicell.org#bib22] can be applied.\n    \n      Highly variable gene (HVG) identification and pathway enrichment analysisacross multiple time points\u00a0\u2013 Dengue case study\n    \nTiming: \u223c 10\u201315\u00a0min (for steps 8 to 14)\n      Here, we describe the process for obtaining HVGs, which represent the\n      variations in transcriptional levels across several biological conditions\n      (typically >2 conditions, in our case, four time points from two\n      DENV-infected patients and two healthy controls) in multiple immune cell\n      types of interest. The inputs of this step are the integrated scRNA-seq\n      Seurat objects from the previous pro-processing and cell type annotation\n      described above (Figure\u00a01[href=https://www.wicell.org#fig1]).\n    \nNote: We first performed a pseudo-bulk\n      expression analysis by calculating the average transcription levels of all\n      the genes. Then, principal component analysis (PCA) of the averaged gene\n      expression in each cell type was performed (Figure\u00a02[href=https://www.wicell.org#fig2]A). This PCA of each cell type was used to extract the genes that\n      demonstrate the largest variation across biological conditions,\u00a0\u2013 to be\n      referred to as \u201cHighly Variable Genes (HVGs)\u201d. In this example, the most\n      apparent differences are between the four time points, whereas those\n      between the two DENV patients are relatively small. Finally, we\n      investigated the biological pathways associated with the HVGs that show\n      common and unique expression dynamics among different cell types and\n      biological conditions (Figure\u00a02[href=https://www.wicell.org#fig2]B). The workflow is\n      described step-by-step below.\n    \nNote: Processed scRNA-seq dataset used in\n      this step is deposited in Mendeley Data:\n      https://data.mendeley.com/datasets/6ry3x7r8hf/3[href=https://data.mendeley.com/datasets/6ry3x7r8hf/3]\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2780-Fig2.jpg\n          Figure\u00a02. The relative expression levels of highly variable genes\n          (HVGs) in each biological process (BP) of interest, across the four\n          time points and major immune cell types, from two dengue-infected\n          patients and two healthy controls\n        \n          (A and B) Figures\u00a0were modified from Arora et\u00a0al., 20221[href=https://www.wicell.org#bib1]\n          (A). PCA of average gene expression in each major immune cell type of\n          interest. (B). Dotplot representing relative expression levels ofHVGs. The BPs of the HVGs across the four major immune cell types of\n          interest here are highlighted in red texts, monocyte-specific BPs are\n          in blue, and B cell-specific BPs are in green.\n        \n        Subset the integrated Seurat object according to their major cell types\n        into monocytes, NK cells, T\u00a0cells, and B cells/plasma cells/plasmablasts\n        - the main immune cell types of interest in this study.\n      \n# Load libraries\nlibrary(Seurat)\nlibrary(gprofiler2)\nlibrary(ggplot2)\n# Set your working directory, pointing to the folder where all your\n          input and output files will be saved\nsetwd(\"PATH/TO/YOUR/WORKING/DIRECTORY\")\n# Load integrated data\nsc_integrated <- readRDS(file\u00a0=\n          \"PATH/TO/YOUR/WORKING/DIRECTORY/sc_integrated.rds\")\n# Subset each cell type\nIdents(sc_integrated) <- \"Cell_Types\"\neach_celltype_list <- list()\neach_celltype <- c(\"Monocytes\" , \"NK cells\" ,\n          \"T cells\" , \"B cells\")\nfor (RN in each_celltype) {\n\u00a0\u00a0each_celltype_list[[RN]] <- subset(sc_integrated ,\n          idents\u00a0= RN)\n}\nnames(each_celltype_list) <- each_celltype\n        Calculate the average gene expression levels in each cell type across\n        biological conditions of interest.\n        \n            Set the object\u2019s identity class based on condition of interest using\n            the Idents() function.\n          \n            Calculate the average gene expression using the\n            AverageExpression() function.\n          \n# Calculate average gene expression in each cell type across severity\n          and time\nAvg_expression_list <- list()\nfor (RN in 1:length(each_celltype_list)) {\n\u00a0\u00a0Idents(each_celltype_list[[RN]]) <-\n          \"ST\"\n\u00a0\u00a0Avg_expression_list[[RN]] <-\n          AverageExpression(each_celltype_list[[RN]] , assays\u00a0= \"RNA\"\n          , slot\u00a0= \"data\")\n\u00a0\u00a0Avg_expression_list[[RN]] <-\n          as.data.frame(Avg_expression_list[[RN]]$RNA)\n}\nnames(Avg_expression_list) <- names(each_celltype_list)\n        Construct the principal components (PCs) of the average gene expression\n        levels in each cell type using the prcomp() function in R (Figure\u00a02[href=https://www.wicell.org#fig2]A).\n      \n# Construct Principal Components (PCs) in each cell type\npca_out <- list()\npca_perc <- list()\ndf_pca <- list()\nfor (RN in 1:length(Avg_expression_list)) {\n\u00a0\u00a0pca_out[[RN]] <-\n          prcomp(t(Avg_expression_list[[RN]]))\n\u00a0\u00a0pca_perc[[RN]] <-\n          round(100\u2217pca_out[[RN]]$sdev\u02c62/sum(pca_out[[RN]]$sdev\u02c62),1)\n\u00a0\u00a0df_pca[[RN]] <- data.frame(PC1\u00a0= pca_out[[RN]]$x[,1],\n          PC2\u00a0= pca_out[[RN]]$x[,2], sample\u00a0=\n          colnames(Avg_expression_list[[RN]]))\n\u00a0\u00a0# Add metadata can be differences in each dataset\n\u00a0\u00a0df_pca[[RN]]$Severity <- c(rep(\"DF\" , 4) ,rep(\"DHF\" , 4) , \"Healthy\" ,\n          \"Healthy\")\n\u00a0\u00a0df_pca[[RN]]$Time <- c(\"Day-2\" ,\n          \"Day-1\" , \"Def\" , \"Wk2\" ,\n          \"Day-2\" , \"Day-1\" , \"Def\" ,\n          \"Wk2\" ,\"Healthy I\" , \"Healthy II\")\n\u00a0\u00a0df_pca[[RN]]$Time <- factor(df_pca[[RN]]$Time ,\n          levels\u00a0= c( \"Day-2\" , \"Day-1\" , \"Def\" ,\n          \"Wk2\" , \"Healthy I\" , \"Healthy\n          II\"))\n}\nnames(pca_out) <- names(Avg_expression_list)\nnames(pca_perc) <- names(Avg_expression_list)\nnames(df_pca) <- names(Avg_expression_list)\nNote: Please add metadata based on your\n      datasets.\n    \n        Visualize the PCA results using the ggplot2 package.14[href=https://www.wicell.org#bib14]\n# Visualize PCA results\nfor (RN in 1:length(df_pca)) {\n\u00a0\u00a0pca_plot<- ggplot(df_pca[[RN]], aes(PC1,PC2, color\u00a0=\n          Time))+ geom_point(aes(shape\u00a0= Severity ), size=6 , stroke\u00a0= 1.4)+\n          labs(x=paste0(\"PC1 (\",pca_perc[[RN]][1],\"%)\"),\n          y=paste0(\"PC2 (\",pca_perc[[RN]][2],\"%)\"))\u00a0+\n          scale_color_manual(values=c(\"darkgoldenrod2\",\n          \"#ff7400\",\"#ff1218\", \"#47849c\" ,\n          \"darkgrey\" , \"gray6\"))\u00a0+ theme(axis.text\u00a0=\n          element_text(size\u00a0= 17 , face=\"bold\" , colour\u00a0=\n          \"black\") , axis.title.y\u00a0=\n          element_text(color=\"black\", size=15, face=\"bold\")\n          , axis.title.x\u00a0= element_text(color=\"black\", size=17,\n          face=\"bold\") , legend.title\u00a0= element_text(face\u00a0=\n          \"bold\" , size\u00a0= 17) , legend.text\u00a0= element_text(size\u00a0= 16)\n          , legend.key.size\u00a0= unit(1, \"cm\") , legend.key.width\u00a0=\n          unit(0.5,\"cm\") , legend.key\u00a0= element_rect(fill\u00a0=\n          \"white\") )\u00a0+ ggtitle(names(df_pca[RN]))\nplot(pca_plot)\n}\nNote: Color and shape can be manually\n      adjusted based on your data.\n    \n        Union the top 500 genes from the first and second PCs of each immune\n        cell type\u00a0\u2013 referred to as \u201cHVGs\u201d herein.\n      \n# Union top 500 genes from PC1 and PC2 from all cell types\nHVGs_each_celltype <- list()\nfor (RN in 1:length(pca_out)) {\n\u00a0\u00a0HVGs_each_celltype[[RN]] <-\n          union(rownames(data.frame(sort(abs(pca_out[[RN]]$rotation[,\"PC1\"]),\n          decreasing=TRUE)[1:500])) ,\n          rownames(data.frame(sort(abs(pca_out[[RN]]$rotation[,\"PC2\"]),\n          decreasing=TRUE)[1:500])))\n}\nnames(HVGs_each_celltype) <- names(pca_out)\n# unique HVGs based on number of cell types\nHVGs <- unique(c(HVGs_each_celltype[[1]] , HVGs_each_celltype[[2]]\n          , HVGs_each_celltype[[3]] , HVGs_each_celltype[[4]]))\nCritical: Edit the\n      unique() function based on the numbers of cell types in your\n      dataset. In this case, we investigated four major immune cell types:\n      monocytes, NK cells, T\u00a0cells, and B cells.\n    \nNote: To find the optimal numbers of topgenes that exhibit high variations in PCs, users can construct and\n      investigate a histogram plot where the y-axis represents PC loading\n      calculated from the prcomp() function in R, and the x-axis shows\n      the numbers of genes (Figure\u00a0S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2780-Mmc1.pdf]).\n    \nExample code.\n# Extract PC loading values calculated from prcomp()\nPC1_mono <- sort(abs(pca_out[[1]]$rotation[,\"PC1\"]),\n          decreasing=TRUE)\nPC1_NK <- sort(abs(pca_out[[2]]$rotation[,\"PC1\"]),\n          decreasing=TRUE)\nPC1_Tcells <- sort(abs(pca_out[[3]]$rotation[,\"PC1\"]),\n          decreasing=TRUE)\nPC1_Bcells <- sort(abs(pca_out[[4]]$rotation[,\"PC1\"]),\n          decreasing=TRUE)\n# Plot\nplot(density(PC1_mono)$y, density(PC1_mono)$x,type=\"l\" ,\n          col\u00a0= \"orange\" , ylab\u00a0= \"PC1 loading values\",\n          xlab\u00a0= \"Number of genes\" , main\u00a0= \" \" , ylim\u00a0=\n          c(0,0.02) , xlim\u00a0= c(0, 2000))\n# Add lines\nlines(density(PC1_NK)$y, density(PC1_NK)$x,type=\"l\" , col\u00a0=\n          \"red\" , lwd=1)\nlines(density(PC1_Tcells)$y, density(PC1_Tcells)$x,type=\"l\"\n          , col\u00a0= \"blue\" , lwd=1)\nlines(density(PC1_Bcells)$y, density(PC1_Bcells)$x,type=\"l\"\n          , col\u00a0= \"green\" , lwd=1)\n# Add a legend\nlegend(1550, 0.020, legend\u00a0= c(\"Monocytes\", \"NK\n          cells\", \"T cells\" , \"B cells\"), fill=c(\n          \"orange\",\"red\",\"blue\",\"green\"\n          ) , box.lty=0 )\n# Add vertical dashed blue line at x\u00a0= 500\nabline(v=500, col=\"blue\" , lty\u00a0= \"dashed\")\n        Perform a pathway enrichment analysis of the HVGs of all cell types\n        using gProfiler2.13[href=https://www.wicell.org#bib13]\n            All the genes in the genome are used as the background gene set.\n          \nPerform the pathway analysis using the gost() function.\n# Pathway enrichment analysis\n# Extract all genes that will be used as the background for pathway\n          analysis\nbg <- rownames(Avg_expression_list[[1]])\n# Pathway enrichment analysis using gProfiler2\nGO_out <- gost(query\u00a0= HVGs , organism\u00a0= \"hsapiens\" ,\n          correction_method\u00a0= \"fdr\" , custom_bg\u00a0= bg , significant\u00a0=\n          TRUE , user_threshold\u00a0= 0.05 , evcodes\u00a0= TRUE , sources\u00a0=\n          \"GO:BP\")\nNote: Beside gProfiler2,13[href=https://www.wicell.org#bib13]\n      alternatively, functional enrichment analysis can be performed using\n      several computational/web tools such as clusterProfiler,23[href=https://www.wicell.org#bib23]\n      Gene Ontology Consortium,24[href=https://www.wicell.org#bib24] Database for Annotation,\n      Visualization and Integrated Discovery (DAVID),25[href=https://www.wicell.org#bib25]\n      Kyoto Encyclopedia of Genes and Genomes (KEGG),26[href=https://www.wicell.org#bib26] and\n      Reactome.27[href=https://www.wicell.org#bib27]\nSave your outputs (optional).# Save outputs\nsaveRDS(Avg_expression_list, file\u00a0=\n          \"PATH/TO/YOUR/WORKING/DIRECTORY/Avg_expression_list.rds\")\nsaveRDS(GO_out , file\u00a0=\n          \"PATH/TO/YOUR/WORKING/DIRECTORY/GO_out.rds\")\nwrite.csv(HVGs , file\u00a0=\n          \"PATH/TO/YOUR/WORKING/DIRECTORY/HVGs.csv\", row.names\u00a0=\n          F)\nwrite.csv(bg , file\u00a0=\n          \"PATH/TO/YOUR/WORKING/DIRECTORY/bg.csv\", row.names\u00a0=\n          F)\n      Investigating dynamic expression patterns of HVGs across all time points\n      and cell types\n    \nTiming: \u223c 5\u201310\u00a0min (for steps 15 to 23)\n      In this section, we describe the normalization method used to obtain\n      relative expression levels of HVGs associated with biological pathways of\n      interest, across four different time points as well as cell types (Figure\u00a02[href=https://www.wicell.org#fig2]B). The integrated Seurat object, average gene expression levels in each\n      cell type, and the selected GO:BP (biological process gene ontology) terms\n      from the pathway enrichment analysis are used as the inputs of this step.\n    \n        Import the integrated Seurat object, average gene expression levels in\n        each cell type, and biological pathways of interest into R.\n      \n# Load libraries\nlibrary(Seurat)\nlibrary(gprofiler2)\nlibrary(ggplot2)\nlibrary(tidyverse)\n# Set your working directory, pointing to the folder where all your\n          input and output files will be saved\nsetwd(\"PATH/TO/YOUR/WORKING/DIRECTORY\")\n# Load objects\nsc_integrated <- readRDS(file\u00a0= file\u00a0=\n          \"PATH/TO/YOUR/WORKING/DIRECTORY/sc_integrated.rds\")\nAvg_expression_list <- readRDS(file\u00a0=\n          \"PATH/TO/YOUR/WORKING/DIRECTORY/Avg_expression_list.rds\")\nselected_GO_out <- readRDS(file\u00a0=\n          \"PATH/TO/YOUR/WORKING/DIRECTORY/selected_GO_out.rds\")\nselected_GO_out <- selected_GO_out$result\n        Calculate the mean value of each gene across all cells from the\n        integrated Seurat object.\n      \n# Calculate the mean value of each gene from all cells\nexp_matrix <- GetAssayData(sc_integrated , slot\u00a0= \"data\"\n          , assay\u00a0= \"RNA\") %>% data.frame() %>%\n          rownames_to_column()\nrownames(exp_matrix) <- exp_matrix$rowname\nexp_matrix$rowname <- NULL\nexp_matrix$Mean <- rowMeans(exp_matrix)\nexp_matrix$Gene <- rownames(exp_matrix)\nMean_all_cells <- exp_matrix[c(\"Gene\" ,\n          \"Mean\")]\n        Create a list of HVGs in each \u201cGene Ontology: Biological processes\n        (GO:BP)\u201d.\n      \n# Create a list of HVGs in each GO:BP\ngenes_each_GO <- list()\nfor (RN in 1:nrow(selected_GO_out)) {\n\u00a0\u00a0temp <-\n          strsplit(unique(paste(as.character(selected_GO_out$intersection[RN]) ,\n          sep\u00a0= \",\")) , \",\")\n\u00a0\u00a0genes_each_GO[[RN]] <- temp[[1]]\n}\nnames(genes_each_GO) <- selected_GO_out$term_nameExtract the average gene expression values of HVGs in each selected\n        GO:BP.\n      \n# Extract average gene expression values of HVGs\nAvg_expression_each_GO <- list()\ntemp_list <- list()\nfor (RN in 1:length(Avg_expression_list)) {\n\u00a0\u00a0for (JA in 1:length(genes_each_GO)) {\n\u00a0\u00a0\u00a0\u00a0temp_list[[JA]] <-\n          Avg_expression_list[[RN]][rownames(Avg_expression_list[[RN]]) %in%\n          genes_each_GO[[JA]],]\n\u00a0\u00a0\u00a0\u00a0names(temp_list)[[JA]] <-\n          names(genes_each_GO)[[JA]]\n\u00a0\u00a0}\nAvg_expression_each_GO[[RN]] <- temp_list\n}\nnames(Avg_expression_each_GO) <- names(Avg_expression_list)\n        Normalize each gene by adding a pseudocount of 1 (to avoid undefined\n        results from a zero denominator\u00a0\u2013 in unexpressed genes), and then divide\n        by the mean value from all cells.\n      \n# Normalise each gene by adding a pseudocount of 1, and divide by\n          mean value from all cells.\nAvg_expression_each_GO_norm <- list()\nfor (RN in 1:length(Avg_expression_each_GO)) {\n\u00a0\u00a0temp <- Avg_expression_each_GO[[RN]]\n\u00a0\u00a0for (JA in 1:length(temp)) {\n\u00a0\u00a0\u00a0\u00a0temp_1\u00a0<-\n          Mean_all_cells[Mean_all_cells$Gene %in% rownames(temp[[JA]]),]\n\u00a0\u00a0\u00a0\u00a0temp_1\n\u00a0\u00a0\u00a0\u00a0temp[[JA]] <- (temp[[JA]]\u00a0+ 1) /\n          (temp_1$Mean\u00a0+ 1)\n\u00a0\u00a0}\n\u00a0\u00a0Avg_expression_each_GO_norm[[RN]] <- temp\n}\nnames(Avg_expression_each_GO_norm) <-\n          names(Avg_expression_list)\n        Calculate the sum of the normalized average gene expression of all HVGs\n        belonging to each GO:BP of interest.\n      \n# Sum of each gene in each GO:BP\nSum_avg_expression_each_GO <- list()\nfor (RN in 1:length(Avg_expression_each_GO_norm)) {\n\u00a0\u00a0temp <- Avg_expression_each_GO_norm[[RN]]\n\u00a0\u00a0for (JA in 1:length(temp)) {\n\u00a0\u00a0\u00a0\u00a0temp[[JA]][names(Avg_expression_each_GO_norm[RN]),]\n          <- colSums(temp[[JA]])\n\u00a0\u00a0}\n\u00a0\u00a0Sum_avg_expression_each_GO[[RN]] <- temp\n}\nnames(Sum_avg_expression_each_GO) <-\n          names(Avg_expression_each_GO_norm)\n        Create a big dataframe, where each row represents each GO:BP and each\n        column represents each cell type in each condition.\n      \n# Create list of summation each GO:BP in each cell type\nSum_exp_each_GO_celltype <- list()\nfor (RN in 1:length(genes_each_GO)) {\n\u00a0\u00a0Sum_exp_each_GO_celltype[[RN]] <-\n          rbind(tail(Sum_avg_expression_each_GO[[1]][[RN]] , 1) ,\n\u00a0\u00a0\u00a0\u00a0tail(Sum_avg_expression_each_GO[[2]][[RN]] ,\n          1) ,\n\u00a0\u00a0\u00a0\u00a0tail(Sum_avg_expression_each_GO[[3]][[RN]] ,\n          1) ,\n\u00a0\u00a0\u00a0\u00a0tail(Sum_avg_expression_each_GO[[4]][[RN]] ,\n          1))\n}\nnames(Sum_exp_each_GO_celltype) <- names(genes_each_GO)\n# Create a big dataframe, each row represents each GO:BP, and each\n          column represents each cell type in each condition.\n# matrix_size\u00a0= 1 : (no_of_cell_types x biological_conditions)\nno_cell_types <- 4\nno_samples <- 10\nmatrix_size <- no_cell_types \u2217 no_samples\nInput_dot_plot <- as.data.frame(matrix(1:matrix_size, nrow\u00a0= 1,ncol\u00a0= matrix_size))\ncolnames(Input_dot_plot) <- c(paste(\"Monocytes\" ,\n          names(Sum_exp_each_GO_celltype[[1]])) , paste(\"NK\" ,\n          names(Sum_exp_each_GO_celltype[[1]])) , paste(\"T\" ,\n          names(Sum_exp_each_GO_celltype[[1]])) , paste(\"B\" ,\n          names(Sum_exp_each_GO_celltype[[1]])))\nfor (RN in 1:length(genes_each_GO)) {\n\u00a0\u00a0temp <- Sum_exp_each_GO_celltype[[RN]]\n\u00a0\u00a0temp <- cbind(temp[1,],temp[2,] , temp[3,] ,\n          temp[4,])\n\u00a0\u00a0colnames(temp) <- c(paste(\"Monocytes\" ,\n          names(Sum_exp_each_GO_celltype[[1]])) , paste(\"NK\" ,\n          names(Sum_exp_each_GO_celltype[[1]])) , paste(\"T\" ,\n          names(Sum_exp_each_GO_celltype[[1]])) , paste(\"B\" ,\n          names(Sum_exp_each_GO_celltype[[1]])))\n\u00a0\u00a0Input_dot_plot <- rbind(Input_dot_plot , temp)\n}\nInput_dot_plot <- Input_dot_plot[-1,]\nrownames(Input_dot_plot) <- names(genes_each_GO)\n        Calculate the z-score across all samples and cell types for\n        visualization (Figure\u00a02[href=https://www.wicell.org#fig2]B).\n      \n# For visualization, calculate z-score by row\nInput_dot_plot_zscore <-\n          t(apply((Input_dot_plot[,1:length(Input_dot_plot)]), 1,\n          function(x){\n\u00a0\u00a0mean <- mean(x)\n\u00a0\u00a0SD <- sd(x)\n\u00a0\u00a0Z_score <- (x-mean)/SD\n\u00a0\u00a0Z_score\n}))\nInput_dot_plot_zscore <-\n          as.data.frame(Input_dot_plot_zscore)\n# Convert to long format for plot\nforplot <- gather(Input_dot_plot_zscore %>%\n          rownames_to_column(\"GO\"), key\u00a0= \"ST\" , value\u00a0=\n          \"Expression\" , -GO)\nforplot <- forplot %>% separate(col\u00a0= \"ST\" , into\u00a0=\n          c(\"Cell_Types\" , \"Patients\" ,\n          \"time\"),sep\u00a0= \" \" , remove\u00a0= F)\nforplot$time <- factor(forplot$time , levels\u00a0= c(\"Day-2\"\n          , \"Day-1\" , \"Def\" , \"Wk2\" ,\n          \"I\" , \"II\"))\nforplot$Cell_Types <- factor(forplot$Cell_Types , levels\u00a0=\n          c(\"Monocytes\" , \"NK\" , \"T\" ,\n          \"B\"))\nforplot$Patients <- factor(forplot$Patients , levels\u00a0=\n          c(\"DF\" , \"DHF\" , \"Healthy\"))\n        Visualize the results using the ggplot2 package.14[href=https://www.wicell.org#bib14]\n# Visualize the relative expression of HVGs over all samples and cell\n          types\nggplot(forplot, aes(x=time, y=GO , color= Expression , size\u00a0=\n          Expression))\u00a0+ geom_point(alpha\u00a0= 1.5)\u00a0+ theme_classic()\u00a0+\n          scale_colour_gradient2( low\u00a0= \"blue\", mid\u00a0=\n          \"white\", high\u00a0= \"red\", space\u00a0= \"Lab\" ,\n          limits\u00a0= c(-max(forplot$Expression),max(forplot$Expression)) )\u00a0+\n          xlab(\"\")\u00a0+ scale_size_continuous(range\u00a0= c(5,5))\u00a0+\n          facet_grid(\u223cCell_Types+Patients , scales\u00a0= \"free\", space\u00a0=\n          \"free\")\u00a0+ labs(color\u00a0= paste(\"z-score\"))\u00a0+\n          theme(axis.text.x\u00a0= element_text(angle\u00a0= 60, hjust=1),\n          axis.text=element_text(size=20) , axis.title.y\u00a0=\n          element_text(size=25), strip.background\u00a0= element_rect(colour\u00a0=\n          \"white\", fill\u00a0= c(\"gray\",\"darkorange3\",\n          \"gray\")), legend.text\u00a0= element_text(size\u00a0= 15),\n          legend.title\u00a0= element_text(size\u00a0= 20),legend.key.size\u00a0= unit(1,\n          \"cm\"))\u00a0+ ylab(\"Significant GO terms\")HVG identification and pathway enrichment analysis across early and late\n      time points in immune cell types - COVID-19 case study\n    \nTiming: \u223c 10\u201315\u00a0min\n      In addition to the dengue data published in our earlier study,1[href=https://www.wicell.org#bib1]\n      here we also demonstrate the application of this framework on another\n      time-course scRNA-seq study. We retrieved processed scRNA-seq data from a\n      COVID-19 study,10[href=https://www.wicell.org#bib10] which was deposited on FASTGenomics\n      (https://www.fastgenomics.org/[href=https://www.fastgenomics.org/]). We selected two COVID-19 patients (\u201ccohort 1\u201d) with the donor IDs\n      \"C19-CB-0009\u2033 and \"C19-CB-0012\u2033, whose samples were obtained at\n      the same \u201cdays after symptom onset\u201d; together with two healthy donors,\n      \"P15F\u2033 and \"P17H\". We used the same cell type annotation as\n      initially characterized by the authors of the study.10[href=https://www.wicell.org#bib10]\nNote: Using our framework to analyze this\n      COVID-19 dataset, we observed distinct overall transcriptome profiles\n      between early and late phases of the infection in each of the cell types\n      of interest, while the differences between the two patients were\n      relatively small (Figure\u00a03[href=https://www.wicell.org#fig3]A). Moreover, the overall\n      expression patterns of the two healthy donors were nicely grouped together\n      and clearly separated from the COVID-19 patients at all the time points\n      (Figure\u00a03[href=https://www.wicell.org#fig3]A). We next identified HVGs and virtualized\n      their dynamic expression patterns, for different groups of biological\n      pathways that the HVGs are associated with (Figure\u00a03[href=https://www.wicell.org#fig3]B). Interestingly, we observed clear up-regulation of the HVGs in the\n      early infection associated with \u201cresponse to type I interferon\u201d,\n      \u201cinterferon-mediated signaling pathway\u201d, and \u201ctype I interferon-mediated\n      signaling pathway\u201d in monocytes (Figure\u00a03[href=https://www.wicell.org#fig3]B, labeled in\n      blue), which have also been described in the original paper,10[href=https://www.wicell.org#bib10]\n      but using a different analytical framework.10[href=https://www.wicell.org#bib10] In\n      addition to this, our analysis and visualization also revealed\n      cell-type-specific expression dynamics of the HVGs associated with certain\n      biological pathways, such as \u201cnatural killer cell mediated cytotoxicity\u201d\n      HVGs being specifically upregulated in NK cells; \u201cT cell receptor V(D)Jrecombination\u201d HVGs being expanded in CD4+ and CD8+ T\u00a0cells, and several B\n      cells-related pathways such as \u201cB cell activation\u201d, \u201chumoral immune\n      response\u201d, and \u201cB cell mediated immunity\u201c being up-regulated specifically\n      in B cells at the early infection (Figure\u00a03[href=https://www.wicell.org#fig3]B), all of\n      which have not been mentioned in the original study.10[href=https://www.wicell.org#bib10]\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2780-Fig3.jpg\n          Figure\u00a03. The relative expression values of HVGs in each biological\n          process (BP) across two COVID-19 patients during the early time point\n          (d9; 9\u00a0days after symptom onset) and late time point (d16; 16\u00a0days\n          after symptom onset), together with two healthy controls\n        \n          (A) PCA of average gene expression in each cell type. Different colors\n          represent the days after symptom onset, while different shapes\n          represent individual samples.\n        \n          (B) Dotplot showing the relative expression levels of HVGs related to\n          each BP. The time-specific BPs described in the original paper10[href=https://www.wicell.org#bib10]\n          are labeled in blue. The NK-specific BP is highlighted in purple,\n          T-specific BP is in dark red, and B-specific BPs are in green. P1\u00a0=\n          covid-19 patient 1; P2\u00a0= covid-19 patient 2; HC\u00a0= healthy controls.\n          d9\u00a0= 9\u00a0days after symptom onset; d16\u00a0= 16\u00a0days after symptom onset.", "PennCNV (Wang et\u00a0al., 2007[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0018]) has been the de facto standard for CNV calling since its initial release 15 years ago. While it is fairly easy to use, implementing a pipeline to process tens or hundreds of thousands of samples can be challenging. Moreover, there are many checks the users need to perform in order to ensure all commands run as intended, as well as some files that are not straightforward to generate or obtain. The pipeline we refined takes care of most of these issues. This protocol details how to run the pipeline and obtain the raw CNV calls and sample QC files to use for further processing.\nRequired Files\nTo run the protocol three main files plus the intensity files are required, and they must be in the correct format. We provide GC content file (requirement 4); however, the user needs to generate the samples list (requirement 3) and the SNPs position file (requirement 2).1.Intensity files. These files store the raw information regarding each Illumina SNP probe. Each file should contain at least the following three columns: \u201cName,\u201d \u201cLog R Ratio,\u201d \u201cB Allele Freq.\u201d Column names must be exact; other columns can be present as long as these are the first three. These columns contain, namely: the name of the SNP markers, the value of Log2 R ratio (a relative measure of the light intensity), and B Allele Frequency (a measure of the allelic composition). Intensity files are also called \u201cFinal Reports\u201d and may contain a multi-line header. Moreover, it is possible that multiple samples are stored in the same file (e.g., an entire genotyping batch) and that the files are compressed. In such cases, PennCNV will not work. All these problems can be very different from case to case, but they can be solved using standard GNU utilities tools (included in every major linux distribution) such as sed or the program awk. To solve the second problem, the PennCNV script split_illumina_report.pl can also be used. Note that in this case the user may need to regenerate a correct samples_list.txt file. Note that it is important to know in which build of the human genome the intensity data is (e.g., \u201chg19\u201d), and that all samples in the same project must be on the same version.2.snppos.txt. This is a tab-separated text file that contains at least the following three columns: \u201cName,\u201d \u201cChr,\u201d and \u201cPosition,\u201d which are the name and genomic location of each SNP marker. The first column must use the same names as in the intensity files. In projects where each intensity file contains this information, any sample can be used to generate the SNP position file. Otherwise, there should be a file called \u201cPFB\u201d (Population Frequency of the B allele) that contains all the columns required by the snppos.txt format.\n3.samples_list.txt. This must be a two-column tab-separated text file with a header. The two columns must be \u201csample_ID\u201d and \u201cfile_path.\u201d \u201csample_ID\u201d needs to be the identifier for the specific sample, and \u201cfile_path\u201d needs to be the complete path (thus starting from root, \u201c/\u201d; therefore avoid using links and the home directory, \u201c\u223c/\u201d, in the path) to the intensity file for that sample. Again, this protocol assumes there is one intensity file per sample. If this is not the case, see step 4. In the case where samples were genotyped in waves or batches, these can be used; otherwise, batches of approximately 2000 samples will be created. In the first instance, an additional column called \u201cbatch\u201d must be present, and the content must be integers indicating the specific batch.\n4.GC content file. This file is used by the PennCNV script cal_gc_snp.pl. This file is not straightforward to generate, but we provide the hg38, hg19 and hg18 version as part of the IBPcnv repository. The hg38 version is available from the PennCNV repository: https://github.com/WGLab/PennCNV/blob/master/gc_file/[href=https://github.com/WGLab/PennCNV/blob/master/gc_file/]. It is also included in the IBPcnv repository for user convenience. Finally, the original hg18 version can also be obtained directly from UCSC Genome Browser at http://hgdownload.cse.ucsc.edu/goldenPath/hg18/database/gc5Base.txt.gz.\nProtocol steps1. Setup. We provide all required software and scripts in a docker/singularity container and a GitHub repository. Excluding the raw data, all files and subdirectories need to be in the same directory ($workingdir hereafter). Support Protocol 2 details the installation process, as well as some suggestions on how to set up directories and the environment for the analysis.\n2. Initial checks and required files generation:\n         \nThis will do the following. First, it will run a series of tests to check that all intensity files are present and in the correct format. Then, it will check if samples were already divided into batches and that this is in the correct format. If not, it will create the batches and separate the samples into groups of approximately 2000 each; the actual number may vary in order to not have the last batch significantly smaller than the rest.\nTo complete this step, move to the main working directory (cd $workingdir) and run:\nsingularity exec ibpcnv.simg Rscript \\\nIBPcnv/penncnv_pipeline/01_preprocess.R \\\n$workingdir 1 2000 $tabix_folder\nwhere $tabix_folder is the complete path to the directory where the tabix-indexed files will be created in step 5. The approximate batch size can be changed via the third parameter; however it is not recommended to use a small batch size, as the population B allele frequency (PFB) files will be created per batch.\n         \nc.If the script fails checking the batches but no errors are found in the intensity files, the second parameter can be set to 0 to skip the initial checks that constitute the slower part of the testing.\nd.If all checks are successfully completed, the script will write the per-batch sample list files needed by PennCNV in $workingdir/listfile/. The script will also print the number of batches that will be used.\n3. Select the SNP markers:This step will do the following. First it will extract the marker names and positions from an intensity file. It will then download the HRC SNP list, selecting only SNPs that are strictly biallelic, known (with a name in dbSNP142, no \u201c.\u201d) and with a minor allele frequency of at least $minMAF (MAF values for each SNP are obtained from the HRC); we suggest 0.001 (0.1%). Then, it will merge the two tables, remove markers with duplicated \u201cSNP_ID,\u201d remove markers that map to the same position, and finally it will create the snppos.txt file needed by PennCNV.\nTo complete this step, move to the main working directory and run:\nsingularity exec ibpcnv.simg Rscript \\\n\u2003\u2003IBPcnv/penncnv_pipeline/02_select_SNPs.R \\\n\u2003\u2003$workingdir $minMAF $hgversion TRUE\nThe removal of duplicated markers can be avoided by setting the last parameter to FALSE. At the moment of this writing, the HRC SNP list is available only in hg19 coordinates; thus, the last argument can take only \u201chg19\u201d as value. This may change in the near future. Any other list of SNPs that follows the same format will work. See the Sanger Institute documentation for details: http://ngs.sanger.ac.uk/README and ftp://ngs.sanger.ac.uk/production/hrc/HRC.r1/README. When using a different SNP list, it is suggested to first use the default one in order to be able to easily check the format.\n4. PennCNV calling pipeline:\nFirst, run the calling pipeline in parallel on each wave. To complete this step, move to the main working directory and run the command:\nbash IBPcnv/penncnv_pipeline/03_penncnv_pipeline.sh \\\n\u2003\u2003$workingdir $ibpcnvdir $n_batches hg19\nwhere $n_batches is the number of batches from step 2.\n         \nb.PennCNV calling parameters (minimum number of SNPs and minimum length of each call) can be changed in the script $ibpcnvdir/penncnv_pipeline/03_2_cnv_calling.sh, before launching the previous step. By default, these are set to 5 and 1000 bp respectively.c.Check that all batches are completed successfully. Catching any PennCNV error is quite straightforward\u2014the following command can be used\ncd $workingdir && grep 'ERROR' pennlogs/*\nHowever, SLURM and PBS problems can be more complex to find. To check that all samples have been processed, run:\n         \nif [$(wc -l samples_list.txt | cut -d ' ' -f1) == \\\n\u2003\u2003$(wc -l results/autosome.qc | cut -d ' ' -f1)]; then\n\u2003\u2003echo \"All good\"; fi\nIf this check fails, the following command can be used to list the samples that have not been processed by PennCNV:\n         \njoin -v2 -1 1 -2 2 <(LANG=C tail -n+2 1 results/autosome.qc | sort -k) \\\n\u2003\u2003<(LANG=C tail -n+2 samples_list.txt | sort -k 2)\nOne common problem is for a full batch or a batch chunk to fail, usually due to the job scheduler. If that is the case, the full batch can be relaunched running (for PBS systems, use qsub instead of sbatch):\n         \nsbatch IBPcnv/penncnv_pipeline/03_1_per_wave.sh $workingdir \\\n$ibpcnvdir $n_wave\nwhere $n_batch is the number of the failed batch. We suggest redoing the whole batch also in the case of a partial failure.\n         \nd.After the calling pipeline is completed, the batches can be combined into two single files (CNV calls and QC) running (for PBS systems, use qsub instead of sbatch)\nsbatch IBPcnv/penncnv_pipeline/04_combine_results.sh \\\n\u2003\u2003$workingdir $ibpcnvdir $maxgap\ne.This will also perform a \u201csoft stitching\u201d step, meaning that for each sample, close calls with the same copy number will be stitched together. This is controlled via the $maxgap parameter; we recommend a value of 0.2 and not higher than 0.4, as higher values could alter the raw call-set. A stronger stitching will be performed in Basic Protocol 2 when selecting putative CNVs in a specific locus.5. Tabix indexing the intensity files. In Basic Protocol 2, we take advantage of the speed of tabix-indexed files as well as of the GC waviness-adjusted LRR values.\n         \nThe GC model file for the specific array in use should have already been generated. If the user is interested only in the second half of the protocol, the following command can be used:\nbash IBPcnv/misc/create_gcmodel.sh $workingdir $ibpcnvdir\nb.To perform the indexing run:\nbash IBPcnv/penncnv_pipeline/05_launch_tabix.sh $workingdir $ibpcnvdir\nThis will also compute the GC waviness-adjusted LRR values for each marker. This can be used in the visual inspection and is also discussed in the Commentary. This step concludes Basic Protocol 1.CNV calling using SNP-array data is an intrinsically imprecise process, and strongly depends on the quality of the initial SNP-array raw data. In particular, it is prone to pick up noise as signal, as well as to \u201cover-segment,\u201d that is, incorrectly splitting a (often large) CNV call into several smaller ones. This ultimately may lead to unreliable results. Moreover, precise CNV boundaries (at the level of 1-2 SNP probes) are very difficult to obtain. Different research groups have developed different strategies to overcome these problems. To counter over-segmentation, it is common practice to \u201cstitch\u201d close, consecutive calls with the same copy number (CN). To reduce noise, some kind of filtering is almost always included. At the CNV call level, the filtering can act on the call length, the number of SNP probes, or, more rarely, on the confidence score. Some filtering is usually also performed at sample level; this may include removing samples with too high an LRR standard deviation (LRRSD), extreme BAF drift, or too many calls. A few studies, such as this protocol, also perform an initial filtering on the SNPs in order to reduce the general noise of the raw data. Notably, these approaches tend to lead to false positive calls being a larger issue than false negative ones. Changing the type and strength of the CNV call level filtering enables researchers to balance between false negatives and the number of calls to validate (i.e., false positives to manually screen). In order to reduce the number of false positives, two main strategies have typically been used. The first is to visually validate all putative CNV calls.This approach is time consuming and prone to human error; however, when done correctly, it is the overall best approach, and it is commonly used when calling CNVs in a limited set of specific genomic loci, such as recurrent CNV loci (Calle S\u00e1nchez et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0002]; Stefansson et\u00a0al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0015]). This approach should always be preferred when the CNVs of interest are rare, and thus a few false positives or negatives could significantly affect estimated prevalence and downstream analysis. The second approach is to use multiple algorithms to perform the CNV calling and intersect the resulting callsets (often using a form of reciprocal overlap in terms of base pairs or probes) and filter the results. An example is using more than one program [such as PennCNV (Wang et\u00a0al., 2007[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0018]), QuantiSNP (Colella et\u00a0al., 2007[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0003]), and others) and considering \u201cvalidated\u201d only calls where two or more programs made the call. In this case the reasoning is that even if some false positives are introduced, with a large sample size, these errors will be equally distributed among, e.g., cases and controls. Such studies usually also perform some kind of grouping before doing any analysis (e.g., deletions affecting at least one gene) and rarely consider individual CNVs on their own.In this protocol, we follow the first approach (i.e., always relying on visual inspection as the final validation step), and we integrate it with newly developed filters that do not depend on the PennCNV output but directly on the raw data. By doing so we are able to reduce the number of putative carriers to inspect, while also minimizing the number of false negatives as much as possible. This approach is particularly suited for large cohorts where the number of false positives can be high. In smaller datasets, it may not be equally useful to perform the filtering step, and the user can simply use our package to standardize the files and then use the graphical interface to validate all putative carriers. It is important to notice that, as stated in the main introduction, this protocol has been designed with a strong focus on recurrent CNV, meaning CNV with relatively fixed and precise start and end positions. As an example, a researcher interested in all recurrent CNVs in the larger 15q region should specify each specific locus individually in the loci.txt file (see required files), while a researcher interested in all CNVs overlapping the NRXN1 gene should use a very low minoverlap value (see step 3c) and avoid the advanced filtering (see step 4d).\nFigure 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0002] shows a schematic representation of the Basic Protocol 2.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/58daf4ff-8d25-4e37-9cd0-14daad6092f5/cpz1621-fig-0002-m.jpg</p>\nFigure 2\nMore detailed schematics of Basic Protocol 2. Colors and icons are kept consistent with Figure 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0001], indicating starting data files, QCtreeCNV, and DeepEYE respectively.\nRequired FilesThe only additional file needed to run the second part of the protocol is a list of the loci of interest. The file loci.txt must be a four-column tab-separated text file with a header. The columns must be called \u201clocus,\u201d \u201cchr,\u201d \u201cstart,\u201d and \u201cend.\u201d The chromosome must be in integer format. This format is used in all the results and intermediate R objects, as well as the tabix-indexed intensity files. It simply consists of integers from 1 to 22 that are used for autosomes, plus 23 for X, 24 for Y, 25 for XY, and 26 for MT.\nProtocol steps\n1. Setup. All software and scripts required are provided in a docker/singularity container and a GitHub repository. Support Protocol 2 details the installation process. It is assumed the user has successfully completed Basic Protocol 1. Throughout this protocol, all files are loaded in R as data.table objects. Please note that they behave slightly differently than data.frame in certain situations. If the user prefers using data.frame to explore the results, after the protocol completion the main objects can be converted by running:\n         \nobjectA_df <- as.data.frame(objectA_dt)\nThis will ensure full consistency when using rbase commands and the tidyverse framework.\n2. QCtreeCNV filtering pipeline. This step is meant to be run interactively in an R session. The user can use the provided commands in an R script; however, we suggest exploring at least a couple of loci interactively before setting the final parameters.\n3. Preparation. All code lines are shown in code block 1.\n         \nLoad data into R. To launch an interactive R session using the provided singularity image, run:\nsingularity exec ibpcnv.simg RAt this point, the four main files can be loaded into R. Assuming the user followed the suggested naming in Basic Protocol 1, this can be done by running lines 1 to 5.\n         \nb.Check formats and select the calls in the loci of interest. These steps can be performed with a single function, qctree_pre(). It takes the four main objects from the previous step and the parameters for stitching close calls. Line 7 shows the code for default values. If there is any problem with the inputs, the function will fail with an error message explaining the specific problem. By default, the function will also take care of multiple calls in a locus from a single locus, keeping only the largest call, regardless of the copy number. The user can avoid this by setting rm_dup = F. Note however that the downstream steps do not support multiple calls per sample in a single locus and will throw an error if any is found.\nc.The stitching function takes three main parameters, minimum number of SNPs for calls to be considered, maximum gap between two consecutive calls with same CN in order to stitch them together, and minimum overlap between a call and a locus for the call to be selected as a putative CNV. Default values are respectively 20, 0.5 and 0.2. These values can be changed with the following parameters: minsnp, maxgap, minoverlap, e.g., pre <- qctree_pre(loci, cnvs, qc, samples, minsnp = 15, maxgap = 0.4, minoverlap = 0.5).d.Compute CNV Regions. We provide two different functions to compute CNVRs (CNV Regions), cnvr_fast() and cnvrs_create(). Additionally, the user is free to use a different method as long as the results are in the correct format. CNVRs are used here to separate groups of largely overlapping calls within a certain locus, in particular groups with different lengths. CNVRs and their computation are further discussed in the Commentary and in the package manuals and vignette. The suggested function can be run as shown in line 9:\n1> library(data.table); setDTthreads(2); library(QCtreeCNV)\n2> cnvs <- fread(\"results/autosome.cnv\")\n3> qc <- fread(\"results/autosome.qc\")\n4> loci <- fread(\"loci.txt\")\n5> samples <- fread(\"samples_list.txt\")\n6>\n7> put_cnvs <- qctree_pre(loci, cnvs, qc, samples)\n8>\n9> cnvrs <- cnvr_fast(put_cnvs)\nCode block 1.\n4. qctree() filtering:\n         \nThe filtering function is structured as a decision-making tree consisting of five main steps, and each step has multiple parameters the user can change. More technical details on each step and how the main parameters are connected with the outputs of Support Protocol 1 (quality control) are further discussed in the Commentary. To run the function with default values type:\ncnvs_out <- qctree(cnvrs[[1]], cnvrs[[2]], loci)\nIf no filtering is deemed necessary (for example when the number of putative calls is small), the user can proceed directly to step 4d.\n         \nb.Step 1 of the filtering tree is removing QC outliers sample-wise, and this is the most accessible step to customize. By default, samples will be removed if they have LRRSD > 0.35, BAF drift > 0.01, or GCWF outside of the window \u20130.02 to 0.02. These values can be changed with the following parameters: maxLRRSD, maxBAFdrift, maxGCWF, minGCWF.c.The resulting table will contain the column \u201cexcl\u201d with the value 0, meaning the line is a good putative CNV call, or 1, meaning the line is a bad putative CNV and can be skipped in the visual inspection step. This table can be exported in the correct format for the visual inspection interface with:\nexport_cnvs(cnvs_out[excl == 0,], \"putative_cnvs.txt\")\nThis will write the file putative_cnvs.txt in the $workingdir containing only the good putative CNV calls in the format expected by DeepEYE. To export all calls, type:\n         \nexport_cnvs(cnvs_out, \"putative_cnvs.txt\")\nd.If no advanced filtering is needed, the user can simply apply the standard filters (LRRSD, BAFdrift, GCWF) when exporting the table. Using the data.table syntax, run:\nexport_cnvs(put_cnvs[LRRSD <= 0.35 & BAFdrift <= 0.01 &\n\u2003\u2003\u2003\u2003between(GCWF, -0.02, 0.02, incbounds=T),],\n\u2003\u2003\"putative_cnvs.txt\")\n5. Visual inspection.\n         \nVisual inspection of the putative CNVs is necessary to validate the true CNV carriers with precision. The in-house program DeepEYE (included in the provided container) provides the user with a graphical interface to assign a label (true, false, unknown) to each CNV candidate. The results are stored in an extra column in the putative CNVs file.\nDeepEYE requires three main inputs\u2014the samples and loci lists, plus the putative CNVs table. It can be run from the singularity image, assuming all files have the standard names described in the protocol:\nsingularity exec ibpcnv.simg python3 /opt/eyeCNV/visualizer.py \\\n$workingdir putative_cnvs.txt loci.txt samples_list.txt GC_YES\nThis will open a graphical window as shown in Figure 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0003].\n         \nInitially, the window will display only a series of buttons and boxes (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0003] left side). In order to start the actual visual inspection (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0003] right side), the user needs to follow these steps:Name the project (box a); the name will dictate the name of the output file, i.e., visual_output_projectname.txt. The project name should be meaningful, e.g., when user \u201cabc\u201d is evaluating deletions in the TAR locus, a good project name could be TAR_del_abc.\nSelect the loci to inspect. Left clicking on button b will open a secondary window (i) where it is possible to select the loci to inspect in the current run.\nSelect the condition. Left clicking on button c will open a menu with the following options: true, false, unknown, all. In a new project \u201call\u201d should be selected; while re-evaluating a previous project the user can select only a portion of the calls (e.g., unknown).\nSelect the type. Left clicking on button d will open a menu with the following options: duplications, deletions, any. This lets the user select a specific type of CNV to inspect.\nOnce this is set, the user can click button e, \u201cstart.\u201d\nc.If at least one CNV call was selected, the right panel will appear. The panel consists of two plots (h); the top is for LRR and the bottom is for BAF. Each dot represents a marker. The red dots are within the locus of interest, the blue dots are outside.\nd.For each plot, the user can evaluate if a CNV is present within the red region and record the decision using the dedicated buttons (f):\nTrue: there is a CNV in the red region (with the correct CN, as shown in g).\nFalse: there is not a CNV in the red region (or the CN is not correct).\nUnknown: it is not possible to tell whether a CNV is present or not, likely due to excessive noise in the region.\nError: useful to mark samples with problematic intensity data.e.The progress bar and text in g mark the session's progress. Once all selected calls have been inspected, the result file will be written in $workingdir. The last column, \u201cvisual_output,\u201d contains the record of the visual inspection as integers: 1 (true), 2 (false), 3 (unknown), \u20137 (error). This file can be used again as a putative CNV file, for example to re-evaluate unknown calls only.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/01639876-46b1-4c58-a096-e1bdca074b79/cpz1621-fig-0003-m.jpg</p>\nFigure 3\nDeepEYE graphical interface (no CNV is present in the region, simulated data). Refer to step 5b of Basic Protocol 2 for the usage instruction.\n6. Visual evaluation concludes Basic Protocol 2. The file visual_output_projectname.txt will contain the results. The visual output codes (step 5e) can be used to filter the relevant CNVs. See also the final section Understanding Results.This protocol, similar to any other CNV calling pipeline, implements a certain set of filters. We propose valid default values based on the scientific literature and our research experience (Calle S\u00e1nchez et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0002]; Stefansson et\u00a0al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0015]). However, since this protocol is mostly aimed at CNV calling in recurrent loci where the actual CNVs are quite rare, we need to be extremely careful that our processing does not introduce any false negatives that may severely bias our estimates (false positives are controlled via visual inspection). In large cohorts, this ultimately means balancing the strength of the filters and the amount of manual validation required. Finally, some filters are applied sample-wise on values that directly reflect the noise in the data, for the specific sample (LRRSD and BAF drift in particular). We found that it is often quite possible to use relaxed filters, thus eliminating fewer samples. However, in doing so, one must be able to assess that the ability to detect CNVs is not significantly different in \u201cnoisier\u201d samples, otherwise biases may be introduced in the results. Here, we show how to produce a series of plots that help identify such potential issues in the results of the protocol, and briefly discuss the interpretation of each one. Specific problems and solutions are then highlighted in the troubleshooting section.\nProtocol steps\n1. Setup. This support protocol is meant to help evaluate the performance of the CNV calling protocol. First, start an R session. From $workingdir:\n         \nsingularity exec ibpcnv.simg R\nLoad the visual inspection results, e.g., assuming the visual inspection results were saved as visual_output_ALL.txt in $workingdir:\n         \nlibrary(data.table); vi_res <- fread(\"visual_output_ALL.txt\")2. Create the plots. The function qc_plots_cnvs() will create three plots for the main filtering arguments and more supporting ones. Here we will discuss briefly only the main one, for the complete discussion refer to the Commentary. As an example, to create the QC plots for the CNVs in all loci, simply run:\n         \nlibrary(QCtreeCNV); qc_plots_cnvs(vi_res, \"all_loci\")\nThis will create the folder all_loci in the working directory and save all QC plots in it. Note that the plots show the results regarding only the CNVs marked as true (visual_ouput == 1).\n3. Interpret the results. Figure 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0004] shows a good and a bad example of the two plot types. Also seeUnderstanding Results for more discussion.\n         \nPlot 1 (example in Figure 4A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0004]) shows the CNV prevalence in different LRRSD chunks (low, medium, high), separated for deletions and duplications. It illustrates the ability to detect true CNVs in different groups of samples from the noise perspective (high LRRSD can be considered the main indication of a noisy sample). Ideally, the prevalence should not strongly differ across the three groups, especially the one with higher LRRSD compared to the rest. A significant change means that the ability to detect CNVs is significantly affected in noisy samples, usually becoming lower. This means the LRRSD filter threshold may need to be increased. On the other hand, if a strong LRRSD filter was used and plot 1 shows very high consistency, the user may want to explore lowering it to possibly exclude fewer samples from the analysis.Plots 2 and 3 (example in Figure 4B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0004]) show the distribution of the number of SNPs and overlap proportion with the locus per call for true CNVs, numsnp, and overlap, respectively. Both these measures are used as filters when selecting putative CNV calls. Ideally, the distribution should not seem to be \u201ccut\u201d at the threshold values for numsnp and overlap. If that is the case, it might indicate that some potential true CNVs are being filtered out, and it may be worth trying to relax the filters. This is especially important for very rare CNV loci, where a small increase in carriers can have an impact on power. Note that plot 2 may be more meaningful when used on individual loci or a group of loci with very similar marker coverage, since it is the absolute number of markers of each call. If different thresholds were used for different loci, they must be treated separately to obtain meaningful results.\nPlots 4 and 5 can be interpreted in the same way as plot 1 (Fig. 4A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0004]) but regarding two other noise measures, BAF drift and GCWF, respectively. They can be considered secondary since these dimensions are less prone to affect the CNV detection accuracy.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6824be90-ef9f-4ad4-8c2f-dd259b0ff62c/cpz1621-fig-0004-m.jpg</p>\nFigure 4\nA good and a problematic example of QC plots 1, 4, and 5 (panel A), and 2 and 3 (panel B). In both plots, the deletion in light red represent the ideal situation, and the duplication in light blue represent the problematic situation. (A) the group of samples with high LRRSD have fewer true duplications than the other two groups, and that the confidence interval is relatively small. (B) The threshold value we selected for numsnp appears to be cutting the left tail of the distribution for the true duplications.4. Deal with possible detection bias. If the dataset includes sample groups selected differently (e.g., cases and controls), it may be a good idea to analyze them separately. The CNV prevalence is often expected to differ in different groups (e.g., some recurrent CNVs are more frequent in neuropsychiatric case groups than in population controls); thus, combining them may lead to confusion in the interpretation of the QC plots, especially if LRRSD distribution also differs across those groups. Possible problems in these plots are described in the last two points of the troubleshooting section. Assuming the sample_IDs for one group are in vector groupA QC analysis can be run separately with:\n         \nqc_plots_cnvs(vi_res[sample_ID %in% groupA,], \"groupA\")\n5. Explore an individual locus or groups of loci. The process described in previous steps 2 and 3 can be applied to groups of loci as well as to individual loci. This is useful especially when there seems to be some problems, but only in a fraction of the loci or in a particular one. For example, to look only at the results for loci \u201cA\u201d and \u201cB\u201d run:\n         \nqc_plots_cnvs(vi_res[locus %in% c(\"A\", \"B\"),], \"loci_A_B\")\nAs a rule, it can be helpful to divide the loci of interest into two or three groups based on length (e.g., small/large) and inspect the QC plots for the groups in addition to the ones for all loci. Groups can also be based on prevalence (very rare/not very rare) or other measures. The general idea is that, while looking at all loci at the same time can give an overall impression of the results quality, it can also mask problems linked to one or very few loci. Smaller groups can help identify those, if any, and the QC plot for individual loci can pinpoint the actual problem.We provide a docker image containing all software required to run the protocol on Docker Hub at https://hub.docker.com/r/sinomem/docker_cnv_protocol[href=https://hub.docker.com/r/sinomem/docker_cnv_protocol]. This container has the following software installed: htslib (Bonfield et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0001]; Danecek et\u00a0al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0005]) (1.14), PennCNV (Wang et\u00a0al., 2007[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0018]) (1.0.5), R (R Core Team, 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0011]) (4.1.2), and DeepEYE2, as well as several R packages, including data.table (Dowle et\u00a0al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0006]), fpc (Hennig, 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0008]), and VariantAnnotation (Obenchain et\u00a0al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0010]).\nRegarding the in-house software used in this protocol, the QCtreeCNV R package is available on GitHub at https://github.com/SinomeM/QCtreeCNV[href=https://github.com/SinomeM/QCtreeCNV]. Instructions to install the package are given in the README. DeepEYE2 is available at https://github.com/XabierCS/eyeCNV[href=https://github.com/XabierCS/eyeCNV]. All other scripts are collected in a GitHub repository at https://github.com/SinomeM/IBPcnv[href=https://github.com/SinomeM/IBPcnv].\nThe statistical programming language R is available at https://www.r-project.org/[href=https://www.r-project.org/]. Tabix is part of the HTSlib suite, together with SAMtools and BCFtools. It can be obtained at https://www.htslib.org/download/[href=https://www.htslib.org/download/]. PennCNV is the de facto standard in CNV calling from array data, in particular Illumina. It is available at http://penncnv.openbioinformatics.org/en/latest/user-guide/download/. In the following section, we detail how to install the docker/singularity image and use it to run the protocol.\nProtocol steps\n1. Setup. Throughout the protocol, $workingdir is used to indicate the main project folder. This folder will contain all the input and output files. For simplicity, the user can define an environmental variable:\n         \nexport workingdir=/path/to/workingdir2. Install singularity. The software should be already installed on most modern HPCs. If not, users should ask the system administrator to install it for them. To install it on a Linux workstation, one should follow the official instructions available at https://sylabs.io/guides/3.0/user-guide/installation.html[href=https://sylabs.io/guides/3.0/user-guide/installation.html]. A precompiled RPM package is available at https://dl.fedoraproject.org/pub/epel/8/Everything/x86_64/Packages/s/[href=https://dl.fedoraproject.org/pub/epel/8/Everything/x86_64/Packages/s/] and the program alien can be used to convert it to DEB (https://github.com/apptainer/singularity/issues/5390[href=https://github.com/apptainer/singularity/issues/5390]). Finally, in systems where the use of conda environments is encouraged or enforced, it should be possible to use the conda package at https://anaconda.org/conda-forge/singularity[href=https://anaconda.org/conda-forge/singularity].\n3. Download the provided container image. We provide the container on DockerHub, and it can be pulled directly by singularity. To do so, first move in the desired folder (cd $workingdir) and then type:\n         \nsingularity pull ibpcnv.simg docker://sinomem/docker_cnv_protocol:latest\nNote that the protocol expects the image to have the suggested name (ibpcnv.simg) and be stored (or linked) in the main working directory ($workingdir). If singularity fails with an error regarding the /tmp folder, it may help to set the environmental variables SINGULARITY_TMPDIR and SINGULARITY_CACHEDIR to some non-protected location (such as \u223c/tmp or $workingdir/tmp).\n4. Download the IBPcnv repository. We provide two versions, one that uses SLURM (srun/sbatch) and one that uses PBS (qsub). They can be obtained running:\n         \nwget https://github.com/SinomeM/IBPcnv/archive/refs/heads/master.zip&&\\\n\u2003\u2003\u2003unzip master.zip && mv IBPcnv-masterIBPcnv && rm master.zip\n# or\nwget https://github.com/SinomeM/IBPcnv/archive/refs/heads/pbs.zip&&\\\nunzip pbs.zip && mv IBPcnv-pbs IBPcnv && rm pbs.zip\nfor the SLURM and PBS versions respectively. For convenience, we can define the environmental variable $ibpcnvdir:\nexport ibpcnvdir=${workingdir}/IBPcnv\n5. Add the SLURM/PBS account if needed. The four scripts that use the job scheduler (03, 03.1, 03.2, and 04 in $ibpcnvdir/penncnv_pipeline/) are designed to be easily edited if the system requires the use of a specific account name. Throughout the text we provide both versions of the commands when run interactively.6. Run the protocol. All scripts assume that the singularity image is used. The pipeline in Basic Protocol 1 is designed to take advantage of the SLURM or PBS job scheduler, depending on which branch of the IBPcnv repository was chosen.\n7. Docker versus singularity. To download the container using docker we run:\n         \ndocker pull sinomem/docker_cnv_protocol:latest\nThen, to print the tabix help page using singularity image or docker we type respectively:\n         \nsingularity exec /path/to/ibpcnv.simg tabix --help\n# or\ndocker run sinomem/docker_cnv_protocol:latest tabix --help\nOne of the main differences is that, conveniently, singularity automatically mounts several file storage locations to the container while Docker does not. Moreover, in order to use docker a user needs to be added to the docker group and this process may require sudo permissions. Refer to the docker documentation for further details, https://docs.docker.com/[href=https://docs.docker.com/].", "Step-by-step method details\nStep-by-step method details\nBuild definition table for target health outcome\nTiming: 2\u20138 h\nHealth outcome information from various data sources / data fields within the main dataset is encoded differently. These relationships have been curated and recorded in the data setting file included in the ukbpheno package. For a target phenotype, survey the various data sources/\u00a0data fields on the Showcase and determine the definitions for the target phenotype in\u00a0UK Biobank. An example definition table to define type 2 diabetes is included in the package.\u00a0This example table can be used as a template for users to define their target health outcomes.\nDownload data setting file (data.settings.tsv) to the project directory from https://github.com/niekverw/ukbpheno/tree/master/inst/extdata/data.settings.tsv[href=https://github.com/niekverw/ukbpheno/tree/master/inst/extdata/data.settings.tsv].\nDownload definition table template to the project directory from https://github.com/niekverw/ukbpheno/tree/master/inst/extdata/definitions_DmRxT2.tsv[href=https://github.com/niekverw/ukbpheno/tree/master/inst/extdata/definitions_DmRxT2.tsv].\nFill in one phenotype (such as DmT2) per row. The column \u201cTRAIT\u201d contains the unique identifier of each phenotype which is case sensitive.\nFor each of code systems e.g., diagnosis codes ICD10 or operation codes OPCS4 as well as codes used in the self-report fields, fill in the corresponding codes in the table.\nEach code should be separated by a comma.\nFor code systems with hierarchical system (refer to data setting file), it is possible to fill in only the parent codes instead of specifying all codes.\nAnnotations of the codes can be made using curly bracket \u201c()\u201d. Figure\u00a03[href=https://www.wicell.org#fig3] illustrates an example for the three rules above.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1733-Fig3.jpg\nFigure\u00a03. Basic syntax for filling in the definition tablesOptional: We included a shiny app to cross-reference codes between systems using the map- ping file provided by UK Biobank. (https://github.com/niekverw/ukbpheno/blob/master/inst/util/shiny.lookup_codes.R[href=https://github.com/niekverw/ukbpheno/blob/master/inst/util/shiny.lookup_codes.R]). Download the code map file (Excel workbook) provided by the UK Biobank (https://biobank.ndph.ox.ac.uk/showcase/refer.cgi?id=592[href=https://biobank.ndph.ox.ac.uk/showcase/refer.cgi?id=592]): (1) locate the shiny app script and run the shiny app, and (2) visit the address returned (usually in the form of http://127.0.0.1:xxxx[href=http://127.0.0.1xxxx]) in a web browser and use the app. A screenshot of the shiny app can be found in Figure\u00a04[href=https://www.wicell.org#fig4].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1733-Fig4.jpg\nFigure\u00a04. The interface of the shiny app \u201cukb code explorer\u201d\nFill in fields with conditions in the \u201cTS\u201d (touchscreen) column.\nFill in field number as Showcase followed by the condition e.g., \u201c6177=3(insulin)\u201d\nTable\u00a02[href=https://www.wicell.org#tbl2] shows the conditions symbols accepted:\u00a0= (equal), != (not equal), <, <=, >, >=, \u2265, \u2264\ntable:files/protocols_protocol_1733_2.csv\nAdd the corresponding age of diagnosis using \u201c[]\u201d following the condition e.g., \u201c4041=1[2976](Gestational diabetes)\u201d\nIt is possible to create a composite phenotype, which involves other phenotypes. Composite phenotypes are constructed using four columns in the definition table (Table\u00a03[href=https://www.wicell.org#tbl3]).\ntable:files/protocols_protocol_1733_3.csv\nIn this example usage: cases with records of \u201cDmT1\u201d (type 1 diabetes) are excluded; Controls with records indicating \u201cRxDm\u201d (use of antidiabetic medication) are excluded; participants with records indicating \u201cRxDmOr\u201d (use of oral antidiabetic medication) will be considered as cases for this composite phenotype.\n\u201cStudy_population\u201d can be used to restrict definition on a subgroup of participants with specific phenotype.\nParticipants with phenotypes in \u201cInclude_definition\u201d will be considered to be a case for the composite phenotype.\nUsers may use the \u201cExclude_from_cases\u201d and \u201cExclude_from_controls\u201d column to exclude participants with certain phenotype(s) from cases and controls respectively.Note: For example, a composite phenotype \u201cdiabetes mellitus\u201d may include two phenotypes \u201ctype 1 diabetes\u201d and \u201ctype 2 diabetes\u201d. Alternatively, for the phenotype \u201ctype 2 diabetes\u201d we may want to exclude any cases with also a \u201ctype 1 diabetes\u201d diagnosis.\nThe definition table template \u201cdefinitions_DmRxT2.tsv\u201d contains definitions constructed for the definition of type 2 diabetes in the UK Biobank.\nTrait \u201cDmT2\u201d, \u201cDmT1\u201d and \u201cDmG\u201d contain specific codes for diabetes type 2, type 1 and gestational diabetes respectively;\n\u201cRxDm\u201d defines the antidiabetic medication which is further divided into \u201cRxDmIns\u201d (Insulin) and \u201cRxDmOr\u201d (oral antidiabetic drugs);\n\u201cDm\u201d captures general codes for diabetes and the remaining definitions are used to differentiate between type 1 and type 2 diabetes within this group.\nLoad input files in R\nTiming: 15\u00a0min\nInput files required by the package include data files from UK Biobank including the main dataset, the metadata file and optionally data tables from Data Portal; the completed definition table and data setting file.\nSpecify data file paths in R.\n# The directory with data files\npheno_dir <-\"mydata/ukb99999/\"\n# Main dataset\nfukbtab <- paste(pheno_dir,\"ukb99999.tab\",sep=\"\")\n# Metadata file\nfhtml <- paste(pheno_dir,\"ukb99999.html\",sep=\"\")\n# Hospital inpatient data\nfhesin <- paste(pheno_dir,\"hesin.txt\",sep=\"\")\nfhesin_diag <- paste(pheno_dir,\"hesin_diag.txt\",sep=\"\")\nfhesin_oper <- paste(pheno_dir,\"hesin_oper.txt\",sep=\"\")\n# GP data\nfgp_clinical <- paste(pheno_dir,\"gp_clinical.txt\",sep=\"\")\nfgp_scripts <- paste(pheno_dir,\"gp_scripts.txt\",sep=\"\")\n# Death registry\nfdeath_portal <- paste(pheno_dir,\"death.txt\",sep=\"\")\nfdeath_cause_portal <- paste(pheno_dir,\"death_cause.txt\",sep=\"\")\n# Participant withdrawal list\nf_withdrawal<-paste(pheno_dir,\"w12345_20210809.csv\",sep=\"\")\nSpecify files paths for the data setting file, the definition table and code maps which are included in the package (extdata/). Alternatively download the files from code repository of ukbpheno hosted at GitHub.\n# Or download the files from\n# https://github.com/niekverw/ukbpheno/tree/master/inst/extdata/[href=https://github.com/niekverw/ukbpheno/tree/master/inst/extdata/]\nextdata_dir<-paste0(system.file(\"extdata\", package=\"ukbpheno\"),\"/\")\nfdefinitions <- paste0(extdata_dir,\"definitions_DmRxT2.tsv\")\nfdata_setting <- paste0(extdata_dir,\"data.settings.tsv\")\nRead data setting file. The pre-curated data setting file specifies the characteristics of each data source which are taken into account in the data harmonization process.\ndfData.settings <- fread(fdata_setting)Run the \u201cread_definition_table()\u201d function to process the definition table.\nThe function \u201cread_definition_table()\u201d expands parent codes using the code maps and sort out codes relevant for inclusion and exclusion accordingly.\nCode maps include all available codes.\nThe function will also cross-check codes entered in the definition with the code maps and warn users of any non-matching codes e.g.,\nA specific ICD10 code may not exist in the UK Biobank ICD10 code map as this code is not present in the data.\nThere may be typos.\ndfDefinitions_processed_expanded<-read_defnition_table(fdefinitions,fdata_setting,extdata_dir)\nOptional: Alternatively download the code maps from the UK Biobank Showcase or create them manually by extracting all unique codes from your data using \u201cget_all_exisiting_codes()\u201d which generates flat-form (non-hierarchical) code maps. Adjust the data setting file accordingly.\n# First input: file path to GP clinical table\n# Second input: corresponding column names from the .txt file\n# Third input: output file-path\nget_all_exsiting_codes(fgp_clinical,c(\"read_2\",\"read_3\"),c(\"gpclinical.read2.code\",\" gpclinical.read3.code\"))\nHarmonize all data from various sources\nTiming: 15\u201345\u00a0min\nAt the harmonization step, we combine all the available data files from various sources and transform them to the format of clinical events to facilitate downstream analyses (Figure\u00a01[href=https://www.wicell.org#fig1]). For individual level data including the self-report data, cancer registry and optionally death registry, the corresponding fields containing the information on the diagnosis and time of diagnosis are extracted (in the corresponding data types) from the main dataset and converted to the episodes of clinical events. Touchscreen data are processed according to the conditions described in the definition table, if one is provided. The record level data, downloaded from the Data Portal, will be parsed and reorganized by the data source and classification system.At the end of the harmonization, all clinical events will be returned in the same episode format. In addition, the original data from main dataset and a full list of participants are also returned.\nLoad, process and harmonize all data files using harmonized_ukb_data().\nThe \u201callow_missing_fields\u201d flag specifies whether field(s) required on the definition table but missing in the main dataset is allowed and ignored. If this flag is set to \u201cFALSE\u201d, the harmonization step will halt in case of any missing field.\nIf the participant withdrawal list is provided, records of these individuals will be removed.\nNote: The function harmonized_ukb_data() harmonizes all available data (minimally works with only the main dataset and meta-data file). Additionally, the function will check if all fields required on the definition table are present in the main dataset and inform the user if any field is missing.\nlst.harmonized.data<-harmonize_ukb_data(f.ukbtab\u00a0= fukbtab,f.html\u00a0= fhtml,dfDefinitions=dfDefinitions_processed_expanded,f.gp_clinical = fgp_clinical,f.gp_scripts\u00a0= fgp_scripts,f.hesin\u00a0= fhesin,f.hesin_diag\u00a0= fhesin_diag,f.hesin_oper\u00a0=fhesin_oper,f.death_portal\u00a0= fdeath_portal,f.death_cause_portal\u00a0= fdeath_cause_portal,f.withdrawal_list=f_withdrawal,allow_missing_fields\u00a0= TRUE)\nNote: Time required to harmonize the data is dependent on size of the files. Factors that should be taken into considerations include number of fields approved for the particular project, number of participants included as well as if record-level primary care data are present.\nExamine the harmonized data which contains 3 objects: \u201clst.data\u201d, \u201cdfukb\u201d and \u201cvct.identifiers\u201d (Figure\u00a05[href=https://www.wicell.org#fig5]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1733-Fig5.jpg\nFigure\u00a05. Screenshot of the lst.harmonized.data object\nView(lst.harmonized.data)\n\u201clst.data\u201d contains data from all sources in same episode format documenting \u201cidentifier\u201d, \u201ccode\u201d,\u201deventdate\u201d and an \u201cevent\u201d column.\nDiagnosis codes without associated actual event date will have date of visit to assessment center (such as self-report diabetes) in the \u201ceventdate\u201d column and \u201c0\u201d in the \u201cevent\u201d indicating that the date does not reflect a true event (Figure\u00a06[href=https://www.wicell.org#fig6]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1733-Fig6.jpg\nFigure\u00a06. Screenshot of the harmonized records\nView(lst.harmonized.data$lst.data)\u201cdfukb\u201d is a subset of the main dataset and contains only columns necessary for the definition of target phenotypes.\n\u201cvct.identifiers\u201d is a vector of identifiers of all participants in the main dataset.\nGenerate phenotype and explore the data\nTiming: 2 h\nTo define case/control status of the participants, we need the phenotype (diabetes) definition, the harmonized data tables, the data settings and the individuals to be included (either specified by a vector of participant identifiers or a data-frame containing identifier in the first column and reference dates in the second column).\n# 1) definition of the target trait \u201cType 2 diabetes\u201d\ntrait<-\"DmRxT2\"\n# 2) harmonized data table - lst.harmonized.data\n# 3) data setting data-frame - dfData.settings\n# 4) individuals specified via df_reference_date\n# Here the dates of baseline visit (f.53.0.0) are taken as reference\ndf_reference_dt_v0<-\nlst.harmonized.data$dfukb[,c(\"identifier\",\"f.53.0.0\")]\nUse \u201cget_cases_controls()\u201d function to obtain the case/control status. The function returns a list of three data.table objects: \u201cdf.casecontrol\u201d, \u201call_event_dt.Include_in_cases\u201d and \u201call_event_dt.Include_in_cases.summary\u201d (Figure\u00a07[href=https://www.wicell.org#fig7]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1733-Fig7.jpg\nFigure\u00a07. Screenshot of the result obtained from the get_cases_controls() function\nlst.DmRxT2.case_control <- get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==trait), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\nView(lst.DmRxT2.case_control)\n\u201cdf.casecontrol\u201d is a data.table object of 16 columns providing summary of the diagnosis per participant (Table\u00a04[href=https://www.wicell.org#tbl4]). Included case/control is marked with 2/1 respectively while excluded case/control will be marked with -2/-1 in this table.\ntable:files/protocols_protocol_1733_4.csv\n\u201call_event_dt.Include_in_cases\u201d is data.table object including all event episodes supporting the diagnosis for the cases included (Table\u00a05[href=https://www.wicell.org#tbl5]).\ntable:files/protocols_protocol_1733_5.csv\n\u201call_event_dt.Include_in_cases.summary\u201d is a data.table object with the same format with \u201cdf.casecontrol\u201d but includes only cases (both included and excluded case).\nGenerate timeline plot to check the relative contribution by various data sources over time (Figure\u00a08[href=https://www.wicell.org#fig8]). Events with known event date will be included.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1733-Fig8.jpg\nFigure\u00a08. Disease timeline of type 2 diabetes by different data sources\nDmRxT2_timeline<-plot_disease_timeline_by_source(definition=dfDefinitions_processed_expanded%>%filter(TRAIT==trait),lst.harmonized.data$lst.data,dfData.settings, df_reference_dt_v0$identifiers)\nDmRxT2_timelineUse \u201cmake_upsetplot()\u201d to examine the overlaps between the data sources at baseline to gain insight on their relationships (Figure\u00a09[href=https://www.wicell.org#fig9]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1733-Fig9.jpg\nFigure\u00a09. UpSet plot of type 2 diabetes at baseline showing the overlaps between different data sources\nupset_plot<-make_upsetplot(definition=dfDefinitions_processed_expanded%>%filter(TRAIT==trait),lst.harmonized.data.gp$lst.data,dfData.settings,df.reference.dates\u00a0= df_reference_dt_v0)\nupset_plot\nGenerate summary descriptions on the events with \u201cget_stats_for_events\u201d. For example, generation of a frequency plot of codes among all events from secondary care may help verify or refine the definition (Figure\u00a010[href=https://www.wicell.org#fig10]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1733-Fig10.jpg\nFigure\u00a010. Frequency plots of type 2 diabetes diagnosis codes from secondary care\nLeft: y-axis in linear scale; Right: y-axis in logarithmic scale.\n# Extract all hospital admission records\nall_DmRxT2_evnt<-lst.DmRxT2.case_control$all_event_dt.Include_in_cases\nDmRxT2_hesin_rec<-all_DmRxT2_evnt[grepl (\"hesin\",all_DmRxT2_evnt$.id)]\n# Get some descriptive statistics on the records on a code level\nhesin_stats<-get_stats_for_events(DmRxT2_hesin_rec)\nhesin_stats$stats.codes.summary.phesin_stats$stats.codes.summary.p\nExplore secondary care code count by individual (Figure\u00a011[href=https://www.wicell.org#fig11]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1733-Fig11.jpg\nFigure\u00a011. Barplot of type 2 diabetes diagnosis code count from secondary care per individual\n# Get some summary statistics on the records on individual level\nDmRxT2_rec_cnt<-DmRxT2_hesin_rec[,.(count=.N),by=c(\"identifier\")]\nmax(DmRxT2_rec_cnt$count)\nmedian(DmRxT2_rec_cnt$count)\nmean(DmRxT2_rec_cnt$count)\nquantile(DmRxT2_rec_cnt$count)\n# Visualize count with barplot with a zoom-in on count between 0-50\nggplot2::ggplot(DmRxT2_rec_cnt, ggplot2::aes(x=count))\u00a0+\n\u00a0\u00a0ggplot2::geom_bar(fill=\"#0073C2FF\")\u00a0+ ggplot2::xlab(\"Number fo secondary care record per person\")\u00a0+\n\u00a0\u00a0ggplot2::ylab(\"Frequency\")\u00a0+ #theme with white background\n\u00a0\u00a0ggplot2::theme_bw()\u00a0+ ggplot2::theme(text\u00a0= ggplot2::element_text(size=22),panel.grid.minor\u00a0=ggplot2::element_blank(),panel.grid.major\u00a0=ggplot2::element_blank())\u00a0+ ggforce::facet_zoom(xlim\u00a0= c(0, 50))\nGenerate a timeline of the codes contributing to diagnosis for a particular individual (please replace the identifier if copied from the cell below) (Figure\u00a012[href=https://www.wicell.org#fig12]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1733-Fig12.jpg\nFigure\u00a012. Diagnosis timeline of a hypothetical participant\n# Plot individual time line\nplot_individual_timeline(df.data.settings\u00a0= dfData.settings,lst.data=lst.harmonized.data$lst.data,ind_identifier\u00a0= 9999999)\nTo make the definition of the type 2 diabetes more precise, we may screen and exclude individuals with evidence of other types of diabetes as well as the use of metformin not due to diabetes.First identify participants with specific diabetes codes (gestational diabetes, type 1 and type 2 diabetes) as well as general diabetes code.\n# Identify individuals with specific DmT2 codes\nlst.DmT2.case_control<-get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==\"DmT2\"), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\n# Identify individuals with specific DmT1 codes\nlst.DmT1.case_control<-get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==\"DmT1\"), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\n# Identify individuals with DmG\nlst.DmG.case_control <- get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==\"DmG\"), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\n# Identify individuals with general diabetes diagnosis codes excl. medication\nlst.Dm.case_control <- get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==\"Dm\"), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\nIdentify use of different anti-diabetic medications. Find individuals on metformin likely due to\u00a0diseases other than diabetes by cross checking with the list of individuals with diabetes diagnoses.\n# Identify individuals with metformin use\nlst.RxMet.case_control <- get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==\"RxMet\"), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\n#Identify use of insulin/oral diabetic med. excl. metformin\nlst.RxDmNoMet.case_control <- get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==\"RxDmNoMet\"), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\n#Identify individuals that are on metformin but no diabetes codes\n#nor medication other than metformin\nRxMet_DmUnlikely<-setdiff(lst.RxMet.case_control$df.casecontrol[Hx==2]$identifier,union(lst.Dm.case_control$df.casecontrol[Hx==2]$identifier,lst.RxDmNoMet.case_control$df.casecontrol[Hx==2]$identifier))\nCross-examine various diagnoses. For example we want to get individuals with young onset diabetes but did not have records supporting a diagnosis of non-type 2 diabetes. Namely these individuals did not have evidence of type 1 diabetes nor gestational diabetes.\nWe identify these individuals via set operations of the relevant diagnoses.\nInspect the records of these individuals for evidence of type 2 diabetes.\n# Identify individuals with self-report insulin <12\u00a0months post-diagnosis\nlst.RxDmInsFirstYear.case_control<-get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==\"RxDmInsFirstYear\"), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\n# Identify young onset self reported diabetes (European origin)\nlst.SrDmYEw.case_control <- get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==\"SrDmYEw\"), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\n# identify young onset self reported diabetes (Caribbean African origin)\nlst.SrDmYSaCa.case_control <- get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==\"SrDmYSaCa\"), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\n# Individuals of young onset diabetes\nind_young_onset<- union(lst.SrDmYSaCa.case_control$df.casecontrol[Any==2]$identifier,lst.SrDmYEw.case_control$df.casecontrol[Any==2]$identifier)\n# Individuals with evidence of other types of diabetes reported\nind_RxInsFirstYear_DmT1_DmG<- union(union(lst.RxDmInsFirstYear.case_control$df.casecontrol[Any==2]$identifier,lst.DmT1.case_control$df.casecontrol[Hx==2]$identifier),lst.DmG.case_control$df.casecontrol[Hx==2]$identifier)\n# Young onset but no DM type 1/ gestational diabetes specific codes nor self report of insulin within first year of diagnosis\ninds_young_onset_possible_DmT2\u00a0<-setdiff(ind_young_onset,ind_RxInsFirstYear_DmT1_DmG)# Check the records of these individuals\nlst.DmRxT2.case_control$all_event_dt.Include_in_cases[identifier %in% inds_young_onset_probable_DmT2]\nGenerate phenotypes in batch\nTiming: 15\u201330\u00a0min\nThis session demonstrates how to generate multiple phenotypes and make a clinical characteristics table with these phenotypes, stratified by type 2 diabetes status. An example definition table with the selected cardiometabolic diseases, family history of these diseases and diabetes medication usage is provided in the package. We additionally extract demographic information namely age and sex as well as biomarkers BMI, blood glucose, glycated hemoglobin and self-report insulin use within one year of diabetes diagnosis from the main dataset.\nRead and process the definition file.\n# Read the definitions table\nfdefinitions <- paste0(extdata_dir,\"definitions_cardiometabolic_traits.tsv\")\ndfDefinitions_processed_expanded<-read_defnition_table(fdefinitions,fdata_setting, extdata_dir)\nExtract only the required fields from the main dataset using read_ukb_tabdata().\nThe metadata provides information such as data type of these fields.\nExtract age at assessment center visit (Field 21003), sex (Field 31), body mass index (Field 21001), glycated hemoglobin level (Field 30750), glucose level (Field 30740), self-report insulin use within the first year of diabetes diagnosis (Field 2986), UK Biobank assessment center visited (Field 54) and Date of attending assessment center (Field 53).\n# Extract clinical variables from the main dataset using read_ukb_tabdata()\n# We need the metadata (.html) file for read_ukb_tabdata()\ndfhtml <- read_ukb_metadata(fhtml)\n# Rename the identifier column in the metadata\ndfhtml[which(dfhtml$field.tab==\"f.eid\"),]$field.tab<-\"identifier\"\n# Age at assessment center visit, sex, BMI, HbA1c, glucose,insulin within 1 year of diagnosis,UK Biobank assessment center location, date of visit\nbaseline_fields<-c(21003,31,21001,30750,30740,2986,54,53)\n# Extract these variables from main dataset\ndfukb_baseline <- read_ukb_tabdata(fukbtab,dfhtml,fields_to_keep\u00a0= baseline_fields)\ngc()\nGenerate the phenotypes for atrial fibrillation, coronary artery disease, type 2 diabetes, hypertrophic cardiomyopathy, heart failure, hypertension and hyperlipidemia with a loop and merge the phenotype information into one table \u201cdfukb_baseline_pheno\u201d.\n# The target disease traits we will generate in batch\ndiseases<-c(\"Af\",\"Cad\",\"DmT2\",\"Hcm\",\"Hf\",\"HtRx\",\"HyperLipRx\")# Make an output folder to store the result\nout_folder<-paste0(pheno_dir,\"output/\")\nif(!dir.exists(file.path(out_folder))){\ndir.create(file.path(out_folder))\n}\ndf_withdrawal<-fread(f_withdrawal)\n# remove withdrawn participants\ndfukb_baseline_pheno<-dfukb_baseline[! identifier %in% df_withdrawal$V1]\n# Loop through the traits, including family history of related diseases and the diabetes medication use\nfor (disease in c(diseases,\"HxDm\",\"HxHrt\",\"HxHt\",\"RxDmOr\",\"RxDmIns\")){\nprint(disease)\nlst.case_control <- get_cases_controls(definitions=dfDefinitions_processed_expanded %>% filter(TRAIT==disease), lst.harmonized.data$lst.data,dfData.settings, df_reference_date=df_reference_dt_v0)\n\u00a0\u00a0# Add the trait to the column names\ncolnames(lst.case_control$df.casecontrol) <- paste(disease,\"0\",colnames(lst.case_control$df.casecontrol), sep\u00a0= \"_\")\n\u00a0\u00a0# Except for participant identifier\nnames(lst.case_control$df.casecontrol)[names(lst.case_control$df.casecontrol)\u00a0== paste(disease,\"0\",\"identifier\", sep\u00a0= \"_\")]<-\"identifier\"\n# Merge these columns with dfukb_baseline_pheno\ndfukb_baseline_pheno<-merge(dfukb_baseline_pheno,lst.case_control$df.casecontrol,by=\"identifier\",all.x\u00a0= TRUE,all.y\u00a0= FALSE)\n}\nReport clinical characteristics at baseline\nTiming: 10\u00a0min\nIn the following example analyses, we investigate the characteristics of participants with type 2 diabetes specific codes. We exclude the cases with type 1 diabetes diagnosis codes and we exclude any controls with non-specific diabetes codes (Table\u00a06[href=https://www.wicell.org#tbl6]).\ntable:files/protocols_protocol_1733_6.csv\nSelect variables to be reported in the clinical characteristics table. Rename the variables in the table to improve readability. Create the clinical characteristics table stratified by type 2 diabetes. Write the clinical characteristic table to a file.\n# Keep only the variables needed for the table\ndfukb_baseline_pheno_fortable1<-dfukb_baseline_pheno[,c('identifier',\"DmT2_0_Hx\",\"f.21003.0.0\",\"f.21001.0.0\",\"f.30740.0.0\",\"f.30750.0.0\",\"DmT2_0_first_diagnosis_days\",\"f.31.0.0\",\"HxDm_0_Any\",\"HxHrt_0_Any\",\"HxHt_0_Any\",\"HtRx_0_Hx\",\"HyperLipRx_0_Hx\",\"Af_0_Hx\",\"Hcm_0_Hx\",\"Hf_0_Hx\",\"RxDmOr_0_Hx\",\"RxDmIns_0_Hx\",\"f.2986.0.0\"),with=FALSE]\n# Negative first diagnosis day indicates history while positive indicates follow-up cases\ndfukb_baseline_pheno_fortable1$DmT2_0_first_diagnosis_years<-(-1\u2217dfukb_baseline_pheno_fortable1$DmT2_0_first_diagnosis_days)/365.25\n# Rename for readability\ncolnames(dfukb_baseline_pheno_fortable1)<-c(\"identifier\",\"Type 2 diabetes\",\"Age\",\"BMI\",\"Glucose\",\"HbA1c\",\"Days since type 2 diabetes diagnosis\",\"Sex\",\"Family history of diabetes\",\"Family history of heart disease\",\"Family history of hypertension\",\"Hypertension\",\"Hyperlipidemia\",\"Atrial fibrillation\",\"Hypertrophic cardiomyopathy\",\"Heart failure\",\"Oral diabetes medication\",\"Insulin\",\"Insulin within 1 year of diagnosis\",\"Years since type 2 diabetes diagnosis\")\n# Below the parameters for CreateTableOne\n# The full variable list\nvars<-c(\"Age\",\"BMI\",\"Glucose\",\"HbA1c\",\"Years since type 2 diabetes diagnosis\",\"Sex\",\"Family history of diabetes\",\"Family history of heart disease\",\"Family history of hypertension\",\"Hypertension\",\"Hyperlipidemia\",\"Atrial fibrillation\",\"Hypertrophic cardiomyopathy\",\"Heart failure\",\"Oral diabetes medication\",\"Insulin\",\"Insulin within 1 year of diagnosis\")\n# The categorical variables on the clinical characteristics table\nfactorVars<-setdiff(vars,c(\"Age\",\"BMI\",\"Glucose\",\"HbA1c\",\"Years since type 2 diabetes diagnosis\"))\n# Create the clinical characteristic table stratified by type 2 diabetestableOne <- CreateTableOne(vars\u00a0= vars, strata\u00a0= \"Type 2 diabetes\", data\u00a0= dfukb_baseline_pheno_fortable1, factorVars\u00a0= factorVars)\nhist(dfukb_baseline_pheno_fortable1$`Years since type 2 diabetes diagnosis`)\ntableOne\ntab1Mat <- print(tableOne, quote\u00a0= FALSE, noSpaces\u00a0= TRUE, printToggle\u00a0= FALSE,nonnormal\u00a0=c(\"Glucose\",\"HbA1c\",\"Years since type 2 diabetes diagnosis\"))\n# Save the table to a CSV file\nwrite.csv(tab1Mat, file\u00a0=paste0(out_folder,\"BaselineTable.csv\"))\nSurvival analysis on heart failure stratified by type 2 diabetes\nTiming: 5\u00a0min\nWith time-to-event data as well as the censoring dates for different data sources for different regions, compute the observed time for each participant.\nThe start time is the date when the participant visited the assessment center;\nObserved time is up to date of event or earliest among date of death and censoring date of hospital inpatient records (last follow up).\n# Get death dates from data\ndeathdt<-unique(lst.harmonized.data$lst.data$tte.death.icd10.primary[,.(identifier,eventdate)])\n# Rename the column and merge\ncolnames(deathdt)<-c(\"identifier\",\"deathdt\")\ndfukb_baseline_pheno<-merge(dfukb_baseline_pheno,deathdt,by=\"identifier\",all.x=TRUE,all.y\u00a0= FALSE)\n# HESIN censoring date are different by regions\n# Use the UK Biobank assessment center location attended by the participants\nengland<-c(\"10003\",\"11001\",\"11002\",\"11007\",\"11008\",\"11009\",\"11010\",\"11011\",\"11012\",\"11013\",\"11014\",\"11016\",\"11017\",\"11018\",\"11019\",\"11020\",\"11021\")\nscotland<-c(\"11004\",\"11005\")\nwales<-c(\"11003\",\"11022\", \"11006\",\"11023\")\n# Corresponding censoring dates\ndfukb_baseline_pheno[dfukb_baseline_pheno$f.54.0.0 %in% england,\"censordateHES\"]<-as.Date(\"2021-03-31\")\ndfukb_baseline_pheno[dfukb_baseline_pheno$f.54.0.0 %in% scotland,\"censordateHES\"]<-as.Date(\"2021-03-31\")\ndfukb_baseline_pheno[dfukb_baseline_pheno$f.54.0.0 %in% wales,\"censordateHES\"]<-as.Date(\"2018-02-28\")\n# Time-to-event/observed time is determined at earliest of date of event, date of death and censoring date of HESIN data (last follow up)\n# This is already calculated for those who have events\nrange(dfukb_baseline_pheno[Hf_0_Fu==2,Hf_0_Fu_days])\n# non-event but died before HESIN censoring date\ndfukb_baseline_pheno[Hf_0_Fu==1 & !is.na(deathdt) & deathdt-censordateHES<=0,Hf_0_Fu_days:=deathdt-as.Date(f.53.0.0)]\n# People censored at last fu\n# non-event but died after censoring date (HESIN),\ndfukb_baseline_pheno[Hf_0_Fu==1 &!is.na(deathdt)& deathdt-censordateHES>0,Hf_0_Fu_days:=censordateHES-as.Date(f.53.0.0)]\n# non-event and alive by censoring date\ndfukb_baseline_pheno[Hf_0_Fu==1 &is.na(deathdt),Hf_0_Fu_days:=censordateHES-as.Date(f.53.0.0)]\nCreate the survival object and Kaplan-Meier plot for new onset heart failure stratified by type 2 diabetes status at baseline.\n#Estimate risk of new onset heart failure by presence/absence of type 2 diabetes at baseline\nfit<-survival::survfit(survival::Surv(Hf_0_Fu_days/365.25,Hf_0_Fu) \u223c DmT2_0_Hx, data\u00a0= dfukb_baseline_pheno[DmT2_0_Hx>0])\n# summary(fit)\n# Make Kaplan-Meier plotggsurvplot(fit, data\u00a0= dfukb_baseline_pheno[DmT2_0_Hx>0], size\u00a0= 0.8,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0break.time.by=2,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0xlab\u00a0= \"Follow up (years)\",\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0censor.size=2,\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0palette\u00a0= c(\"#072A6C\", \"#FF8400\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0conf.int\u00a0= TRUE, # Add confidence interval\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0pval\u00a0= TRUE, # Add p-value\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0risk.table\u00a0= TRUE, # Add risk table\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0risk.table.col\u00a0= \"strata\", # Risk table color by groups\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0legend.labs\u00a0= c(\"No type 2 diabetes at baseline\",\"Type 2 diabetes at baseline\"),\n\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0risk.table.height\u00a0= 0.2)\nCase-control matching with MatchIt\nTiming: 5\u00a0min\nSometimes it may be desirable to match cases and controls by characteristics such as age and sex\u00a0in certain studies. Here we demonstrate how to further process phenotypes created in ukbpheno to generate matched case-control pairs. We utilize an R package MatchIt for the matching task.\nTo match type 2 diabetes case to control by age, sex and body mass index. We extract those variables and remove individuals with missing values in either the target phenotype or any covariates.\n########################################\n# 1:2 case control matching with MatchIt\n########################################\n#library(\u201cMatchIt\u201d)\n# Remove individuals with either missing or excluded phenotype for target phenotype (type 2 diabetes at baseline)\ndf_to_matchit<-dfukb_baseline_pheno[!is.na(DmT2_0_Hx) & DmT2_0_Hx>0]\n# Pick three covariates age at assessment center visit, sex and BMI for matching\ndf_to_matchit<-na.omit(df_to_matchit[,.(identifier,DmT2_0_Hx,f.21003.0.0,f.31.0.0,f.21001.0.0)])\nFormat the coding of the phenotype and name the rows by participant identifier in preparation for the matchit() function. Run the matchit() function to match 2 controls to each case. Examine the result.\n# Format the data for the matchit function\n# Control/case: 1/2 to 0/1\ndf_to_matchit$DmT2_0_Hx<-df_to_matchit$DmT2_0_Hx-1\n# Name the rows\nrownames(df_to_matchit)<-df_to_matchit$identifier\ncolnames(df_to_matchit)<-c(\"identifier\",\"Type 2 diabetes\",\"Age\",\"Sex\",\"BMI\")\n# Run matchit\nm.dm2<-matchit(`Type 2 diabetes`\u223cAge\u00a0+ Sex+BMI,data=df_to_matchit,ratio=2)\n#Check result\nsummary(m.dm2)\n# Each row in the match.matrix shows identifier of one case with 2 matched controls\nm.dm2$match.matrix"], "result": {"Volume": ["100mL", "0.5\u00a0mL", "0.5\u00a0mL", "600\u00a0\u03bcL", "8\u00a0mL", "500\u00a0\u03bcL", "5\u00a0\u03bcL", "1500\u00a0\u03bcL", "650\u00a0\u03bcL", "100\u00a0\u03bcL", "20\u00a0\u03bcL", "27\u00a0\u03bcL", "3\u00a0\u03bcL", "30\u00a0\u03bcL", "45.5\u00a0\u03bcL", "2.5\u00a0\u03bcL", "100\u00a0\u03bcL", "block solution", "1.5-mL microfuge tube", "Measure the volume of each SECM sample", "16.6\u00a0\u03bcL", "5\u00a0\u03bcL", "20\u00a0\u03bcL", "130\u00a0\u03bcL", "10\u00a0\u03bcL", "600\u00a0\u03bcL", "150\u00a0\u03bcL", "400\u00a0\u03bcL", "200\u00a0\u03bcL", "21.5\u00a0\u03bcL", "1\u00a0\u03bcL", "25\u00a0\u03bcL", "26\u00a0\u03bcL", "200\u00a0mL", "21.5\u00a0\u03bcL", "installation of PAN2HGENE", "installation of other programs", "Ficoll-Paque PREMIUM", "RPMI 1640 complete medium", "Master Mix", "10\u201320 column volumes", "50-\u03bcL", "2\u00a0mL", "3 CVs", "5 CVs"], "Software": ["Reactome Web site", "Web browser", "Anaconda-Navigator", "Jupyter Notebook", "Th"], "Computer": ["Hardware"], "Web browser": ["Firefox", "Safari", "Chrome"], "Web page": ["Reactome home page", "Reactome Pathway Browser"], "Figure": ["Fig. 1"], "Image": ["Reactome logo"], "Web page element": ["Navigation bar"], "Description": ["About"], "Links": ["Content", "Tools"], "Access": ["Docs", "Download", "API and data access"], "Information": ["Community", "Documentation", "Twitter feed"], "Feature": ["Pathway Browser", "Analysis Tools", "ReactomeFIViz", "Reactome Research Spotlight"], "Scientific Quantity": ["ENCODE datasets", "Hardware", "Computer", "Software", "Experiment matrix page", "Experiment matrix", "Experiments", "K562", "Status", "Released", "Revoked", "Archived", "Assay title", "TF ChIP-seq", "Biosample term name", "Proband_ID", "Father_ID", "Mother_ID", "AC", "GT", "AD", "DP", "GQ", "PL", "REF", "ALT", "QUAL", "FILTER", "INFO", "FORMAT", "CHROM", "POS", "ID", "Timing", "genome index", "genome index", "reference genome", "gene expression", "neuroimaging", "mass-univariate Spearman correlation analysis", "multi-variate PLS regression analysis", "Desikan-Killiany (DK) parcellation", "signal", "voxels", "genetic probes", "genes", "tissue samples", "brain regions", "MNI coordinates", "matrix", "gold-standard methods", "null spatial maps", "spin rotations", "ensemble gene set enrichment analyses", "positron emission tomography (PET)", "serotonin receptor 5-HT2A", "NIfTI", "text files", "voxel size", "EEG", "IUPred2A", "RPB1 (RNA Pol II largest subunit)", "CTCF", "CTD of RPB1 (aa 1593-1970)", "CTD of CTCF (aa 573-727)", "DBD (aa 266-577)", "Gibson ligation kit", "SDS-PAGE", "mass.c (g/L)", "NanoDrop", "BCA method", "mol. wt.", "mol.c", "staining", "tag digestion", "buffer exchange", "hyperfiltration tube", "Pol II CTD-mEGFP", "Dilution buffer", "confocal microscope", "Nikon A1RSi+", "Table 1", "Table 2", "20", "the i-th spike", "N\u00d7P matrix STE", "20", "STE", "k", "N\u00d7K weight matrix W", "K\u00d7P module matrix M", "20", "subunit", "40", "mean generator signal", "mean spike rate", "subTemporalKernel.mat", "Datapre.mat", "Data.mat", "gainNL.mat", "temporalfilter.exe", "GainandNL.exe", "W j", "ON-OFF attribute", "module_weight.mat", "STE.mat", "unit", "Ngood", "module_weight.exe", "spklist", "W_matrix", "spklist_sub", "subunit_sp", "subunit_sp.mat", "database", "stoichiometry", "volume", "PhosphoExperiment", "Sequence window", "Residue", "Site", "Sequence", "colData", "Uniprot ID", "Localization", "Position of the phosphosite", "Residue of the phosphosite", "Gene symbol", "scImpute", "tImpute", "Log transformation", "log2 transformation", "germline genotyping data", "SNPs", "X chromosome homozygosity estimate (XHE)", "F coefficient", "Heterozygosity rate", "proportion of missing genotypes", "log10 p-value", "log10 MAF", "call confidence values", "probeset ID", "genotype call", "call confidence", "allele calls", "nuclei suspension", "Timing", "volume", "embryos", "dissecting microscope", "B5 agar media", "lysis buffer", "seedlings", "forceps", "slurry", "filter", "collection tubes", "FACS", "DAPI", "photomultiplier tube (PMT) voltages", "gates", "BD collect tube", "centrifuge", "wash buffer", "PBS buffer", "ATAC-seq library", "TruePrep DNA Library Prep Kit V2 for Illumina", "TTBL and TTE Mix V50", "MinElute Reaction Cleanup Kit", "PCR tube", "thermal cycler", "spin column"], "Cell count": ["10\u02c67 cells"], "Time": ["5\u00a0min", "2\u00a0min", "10 h", "2\u00a0h", "3\u00a0min", "60\u00a0min", "3\u00a0min", "20\u00a0min", "2\u00a0s", "30\u00a0s", "1\u00a0day", "1\u00a0h 30\u00a0min", "16 h", "4\u201355 h", "4\u20135 h", "5 s", "2\u00a0min", "25\u00a0min to 120\u00a0min", "1\u00a0min", "30 s", "22\u00a0min", "2\u20133 h", "120\u00a0min", "85\u00a0min", "45\u00a0min", "4 h", "time window", "1 h", "Timing", "day 11 of development", "5 h", "30 s", "20 s", "30\u00a0min", "1\u00a0h", "5\u20137 h", "8\u00a0min", "3.5\u00a0h", "4 h", "3 h", "30 s", "Timing", "60 h", "4 hours ago", "Trim Galore", "Bismark", "Timing", "2 h", "2 h", "Day 0", "D4-D7", "3\u20134 h", "2 h", "Timing", "2 h", "16 h", "42 h", "24 h", "24 h", "0\u20134\u00a0h", "18\u201324\u00a0h", "2\u20134\u00a0days", "0.5\u20131.5\u00a0h", "1\u00a0h", "10\u00a0min", "40 s", "16 h", "4\u00b0C overnight (\u223c16 h)", "2 h"], "Concentration": ["150\u00a0mM NaCl", "0.05% NP-40", "4\u00a0M", "5%", "12\u00a0ng/\u03bcL", "0.05%", "collagenase type 1 stock solution", "collagenase type 1", "10% fetal calf serum (FCS)", "1% penicillin/streptomycin (PS)", "SYBR\u00aeSafe DNA gel stain", "70% ethanol", "200\u00a0\u03bcM", "20% dextran", "10\u00a0\u03bcM", "1,000 cells/\u03bcL", "5% CO2", "Emla\u00ae cream 5% (Lidocaine/Prilocaine)", "0.1% Triton X-100", "2% PFA", "Tris buffer", "1% BSA in PBS", "0.5% SDS", "80% alcohol", "5% CO2"], "Centrifugal force": ["12,000 g", "500 \u00d7 g", "15,000 \u00d7 g"], "Percentage": ["10%\u201320%", "95%", "5% CO2", "80%", "20%", "5%"], "Phred quality score": ["<15"], "Length": ["<25\u00a0bp", "20 Kb", "98 nucleotides", "50 nucleotides", "91 nucleotides", "4 cm", "\u223c200 nt", "124-bp", "20 nucleotides", "nt", "9 nucleotides", "8 nucleotides", "15 nucleotides", "TFs", "100 nt"], "Sample name": ["dRPB3_IAA_0h_rep1"], "File path": ["path/to/output_directory"], "Adapter sequence": ["CCCCCCCCCAGATCGGAAGAGCACACGTCTGAACTCCAGTCAC", "AGATCGGAAGAGCGTCGTGTAGGGAAAGAGTGT"], "Accession number": ["BK000964.3"], "Number of samples": ["24 samples", "96 samples", "<100"], "Temperature": ["20\u00b0C\u201322\u00b0C", "70\u00b0C", "37\u00b0C", "4\u00b0C", "65\u00b0C", "65\u00b0C", "20\u00b0C", "21\u00b0C\u201322\u00b0C", "4\u00b0C", "37\u00b0C", "4\u00b0C", "\u221280\u00b0C", "30\u00b0C", "37\u00b0C", "4\u00b0C", "55\u00b0C", "95\u00b0C", "\u223c36\u00b0C", "50\u00b0C", "75\u00b0C", "\u221280\u00b0C", "37\u00b0C", "98\u00b0C", "64\u00b0C", "\u221220\u00b0C", "55\u00b0C", "65\u00b0C", "25\u00b0C", "\u221280\u00b0C", "25\u00b0C", "25\u00b0C", "25\u00b0C", "25\u00b0C", "25\u00b0C", "37\u00b0C", "20\u00b0C\u201325\u00b0C", "4\u00b0C", "\u221280\u00b0C", "6", "\u221280\u00b0C", "60\u00b0C", "20\u00b0C", "Core body temperature", "36\u00b0C and 38\u00b0C", "29\u00b0C\u201332\u00b0C", "6\u00b0C\u20139\u00b0C", "24\u00b0C\u201328\u00b0C", "37\u00b0C", "37\u00b0C", "4\u00b0C", "\u221280\u00b0C", "25\u00b0C", "\u221280\u00b0C", "\u221280\u00b0C", "-80\u00b0C", "24\u00b0C", "4\u00b0C", "22\u00b0C", "4\u00b0C", "65\u00b0C", "37\u00b0C", "\u221280\u00b0C", "4\u00b0C"], "Mass": ["2\u00a0g", "15\u00a0mg", "1 g"], "Speed": ["700\u00a0rpm", "low speed"], "Acceleration": ["11,000\u00a0\u00d7 g"], "Number of cycles": ["15 cycles"], "Cells": ["PC-3 cells", "Drosophila S2 cell chromatin"], "Instrument": ["Qubit dsDNA quantification system", "NEB Next Ultra II DNA Library Preparation Kit", "Samtools", "stereoscope", "water bath", "shaker", "thermocycler", "UV transilluminator"], "Quantity": ["15", "30 million", "11 entity types", "45 relation types", "107", "7AAD", "2000", "42\u201345 somite pairs", "somite pairs", "DNA molecular weight marker", "7\u00a0\u00d7\u00a0103 cells", "calibration spike-in mix", "poly A", "qPCR", "3 replicates", "injection needle", "92871 sgRNAs", "46", "7 passages"], "Version": ["Illumina\u2019s RTA version 2.7.7", "3.0.0"], "Format": ["FASTQ format"], "Sequence": ["human/hg38", "Drosophila/dm6"], "File": ["hg38.fa.gz", "dm6.fa.gz", "Short sequencing reads", "ecoli_shasta.fa", "annotation file", ".bam file", "annotation file", "reads.tagged.1.fastq.gz", "reads", "SIM_template.xlsx"], "Data": ["CSV files", "TSV file", "multivariate data", "neural data", "Raw data", "scRNA-seq data", "Data S1"], "Model": ["TransE", "TransR", "ComplEx", "DistMult", "decoding model", "generative model"], "Variable": ["DGLBACKEND", "stimulus+choice", "independent variables", "dummy variables", "continuous variables", "eye tracks", "dummy independent variables", "task phases"], "Measurement": ["HITS@k", "Mean Rank", "MRR", "Glycemia", "OD600", "pH"], "Magnification": ["20\u00d7", "100\u00d7"], "Numeric value": ["7"], "Frequency": ["3 frames per second"], "Operating System": ["64-bit Linux", "Mac"], "Resource": ["RAM"], "Software Package": ["sra-tools v2.9.1+", "ntHits v0.0.1+", "ntEdit v1.3.5+", "ABySS v2.3.2+", "ntEdit+Sealer protocol v1.0.0+", "QUAST v5.0.0+"], "Tool": ["fasterq-dump", "quast"], "File Format": ["FASTQ format", "FASTA file"], "Data Set": ["E. coli strain NDM5 draft genome assembly", "ONT MinION reads", "Illumina MiSeq reads", "E. coli strain K-12 substr. MG1655 reference genome assembly", "training set", "test set", "training set", "test set"], "Accession Number": ["SRR15859208"], "Bloom Filter Size": ["200 MB"], "hours to days": ["Timing"], "Single-cell technology": ["10xv2", "10xv3", "Drop-seq"], "Number of reads": ["386 million", "1 million"], "Company": ["10x Genomics"], "Genome version": ["GRCh38-2020-A"], "Some quantities mentioned in the text may not be explicitly given in a numerical value or unit, but are still relevant to physical chemistry experiments.": ["Note"], "Adjusted p value (padj)": ["0.1"], "Number of cells": ["74,973", "5,506", "5,429"], "Classification Method": ["linear Support Vector Machine (SVM)"], "Distributions": ["non-Gaussian distributions"], "Number of variables": ["tens to hundreds of units"], "Classification Model": ["linear model", "logistic regression"], "Cross-validation Method": ["Monte-Carlo CV"], "Count": ["spike count", "Original read count"], "Neuron": ["neuron"], "Mean": ["mean"], "Standard Deviation": ["standard deviation"], "Z-scored Counts": ["z-scored spike counts"], "Number of neurons": ["N"], "Classification": ["binary classification"], "Control": ["control"], "Time Window": ["target and/or the delay period"], "Activity": ["neural activity"], "Measure": ["performance measure"], "Quality": ["Quality control", "QC analysis"], "Report": ["QC report"], "Number": ["Estimated Number of Cells", "5 strokes", "15 strokes", "30 passages (P30)"], "Reads": ["Mean Reads per cell"], "Genes": ["Median genes per cell"], "Fraction": ["Fraction reads in cells"], "Barcodes": ["Valid barcodes", "inDrop V1 and V2 barcodes"], "UMIs": ["Valid UMIs"], "Bases": ["Q30 Bases in RNA Read"], "Normalization": ["Normalization"], "Analysis": ["Clustering analysis"], "Dimensional reduction": ["PCA"], "Dimensionality": ["JackStraw"], "Neighbors": ["FindNeighbors"], "Clusters": ["FindClusters"], "Visualization": ["UMAP"], "Size": ["1024", "\u03bcm filter", "1.56GB", "40-um cell strainer", "15-cm", "15-mL", "15-cm"], "Ratio": ["0.5", "1", "0.2", "0.39", "trypan blue ratio", "1,000-fold coverage"], "Threshold": ["0.5", "0.5"], "Buffer solution": ["PBS", "PBS/FCS/PS", "PBS/FCS/PS", "TAE Buffer"], "Process": ["dissect", "PCR", "single-cell alignment process"], "Sample": ["embryos", "tissue", "AGMs", "cells", "DNA", "mesenchymal stem/stromal cells (MSCs)", "PDGFR\u03b2-KO AGM", "single-cell suspension"], "Procedure": ["program"], "Container": ["PCR reaction tubes"], "Document": ["Chromium Next GEM Single Cell 3\u2032 Reagent Kits User Guide"], "Company name": ["10x Genomics"], "Chip": ["Chromium Next Gem Chip G"], "Quantity of reads": ["120 million reads"], "Number of cores": ["12 cores"], "Memory": ["48 GB"], "Technique": ["scRNA-seq"], "Library": ["dropEst"], "Aligner": ["STAR"], "Matrix": ["barcode by gene count matrix", "droplet matrix", "regressors matrix X"], "Package": ["scRNABatchQC"], "Directory": ["genomeDir"], "Maximum number": ["outSAMmultNmax"], "Number of threads": ["runThreadN"], "Separating characters": ["readNameSeparator"], "Output configuration": ["outSAMunmapped"], "Output type": ["outSAMtype"], "Output file name prefix": ["outFileNamePrefix"], "Command": ["readFilesCommand"], "Metadata": ["Table 1", "Table 1"], "Percentage concentration": ["4% isoflurane"], "Needle size": ["30 gauge"], "Centrifugation force": ["9,000\u00a0g", "300\u00a0g"], "Reagent": ["CT Conversion Reagent", "Chromium Next GEM Single Cell V(D)J Reagent Kits v1.1"], "Kit": ["MagPrep kit"], "Temperature range": ["20\u00b0C\u201325\u00b0C", "18\u00b0C\u201327\u00b0C"], "Material": ["XP beads"], "None": ["sudo apt-get install make", "sudo apt-get install build-essential", "sudo apt-get install curl", "sudo cpan install YAML.pm", "sudo apt-get install screen", "sudo apt-get install bowtie2", "sudo apt-get install blast2", "sudo apt-get install samtools", "sudo apt-get install python3-distutils", "sudo apt-get install python", "sudo cpan install DBI", "sudo apt-get install mafft", "sudo apt-get install mcl", "sudo apt-get install phylip", "SPAdes Installation", "PGAP Installation", "tar -xzf PGAP-1.2.1.tar.gz", "mv PGAP-1.2.1/ PGAP/", "sudo mv PGAP/ /opt/", "cd /opt/", "sudo chmod 777 -R PGAP/", "sudo gedit /opt/PGAP/PGAP.pl", "system(\"perl ./multiparanoid.pl -species \".join(\".pep+\",@species).\".pep -unique 1\")", "system(\"perl ./Blast_Filter.pl All.blastp All.pep $coverage $identity $score | $mcl - --abc -I 2.0 -o All.cluster\")", "system(\"perl /opt/PGAP/multiparanoid.pl -species \".join(\".pep+\",@species).\".pep -unique 1\")", "system(\"perl /opt/PGAP/Blast_Filter.pl All.blastp All.pep $coverage $identity $score | $mcl - --abc -I 2.0 -o All.cluster\")", "system(\"perl /opt/PGAP/inparanoid.pl $blastall $thread $formatdb $score $global $local $species[$i].pep $species[$j].pep\")", "/opt/PGAP/PGAP.pl", "Prokka Installation", "sudo apt-get install libdatetime-perl libxml-simple-perl libdigest-md5-perl git default-jre bioperl", "sudo chmod 777 -R /opt/prokka", "/opt/prokka/bin/prokka --setupdb", "sudo chmod 777 -R /opt/prokka", "/opt/prokka/bin/prokka", "R Installation", "sudo apt-get update", "sudo apt-get install r-base r-base-dev", "install.packages(\"plotrix\")", "install.packages(\"minpack.lm\")", "install.packages('ctv')", "library('ctv')", "install.views('Phylogenetics')", "update.views('Phylogenetics')", "Tbl2asn Installation", "gunzip linux64.tbl2asn.gz", "mv linux64.tbl2asn /usr/local/bin/tbl2asn", "tbl2asn --help", "MySQL Installation", "sudo apt-get install mysql-server", "sudo mysql_secure_installation", "ATAC-seq", "MACS2 callpeak", "RNA-seq", "Differential expression", "Bioconductor package DESeq2", "CSV format", "pCHiC raw data", "HiCUP pipeline", "BAM files", "Bioconductor package CHiCAGO", "CHINPUT files", "CHiCAGO scores"], "Individuals": ["HIV patients"], "Antiretroviral treatment": ["ART"], "Blood volume": ["PBMCs"], "Tube": ["EDTA-anticoagulant tube"], "Buffer volume": ["HBSS buffer"], "Cell concentration": ["1000 cells/\u03bcL"], "Platform": ["NovaSeq 6000 platform"], "Quantity of Runs/Iterations": ["16 MP and 16 bootstrap runs"], "Options": ["numerical options"], "Evolutionary Scenario": ["most parsimonious evolutionary scenario"], "Derived Characters": ["apomorphies"], "Figures": ["Figures S9 and S10"], "Tables": ["Tables S2 and S3"], "Simulation": ["bootstrap simulation"], "Statistical Method": ["BMCMC"], "Reconstruction of Phylogeny": ["phylogeny reconstruction"], "Runs/Iterations": ["MP runs"], "Character Weights": ["equal weight", "equal character weights"], "Character We-Weights": ["Rescaled Consistency Index RC reweight"], "Character Types": ["Fitch, Wagner, Camin-Sokal or Dollo", "Dollo and Irrev types"], "Taxa/Species": ["multistate taxa"], "Search Algorithm": ["Exhaustive Search", "Branch-and-Bound Search", "Heuristic search"], "Search Methods": ["simple stepwise addition and TBR branch swapping"], "Setting for Fitch Parsimony": ["unordered Fitch setting"], "Setting for Wagner Parsimony": ["Wagner ordered setting"], "Criterion for Camin-Sokal Parsimony": ["Camin-Sokal criterion"], "Author's Name": ["Simpson"], "File size": ["CSV file"], "scientific quantity": ["Timing"], "Library initialization": ["Step-1"], "< 5 min": ["Timing"], "CNV analysis": ["Step-2"], "Gene expression analysis": ["Step-3"], "Survival analysis": ["Step-4"], "Correlation analysis on protein RNA levels": ["Step-7", "Step-8"], "Mouse genotype": ["SSTCre/+", "SST-Cre", "C57BL/6J"], "Age range of mouse pups": ["P1-P2"], "Scientific concept, not a specific quantity": ["Analgesic treatment", "Neonatal pial-surface electroporation", "Chlorhexidine soap solution"], "Statistical model": ["Generalized linear models (GLMs)"], "Statistical distribution": ["Poisson distribution"], "Spike count": ["spike counts Y", "spike count Y vector"], "Time interval": ["bin width"], "MATLAB function": ["histcounts MATLAB function"], "Spike history": ["spike history"], "Modification": ["m6A modification"], "RNA": ["mRNA", "ribo- RNA", "longer (>200 nt) RNA", "ligated RNA"], "Buffer": ["MjDim1 reaction buffer", "RT buffer", "T4 RNA ligase buffer"], "Inhibitor": ["SUPERase In RNase Inhibitor", "RNaseOUT"], "Compound": ["Allylic SAM", "I2", "Na2S2SO3", "dNTP", "MgCl2", "DTT", "ATP", "PEG8000", "EtOH"], "Enzyme": ["MjDim1 enzyme", "HIV-RT enzyme", "RNase H", "T4 RNA ligase 1"], "DNA": ["cDNA"], "Adapter": ["cDNA 3\u2032adapter", "NEBNext adaptors"], "Mix": ["NEBNext\u00ae Ultra\u2122 II Q5\u00ae Master Mix"], "Beads": ["Ampure beads"], "Solution": ["RNase-free H2O"], "-": ["Extracting adjacent or overlapping bisulfite reads", "SAM flags", "DNA molecule length", "ROI coordinates", "TF binding sites", "States"], "Duration": ["Timing"], "This list includes only the physical chemistry scientific quantities mentioned explicitly in the experimental procedure. There may be other scientific quantities involved in the procedure that are not mentioned explicitly.": ["Note"], "Apparatus": ["Dounce homogenizer", "Trypan Blue"], "Centrifuge speed": ["100 g", "1000 g"], "Substance": ["MboI", "DpnII", "NlaIII", "DNA ligase", "Hi-C Lysis Buffer"], "Accuracy": ["5%"], "Mode": ["sequential mode"], "Minor Allele Frequencies (MAF)": ["Calculate Minor Allele Frequencies"], "10\u201330 min": ["Timing"], "Principal Component Analysis (PCA)": ["Predict"], "Canonical Correspondence Analysis (CCA)": ["Merge", "Null Model CCA"], "1 h": ["Timing"], "2 h": ["Timing"], "args": ["command line argument"], "inFile": ["input file"], "outFile": ["output file"], "n": ["number of rows"], "input": ["path of the input file"], "output": ["path of the output file"], "nrow": ["the number of row"], "Hardware/Software": ["64-bit"], "RAM": ["16 GB"], "Input Files": ["FASTQ format"], "Relative abundance": ["16%"], "Energy": ["100 J/cm2"], "Plate size": ["384-well"], "Chemical reagent": ["Trizol reagent"], "Laboratory equipment": ["Phasemaker Tubes"], "Chemical solution": ["monophasic solution of Trizol"], "Chemical substance": ["gel polymer", "glycoblue", "isopropanol"], "Humidity": ["50% humidity"], "Resolution": ["1280 by 1024 pixels"], "Frame rate": ["66.6 frames/s"], "Minimum number of features": ["min.features"], "Minimum number of cells": ["min.cells"], "Counts": ["counts"], "Fragments": ["fragments"], "Assay": ["assay"], "Meta data": ["meta.data"], "Quantity of CPU cores": ["CPU core", "32 CPU cores"], "Amount of random-access memory (RAM)": ["one gigabyte (GB)"], "Amount of free storage": ["100 GB"], "Base clock speed": ["2 GHz"], "Amount of RAM": ["50 GB"], "Time, in hours": ["6 hr"], "The given experimental procedure contains several physical chemistry scientific quantities, such as temperature (in degrees Celsius), volume (in mL, \u03bcL, L), concentration (in ng/\u03bcL, mM), time (in s, min, h), and measurement (in OD600, CVs, A280, A260, g).": ["Note"], "Volume, Light intensity, Allelic composition": ["Intensity files"], "Relative measure of light intensity": ["Log R Ratio"], "Allelic composition": ["B Allele Freq"], "Name, Chromosome, Position": ["snppos.txt"], "sample_ID, file_path": ["samples_list.txt"], "GC content": ["GC content file"]}}