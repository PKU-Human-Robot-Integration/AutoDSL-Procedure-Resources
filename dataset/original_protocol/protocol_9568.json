{
  "id": 9979,
  "origin_website": "Jove",
  "title": "Superior Auto-Identification of Trypanosome Parasites by Using a Hybrid Deep-Learning Model",
  "procedures": [
    "Archived blood films and project design were approved by the Institutional Biosafety Committee, the Institutional Animal Care and Use Committee of the Faculty of Veterinary Science, Chulalongkorn University (IBC No. 2031033 and IACUC No. 1931027), and Human Research Ethics Committee of King Mongkut's Institute of Technology Ladkrabang (EC-KMITL_66_014).\n1. Preparation of raw images\nThe image dataset preparation\n\t\nObtain at least 13 positive slides with blood-parasite infections, including T. brucei, T. cruzi, and T. evansi, confirmed by parasitologist experts. Separate the 13 slides for training (10 slides) and testing (three slides).\nAcquire images of the Giemsa stained-thin blood films described above under an oil-immersion field of a light microscope with a digital camera. Obtain images containing multiple objects of the trypomastigotes of all three parasite species under microscopic examination; look for a slender shape, long tails, an undulating membrane, and a kinetoplast at the anterior end.\n\t\tNOTE: Creating both thick and thin smears would enhance the detection of acute phase trypanosomiasis31. The blood collection by finger-prick is recommended by WHO32. Nevertheless, thin films are more effective in identifying Trypanosoma cruzi and other species, as these organisms tend to become distorted in thick films33. In light of this, we utilized thin blood film images to maintain the appropriate morphology of the parasites for this study.\nStore all images in a parasite-specific folder with the following specifications: 1,600 x 1,200 pixels, 24-bit depth, and JPG file format. Split the images into the training and test sets at a ~6:1 ratio.\n\t\tNOTE: See https://gitlab.com/parasite3/superior-auto-identification-of-medically-important-trypanosome-parasites-by-using-a-hybrid-deep-learning-model/-/blob/main/JOVEimage.zip; 650 images were split to train (560 images) and test (90 images) model.",
    "Define the region of interest as a rectangular label for two classes: trypanosomes and non-trypanosomes. Use the auto-cropping module to crop all detected images by using the well-trained object detection model. The auto-cropping module is the module developed in the in-house CiRA CORE program (see Table of Materials). Collect a single object per image for training the object classification.\n\t\tNOTE: For this paper, 1,017 images were split for training (892 images) and testing (126 images). The model training was performed with four labeled classes, including leukocyte, T. brucei, T. cruzi, and T. evansi.\n2. Training process with in-house CiRA CORE platform\nStarting a new project\n\t\nOpen the CiRA CORE application from the computer desktop (see Table of Materials) and create a new project by double-clicking on the program's icon.\nChoose the operation icon on the left vertical toolbar to select the required tools.\nObject detection model training\n\t\nSelect the training-DL model function for data labeling and training by using the drag-and-drop method. Go to the General toolbar | CiRA AI | Drag DeepTrain | Drop DeepTrain on the screen (right-hand side).\n\t\tNOTE: For additional options, right-click on the selected tool and perform the appropriate functions: Copy, Cut, or Delete.\nImport the images using DeepTrain tool's settings. Click on the Load images button and navigate to the image directory. Label the objects by holding the left-click and naming the selected object. Adjust the rectangle line thickness and font size by clicking on the Display Setting button and Save GT as a .gt file in the same directory.\n\t\tNOTE: Save as needed to avoid any undesired conditions such as power shortage, automatic program closures, and hanging within the labeling process.",
    "Prior to model training, expand the data to gather sufficient information using the four augmentation techniques: Rotation, Contrast, Noise, and Blur. Click the Gen Setting button to access this feature.\nInitiate model training by clicking the Training button in the DeepTrain tool. The training part has two sub-functions: Generate Training Files and Train. Under the Generate Training Files function, select the desired models, batch size, and subdivisions. Click the Generate button to generate data and save it in the directory. In the Train function, choose the following options: i) use another generated training location for conditions and backup, ii) use prebuilt weights for continued training, or iii) override parameters for current training design. This will design the model configuration and training conditions.\n\t\tNOTE: The generation process time depends on the image file size, augmentation usage, and available memory space.\nOnce all necessary configurations are complete, begin the model training by clicking on the Train button. Allow the program to continuously execute, evaluating the training loss and adjusting the weight of the dataset during the training process. If the model achieves optimal loss, save the trained weight file in the specified directory by clicking on the Export button.\n3. Object detection model evaluation\nSelect the object detection model evaluation function for model evaluation using the drag-and-drop method. Go to the Plugin toolbar | Evaluate | Drag EvalDetect | Drop EvalDetect on the screen (right-hand side).\nClick on Setting and wait for three functions: Detection, Evaluate, and Plot. Initiate model evaluation by importing the trained weight file from the directory (step 2.2.5) by clicking on Load Config.\nUnder the Detection function, select non-maximum suppression (NMS) value to enhance accuracy by eliminating redundant false positive (FP) detections. NMS removes duplicate model-generated detections for improved reliability.",
    "Proceed with the following steps under the Evaluation function:\n\t\nImport test images from the image file directory by clicking on Browse. Import the GT file from the directory where it was saved in step 2.2.2 by clicking on Load GT.\nChoose the Intersection over Union (IoU) value to assess accuracy on the specific image test dataset.\nClick the Evaluation button to assess the detection model in the specified directory. Once the evaluation is completed, the results will be automatically saved as a CSV file in the same directory, sorted by class name. This CSV file will provide essential parameters such as True Positive (TP), False Positive (FP), False Negative (FN), Recall, and Precision for each class.\nTo plot the Precision-Recall (PR) curve, follow these steps under the Plot function: Import the CSV files from the previous section (step 3.4) directory by clicking on Browse. Choose classes from the list and click the Plot button to display the editable PR curve image.\nFinally, to save an image with the AUC values of the PR curve in the required image format at the specified directory, click on the Save button of the image.\n4. Image cropping for a single object per image\nPrior to cropping the images, complete the following steps:\n\t\nImport the images from the image file directory by accessing the settings of the Image Slide tool.\nImport the trained weight file (saved in step 2.2.8) by accessing the settings of the Deep Detect tool. Click on the Config button | + button, select the backend (CUDA or CPU), provide a name, click OK, choose the weight file directory, and click Choose. Within the Deep Detect tool, select the detection parameters (threshold and non-maxima suppression (nms)); drawing parameters; tracking parameters; and region of interest (ROI) parameters.",
    "Select the directory where the cropped images will be saved by accessing the settings of the Deep Crop tool. Click Browse | choose the directory to save the cropped images | click Choose | select the image format (jpg or png) | enable the Auto Save option.\nCrop images to obtain a single object per image for image classification and segmentation. To carry out this process, utilize four tools and establish connections between them: go to the General toolbar | General | Button Run. Next, navigate to General toolbar | CiRA AI | DeepDetect; then, go to General toolbar | CiRA AI | DeepCrop. Finally, go to Image toolbar | Acquisition | ImageSlide.\nOnce all the necessary settings are in place, initiate the image cropping process by clicking on the Button Run tool.\nObtain a new image training dataset consisting of single-object images with a size of 608 x 608.\n5. Image classification as model training\nUse drag-and-drop to select the image classification model training function for data training. Go to the Image toolbar | DeepClassif | Drag ClassifTrain | Drop ClassifTrain on the screen.\nImport images for model training using the ClassifTrain tool's settings. Click on the Open folder button and navigate to the desired image directory. Before training, expand the data by clicking on the Augmentation button for more information using techniques such as Rotation, Contrast, Flipping (horizontal and/or vertical), Noise, and Blur.\nTo commence model training, click on the GenTrain button of the ClassifTrain tool. Under the GenTrain function, select the models, batch size, and subdivisions. Assign a directory to save the generated file. Click the Generate button to proceed with data for training. In the Train function, tick the appropriate options: Continue training with default weight or custom weight.",
    "NOTE: The generation process may take time depending on factors such as image file size, augmentation usage, class balancing, and available memory space.\nOnce all preparations are complete, initiate the model training by clicking the Start button. Allow the program to execute continuously, evaluating the training loss and adjusting the weight of the dataset during the training process. If the model achieves the desired level of loss, save the trained weight file to the specified directory by clicking on the Export button.\n6. Classification model evaluation\nSelect the image classification model evaluation function for model evaluation using the drag-and-drop method. Go to the Plugin toolbar | Evaluate | Drag EvaluateClassif | Drop EvaluateClassif on the screen (the right-hand side).\nClick on Setting to access additional functions within the EvaluateClassif tool, namely Evaluate and PlotROC.\nTo initiate model evaluation, click on the Evaluate button in the EvaluateClassif tool. Follow these steps under the Evaluate function.\n\t\nImport the test images from the image file directory by clicking on the Load folder image. Import the trained weight file from the directory (saved in step 5.4) by clicking on Load Config. Click the Start button to evaluate the classification model.\nOnce the evaluation is complete, save the evaluated file as CSV in the specified directory by clicking on the Export to CSV button. For evaluation of data at every threshold, save the CSV file with class names in the specified directory by clicking on Start all threshold. The saved CSV file includes parameters such as Recall (True Positive Rate), False Positive Rate, and Precision for each class.\nTo plot the Receiver Operating Characteristics (ROC) curve, click on the PlotROC button within the EvaluateClassif tool. Follow these steps under the PlotROC function.",
    "Import CSV files from the directory obtained earlier by clicking on Browse. Inspect the imported class list and select each class label to plot the ROC curve.\nClick the Plot button to visualize the ROC curve as an image. Make the desired edits to adjust image properties, including font size, font colors, rounding the decimal, line styles, and line colors.\nFinally, save an image of the ROC curve with the AUC values in the required image format at the specified directory by clicking on the Save button.\n7. Testing the process with the CiRA CORE application\nObject detection as model testing\n\t\nTo perform model testing, utilize four tools and establish connections between them. Go to the General toolbar | General | Button Run. Then, General toolbar | General | Debug. After that, click on General toolbar | CiRA AI | DeepDetect, and finally Image toolbar | Acquisition | ImageSlide.\nBefore testing the images, follow these steps:\n\t\t\nImport the test images from the image file directory by clicking on the Setting option in the Image Slide tool.\nImport the saved trained weight file from step 2.2.8 by clicking on the Setting option in the DeepDetect tool. Click on the Config button, then the + button, select the backend (CUDA or CPU), provide a name, click OK, choose the weight file directory, and click Choose. Under the DeepDetect tool, select the detection parameters (Threshold and nms), drawing parameters, tracking parameters, and ROI parameters.\nView the test image results by clicking on the image function in the Debug tool.\nFinally, check the predicted results for each image by clicking on the Run button on the Button Run tool.\nImage classification as model testing",
    "To perform model testing, utilize four tools and establish connections between them. Go to the General toolbar | General | Button Run; then, General toolbar | Debug. After that, navigate to Image toolbar | Acquisition | ImageSlide, and finally, Image toolbar | DeepClassif | DeepClassif.\nBefore testing the images, follow these steps:\n\t\t\nImport the test images from the image file directory by clicking on the Setting option in the Image Slide tool.\nImport the saved trained weight file from section 5.5 by clicking on the Setting option in the DeepClassif tool. Click on the Config button | + button | select the backend (CUDA or CPU) | provide a name | click OK | choose the weight file directory | click Choose. Under the DeepClassif tool, select the classification parameters (Threshold and number of top-class predictions), Guide map parameters (threshold, alpha, beta, and color map), and various parameters in the color map.\nView the test image results by clicking on the image function in the Debug tool.\nFinally, check the predicted results for each image by clicking on the Run button on the Button Run tool.\n8. Hybrid (detection and classification) as model testing\nTo perform this model testing, utilize four tools and establish connections between them. Go to the General toolbar | General | ButtonRun. Then, General toolbar | General | Debug. After that, Image toolbar | Acquisition | ImageSlide, and finally, Image toolbar | DeepComposite | DeepD->C.\nBefore testing the images, follow these steps: Import test images from the image file directory by clicking on the Setting option in the Image Slide tool. Import the two saved trained weight files from section 2.1.5 and section 4.4 by clicking on the Setting option in the DeepD->C tool:",
    "For the Detect function, click on the Config button |+ button, select the backend (CUDA or CPU) | provide a name | click OK | choose the weight file directory | click Choose. Under the Detect function, select the detection parameters (Threshold and nms), drawing parameters, tracking parameters, and ROI parameters.\nFor the Classif function, click on the Config button |+ button, select the backend (CUDA or CPU) | provide a name | click OK | choose the weight file directory | click Choose. Under the Classif function, select the classification parameters (Threshold and number of top-class predictions) and Guide map parameters (threshold, alpha, beta, and color map).\nView the test image results by clicking on the image function in the Debug tool. Finally, check the predicted results for each image by clicking on the Run button on the Button Run tool.\n9. Five-fold cross-validation\nNOTE: To validate the performance of the proposed model more effectively, K-fold cross-validation is used.\nDivide the dataset into five sections, corresponding to the five folds of cross-validation. During each iteration of model training and testing, use one section as the validation set for testing and the remaining four sections for training. Repeat this process five times, with each fold being used as the validation set once.\nFor Folds 1 through 5:\n\t\nRepeat section 5 to train the model using the training data from the four folds.\nRepeat section 7.2 to test the model using the remaining fold as the test set.\n10. Model evaluation\nConfusion matrix\n\t\nBased on the test results, the four conditions will happen as follows:\n\t\t\nTrue Positive (TP): When the input image is true, and the prediction is also true.\nFalse Positive (FP): When the input image is false, but the prediction is true.",
    "False Negative (FN): When the input image is true, but the prediction is false.\nTrue Negative (TN): When the input image is false, and the prediction is also false.\nUsing these four conditions, evaluate the performances with the confusion matrix.\nPerformance evaluations\n\t\nThe most commonly used classification performance metrics are accuracy, precision, recall, specificity, and F1-score values. Calculate all evaluation metrics in equations (1-6) used to evaluate model performance from values from the confusion matrix.\nimgsrc://cloudfront.jove.com/files/ftp_upload/65557/65557eq01.jpg    (1)\nimgsrc://cloudfront.jove.com/files/ftp_upload/65557/65557eq02.jpg    (2)\nimgsrc://cloudfront.jove.com/files/ftp_upload/65557/65557eq03.jpg    (3)\nimgsrc://cloudfront.jove.com/files/ftp_upload/65557/65557eq04.jpg    (4)\nimgsrc://cloudfront.jove.com/files/ftp_upload/65557/65557eq05.jpg    (5)\nimgsrc://cloudfront.jove.com/files/ftp_upload/65557/65557eq06.jpg    (6)\nROC curve\n\tNOTE: The ROC curve is a performance measure for classification problems with different threshold settings. The area under the ROC curve (AUC) represents the degree or measure of separability, while the ROC is a probability curve.\n\t\nThe ROC curve is a two-dimensional graph with the true positive rate (TPR) and false positive rate (FPR) values plotted on the Y and X axes, respectively. Construct the ROC curves using the TPR and TFR values obtained from the confusion matrix. The TPR value is the same as the sensitivity; calculate the FPR value using the equation (7).\nimgsrc://cloudfront.jove.com/files/ftp_upload/65557/65557eq07.jpg    (7)\nAfter obtaining the TPR and FPR values, plot the ROC curve using the Jupyter Notebook open-source web tool in a Python environment. The AUC is an effective way to assess the performance of the proposed model in ROC curve analysis.\nPR curve\n\t\nUse the PR curve to evaluate models by measuring the area under the PR curve. Construct the PR curve by plotting the models' precision and recall using the model's confidence threshold functions. Because the PR curve is also a two-dimensional graph, plot Recall on the x-axis and Precision on the y-axis.",
    "Plot the PR curve, like the ROC curve, using the open-source Jupyter Notebook web tool in a Python environment. The area under the Precision-Recall curve (AUC) score is also helpful in multilabel classification.\nSubscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Bioengineering"
  ],
  "bigAreas": [
    "Bioengineering & Technology"
  ]
}