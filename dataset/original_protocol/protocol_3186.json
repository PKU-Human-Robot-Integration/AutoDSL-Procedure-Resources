{
  "id": 3375,
  "origin_website": "Cell",
  "title": "Mapping neuronal ensembles and pattern-completion neurons through graphical models",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nHere we describe step-by-step details in the analysis of an example dataset, from the importing of the dataset, to parameter selection, data validation, structural learning, parameter estimation, model selection, the identification and evaluation of neuronal ensembles, and the identification of pattern completion neurons. We have indicated the estimated time required for the example dataset. A comprehensive benchmarking can be found here[href=https://www.jneurosci.org/content/41/41/8577] to estimate the computation time for your own datasets.\nInitialization\nTiming: <5 min\nIn this portion of the protocol, we import our data, select dataset-specific parameters, and validate the compatibility of our dataset (Figure 2[href=https://www.wicell.org#fig2]A). For this tutorial, we will utilize data collected from the calcium imaging of excitatory neurons in the visual cortex during the presentation of drifting gratings with 0°, 45°, 90°, and 135° orientations on an LCD screen. More information on this dataset can be found here.[href=https://github.com/darikoneil/Pat-Map/blob/main/example_datasets/drifting_gratings/metadata.txt]\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2958-Fig2.jpg\nFigure 2. Importing data\n(A) Import interface for loading and validating data. Note separate buttons for loading the spike matrix, user-defined features, and ROI coordinates.\n(B) Logging console confirming successful import and validation of data.",
    "Note: All relevant files can be found in the ‘drifting_gratings’ folder within ‘example datasets’. The folder contains five files: ‘visual_cortex_data.mat’, ‘visual_cortex_udf.mat’, ‘visual_cortex_rois.mat’, ‘visual_cortex_params.mat’, ‘metadata.txt’. The ‘data’ file contains an M sample by N neuron binary matrix where each element indicates whether the n-th neuron was spiking during the m-th sample. The ‘udf’ file contains two variables. The first variable ‘udf’ is an M sample vector of natural numbers indicating which drifting grating was presented during the m-th sample. The ‘udf_labels’ variable contains a cell array mapping the natural numbers in ‘udf’ and the presented drifting gratings. The ‘rois’ file is an optional file that contains the pixel masks for each neuron within the dataset. The parameters file is a structure containing the parameters for the analysis. The provided parameters file is simply the default parameters and is only provided to provide an example on loading a set of saved parameters. Finally, the ‘metadata.txt’ file contains a short description of the experiment.\nCritical: Imported data files must be an M sample by N neuron binary matrix where each value indicates the state of the neuron in the sample. Intuitively, the state of the neuron is a simple binary indication of spiking (i.e., active or inactive). Nevertheless, the matrix does not need to directly map to discrete spikes. Any binary measure of neural activity is sufficient. This ‘data’ matrix must be saved in the form of a ‘.mat’ file (MATLAB binary file).",
    "Critical: Imported udf files must contain either an M sample vector of natural numbers where each element indicates the user-defined feature present in the m-th sample or an M sample by N user-defined feature binary matrix where each element indicates whether the n-th user-defined feature was present during the m-th sample. It is critical that each sample is directly matched with the samples of the dataset. An optional cell array of length N user-defined features may be provided that maps each user-defined feature with a text label.\nCritical: Imported ROIs must be properly formatted in the form of a ‘.mat’ (MATLAB binary file). Acceptable formats include: (1) a 2D matrix in which the rows are neurons and the columns represent the center point in Cartesian coordinates with respect to the image (2) a 2D matrix in which the rows are neuron, the first two columns represent the center point in Cartesian coordinates with respect to the image, and the third column representing the imaging plane or (3) an exported Suite2P structure.\nImporting your dataset.\nPress the browse button to open a pop-up file explorer.\nLocate your dataset within the file explorer and press the open button.\nNote: In this case, we will be selecting the ’visual_cortex_data.mat’ file in the ’drifting_gratings folder located within ‘example_datasets’ folder.\nNote: The path to your file will be reflected in the associated textbox.\nClick the load button to import your dataset.\nNote: A preview of the dataset will be generated below.\nImporting (potentially) encoded features, referred to as user-defined features (UDFs)\nOnce again, press the browse button to open a pop-up file explorer.\nLocate your UDFs within the file explorer and press the open button.\nNote: In this case, we will be selecting the ‘visual_cortex_udf.mat’ file in the ‘drifting_gratings’ folder located within ‘example_datasets’ folder.",
    "Note: The path to your file will be reflected in the associated textbox.\nClick the load button to import the UDFs.\nOptional: a file containing ROI coordinates can be imported for visual identification of ensemble neurons and pattern completion neurons, and superimposing the learned graphical structure onto the rois. If one is not provided, simulated rois will be generated.\nPress the browse button to open a pop-up file explorer.\nLocate your ROIs file within the file explorer and press the open button.\nNote: In this case, we will be selecting the ‘visual_cortex_rois.mat’ file in the ‘drifting_gratings’ folder located within ‘example_datasets’ folder.\nNote: The path to your file will be reflected in the associated textbox.\nClick the load button to import the ROIs. The ROIs will be plotted below.\nSelecting Parameters.\nNote: The right panel displays the major parameters of the analysis pipeline. Relevant parameters are additionally displayed on the submenu of each step and are described as the user performs those steps later. These parameters are linked (i.e., changing one changes the other).\nNote: In this tutorial we will demarcate and outline parameter choices during the step they are relevant.\nPress the browse button to open a pop-up file explorer.\nLocate the parameters file within the file explorer and press the open button.\nNote: In this case, we will be selecting the ‘visual_cortex_params.mat’ file in the ‘drifting_gratings’ folder located within the ‘example_datasets’ folder.\nClick the load button to import the parameters. Do not worry if the parameters do not change. The supplied example parameters are simply the defaults.\nValidating the Dataset.\nFirst, we specify the three parameters relevant for this stage.\n‘Shuffle Data’ indicates whether to shuffle the dataset before learning. Shuffling the data here mitigates the influence of potential longitudinal changes in the dataset (e.g., unstable or dynamic representation).",
    "Note: It may seem that one would always wish to shuffle the dataset, however, there are situations where this is not the case. For example, one might wish to train a model on some behaviorally relevant portion of the dataset and consider whether it generalizes to a different portion. For example, before and after extinction of a conditioned stimulus.\n‘Training-Testing Split’ indicates the fraction of samples used for training the graphical model. To mitigate overfitting of the model, a subset of the data should always be withheld at this stage (i.e., the testing data).\n‘Validation-Training Split’ indicates the fraction of training data withheld for hyperparameter optimization. Hyperparameters are parameters whose values shape the learning of the model. It is always best practice to determine the best combination of hyperparameters on a dataset that is separate from the testing data to mitigate overfitting.\nSecond, we press the “validate dataset” button.\nNote: This step ensures that data is appropriately structured as described in the preceding sections labeled critical. Specifically, this step ensures the data is binary (contains only 0′s and 1′s), that every neuron fires at least twice in the testing and training datasets, and that each observation contains at least one neuron firing. In the event of failed validation, the validator will attempt automatic solutions if applicable. Successful validation will be confirmed to the user in the logging console (Figure 2[href=https://www.wicell.org#fig2]B). Otherwise, an error will appear.\nNote: Pressing the ‘Run Model’ button will launch the entire analysis with a single-click (including validation). However, in this tutorial, we will proceed manually through each step.\nStructural learning\nTiming: 1–5 min",
    "The objective of the structural learning step is to generate a set of structures which approximate the functional connectivity of the neural circuit. We accomplish this by conducting neighborhood-based regressions implemented through highly-efficient elastic nets. The most optimal structure for the graphical structure will be determined during the hyperparameter optimization step.\nClick on the ‘Training’ tab to proceed to the training stage.\nClick on the “Structural Learning” tab to open the structural learning submenu (Figure 3[href=https://www.wicell.org#fig3]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2958-Fig3.jpg\nFigure 3. Structural learning and parameter estimation\n(A) Structural learning panel with an interface for initiating structural learning and a drop-down box highlighting a pseudo-random selection of s-lambda values to be passed to parameter estimation.\n(B) Parameter estimation panel with an interface for initiating parameter estimation and a drop-down box highlighting selected p-lambda values.\n(C) Console feedback during parameter estimation reporting progress and computation time.\nSetting Structural Learning Parameters.\n‘Parallel Learning’ indicates whether to learn the set of structures in parallel. This is step can save considerable time at the expense of using more memory. By default, it is set to 0 (off).\n‘Alpha’ indicates the balance of    l 1   -to-   l 2    regularization in the elastic net during structural learning. We set this parameter to the default value of 1. This parameter does not need to be changed but is provided for advanced users.\n‘Number of sLambda’ indicates the number of distinct sLambda values used to minimize the structural learning objective.\n‘Minimum sLambda’ and ‘Maximum sLambda’ set the range from which sLambda parameters are selected. By default, this range is set to 1e-05 to 5e-01. These parameters should rarely need changed.",
    "‘sLambda distribution’ sets the distribution used to randomly select sLamba values. By default, this parameter is set to “1” to utilize a log distribution. This allows for more samples to be selected at smaller sLambda where the complexity of output structures is greater. Users should only set this to parameter to “0” (uniform distribution) if structural learning is prohibitively time-consuming for a particular dataset. Setting this parameter to “0” will improve performance at the expense of accuracy.\n‘Edge Constraints’ controls the connectivity of user-defined feature nodes within the learned structure. By default, this parameter is set to “1” to constrain the structure such that user-defined feature nodes cannot connect to each other. This parameter does not need to be changed but is provided for advanced users who may be interested in exploring interactions between user-defined features.\n‘Absolute Value Ranking’ formulates structures based on the absolute value of their coefficients rather than their positive magnitude. We will set this parameter to 0. However, it could be useful in mixed datasets including excitatory and inhibitory neurons and has been left open to the user.\n‘Number of Seed Structures’ determines how many structures to randomly select as seeds for parameter estimation and hyperparameter optimization.\n‘Density’ sets an optional threshold for the density of the learned structure. The number of edges kept in the structure is equal to this value times the number of possible non-zero edges.\nPress the run Structural Learning to generate a set of structures to pass to parameter estimation and hyperparameter optimization.\nNote: A progress bar indicating progress will be displayed in the terminal.\nParameter estimation\nTiming: 10–30 min\nThe object of this step is to infer the potentials which dictate the magnitude of the interactions identified during structural learning. This objective is accomplished through maximum likelihood estimation.",
    "Click on the “Parameter Estimation” tab to open the parameter estimation submenu (Figure 3[href=https://www.wicell.org#fig3]B).\nSetting Parameter Estimation parameters.\n‘Implementation Mode’ sets the implementation of parameter estimation. Changing the default “1” to “2” conducts parameter estimation on multiple potential models in parallel at the expense of increased memory requirements. Setting the mode to “3” or “4” conducts sequential or parallel parameter estimation on seed models only. This setting is useful to get a quick gauge of the amount of time it takes to learn models of a particular size or composition before launching a more-expansive optimization.\n‘fVal Epsilon’ sets the convergence criterion and is best left at default. It is provided to allow advanced users to relax the criterion on exceptionally large or complex datasets.\n‘Reweight Denominator’ sets the method used to relax the approximation of the partition function to facilitate learning. It is best left at the default. It is provided to allow advanced users to tailor performance when using exceptionally large or complex datasets.\n‘Number of pLambda’ sets the number of pLambda values used as seeds during parameter estimation.\n‘Minimum pLambda’ and ‘Maximum pLambda’ set the range from which pLambda values are selected.\n‘pLambda Distribution’ sets the distribution from which seed pLambda values are selected. Set this parameter as “0” for a uniform distribution or “1” for a log space.\n‘Max Iterations’ sets the upper limit for the number of iteration to allow during parameter estimation. It is best set at default, but can be decreased to improve performance for large or complex datasets",
    "‘Max Time’ sets the upper limit for the amount of time spent on a single model during parameter estimation. While the example dataset will converge relatively quickly, I can be advantageous to set this parameter to a few thousand seconds to ensure poorly-suited models don’t’ absorb valuable computation time.\n‘Print Interval’ sets the regularity of user feedback during learning. It does not affect performance and can be left at the default.\n‘SMBO Max Evaluations’ sets the maximum number of models to evaluate during hyperparameter estimation. The default value is usually sufficient.\n‘SMBO Max Time’ sets the maximum amount of time permitted to optimize the hyperparameters.\nPress the “perform parameter estimation” button to estimate parameters.\nNote: Verbose feedback will be printed in the command window (Figure 3[href=https://www.wicell.org#fig3]C).\nModel evaluation\nTiming: <5 min\nThe objective of this step is to identify the best model from the pool of all learned models. The best model is selected by assessing the performance of each model on a withheld dataset.\nClick on the “Evaluate Model” Tab to open the model evaluation submenu (Figure 4[href=https://www.wicell.org#fig4]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2958-Fig4.jpg\nFigure 4. Leveraging learned models for identifying pattern completion neurons\n(A) Model evaluation panel displaying the statistics and properties of the optimal model. Note the button to evaluate the model’s ability to predict the occurrence of user-defined features.\n(B) Panel for identifying neuronal ensembles.\n(C) Panel for identifying pattern completion neurons.\nNote: The best model is automatically selected following parameter estimation.\nPress the “Evaluate Best Model” button to evaluate the learned model.\nNote: A table will be produced describing the performance of the model in predicting user-defined features. An interactive visualization will be produced allowing users to highlight specific nodes within the graphical structure to observe their connectivity.\nNote: Select which UDF to visualize using the UDF edit field.",
    "Identification of neuronal ensembles\nTiming: <5 min\nThe objective of this step is to identify the groups of neurons (neuron ensembles) specifically relevant to each UDF.\nClick on the “Identify Ensembles” tab to open the Identifying Neuronal Ensembles submenu (Figure 4[href=https://www.wicell.org#fig4]B).\nSetting identification of neuronal ensembles parameters.\n‘Tuning Criterion’ sets the criterion for assessing the performance of individual neuronal nodes. The default criterion is ‘AUC’ for the area-under-the-curve of the receiver operating characteristic (true positive rate vs. false positive rate). In some cases, the dataset may be heavily imbalanced. That is, the empirical probability of a user-defined feature being present is low (∼10%). In this case, the criterion ought to be set to ‘PR’ to utilize a precision-recall curve.\n‘Ensemble size’ controls the size of random ensembles used for comparison. By default, the random ensemble size is equal to the size of the node with the highest degree. However, it can also be set to the size of the maximum coactivation of neurons or the size of the maximum coactivation of neurons during UDFs.\n‘Number of Random Controls’ sets the number of random ensembles used for comparison. The default value of 100 is sufficient, but the user may increase this value if desired.\nPress ‘Evaluate Neuronal Contributions’ to quantify the individual contributions of individual neurons in the model.\nNote: Here, the ratio of the log-likelihood of a model with and without the neuron active are compared.\nPress ‘Evaluate Node Performance’ to predict the state of UDFs using the change in the contributions of individual neurons.\nPress ‘Compare to Random Ensembles’ to compare these predictions to the performance of random ensembles and identify neuronal ensembles.\nNote: Neurons are added to an ensemble when they surpass a threshold calculated using the performance of randomized ensembles.",
    "Note: The threshold for ensembles neurons can be modified and new ensembles extracted through the ‘Recompare to Random Ensembles’ buttons.\nIdentifying pattern completion neurons\nTiming: <5 min\nThe objective of this step is to identify the pattern completion neurons for each UDF.\nFirst press the ‘Identify Pattern Completion’ tab to open the Pattern Completion Neurons submenu.\nTo identify pattern completion neurons within the identified ensembles (if present), press the ‘Identify Pattern Completion Neurons’ button (Figure 4[href=https://www.wicell.org#fig4]C).\nNote: Pattern completion neurons are defined as those neurons surpassing a threshold for both predicting the UDF and the strength of their influence within the network—defined as the node strength, the sum of their Phi-11 coactivity potentials.\nNote: The results will be automatically plotted. To select which UDF is currently plotted, change the ‘UDF’ field.\nPress the export button to save and export results.\nCritical: The identity of ensemble and pattern completion neurons will be saved in cell arrays named ‘ensemble_nodes’ and ‘pattern_completion_nodes’. The length of each cell array is equal to the number of user defined features. Each cell contains an index of the identified ensemble or pattern completion nodes. A snap-shot of all the calculated data, including sub-optimal models, is also exported. Descriptions of these exported variables and parameters can be found here.[href=https://github.com/darikoneil/Pat-Map/blob/main/tutorial/output_documentation]"
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Neuroscience",
    "Behavior"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Ecology & Environmental Biology"
  ]
}