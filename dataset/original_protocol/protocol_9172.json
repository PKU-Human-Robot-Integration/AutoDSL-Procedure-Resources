{
  "id": 9583,
  "origin_website": "Jove",
  "title": "MPI CyberMotion Simulator: Implementation of a Novel Motion Simulator to Investigate Multisensory Path Integration in Three Dimensions",
  "procedures": [
    "1. KUKA Roboter GmbH\nThe MPI CyberMotion Simulator consists of a six-joint serial robot in a 3-2-1 configuration (Figure 1). It is based on the commercial KUKA Robocoaster (a modified KR-500 industrial robot with a 500 kg payload). The physical modifications and the software control structure needed to have a flexible and safe experimental setup have previously been described, including the motion simulator's velocity and acceleration limitations, and the delays and transfer function of the system 9. Modifications from this previous setup are defined below.\nimgsrc://cloudfront.jove.com/files/ftp_upload/3436/3436fig1.jpg\nFigure 1. Graphical representation of the current MPI CyberMotion Simulator work space.\nComplex motion profiles that combine lateral movements with rotations are possible with the MPI CyberMotion Simulator. Axes 1, 4 and 6 can rotate continuously. 4 pairs of hardware end-stops limit Axis 2, 3 and 5 in both directions. The maximum range of linear movements is strongly dependent on the position from which the movement begins. The current hardware end-stops of the MPI CyberMotion Simulator are shown in Table 1.\n\ttable:\n﻿0,1,2\nAxis,Range [deg],Max. velocity [deg/s]\nAxis 1,Continuous,69\nAxis 2,-128 to -48,57\nAxis 3,-45 to +92,69\nAxis 4,Continuous,76\nAxis 5,-58 to +58,76\nAxis 6,Continuous,120\nTable 1. Current technical specifications of the MPI CyberMotion Simulator.\nBefore any experiment is performed on the MPI CyberMotion Simulator, each experimental motion trajectory undergoes a testing phase on a KUKA simulation PC (Office PC). The \"Office PC\" is a special product sold by KUKA which simulates the real robot arm and includes the identical operating system and control screen layout as the real robot. A schematic overview of the control system of the MPI CyberMotion Simulator for an open-loop configuration is shown in Figure 2.\nimgsrc://cloudfront.jove.com/files/ftp_upload/3436/3436fig2.jpg\nFigure 2.  Schematic overview of the open-loop control system of the MPI CyberMotion Simulator. Click here for larger figure[href=http://www.jove.com/files/ftp_upload/3436/3436fig2large.jpg].",
    "The details of the control structure can be found here 9. In brief, for an open-loop configuration such as that used in the current experiment, trajectories are pre-programmed by converting input trajectories in Cartesian coordinates to joint space angles through inverse kinematics (Figure 2).\nThe MPI control system reads in these desired joint angle increments and sends these to the KUKA control system to perform axis movements via motor currents. Joint resolver values are sent to the KUKA control system which determines the current joint angle positions at an internal rate of 12ms, which in turn trigger the next joint increment to be read in from file by the MPI control system as well as write the current joint angle positions to disk. Communication between the MPI and KUKA control systems is by an Ethernet connection using the KUKA-RSI protocol.\nA racecar seat (RECARO Pole Position) equipped with a 5-point safety belt system (Schroth) is attached to a chassis which includes a footrest. The chassis is mounted to the flange of the robot arm (Figure 3a). Experiments are also possible by seating participants within an enclosed cabin (Figure 3b).\nimgsrc://cloudfront.jove.com/files/ftp_upload/3436/3436fig3.jpg\nFigure 3. MPI CyberMotion Simulator setup. a) Configuration for current experiment with LCD display. b) Configuration for experiments requiring an enclosed cabin with front projection stereo display. c) Front projection mono display. d) Head mounted display.\nAs the experiment is performed in darkness, infrared cameras allow visual monitoring from the control room.\n2. Visualization\nMultiple visualization configurations are possible with the MPI CyberMotion Simulator including LCD, stereo or mono front projection, and head mounted displays (Figure 3). For the current experiment visual cues to self-motion are provided by an LCD display (Figure 3a) placed 50 cm in front of the observers who were otherwise tested in the dark.",
    "The visual presentation was generated using Virtools 4.1 software and consisted of a random, limited life time dot-field. A cuboid extending eight virtual units to the front, right, left, upwards and downwards from the point of view of the participant (i.e., 16 x 16 x 8 units in size) was filled with 200,000 equal size particles consisting of white circles 0.02 units in diameter in front of a black background. The dots were randomly distributed across the space (homogeneous probability distribution within the space). Movement in virtual units was scaled to correspond 1 to 1 with physical motion (1 virtual unit = 1 physical meter).\nEach particle was shown for two seconds before vanishing and immediately showing up again at a random location within the space. Thus half of the dots changed their position within one second. Dots between a distance of 0.085 and 4 units were displayed to the participants (corresponding visual angles: 13° and 0.3°).\nMovement within the dot field was synchronized with physical motion by receiving motion trajectories from the MPI control computer transmitted by an Ethernet connection using the UDP protocol. When moving through the dot-field the average number of dots stayed constant for all movements. This display provided no absolute size scale, but optic flow and motion parallax as dots were spheres with a fixed size; looking smaller according to their distance relative to the observer.\n3. Experimental Design\n16 participants, who were naïve to the experiment with the exception of one author (MB-C), wore noise-cancelling headphones equipped with a microphone to allow two-way communication with the experimenter. Additional auditory noise was continuously played through the headphones to further mask noise produced by the robot.",
    "Participants used a custom built joystick equipped with response buttons with data transmitted by an Ethernet connection using the UDP protocol.\nThe angle of the two movement segments was either 45° or 90°. Movements in the horizontal, sagittal and frontal planes consisted of: forward-rightward (FR) or rightward-forward (RF), downward-forward (DF) or forward-downward (FD), and downward-rightward (DR) or rightward-downward (RD) movements respectively (Figure 4a).\nimgsrc://cloudfront.jove.com/files/ftp_upload/3436/3436fig4.jpg\nFigure 4. Procedure. a) Schematic representation of trajectories used in the experiment. b) Sensory information provided for each trajectory type tested. c) Pointing task used to indicate the origin of where participants thought that they had moved from. Click here for larger figure[href=http://www.jove.com/files/ftp_upload/3436/3436fig4large.jpg].\nSensory information was manipulated by providing visual (optic flow, limited lifetime star field), vestibular-kinaesthetic (passive self motion with eyes closed), or visual and vestibular-kinaesthetic motion cues (Figure 4b).\nMovement trajectories consisted of two segment lengths (1st: 0.4 m, 2nd: 1 m; ±0.24 m/s2 peak acceleration; Figure 4b). Trajectories consisted of translation only. No rotations of the participants occurred. To reduce possible interference from motion prior to each trial and ensure that the vestibular system was tested starting from a steady state, a 15 s pause preceded each trajectory.\nObservers pointed back to their origin by moving an arrow that was superimposed on an avatar presented on the screen (Figure 4c). Movement of the arrow was constrained to the trajectory's plane and controlled by the joystick. The avatar was presented from frontal, sagittal and horizontal viewpoints. Observers were allowed to use any or all viewpoints to answer. The starting orientation of the arrow was randomized across trials.",
    "As the pointing task required participants to mentally transform their pointing perspective from an egocentric to an exocentric representation, participants were given instructions on how to point back to their origin with reference to the avatar prior to practice and experimental trials. Participants were told that pointing should be made as if the avatar were their own body. Participants were then instructed to point to physical targets relative to the self using the exocentric measurement technique. For example, participants were instructed to point to the joystick resting on their lap half-way between themselves and the screen, which required participants to point the arrow forward and down relative to the avatar. All participants were able to perform these tasks without expressing confusion.\nEach experimental condition was repeated 3 times and presented in random order. Signed error and response time were analyzed as dependent variables in two separate 3 (plane) * 2 (angle) * 3 (modality) repeated measures ANOVA. Response times from one extreme outlier participant were removed from analysis.\n4. Representative Results",
    "Signed error results are collapsed across modalities and angles as no significant main effects were found for these factors. Figure 5a shows the significant main effect of movement plane (F(2,30) = 7.0, p = 0.003) where observers underestimated angle size (average data less than 0°) for movement in the horizontal plane (-8.9°, s.e. 1.8). In the frontal plane observers were more likely on average to overestimate angle size (5.3°, s.e. 2.6), while there was no such bias in the sagittal plane (-0.7°, s.e. 3.7). While main effects of angle and modality were not significant, angle was found to significantly interact with plane (F(2,30) = 11.1, p < 0.001) such that overestimates in the frontal plane were larger for movements through 45° (7.9°, s.e. 2.6) than through 90° (2.8°, s.e. 2.7), while such a discrepancy was absent for the other planes. In addition, modality was found to significantly interact with angle (F(2,30) = 4.7, p = 0.017) such that underestimates from vestibular information alone for movements through 90° were significantly larger (-4.3°, s.e. 2.1) compared to the visual (-2.0°, s.e. 2.4) and vestibular and visual information combined (2.3°, s.e. 2.2) conditions, while such discrepancies were absent for movements through 45°. No significant between subjects effect was for signed error (F(1,15) = 0.7, p = 0.432). Figure 5b shows the response time results. There was a significant main effect of modality (F(2,28) = 22.6, p < 0.001) where observers responded slowest when answering based on vestibular-kinaesthetic information alone (11.0 s, s.e. 1.0) compared to the visual (9.3 s, s.e. 0.8) and combined (9.0 s, s.e. 0.8) conditions. There was also a significant main effect of plane (F(2,28) = 7.5, p = 0.002) where observers responded slowest when moved in the horizontal plane (10.4 s, s.e. 1.0) compared to the sagittal (9.",
    "4 s, s.e. 0.8) and the frontal (9.4 s, s.e. 0.9) planes. There was no significant main effect of segment angle or any interactions. A significant between subjects effect was found for response time (F(1,14) = 129.1, p < 0.001).",
    "imgsrc://cloudfront.jove.com/files/ftp_upload/3436/3436fig5.jpg\nFigure 5.  Results. a) Signed error collapsed across modality for the planes tested. b) Response time collapsed across movement planes for the modalities tested. Error bars are +/- 1 s.e.m."
  ],
  "subjectAreas": [
    "Neuroscience"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}