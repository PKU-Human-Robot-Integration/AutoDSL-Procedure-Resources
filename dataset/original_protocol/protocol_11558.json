{
  "id": 12510,
  "origin_website": "Jove",
  "title": "Creating Virtual-hand and Virtual-face Illusions to Investigate Self-representation",
  "procedures": [
    "All studies conformed to the ethical standards of the declaration of Helsinki and the protocols were approved by the Leiden University Human research ethics committee. Each condition tested about 20 participants.\n1. Virtual-hand Illusion\nExperimental Setup\nWelcome the participant and collect additional information, like age, gender, etc.\nEstablish an experimental setup that includes a virtual reality programming environment; a right-handed dataglove with six programmable vibration stimulators attached to the middle of palm and to the outside of the medial (second) phalanges of each of the five fingers (see Materials List); a 3-Degrees of Freedom (DOF) orientation tracker; SCR (skin conductance response) measurement equipment; a black box (depth: 50 cm; height: 24 cm; width: 38 cm) with a computer screen lying on top horizontally (serving to present the virtual reality environment); and a cape to cover the participant's hand.\nAsk the participant to put the dataglove on his or her right hand and the orientation tracker on the right wrist. Attach a SCR remote transmitter with a strap to the left wrist. Put the SCR electrodes on the medial (second) phalanges of the index and middle fingers of the left hand (see Figure 1A and B for an illustration of the setup).\nSeat the participant in front of the desk on which the box with the computer screen on top is placed. Ask the participant to put his or her right hand into the box along the depth axis, as to shield it from their view.\nPut a cape over the participant's right shoulder and cover the space between screen and participant. Ask the participant to rest his or her left hand on an empty part of the desk.",
    "Connect the cables of dataglove and orientation tracker to the computer, and start the virtual reality programming environment. Run the pre-written command script in the command window by clicking the \"run\" button in the virtual reality environment interface, so that the virtual reality environment starts. Monitor that the participant follows the instructions shown on the computer screen in front of the participants. Wait until the pre-written command script quits automatically.\nimgsrc://cloudfront.jove.com/files/ftp_upload/54784/54784fig1.jpg\nFigure 1: (A) Participants wore an orientation tracker and a dataglove on their right hand, and SCR remote transmitter on their left hand. (B) Setup of the virtual-hand illusion experiment. (C) Setup of the virtual-face illusion experiment. (D) A screenshot of the computer screen. Please click here to view a larger version of this figure.[href=http://cloudfront.jove.com/files/ftp_upload/54784/54784fig1large.jpg]\nVirtual Hand Design\n\tNOTE: Use Python command scripts in the command window of the virtual reality software and save them. Make sure that the main command script, the import commands, the module scripts, and other commands described below are part of the same script file. For the complete python script and necessary files see the attached \"Virtual Hand Illusion.zip\" file (NB: the zip-file is a supplemental materials of the manuscript and not part of the software package. Furthermore, it excludes the required plugins for the dataglove and orientation tracker, and any other python modules used throughout the script). In order to execute the experiment first unpack the contents of this file to any folder (e.g. the desktop) and then double click the \"virtual-hand illusion_54784_R2_052716_KM.py\" file to start the experiment. Note that the script is designed to work with the stated virtual reality programming environment and will not work using other programs.",
    "Import a pre-made virtual hand model and a pre-written hand script module (which can be found in the installment file of the virtual-reality-environment software package) into the virtual reality environment. The hand script module tracks the finger joint gesture and angles of the dataglove and feeds the information into the virtual hand model, which allows controlling the movements of the virtual hand by moving the real hand wearing the dataglove.\n\t\t\nManually change the size and appearance of the virtual hand if necessary by specifying its parameters in the script, such as its x, y, and z scaling to change its size or change the mapped image.\nFor synchrony conditions, use no transformation, so that the virtual hand moves the same way as the real hand and at (about) the same time. To create asynchrony, add a delay of 3 s, so that the virtual hand moves as the real hand but with a noticeable delay.\nIdentify a suitable pre-made orientation tracker plugin in the virtual-reality-environment installment file and import it in the command scripts. Note that running the command scripts makes the orientation tracker module track the orientation changes of the real hand (provided by the orientation tracker participants wear on their right wrist), which can then be used to control the orientation changes of the virtual hand by setting the yaw, pitch, and roll data of the virtual hand in the command window. Channel the data tracked by the orientation tracker directly into the virtual-hand model for synchrony conditions but insert a delay of 3 s for asynchrony.",
    "Design the required additional virtual objects and their movement trajectories, so that they move to and from the virtual hand (here, design and import additional models for a stick, rectangle, ball, and knife, to be used during various parts of the experiment; see \"Experimental Conditions\"). Manually change the size, appearance and position for each of these objects in the command script in the same way as the parameters for the virtual hand are set. Set the required movement trajectories using the appropriate commands to set the start and end position of the movement trajectories for an object and the speed at which it should move.\nDetermine the vibration strength and timing of each vibration stimulator in the command script; either without a delay for synchrony conditions (i.e., vibration starts exactly when the virtual hand is being contacted by the other virtual object) or with a delay of 3 s for asynchrony. All vibrators vibrate at the same time as the virtual hand is touched by the other virtual object (or at the delayed time point). Set the vibration strength to a medium level (i.e. to 0.5 on a scale of 0-1). Note that the actual strength of vibration depends on the programming environment and vibrators used for the experiment, and that a medium level of vibration in our experiment does not necessarily match the actual strength of vibration when different hardware (i.e. vibrators/dataglove) or software is used.\nAdd a second part to the experiment script that is identical to the previous steps except for the following changes:\n\t\t\nReplace the virtual hand model with a virtual rectangle of a similar size as the virtual hand (so to realize the appearance factor of the experiment)",
    "Make sure that the rotation of the real hand as picked up by the orientation tracker is translated into rotational movements of the rectangle.\nMake sure that the opening and closing of the real hand as picked up by the dataglove is translated into color changes of the rectangle using the appropriate command for changing the color of an object in your programming environment (e.g., present the rectangle in green when the hand is completely closed, in red when it is completely opened, and let the color gradually change from red to green or green to red as the hand opens or closes).\nExperimental Conditions\nRun the eight experimental conditions (resulting from crossing the three experimental factors synchrony, appearance of the virtual effector, and active/passive) in an order that is either balanced across participants or randomized.\nFor each condition, include three phases of about 2 to 3 min each to induce the virtual-hand illusion and a threat phase to measure electrophysiological skin responses (SCR). The concrete protocol differs somewhat for the eight conditions and is described below.\nVirtual hand/active/synchrony",
    "Configure the system such that the delay between the following events is close to zero and not noticeable: (a) the movements and orientation changes of the real hand and the corresponding movements and orientation changes of the virtual hand in the visuo-motor correlation phase; (b) the time points of contact between the virtual hand and the additional virtual object on the screen and the corresponding time points of vibration-induced stimulation of the real hand in the visuo-tactile phase; and (c) the movements and orientation changes of the real hand and the corresponding movements and orientation changes of the virtual hand; and the time points of contact between the virtual hand and the additional virtual object on the screen and the corresponding time points of vibration-induced stimulation of the real hand in the visuo-motor-tactile phase.\nFor the visuo-motor correlation phase, have participants freely move or rotate their real right hand, including opening, closing, and rotating their real hand, and moving each finger individually. Have participants watch the corresponding movements of the virtual hand on the computer screen.\nFor the visuo-tactile stimulation phase, have participants keep their real hand still while watching the screen. Present another virtual object on the screen, such as a virtual ball or stick (which was created in 1.2.3) that moves to and from the virtual hand, producing the impression of touching and not touching the virtual hand.",
    "Accompany each contact between this additional virtual object and the virtual hand by vibrator activity on the dataglove. Have the vibrator stimulate that part of the real hand that corresponds to the part of the virtual hand that is being touched by the additional virtual object (e.g., if the virtual object seems to touch the palm of the virtual hand, the palm of the participant's real hand should be stimulated by the vibrator16).\nFor the visuo-motor-tactile correlation phase, have the participants move the virtual hand by moving their real hand in order to touch a virtual vibrating stick or similar object (see 1.2.3). Ensure that each contact between virtual hand and virtual stick/object is accompanied by vibration-induced stimulation of the participant's real hand as described in 1.3.3.3.\nFor the threat phase, have participants keep their real right hand still while watching a virtual knife or needle appear on the computer screen. Make the virtual knife or needle go to and from the virtual hand. Ensure that each contact results in a visible apparent \"cutting\" or \"puncturing\" of the virtual hand.\n\t\t\t\nStimulate that part of the real hand that corresponds to the cut or punctured part of the virtual hand by using the vibrators of the dataglove as described in 1.3.3.3.\nVirtual hand/active/asynchrony\n\t\t\nRun the procedure described under 1.3.3 after configuring the system such that the delay between the critical events is three seconds instead of close to zero.\nVirtual rectangle/active/synchrony\n\t\t\nRun the procedure described under 1.3.3 but with the virtual rectangle instead of the virtual hand.\nVirtual rectangle/active/asynchrony\n\t\t\nRun the procedure described under 1.3.4 but with the virtual rectangle instead of the virtual hand.\nVirtual hand/passive/synchrony\n\t\t\nRun the procedure described under 1.3.3 but ask the participant to keep his or her real hand still throughout all phases.\nVirtual hand/passive/asynchrony",
    "Run the procedure described under 1.3.4 but ask the participant to keep his or her real hand still throughout all phases.\nVirtual rectangle/passive/synchrony\n\t\t\nRun the procedure described under 1.3.5 but ask the participant to keep his or her real hand still throughout all phases.\nVirtual rectangle/passive/asynchrony\n\t\t\nRun the procedure described under 1.3.6 but ask the participant to keep his or her real hand still throughout all phases.\nData Collection\nCollect SCR data using the measurement equipment (see Materials List) and its software. The recording frequency is every 0.1 ms.\nAsk the participant to fill out the questionnaire measuring sense of ownership, agency, location and appearance for the respective condition. Use either a paper version, in which each question (as described in 1.4.2.1 and 1.4.2.2) is printed, together with a Likert scale (as described in 1.4.2.3), and which can be filled in with a pen; or use a computerized version, in which each question is shown on the screen, together with the Likert scale, and in which the chosen scale value can be typed in.\n\t\t\nInclude a questionnaire that minimally includes one or more ownership questions2; use the following four:\n\t\t\t(O1) \"I felt as if the hand on the screen were my right hand or part of my body\";\n\t\t\t(O2) \"It seemed as if what I were feeling on my right hand was caused by the touch of the stick on the hand on the screen that I was seeing\";\n\t\t\t(O3) \"I had the sensation that the vibration I felt on my right hand was on the same location where the hand on the screen was touched by the stick\";\n\t\t\t(O4) \"It seemed my right hand was in the location where the hand on the screen was\".\nConsider including further questions regarding agency questions; use the following:",
    "(A1) \"I felt I can control this virtual hand\" (for the active condition);\n\t\t\t(A1) \"It seemed like I could have moved the hand on the screen if I had wanted to, as if it were obeying my will\" (for the passive condition); .\n\t\t\tNote that the items listed in 1.4.2.1 and 1.4.2.2 refer to the hand condition. For the rectangle condition, replace all references to the virtual hand by references to the virtual rectangle.\nUse a Likert scale2 for each question (e.g., 1-7), so that participants can score the degree to which they agreed to the question; e.g., use 1 for \"strongly disagree\" and 7 for \"strongly agree\". Make sure each question appears on screen and can be responded to with the numbers 1 to 7 corresponding to the 7 response options of the Likert scale; appearance and response options are programmed in the experiment script.\n2. Virtual-face Illusion\nExperimental Setup\nWelcome the participant and collect additional information, like age, gender, etc.\nEstablish an experimental setup that includes a virtual reality programming environment; a head position tracking system, including corresponding hardware and software17; and a 3-DOF orientation tracker attached to the top of a hat or baseball cap.\n\t\tNOTE: Using this experimental setup, participants can freely move or rotate their own head to control the position and orientation of the virtual face but they cannot control the facial expressions of the virtual face\nAsk the participant to sit on the chair 2 meters in front of the computer screen. See Figure 1C and 1D for an illustrations of the experimental setup.\nAsk the participant to put on the cap with the attached orientation tracker.",
    "Connect position tracking system and orientation tracker to the computer and run the pre-written command script in the command window by clicking the \"run\" button in the virtual reality environment interface, so that the virtual reality environment starts. Monitor that the participant follows the instructions shown on the computer screen in front of the participants. Wait until the pre-written command script quits automatically.\nVirtual Face Design\n\tNOTE: For the complete python script and necessary files see the attached \"Virtual Face Illusion.zip\" file (NB: the zip-file is a supplemental materials of the manuscript and not part of the software package; it does not include the required plugins used for position and orientation tracking and any other python modules used throughout the script). In order to execute the experiment, first unpack the contents of this file to any folder (e.g. the desktop) and then double click the \"virtual-face illusion_54784_R2_052716_KM.py\" file to start the experiment. Note that the script is designed to work with the virtual reality programming environment presented here and will not work using other programs.\n\t\nUse a virtual face building program to design virtual faces with the appropriate age, race, and genders (corresponding to the participants being tested) by selecting the best fitting values on the corresponding scales of the program\nCreate two versions of each face, one with a neutral facial expression and one with a smile, by selecting the corresponding values on the corresponding scales of the program (which varies expressions by changing eye size, curvature of the mouth and some other face muscles)\nFor testing university students, create four 20-year-old virtual faces with the virtual face building program, one male face with a neutral facial expression, one male face that is smiling, one female face with a neutral facial expression, and one female face that is smiling",
    "In the virtual face building program export the faces to 3D VRML-formatted files.\nUsing the appropriate commands of the virtual reality programming environment import the created VRML files, i.e., the virtual faces, into the virtual reality environment for use during the experiment. Vary their size or scale by setting their parameters accordingly using the appropriate commands.\nFind the pre-written tracking module for the head position tracking system in the installment file of the virtual environment and import it, which allows tracking the head positions of participant. In the scripts, change the data of the head positions and determine the time point of when head positions are translated into virtual-face positions (use a 0 ms delay for synchrony conditions and a 3 s delay for asynchrony).\nFind a pre-made orientation tracker plugin in the installment file of the virtual environment and import it in the command scripts. Note that, again, the script allows introducing temporal delays with respect to the time point of when orientation changes of the participant's head are translated into orientation changes of the virtual head (use a 0 ms delay for synchrony conditions and a 3 s delay for asynchrony).\nDesign additional virtual objects (such as a virtual stick) and their motion trajectories, so they move to and from the virtual face. Set the size of the virtual object to be similar to the size of a virtual finger.\nConnect hardware and implement the saved command scripts, and then start the experiment.\nExperimental Conditions\nRun the command scripts and track the participant's head position by means of the head position tracking system and the participant's head orientation by means of a 3-DOF orientation tracker attached to a cap.",
    "Expose the participant to the virtual face for 30 s and instruct participants not to move. Once face has disappeared, have participants respond to the IOS scale (described under Data Collection) to assess how he or she perceives the relationship between him- or herself and the virtual face.\nRun the four experimental conditions (described below) in an order that is either balanced across participants or randomized. Each condition includes three phases of about 2 to 3 min each to induce the virtual-face illusion.\nNeutral/synchrony\n\t\t\nConfigure the system such that the delay between the following events is close to zero and not noticeable: (a) the movements of the real head and the corresponding movements of the virtual head in the visuo-motor correlation phase and (b) the time points of contact between the participant's real hand and the participant's real cheek and between the virtual object and the virtual head in the visuo-tactile stimulation phase.\nFor the visuo-motor correlation phase, have participants put on the cap with the attached orientation tracker. Ask them to keep moving or rotating their own head to control the position and orientation of the virtual face.\nFor the visuo-tactile stimulation phase, have participants stretch their right arm to the right and back repeatedly, to touch their right cheek, while watching the screen. The touch is only momentary: participants touch the cheek, let go and stretch their right arm to the right, and repeat for the duration of this visuo-tactile stimulations phase.",
    "On the screen, present the virtual face being repeatedly touched at the cheek by a virtual object, such as a virtual ball. The touch is (or rather hand movement in general) is synchronized with the virtual object through the motion system that can track the location of a participant's limb (e.g. hand) in 3D space, which allowed us to directly map the participant's hand movements on to the trajectory of the of the virtual object, resulting in a synchronized movement of the participant's real hand movement trajectory and the virtual object's movement trajectory. Thus when the virtual object touches the virtual avatar, this corresponds to the participant touching their own cheek.\nNeutral/asynchrony\n\t\t\nRun the procedure described under 2.3.4 after having configured the system such that the delay between the critical events is 3 s instead of close to zero.\nSmiling/synchrony\n\t\t\nRun the procedure described under 2.3.4 after having configured the system to present the smiling face instead of the face with a neutral expression.\nSmiling/asynchrony\n\t\t\nRun the procedure described under 2.3.6 after having configured the system such that the delay between the critical events is 3 s instead of close to zero.\nData Collection\nAsk the participant to fill out the questionnaire measuring sense of ownership and agency for the respective condition.\n\t\t\nInclude a questionnaire that minimally includes one or more ownership questions; use the following four:\n\t\t\t(O1) \"I felt like the face on the screen was my own face\";\n\t\t\t(O2) \"It seemed like I was looking at my own reflection in a mirror\";\n\t\t\t(O3) \"It seemed like I was sensing the movements and the touch on my face in the location where the face on the screen was\";",
    "(O4) \"It seemed like the touch I felt on my face was caused by the ball touching the face on the screen\".\nConsider including agency questions; use the following two:\n\t\t\t(A1) \"It seemed as though the movements I saw on the face on the screen was caused by my own movements\";\n\t\t\t(A2) \"The face on the screen moved just like I wanted it to, as if it was obeying my will\".\nInclude the \"Inclusion of Other in the Self\" (IOS) scale18, which is created by using a 7-point (1-7) Likert scale2 on which each score is indicated to correspond to a different degree of self-other overlap. Indicate the degree of overlap graphically through the overlap of two circles with one representing the \"Self\" and the other circle the \"Other\". Characterize the lowest score of the scale by zero-overlap of the two circles and the highest score by perfect overlap. Higher ratings thus represent a higher degree of self-other overlap.\nOptionally, include the Affect Grid19 to assess mood.\n\t\t\nCreate a 2-dimensional (valence by arousal) Likert-kind grid, in which one dimension corresponds to valence (ranging from -4 for feeling unpleasant to +4 for feeling pleasant) and the other to arousal (ranging from -4 for feeling sleepy to +4 for feeling highly aroused).\nHave participants choose one point (e.g., with a pen) that corresponds to how pleasant and how aroused they currently feel.\n\t\t\tNOTE: The questionnaires, IOS and Affect Grid appear on screen after each of the experimental phases is finished. Participants used the keyboard to respond (identical to the Virtual Hand Illusion experiment).\nOptionally, include the Alternative Uses Task (AUT)20.",
    "Ask participants to list as many possible uses for a common household item such as a newspaper. The task is performed with pen and paper. Have participants to write down as many uses for the object as they can in 5 min.\nRepeat for another object (e.g., a brick). Score the outcomes later according to fluency (number of uses), flexibility (number of categories of uses), elaboration (how much detail or explanation that is provided for the use), and originality (how unique the use is). Ensure that higher scores indicate higher divergent thinking performance for all items. Use two different scorers and ensure that the inter-scorer correlation is high. Focus on the flexibility score for further analyses, as this is the most consistent and theoretically most transparent score of the task.\nUse the AUT as an implicit (and demand-characteristic-free) measure indicating mood, as performance in this task increases with better mood21.\n\t\t\tNOTE: If the AUT is to implemented change the script such that the virtual face remains on screen, is visible to and remains under the control of the participant while they are doing the AUT."
  ],
  "subjectAreas": [
    "Behavior"
  ],
  "bigAreas": [
    "Ecology & Environmental Biology"
  ]
}