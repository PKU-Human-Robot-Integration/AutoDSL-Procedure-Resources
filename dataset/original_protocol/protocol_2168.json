{
  "id": 2284,
  "origin_website": "Cell",
  "title": "A deep learning based CT image analytics protocol to identify lung adenocarcinoma category and high-risk tumor area",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nGenerally speaking, the execution of the KDBBN project is composed of three stages, data standardization, data preprocessing, and model implementation. To illustrate these stages, we offer an illustrative example based on taking Dataset 1 as the training and test sets as well as Dataset 2 as the validation set.\nData standardization\nTiming: 1 h\nThe following steps explain the process of transforming an original dicom-formatted CT dataset into the dataset used in a deep learning model. In the original CT dataset, several images contain 4 subfigures (Figure 1[href=https://www.wicell.org#fig1]), in which only the thin-slice CT image (Figure 2[href=https://www.wicell.org#fig2]) is needed. Subfigure extraction has also been included at this stage.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig1.jpg\nFigure 1. An example CT image containing 4 subfigures\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig2.jpg\nFigure 2. Extracted subfigure\nCheck if there are some images containing 4 subfigures in the dataset, and extract the needed subfigure. Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], 3[href=https://www.wicell.org#sec6.5], and 4[href=https://www.wicell.org#sec6.7].\nOpen the ‘readDCM.py’ in the ‘./lung_code/trans_data’ folder.\nNote: The script has three custom parameters, in which the ‘path’ is the path of the original input dataset, ‘resultpath1’ and ‘resultpath2’ are two empty folders prepared for saving the outputs. There are no subordinate relationships between the three folders.\nRun the ‘readDCM.py’ after setting the custom parameters, or in command line run:\n>python readDCM.py -p [path] -r1 [resultpath1] -r2 [resultpath2]\nOpen the ‘crop4.py’ in the ‘./lung_code/trans_data’ folder. Run the ‘crop4.py’ after setting the ‘path’ the same as ‘resultpath2’, or in command line run:\n>python crop4.py -p [resultpath2]\nTransform dicom files to standardized jpg files and change the dataset division. The original dataset is divided by the patient id (Figure 3[href=https://www.wicell.org#fig3]), while the dataset used for the deep learning model should be divided according to the adenocarcinoma categories (Figure 4[href=https://www.wicell.org#fig4]). Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], 3[href=https://www.wicell.org#sec6.5], and 4[href=https://www.wicell.org#sec6.7].",
    "Open the ‘trans2jpg.py’ in the ‘./lung_code/trans_data’ folder.\nNote: The script has four custom parameters, in which the ‘metadata’ is the path of the dataset metadata, and ‘file_dir1’ is the same as ‘resultpath1’, while ‘file_dir2’ is the same as ‘resultpath2’ in step 1, and ‘re_dir’ is one empty folder prepared for saving the outputs. There is no subordinate relationship between the two folders.\nRun the ‘trans2jpg.py’ after setting the custom parameters, or in command line run:\n>python trans2jpg.py -m [metadata] -f1 [resultpath1] -f2 [resultpath2] -r [re_dir]\nNote: The standardized dataset will be located in the ‘re_dir’.\nRepeat steps 1 and 2 to standardize other datasets (e.g., validation dataset).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig3.jpg\nFigure 3. Originally organized dataset, divided by patient id\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig4.jpg\nFigure 4. Standardized dataset, classified by lung adenocarcinoma categories\nData preprocessing\nTiming: 2 h\nThese steps correspond to the preprocessing stage in the original publication (Chen et al., 2022[href=https://www.wicell.org#bib1]), including the segmentation unit and rebalancing unit. The parameters miu and miut in step 4 correspond to the parameters μ and ρ in the original publication.\nRun the three extractors in the segmentation unit (Figure 5[href=https://www.wicell.org#fig5]). Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], and 3[href=https://www.wicell.org#sec6.5].\nOpen the ‘./lung_code/preprocessing’ folder.\nOpen the ‘contour.py’ (corresponding to the black SROI images) in the folder.\nNote: The script has two custom parameters, in which ‘re_dir’ is the same as that in step 2, and ‘data_dir2’ is one empty folder prepared for saving the outputs.\nRun the ‘contour.py’ after setting the custom parameters, or in command line run:\n>python contour.py -r [re_dir] -d [data_dir2]\nOpen the ‘background.py’ (corresponding to the white SROI images) in the folder.\nNote: The script has two custom parameters, in which ‘data_dir2’ is the same as that in step b, and ‘data_dir3’ is one empty folder prepared for saving the output.",
    "Run the ‘background.py’ after setting the custom parameters, or in command line run:\n>python background.py -d2 [data_dir2] -d3 [data_dir3]\nOpen the ‘crf.py’ (corresponding to the CRF extractor) in the folder.\nNote: The script has two custom parameters, in which ‘re_dir’ is the same as that in step 2, and ‘data_dir1’ is one empty folder prepared for saving the output.\nRun the ‘crf.py’ after setting the custom parameters, or in command line run:\n>python crf.py -r [re_dir] -d1 [data_dir1]\nOpen the ‘cut.py’ (corresponding to the Crop Background extractor) in the folder.\nNote: The script has four custom parameters, in which the ‘re_dir’ is the same as that in step 2, ‘cut_dir’ is an empty folder prepared for output, while ‘miu’ and ‘miut’ are two parameters to finetune the extractor. Create three new paths ‘data_dir4’, ‘data_dir5’, and ‘data_dir6’ for three empty folders.\nRun the ‘cut.py’ after setting the ‘cut_dir’ the same as ‘data_dir4’ while setting miu as 180 and miut as 100. Or in command line run:\n>python cut.py -r [re_dir] -c [data_dir4] -m 180 -mt 100\nRun the ‘cut.py’ after setting the ‘cut_dir’ the same as ‘data_dir5’ while setting miu as 200 and miut as 100. Or in command line run:\n>python cut.py -r [re_dir] -c [data_dir4] -m 200 -mt 100\nRun the ‘cut.py’ after setting the ‘cut_dir’ the same as ‘data_dir6’ while setting miu as 230 and miut as 160. Or in command line run:\n>python cut.py -r [re_dir] -c [data_dir4] -m 230 -mt 160\nOpen the ‘mix.py’.",
    "Note: The script has seven paths, in which ‘data_dir1’, ‘data_dir2’, ‘data_dir3’, ‘data_dir4’, ‘data_dir5’ and ‘data_dir6’ are the same as those in the previous steps, and ‘target_dir’ is one empty folder prepared for saving the output. Moreover, there are six parameters in finetuning the corresponding rebalance unit in the original publication, default values have been set while the users can change them accordingly.\nRun the ‘mix.py’ after setting the custom parameters, or in command line run:\n>python mix.py -t [target_dir] -d1 [data_dir1] -d2 [data_dir2] -d3 [data_dir3] -d4 [data_dir4] -d5 [data_dir5] -d6 [data_dir6]\nNote: There are no subordinate relationships between the following folders, ‘re_dir’, ‘data_dir1’, ‘data_dir2’, ‘data_dir3’, ‘data_dir4’, ‘data_dir5’, ‘data_dir6’ and ‘target_dir’.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig5.jpg\nFigure 5. Visualization of extracted ROI from the lung CT image, for more details please refer to the original publication.\n(A) The ROI extracted by CRF extractor.\n(B) The black ROI extracted by SROI extractor.\n(C) The white ROI extracted by SROI extractor.\n(D–F) are generated by Crop-Background extractor with different miu and miut: (D) miu=180, miut>100 (E) miu=200, miut>100 (F) miu=230, miut>160.\nModel implementation\nTiming: 6 h or more\nThese steps show details of implementing the KDBBN including training and testing on the bilateral-branch network as well as validating the deep network through a knowledge distillation procedure.\nOpen the ‘./lung_code’ folder.\nTrain and test the bilateral-branch network. Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], and 3[href=https://www.wicell.org#sec6.5].\nOpen the ‘implement_concat.py’. If there exists any GPU for training, set ‘os.environ[“CUDA_VISIBLE_DEVICES”]’ as the serial number of the GPU.\nSet ‘re_dir’ and ‘target_dir’ the same as those in the steps before.\nSet ‘TRAIN’ as TRUE, and create one new path ‘filepath’ of one empty folder. Run the ‘implement_concat.py’ after setting the three paths, or in the command line run troubleshooting 5[href=https://www.wicell.org#sec6.9].\n>python implement_concat.py -r [re_dir] -t [target_dir]",
    "Note: The best bilateral-branch network model after the specific training epochs is stored in the path ‘filepath’. The training accuracy of the bilateral-branch network is obtained in this step as well.\nSet ‘TRAIN’ as FALSE, and set ‘modelpath’ the same as ‘filepath’, or any other path of the predicting model. Run the ‘implement_concat.py’ after setting the ‘modelpath’, or in the command line run troubleshooting 5[href=https://www.wicell.org#sec6.9].\n>python implement_concat.py -m [modelpath]\nNote: The testing accuracy of the chosen bilateral-branch network model and the confusion matrix of the testing dataset is obtained in this step.\nCritical: The hyperparameters, such as the ‘batch_size’, ‘nb_epoch’, ‘alpha’, etc., have been finetuned and set the best choices as the default values. Or you can set your custom values.\nValidate the bilateral-branch network through a knowledge distillation procedure. Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], and 3[href=https://www.wicell.org#sec6.5].\nOpen the ‘KD.py’.\nThere are three custom paths. Set ‘valpath’ as the path of the standardized validation dataset. The ‘unpath’ is the path of the standardized unlabeled dataset. The ‘Tmodelpath’ is the path of the best saved bilateral-branch network model in step 6.\nNote: The validation dataset can be any labeled dataset different from the training and test datasets. In the original publication, it can be Dataset 2 or 3 or the validation part in Dataset 1. The unlabeled dataset can be any unlabeled lung adenocarcinoma CT images with lesion area, in the original publication, it is Dataset 4.\nRun the ‘KD.py’ after setting the paths, or in the command line run troubleshooting 5[href=https://www.wicell.org#sec6.9].\n>python KD.py -v [valpath] -u [unpath] -m [Tmodelpath]\nNote: The validation accuracy of the KDBBN and the confusion matrix of the validation dataset are obtained in this step.\nResults display through CAM heatmap\nTiming: 1 h",
    "The following steps explain the process of visualizing the high-risk area in the CT images through CAM or Grad-CAM heatmap (Selvaraju et al., 2017[href=https://www.wicell.org#bib3]). Both CAM and Grad-CAM are designed to locate category-related areas in the image. The final convolution layer output in CAM must be connected to the GAP Layer, which restricts the model construction and may reduce the model accuracy. However, Grad-CAM does not have this restriction and it can be regarded as the generalization of CAM.\nDraw the heatmap through CAM or Grad-CAM (Figure 6[href=https://www.wicell.org#fig6]). Troubleshooting 1[href=https://www.wicell.org#sec6.1], 2[href=https://www.wicell.org#sec6.3], and 3[href=https://www.wicell.org#sec6.5].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1751-Fig6.jpg\nFigure 6. We present the original labels of the CT images which were diagnosed by skilled doctors through pathological examinations, and the corresponding probability scores for 3 categories\nThe detected high-risk area by the framework is also shown by the CAM heatmaps and detection results, for more details please refer to the original publication.\nOpen the ‘./lung_code’ folder.\nOpen the ‘cam.py’ for CAM heatmap or ‘apply_gradcam.py’ for Grad-CAM heatmap.\nThere are three paths in the scripts. Set ‘inputpath’ as the path of any standardized dataset you want to display, while ‘modelpath’ is the path of the chosen model. Create one new path ‘outputpath’ for one empty folder.\nRun the ‘cam.py’ or ‘apply_gradcam.py’ chosen in step b after setting the paths, or in command line run:\n>python apply_gradcam.py -i [inputpath] -m [modelpath] -o [outputpath]"
  ],
  "subjectAreas": [
    "Bioinformatics",
    "Systems Biology",
    "Computer Sciences",
    "Health Sciences",
    "Biotechnology And Bioengineering",
    "Cancer"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}