{
  "id": 19427,
  "origin_website": "Jove",
  "title": "An Open-Source Virtual Reality System for the Measurement of Spatial Learning in Head-Restrained Mice",
  "procedures": [
    "All procedures in this protocol were approved by the Institutional Animal Care and Use Committee of the New York State Psychiatric Institute.\nNOTE: A single-board computer is used to display a VR visual environment coordinated with the running of a head-restrained mouse on a wheel. Movement information is received as serial input from an ESP32 microcontroller reading a rotary encoder coupled to the wheel axle. The VR environment is rendered using OpenGL hardware acceleration on the Raspberry Pi GPU, which utilizes the pi3d Python 3D package for Raspberry Pi. The rendered environment is then output via a projector onto a compact wraparound parabolic screen centered on the head-restrained mouse's visual field15,16, while the behavior (e.g., licking in response to spatial rewards) is measured by a second behavior ESP32 microcontroller. The graphical software package enables the creation of virtual linear track environments consisting of repeated patterns of visual stimuli along a virtual corridor or hallway with a graphical user interface (GUI). This design is easily parameterized, thus allowing the creation of complex experiments aimed at understanding how the brain encodes places and visual cues during spatial learning (see section 4). Designs for the custom hardware components necessary for this system (i.e., the running wheel, projection screen, and head-restraint apparatus) are deposited in a public GitHub repository (https://github.com/GergelyTuri/HallPassVR). It is recommended to read the documentation of that repository along with this protocol, as the site will be updated with future enhancements of the system.\n1. Hardware setup: Construction of the running wheel, projection screen, and head-fixation apparatus",
    "NOTE: The custom components for these setups can be easily manufactured if the user has access to 3D-printing and laser-cutting equipment or may be outsourced to professional manufacturing or 3D prototyping services (e.g., eMachinehop). All the design files are provided as .STL 3D files or .DXF AutoCAD files.\nRunning wheel and behavioral setup (Figure 1)\n\tNOTE: The wheel consists of a clear acrylic cylinder (6 in diameter, 3 in width, 1/8 in thickness) centered on an axle suspended from laser-cut acrylic mounts via ball bearings. The wheel assembly is then mounted to a lightweight aluminum frame (t-slotted) and securely fastened to an optical breadboard (Figure 1C-E).\n\t\nLaser-cut the sides of the wheel and axle mounts from a 1/4 in acrylic sheet, and attach the wheel sides to the acrylic cylinder with acrylic cement. Screw the axle flange into the center of the wheel side piece.\nInsert the axle into the wheel center flange, snap the ball bearings into the axle mounts, and attach them to the vertical aluminum support bar.\nInsert the wheel axle into the mounted ball bearings, leaving 0.5-1 inch of the axle past the bearings for the attachment of the rotary encoder.\nAttach the rotary encoder mount to the end of the axle opposite the wheel, and insert the rotary encoder; then, use the shaft coupler to couple the wheel axle to the rotary encoder shaft.\nAttach the lick port to the flex arm, and then affix to the aluminum wheel frame with t-slot nuts. Use 1/16 inch tubing to connect the lick port to the solenoid valve and the valve to the water reservoir.\n\t\tNOTE: The lick port must be made of metal with a wire soldered to attach it to the capacitive touch sensing pins of the behavior ESP32.\nProjection screen",
    "NOTE: The VR screen is a small parabolic rear-projection screen (canvas size: 54 cm x 21.5 cm) based on a design developed in Christopher Harvey's laboratory15,16. The projection angle (keystone) of the LED projector used is different from that of the laser projector used previously; thus, the original design is slightly modified by mounting the unit under the screen and simplifying the mirror system (Figure 1A, B). Reading the Harvey lab's documentation along with ours is highly recommended to tailor the VR environment to the user's needs15.\n\t\nLaser-cut the projection screen sides from 1/4 in black matte acrylic sheets. Laser-cut the back projection mirror from 1/4 in mirrored acrylic.\nAssemble the projection screen frame with the aluminum bars, and laser-cut the acrylic panels.\nInsert the translucent projection screen material into the parabolic slot in the frame. Insert the rear projection mirror into the slot in the back of the projection screen frame.\nPlace an LED projector on the bottom mounting plate inside the projection screen frame. Align the projector with mounting bolts to optimize the positioning of the projected image on the parabolic rear projection screen.\nSeal the projector box unit to prevent light contamination of the optical sensors if necessary.\nHead-restraint apparatus\n\tNOTE: This head-restraint apparatus design consists of two interlocking 3D-printed manifolds for securing a metal head post (Figure 1E, F).\n\t\nUsing a high-resolution SLM 3D printer, 3D print the head post holding arms.\n\t\tNOTE: Resin-printed plastic is able to provide stable head fixation for behavior experiments; however, to achieve maximum stability for sensitive applications like single-cell recording or two-photon imaging, it is recommended to use machined metal parts (e.g., eMachineShop).\nInstall the 3D-printed head post holder onto a dual-axis goniometer with optical mounting posts so that the animal's head can be tilted to level the preparation.",
    "NOTE: This feature is indispensable for long-term in vivo imaging experiments when finding the same cell population in subsequent imaging sessions is required. Otherwise this feature can be omitted to reduce the cost of the setup.\nFabricate the head posts.\n\t\tNOTE: Two types of head posts with different complexity (and price) are deposited in the link provided in the Table of Materials along with these instructions.\n\t\t\nDepending on the experiment type, decide which head post to implement. The head bars are made of stainless steel and are generally outsourced to any local machine shop or online service (e.g., eMachineShop) for manufacturing.\n2. Setup of the electronics hardware/software (single board computer, ESP32 microcontrollers, Figure 2)\nConfigure the single-board computer.\n\tNOTE: The single-board computer included in the Table of Materials (Raspberry Pi 4B) is optimal for this setup because it has an onboard GPU to facilitate VR environment rendering and two HDMI ports for experiment control/monitoring and VR projection. Other single-board computers with these characteristics may potentially be substituted, but some of the following instructions may be specific to Raspberry Pi.\n\t\nDownload the single-board computer imager application to the PC, and install the OS (currently Raspberry Pi OS r.2021-05-07) on the microSD card (16+ GB). Insert the card, and boot the single-board computer.\nConfigure the single-board computer for the pi3d Python 3D library: (menu bar) Preferences > Raspberry Pi Configuration.\n\t\t\nClick on Display > Screen Blanking > Disable.\nClick on Interfaces > Serial Port > Enable.\nClick on Performance > GPU Memory > 256 (MB).\nUpgrade the Python image library package for pi3d: (terminal)> sudo pip3 install pillow --upgrade.\nInstall the pi3d Python 3D package for the single board computer: (terminal)> sudo pip3 install pi3d.\nIncrease the HDMI output level for the projector: (terminal)> sudo nano /boot/config.txt, uncomment config_hdmi_boost=4, save, and reboot.",
    "Download and install the integrated development environment (IDE) from arduino.cc/en/software (e.g., arduino-1.8.19-linuxarm.tar.gz), which is needed to load the code onto the rotary encoder and the behavior ESP32 microcontrollers.\nInstall ESP32 microcontroller support on the IDE:\n\t\t\nClick on File > Preferences > Additional Board Manager URLs = https://raw.githubusercontent.com/espressif/arduino-esp32/gh-pages/package_esp32_index.json\nClick on Tools > Boards > Boards Manager > ESP32 (by Espressif). Install v.2.0.0 (upload currently fails on v2.0.4).\nDownload and install the Processing IDE from https://github.com/processing/processing4/releases (e.g., processing-4.0.1-linux-arm32.tgz), which is necessary for the recording and online plotting of the mouse behavior during VR.\n\t\tNOTE: The Arduino and Processing environments may be run on a separate PC from the VR single-board computer if desired.\nSet up the rotary encoder ESP32 connections.\n\tNOTE: The rotary encoder coupled to the wheel axle measures the wheel rotation with mouse locomotion, which is counted with an ESP32 microcontroller. The position changes are then sent to the single-board computer GPIO serial port to control the movement through the virtual environment using the graphical software package, as well as to the behavior ESP32 to control the reward zones (Figure 2).\n\t\nConnect the wires between the rotary encoder component and the rotary ESP32. Rotary encoders generally have four wires: +, GND, A and B (two digital lines for quadrature encoders). Connect these via jumper wires to ESP32 3.3 V, GND, 25, 26 (in the case of the attached code).\nConnect the serial RX/TX wires between the rotary ESP32 and the behavior ESP32. Make a simple two-wire connection between the rotary ESP32 Serial0 RX/TX (receive/transmit) and the Serial2 port of the behavior ESP32 (TX/RX, pins 17, 16; see Serial2 port on the right of OMwSmall PCB). This will carry movement information from the rotary encoder to the behavior setup for spatial zones such as reward zones.",
    "Connect the serial RX/TX wires between the rotary ESP32 and the single-board computer GPIO (or direct USB connection). Make a two-wire connection between the single-board computer GPIO pins 14, 15 (RX/TX) and the rotary ESP32 Serial2 (TX/RX, pins 17, 16). This will carry movement information from the rotary encoder to the graphical software package running on the single-board computer.\n\t\tNOTE: This step is only necessary if the rotary ESP32 is not connected via a USB (i.e., it is a GPIO serial connection at \"/dev/ttyS0\"), but the HallPassVR_wired.py code must otherwise be modified to use \"/dev/ttyUSB0\". This hardwired connection will be replaced with a wireless Bluetooth connection in future versions.\nPlug the rotary ESP32 USB into the single-board computer USB (or other PC running the IDE) to upload the initial rotary encoder code.\nSet up the behavior ESP32 connections with the behavioral hardware (via OpenMaze PCB).\n\tNOTE: The behavior ESP32 microcontroller will control all the non-VR animal interactions (delivering non-VR stimuli and rewards, detecting mouse licks), which are connected through a general PCB \"breakout board\" for the ESP32, \"OMwSmall\", designs of which are available through the OpenMaze website (www.openmaze.org). The PCB contains the electronic components necessary for driving the electromechanical components, such as the solenoid valves used to deliver liquid rewards.\n\t\nConnect the 12 V liquid solenoid valve to the ULN2803 IC output on the far left of the OMwSmall PCB (pin 12 in the example setup and code). This IC gates 12 V power to the reward solenoid valve, controlled by a GPIO output on the behavior ESP32 microcontroller.",
    "Connect the lick port to the ESP32 touch input (e.g., T0, GPIO4 in the example code). The ESP32 has built-in capacitive touch sensing on specific pins, which the behavior ESP32 code uses to detect the mouse's licking of the attached metal lick port during the VR behavior.\nConnect the serial RX/TX wires between the behavior ESP32 Serial2 (pins 16, 17) and rotary encoder ESP32 Serial0 (see step 2.2.2).\nPlug the USB into the single-board computer's USB port (or other PC) to upload new programs to the behavior ESP32 for different experimental paradigms (e.g., number/location of reward zones) and to capture behavior data using the included Processing sketch.\nPlug the 12 V DC wall adapter into the 2.1 mm barrel jack connector on the behavior ESP32 OMwSmall PCB to provide the power for the reward solenoid valve.\nPlug the single-board computer's HDMI #2 output into the projector HDMI port; this will carry the VR environment rendered by the single-board computer GPU to the projection screen.\n(optional) Connect the synchronization wire (pin 26) to a neural imaging or electrophysiological recording setup. A 3.3 V transistor-transistor-logic (TTL) signal will be sent every 5 s to align the systems with near-millisecond precision.\nSet up the software: Load the firmware/software onto the rotary encoder ESP32 (Figure 2B) and behavior ESP32 (Figure 2E) using the IDE, and download the VR Python software onto the single-board computer. See https://github.com/GergelyTuri/HallPassVR/software.\n\t\nPlug the rotary encoder ESP32 into the single-board computer's USB port first-this will automatically be named \"/dev/ttyUSB0\" by the OS.\nLoad the rotary encoder code: Open the file RotaryEncoder_Esp32_VR.ino in the IDE, and then select the ESP32 under Tools > Boards > ESP32 Dev Module. Select the ESP32 port by clicking Tools > Port > /dev/ttyUSB0, and then click on Upload.",
    "Plug the behavior ESP32 into the single-board computer's USB port next-this will be named \"/dev/ttyUSB1\"by the OS.\nLoad the behavior sequence code onto the behavior ESP32 (IDE, ESP32 Dev Module already selected), then click on Tools > Port > /dev/ttyUSB1, and click on Upload: wheel_VR_behavior.ino.\nTest the serial connections by selecting the serial port for each ESP32 in the IDE (Tools > Port > /dev/ttyUSB0, or /dev/ttyUSB1) and then clicking on Tools > Serial Monitor (baud rate: 115,200) to observe the serial output from the rotary board (USB0) or the behavior board (USB1). Rotate the wheel to see a raw movement output from the rotary ESP32 on USB0 or formatted movement output from the behavior ESP32 on USB1.\nDownload the graphical software package Python code from https://github.com/GergelyTuri/HallPassVR/tree/master/software/HallPassVR (to /home/pi/Documents). This folder contains all the files necessary for running the graphical software package if the pi3d Python3 package was installed correctly earlier (step 2.1).\n3. Running and testing the graphical software package\nNOTE: Run the graphical software package GUI to initiate a VR linear track environment, calibrate the distances on the VR software and behavior ESP32 code, and test the acquisition and online plotting of the mouse's running and licking behavior with the included Processing language sketch.\nOpen the terminal window in the single-board computer, and navigate to the HallPassVR folder (terminal:> cd /home/pi/Documents/HallPassVR/HallPassVR_Wired)\nRun the VR GUI: (terminal)> python3 HallPassVR_GUI.py (the GUI window will open, Figure 3A).\nGraphical software GUI\n\t\nSelect and add four elements (images) from the listbox (or select the pre-stored pattern below, and then click on Upload) for each of the three patterns along the track, and then click on Generate.\n\t\tNOTE: New image files (.jpeg) can be placed in the folder HallPassVR/HallPassVR_wired/images/ELEMENTS before the GUI is run.",
    "Select floor and ceiling images from the dropdown menus, and set the length of the track as 2 m for this example code (it must equal the trackLength in millimeters [mm] in the behavior ESP32 code and Processing code).\nName this pattern if desired (it will be stored in HallPassVR_wired/images/PATH_HIST).\nClick the Start button (wait until the VR window starts before clicking elsewhere). The VR environment will appear on Screen #2 (projection screen, Figure 3B, C).\nRun the Processing sketch to acquire and plot the behavioral data/movement.\n\t\nOpen VRwheel_RecGraphSerialTxt.pde in the Processing IDE.\nChange the animal = \"yourMouseNumber\"; variable, and set sessionMinutes equal to the length of the behavioral session in minutes.\nClick on the Run button on the Processing IDE.\nCheck the Processing plot window, which should show the current mouse position on the virtual linear track as the wheel rotates, along with the reward zones and running histograms of the licks, laps, and rewards updated every 30 s (Figure 3D). Advance the running wheel by hand to simulate the mouse running for testing, or use a test mouse for the initial setup.\nClick on the plot window, and press the q key on the keyboard to stop acquiring behavioral data. A text file of the behavioral events and times (usually <2 MB in size per session) and an image of the final plot window (.png) is saved when sessionMinutes has elapsed or the user presses the q key to quit.\n\t\tNOTE: Due to the small size of the output .txt files, it is estimated that at least several thousand behavior recordings can be stored on the single-board computer's SD card. Data files can be saved to a thumb drive for subsequent analysis, or if connected to a local network, the data can be managed remotely.",
    "Calibrate the behavior track length with the VR track length.\n\t\nAdvance the wheel by hand while observing the VR corridor and mouse position (on the Processing plot). If the VR corridor ends before/after the mouse reaches the end of the behavior plot, increase/decrease the VR track length incrementally (HallPassVR_wired.py, corridor_length_default, in centimeters [cm]) until the track resets simultaneously in the two systems.\n\t\t​NOTE: The code is currently calibrated for a 6 inch diameter running wheel using a 256-position quadrature rotary encoder, so the user may have to alter the VR (HallPassVR_wired.py, corridor_length_default, in centimeters [cm]) and behavior code (wheel_VR_behavior.ino, trackLength, in millimeters [mm]) to account for other configurations. The behavioral position is, however, reset on each VR lap to maintain correspondence between the systems.\n4. Mouse training and spatial learning behavior\nNOTE: The mice are implanted for head fixation, habituated to head restraint, and then trained to run on the wheel and lick consistently for liquid rewards progressively (\"random foraging\"). Mice that achieve consistent running and licking are then trained on a spatial hidden reward task using the VR environment, in which a single reward zone is presented following a visual cue on the virtual linear track. Spatial learning is then measured as increased licking selectivity for positions immediately prior to the reward zone.\nHead post implantation surgery: This procedure is described in detail elsewhere in this journal and in others, so refer to this literature for specific instructions7,17,18,19,20,21.\nWater schedule",
    "Perform water restriction 24 hours prior to first handling (see below), and allow ad libitum water consumption following each session of habituation or head-restrained behavior. Decrease the time of water availability gradually over three days during habituation to around 5 minutes, and adjust the amount for individual mice such that their body weight does not fall below 80% of their pre-restriction weight. Monitor the weight of each animal daily and also observe the condition of each mouse for signs of dehydration22. Mice that are not able to maintain 80% of their pre-restriction body weight or appear dehydrated should be removed from the study and given free water availability.\n\t\tNOTE: Water restriction is necessary to motivate the mice to run on the wheel using liquid rewards, as well as to use spatial licking as an indication of learned locations along the track. Institutional guidelines may differ on specific instructions for this procedure, so the user must consult their individual institutional animal care committees to assure animal health and welfare during water restriction.\nHandling: Handle the implanted mice daily to habituate them to human contact, following which limited ad libitum water may be administered as a reinforcement (1-5 min/day, 2 days to 1 week).\nHabituation to the head restraint\n\t\nHabituate the mice to the head restraint for increasing amounts of time by placing them in the head restraint apparatus while rewarding them with occasional drops of water to reduce the stress of head fixation.",
    "Start with 5 min of head fixation, and increase the duration by 5 min increments daily until the mice are able to tolerate fixation for up to 30 min. Remove the mice from the fixation apparatus if they appear to be struggling or moving very little. However, mice generally begin running on the wheel spontaneously within several sessions, which means they are ready for the next stage of training.\n\t\tNOTE: Mice that repeatedly struggle under head restraint or do not run and lick for rewards should be regressed to earlier stages of training and removed from the study if they fail to progress for three such remedial cycles (see Table 1).\nRun/lick training (random foraging)\n\tNOTE: To perform the spatial learning task in the VR environment, the mice must first learn to run on the wheel and lick consistently for occasional rewards. The progression in the operant behavioral parameters is controlled via the behavior ESP32 microcontroller.\n\t\nRandom foraging with non-operant rewards\n\t\t\nRun the graphical software GUI program with a path of arbitrary visual elements (user choice, see step 3.3).\nUpload the behavior program to the behavior ESP32 with multiple non-operant rewards (code variables: isOperant=0, numRew=4, isRandRew=1) to condition the mice to run and lick. Run the mice in 20-30 min sessions until the mice run for at least 20 laps per session and lick for rewards presented in random locations (one to four sessions).\nRandom foraging with operant rewards on alternate laps\n\t\t\nUpload the behavior program with altOpt=1 (alternating operant/non-operant laps), and train the mice until they lick for both non-operant and operant reward zones (one to four sessions).\nFully operant random foraging",
    "Upload the behavior program with four operant random reward zones (behavior ESP32 code variables: isOperant=1, numRew=4, isRandRew=1). By the end of this training step, the mice should be running consistently and performing test licks over the entire track length (one to four sessions; Figure 4A).\nSpatial learning\n\tNOTE: Perform a spatial learning experiment with a single hidden reward zone some distance away from a single visual cue by selecting a 2 m long hallway with dark panels along the track and a single high-contrast visual stimulus panel in the middle as a visual cue (0.9-1.1 m position), analogous to recent experiments with spatial olfactory cues20. Mice are required to lick at a reward zone (at a 1.5-1.8 m position) located a distance away from the visual cue in the virtual linear track environment.\n\t\nRun the graphical software program with a path of a dark hallway with a single visual cue in the center (e.g., chessboard, see step 3.3, Figure 3A).\nUpload the behavior program with a single hidden reward zone to the behavior ESP32 (behavior ESP32 code variables: isOperant=1, isRandRew=0​, numRew=1, rewPosArr[]= {1500}).\nGently place the mouse in the head-fixation apparatus, adjust the lick spout to a location just anterior to the mouse's mouth, and position the mouse wheel into the center of the projection screen zone. Ensure that the head of the mouse is ~12-15 cm away from the screen after the final adjustments.\nSet the animal's name in the Processing sketch, and then press run in the Processing IDE to start acquiring and plotting the behavioral data (see step 3.4).\nRun the mouse for 30 min sessions with a single hidden reward zone and single visual cue VR hallway.",
    "Offline: download the .txt data file from the Processing sketch folder and analyze the spatial licking behavior (e.g., in Matlab with the included files procVRbehav.m and vrLickByLap.m).\n\t\tNOTE: The mice should initially perform test licks over the entire virtual track (\"random foraging\") and then begin to lick selectively only near the reward location following the VR visual cue (Figure 4).\nSubscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Neuroscience"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}