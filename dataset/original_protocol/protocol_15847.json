{
  "id": 19673,
  "origin_website": "Wiley",
  "title": "Bioinformatic Analysis to Investigate Metaproteome Composition Using Trans-Proteomic Pipeline",
  "procedures": [
    "Here we describe how to access and download various data resources required for analysis, including the collection of MS data, reference data, and taxonomic information, from the most common publicly available databases.\nThe Proteomics IDEntifications (PRIDE) database is one of the most common MS data repositories, and while other specialized databases also exist based on their disease classification, such as the Clinical Proteomic Tumor Analysis Consortium (CPTAC) data portal (https://proteomics.cancer.gov/data-portal[href=https://proteomics.cancer.gov/data-portal]), which holds data from various cancer tumors (Edwards et al., 2015[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0010]), these will not be discussed here. Of note, annotation quality varies by dataset, as does the methodology implemented in data acquisition, which should be taken into consideration when planning your own analyses. In particular, it is recommended to select datasets where mechanical disruption (e.g., ultrasonication, bead beating) and detergent (e.g., SDS) have been used for protein extraction (Zhang & Figeys, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0034]).",
    "The UniProt (https://www.uniprot.org/proteomes/[href=https://www.uniprot.org/proteomes/]) database is one of the largest repositories for protein reference sequences (Bateman, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0004]). UniProt has the advantage of providing a comprehensive, high-quality, and freely accessible resource of protein sequence and functional information. It also incorporates both the manually annotated and curated SwissProt resource as well as the computationally analyzed TrEMBL data, awaiting full manual annotation. UniProt is also regularly updated and contains microbial (bacterial, viral as well as fungal) proteomes. While the use of existing reference data has the advantage of being easily generalized to most experimental designs, it comes at the expense of database specificity. Other methods to prepare reference databases include the use of prior sample metagenome/metatranscriptome data for construction, or of alternative reference database sources that are specific to certain biological niches. Several examples of such human and murine microbiome databases, which can substitute for UniProt, have been provided in Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-tbl-0001]. The databases selected for inclusion here all have protein FASTA sequence files readily available for reference database construction. While the level of protein annotation for the mouse reference gut microbiome (MRGM; Kim et al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0019]) is somewhat lacking, it still remains one of the more comprehensive murine gut reference sources. While bacterial reference data is readily available from these databases, there currently remain limited options for fungal and viral components of the microbiome in these repositories, representing potential avenues for future research and development. Additionally, it is important to note that as new organisms continue to be sequenced, reference databases need to be updated regularly. To construct up-to-date and niche reference databases, alternative methods of database construction, comprehensively reviewed elsewhere (Blakeley-Ruiz & Kleiner, 2022[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0005]), may need to be adopted.\nTable 1.\n                Alternative Targeted Microbiome Databases/Datasets for Metaproteomic Reference Sequences\ntable:\n﻿0,1,2\nDatabase,Utility,Limitations",
    "\"Expanded human oral microbiome database (Escapa et al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0011])\",- Curated information on bacteria present in the human mouth and aerodigestive tract based on metagenomic sequencing - Corresponding NCBI taxonomy IDs provided for identified species - Periodically updated with corresponding release notes,- Lack of information regarding fungal and viral species\n\"Unified human gastrointestinal protein catalog (Almeida et al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0001])\",\"- Curated information on prokaryotic microbiota of the human gut based on metagenomic sequencing - Protein coding sequences are available for individual organisms, or collectively clustered at 100, 95, 90, and 50% amino acid identity - Periodically updated approximately every 6–12 months\",- Lack of information regarding fungal and viral species - Not readily able to map species information to existing NCBI taxonomy IDs\n\"Mouse oral microbiome database (Joseph et al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0017])\",- Curated information on bacteria present in the murine oral cavity based on metagenomic sequencing - Corresponding NCBI taxonomy IDs provided for identified species,\"- Lack of information regarding fungal and viral species - Evidence of updates, though schedule/frequency is unclear\"\n\"Mouse reference gut microbiome (Kim et al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0019])\",\"- Curated information on bacteria present in mouse gut based on metagenomic sequencing - Protein coding sequences available clustered at 100%, 95%, 90%, 70%, and 50% amino acid identity with lowest common ancestor identified - Functional annotation based on EggNOG database (Huerta-Cepas et al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0015])\",\"- Lack of information regarding fungal and viral species - Lack of protein annotation in .faa FASTA files - Not readily able to map species information to existing NCBI taxonomy IDs - Evidence of updates, though schedule/frequency is unclear\"\nFinally, the National Center for Biotechnology Information (NCBI) taxonomy database (https://www.ncbi.nlm.nih.gov/taxonomy/[href=https://www.ncbi.nlm.nih.gov/taxonomy/]) is one of the largest sources of taxonomic information (Schoch et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0029]), and also is used by UniProt for taxonomy.\nThe current protocol topic will be broken down into the following:",
    "1.Collection of raw MS data from PRIDE\n2.Collection of microbial proteomes from UniProt for reference database creation\n3.Collection of taxonomic information from NCBI.\nNecessary Resources\nHardware\nAny computer system with Internet access and a browser. Depending on the size of the datasets to be downloaded, additional storage may be required in the form of external hard drives.\nSoftware\nFor File Transfer Protocol (FTP) downloads, depending on the organization of the data, the freely available software Filezilla may be used to streamline the download of many files. Filezilla is freely available at https://filezilla-project.org/[href=https://filezilla-project.org/] and is supported on Windows, Mac OS, and Linux. Additionally, software capable of extracting files from .gz file formats is required. 7-zip is a free option available at https://www.7-zip.org/[href=https://www.7-zip.org/]. Depending on the size of the microbial database being implemented, additional software capable of reading and editing large text files, such as EmEditor (https://www.emeditor.com[href=https://www.emeditor.com]), is also recommended.\nCollection of raw MS data from PRIDE\n1. Using your Internet browser, navigate to the PRIDE database (https://www.ebi.ac.uk/pride/[href=https://www.ebi.ac.uk/pride/]) (Martens et al., 2005[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0023]). Enter a search query to call relevant datasets. If a publication has uploaded its dataset to PRIDE, this can instead be called by entering its PRIDE identifier. This is often in the format “PXD”, followed by a 6-digit identifier (e.g., PXD123456).\n2. Clicking on an entry will bring up an information summary about the dataset. Scrolling down to the bottom of the page will show the project files that are available for download. The project files from PXD007232 (https://www.ebi.ac.uk/pride/archive/projects/PXD007232[href=https://www.ebi.ac.uk/pride/archive/projects/PXD007232]), an MS study on oral cancer (Carnielli et al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0006]), are shown as an example in Figure 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0001].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/d5cb5860-fd7b-4b6a-9b42-8401abde28f9/cpz1506-fig-0001-m.jpg</p>\nFigure 1",
    "Screenshot of PRIDE PXD007232 database project files. (A) Input field to search for specific files within a project. (B) Link to open the FTP page of a project dataset. (C) Link to initiate download of an individual file. (D) Drop-down menu to change the number of files displayed per page.\n3. Individual files from a project dataset can be selectively downloaded by clicking “FTP” next to the particular file. When there are a large number of files to download, it is recommended to click “Project FTP” and utilize Filezilla FTP download software, freely available from https://filezilla-project.org/[href=https://filezilla-project.org/].\n4. Clicking “Project FTP” will open a new window with a list of all available files for a particular project. Open Filezilla and copy the website URL into the “Host” field in Filezilla to establish a remote connection with the PRIDE FTP server (Fig. 2A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0002]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/dc53cdf4-1ca1-44a6-b27a-90086be51681/cpz1506-fig-0002-m.jpg</p>\nFigure 2\nFilezilla software with remote connection to PRIDE database FTP server project PXD007232. (A) Input field to connect to host; when copying URLs, ensure that the input begins with “ftp”. (B) Local directory pane. Navigate to the desired location to which the files will be downloaded; in this case files will be downloaded to the folder “MS_dataset”. (C) Remote site pane; shows the directory layout of the FTP server that has been connected to. (D) Remote file directory. Shows files in the currently selected folder from (C). Multiple files can be selected, and then actions can be performed by right-clicking.",
    "5. Once the connection is established, use the local navigation pane to select the destination to which files will be downloaded (Fig. 2B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0002]). If the URL has correctly been entered from step 4, there should be no need to use the remote site navigation (Fig. 2C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0002]), and the project files should be visible (Fig. 2D[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0002]). After selecting all desired files, right-click and select “Download” to begin downloading the selected files. This can be more convenient when a large number of MS data files are required to be downloaded.\nCollection of microbial proteomes from UniProt for reference database creation\n6. Open your preferred Internet browser and navigate to UniProt proteomes (https://www.uniprot.org/proteomes[href=https://www.uniprot.org/proteomes]).\n7. On the left-hand sidebar under “Filter by”, select “Reference proteomes” (Fig. 3A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0003]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/fc70e95a-da40-40f1-a7f3-e03b81f08f3a/cpz1506-fig-0003-m.jpg</p>\nFigure 3\nScreenshot of UniProt. (A) Sidebar selection to filter by reference proteomes. (B) Sidebar selection to filter by either Bacteria or Viruses, depending on the desired microbes to be searched for. (C) Sidebar selection to show all proteins mapped to the currently selected species’ proteomes.\n8. On the same left-hand sidebar, select “Bacteria” or “Viruses” to filter by the desired microbe. Here, “Viruses” will be used as an example (Fig. 3B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0003]).\n9. With all viral reference proteomes now selected, on the left-hand sidebar under “Map to”, select “UniProtKB”. This will show all protein entries mapping to the selected viral proteomes. Selecting “Download” from above the results window will allow all returned entries to be downloaded into a single FASTA file, which can be used as the reference database. For the example reference database, all viral reference proteomes were downloaded in FASTA format, totaling 594,570 protein sequences (515,513 viral proteins, 79,057 human proteins; files downloaded 15th Feb 2022).",
    "10. Of note, as fungi are grouped in Eukaryota on the sidebar, a separate query, “taxonomy:Fungi [4751]”, must be entered into the search bar to obtain fungal sequences. After this, select “Reference proteomes” and map to “UniProtKB” as previously described before downloading as FASTA.\n11. Assuming that metaproteomic analysis is being performed on human samples, in order to improve FDR, a copy of the human reference proteome should also be downloaded (available from https://www.uniprot.org/proteomes/UP000005640[href=https://www.uniprot.org/proteomes/UP000005640]) and appended to the previously downloaded FASTA file reference database using a copy and paste command. If an alternate host species is being used (e.g., mouse), then the corresponding reference proteome should be appended instead.\nIf the database file exceeds approximately 1 GB in file size, terminal commands or additional reading/editing software such as EmEditor may be required to perform this step. If the final database file exceeds 2 GB in size, it is recommended to split this database into multiple smaller files of approximately 2 GB using software such as EmEditor.\nCollection of taxonomic information from NCBI\n12. The National Center for Biotechnology Information (NCBI) taxonomy database, on which UniProt's taxonomy is based, can be downloaded from https://ftp.ncbi.nih.gov/pub/taxonomy/new_taxdump/[href=https://ftp.ncbi.nih.gov/pub/taxonomy/new_taxdump/]. Download one of either “new_taxdump.tar.Z”, “new_taxdump.tar.gz”, or “new_taxdump.zip”; all three contain the same data but are intended to provide convenience in unpacking on different operating environments. Opening the “taxdump_readme.txt” will provide more information.\n13. Extract “rankedlineage.dmp” from the downloaded file. The .dmp file contains the taxonomic information for all organisms in the NCBI database, and will be used in Basic Protocol 3",
    "Here we describe the use of various TPP modules for the analysis of the downloaded MS data using the constructed reference database. This includes the conversion of the MS data, database searching, and then peptide and protein validation. A breakdown of the following workflows will be provided in this protocol:\n         \n1.Starting up TPP\n2.Conversion of proprietary file formats using msconvert\n3.Database searching using Comet\n4.Peptide validation using PeptideProphet\n5.Protein inference using ProteinProphet.\nNecessary Resources\nHardware\nA computer system with minimum 16 GB RAM and i7 or newer processor is recommended for the analysis, to reduce processing time. Depending on the size of the dataset to be analyzed, additional memory storage may also be required.\nSoftware\nTPP software is used for the MS data analysis. It can be freely downloaded from the Seattle Proteome Center website (http://tools.proteomecenter.org/wiki/index.php?title=Software:TPP[href=http://tools.proteomecenter.org/wiki/index.php?title=Software:TPP]) and is supported on Windows, Mac OS, and Linux. The current protocol will assume TPP (version 6.0.0) is being run using the Windows OS.\nStarting up TPP\n1. Prior to running the TPP software, the requisite files need to be moved into the correct TPP directories. Navigate to the TPP folder (default installation path is to the C: drive).\n2. Move the downloaded RAW MS files into the TPP/data folder and the FASTA format reference database to TPP/data/dbase.\nHere, RAW MS data files of oral cancer stroma from the invasive tumor front (Carnielli et al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0006]) will be analyzed against a reference database containing UniProt viral proteomes. Information regarding these files can be viewed in Supplementary File 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#support-information-section] in Supporting Information[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#support-information-section].",
    "3. From the Desktop, open TPP by double clicking the “Trans-Proteomic Pipeline” shortcut icon (created by default during installation). TPP will open in a new Internet browser window. Alternatively, open your preferred internet browser and enter the URL http://localhost:10401/tpp/[href=http://localhost:10401/tpp/]; this assumes default settings during the TPP installation.\n4. Select “Petunia”, which is the main TPP graphical user interface (GUI). When prompted to login, enter “guest”, which acts as both the default user name and password.\n5. From the main TPP home page, links to the primary tools for MS analysis can be found under “TPP Tools” on the menu bar (Fig. 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0004]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/1856f653-7606-4cc5-b01c-672fe33eed87/cpz1506-fig-0004-m.jpg</p>\nFigure 4\nScreenshot of TPP home page and TPP Tools menu. (A) Menu bar for TPP, providing access back to the home page (Home), to software modules (TPP Tools), additional optional software modules (External Tools), accounts and password settings (Account), and a log of all completed and in-progress analyses (Jobs). (B) Link to the msconvert module for file conversion to mzML. (C) Link to Comet for database searching. (D) Link to PeptideProphet for the analysis and validation of peptides. (E) Link to ProteinProphet for protein analysis and inference.\nConversion of proprietary file formats using msconvert\n6. First, the RAW MS data files need to be converted to mzML format. Select the “Generate mzML” option under TPP Tools to open the msconvert module.\n7. Select the input file format (default is Thermo RAW) and then select “Add Files” to open a new navigation screen. Navigate to your TPP data directory and select the RAW MS files to be converted. Doing so will return you to the previous screen.",
    "8. Click “convert to mzML” to begin the job. This process may take several minutes to several hours depending on the number of files to convert and specifications of your computer.\nOutput files\n9. Once the msconvert module has finished running, .mzML files corresponding to each data file input will be produced. The output files will appear in the same directory as the input files.\nDatabase searching using Comet\n10. From the TPP home page, select “Comet Search” under TPP Tools to open the Comet database search module. In the Comet screen, there will be three prompts to choose mzML input files, a Comet parameters file, and a sequence database (Fig. 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0005]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/5604598e-950f-45a4-961e-9d14174378b7/cpz1506-fig-0005-m.jpg</p>\nFigure 5\nScreenshot of TPP Comet database search tool. (A) Field for input files; multiple files can be uploaded, provided they are in mzML format. (B) Field for Comet parameters; settings for the Comet parameters are stored as separate .param format files that have to be input into Comet. These .param files control settings such as peptide mass tolerance and residue variable modifications. Only one .param file can be input at a time. (C) Field for sequence database input; a single FASTA-format reference database is required as input for the search to run against.\n11. Under “choose mzML input files”, click “Add Files” to open up a new directory navigation screen. Navigate to the .mzMLfiles produced by msconvert and select them as input files. This will return you to the Comet search page.",
    "12. Select “Add Files” under “choose Comet parameter files” to similarly open directory navigation. A default set of Comet parameters can be found in TPP/data/param/comet.params. This can be edited by selecting the “Params” link next to the file. From here, various parameters of the database search can be adjusted such as peptide mass tolerance, decoy search, missed cleavages, and amino acid variable modifications. The Comet search parameters used for the example analysis can be found in Supplementary File 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#support-information-section] (see Supporting Information[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#support-information-section]).\nSaving a new params file will take you from the Comet database search to the TPP Files page; select TPP Tools from the menu bar to navigate back to the Comet search page.\n13. Selecting “Add Files” under “Choose a sequence database” will open directory navigation. Navigate to the reference database prepared earlier (TPP/data/dbase) and select it.\nThe example viral reference database prepared in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-prot-0001] containing 594,570 protein sequences was used here.\n14. With the input files, Comet parameters, and reference database selected, the “Run Comet Search” button will appear at the bottom of the page. Clicking this will begin the database search. Checking the “Preview” box next to this will show the command issued to TPP, but will not run the actual analysis. Depending on the number of files to be analyzed, size of the database, search parameters, and computer hardware being used, the database search may take several minutes to several hours. Using the example data here, the Comet database search required approximately 3 hr to complete.\n15. While running, the progress of the analysis can be viewed from the Jobs tab on the menu bar (Fig. 4A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0004]).\nOutput files",
    "16. Once the analysis is complete, a pep.xml file will be produced for each .mzML file in the same directory as the Comet parameters file used. The results of each analysis, prior to peptide validation and protein inference, can be viewed using the File option on the TPP menu bar.\nPeptide validation using PeptideProphet\n17. From the TPP Home page, using the menu bar select “TPP Tools” > “Analyze Peptides” to access the PeptideProphet module (Fig. 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0006]; Ma, Vitek, & Nesvizhskii, 2012[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0022]). There will be multiple fields corresponding to file input, output options, and PeptideProphet settings, followed by options for additional software modules including iProphet, PTMProphet, XPRESS, ASAPRatio, and Libra. While these modules have useful applications depending on the analysis (iProphet for concatenation of multiple database searching, PTMProphet for post-translational modifications, XPRESS and ASAPRatio for relative quantitation of isotopically labeled data, and Libra for isobaric-tagged quantitation), they will not be explored here.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4f221394-87de-4a58-8812-aee23589c031/cpz1506-fig-0006-m.jpg</p>\nFigure 6\nScreenshot of TPP PeptideProphet module. Options for additional software modules iProphet, PTMProphet, XPRESS, ASAPRatio, and Libra are not shown. (A) “File(s) to analyze” section for file input; selecting Add Files will open directory navigation. (B) Output file and filter options; allows the user to define the name and directory of output files, as well as filtering of results based on probability and peptide length. (C) PeptideProphet options: “Run PeptideProphet” and “Use accurate mass binning” are enabled by default, but the latter depends on whether high-accuracy data is being used. Additional parameters can be adjusted at the user's discretion. (D) A parameter of PeptideProphet; enabling this option defines which database sequences are decoys, as denoted by a user-defined prefix. While PeptideProphet can be run without decoy sequences, enabling this option can improve modeling and FDR estimation.",
    "18. In the “Files to analyze” section, selecting “Add Files” will open up the directory navigator. Locate and select the pep.xml file outputs from the Comet search for input into PeptideProphet.\n19. Under “Output file and filter options”, select the directory where the output files will be produced, and the output filename (when changing the output filename, ensure that it ends with the .pep.xml extension).\nAdditional filters are available to remove results below a specific PeptideProphet probability cut-off and/or peptide length cut-off. Additionally, if multiple files have been input, an option will exist to either process them separately or to concatenate the analysis and produce a single results output. For the example data, a minimum peptide length cut-off of nine amino acid residues was implemented, while all other settings were left as default.\n20. Under “PeptideProphet Options”, additional parameters can be modified. The “Run PeptideProphet” and “Use accurate mass binning, using: PPM” settings are enabled by default. If a decoy search was used during the Comet database search, enable the “Use decoy hits to pin down the negative distribution” option and input the decoy prefix used into the “Known decoy protein names begin with” field.\n21. Click Run XInteract to initiate the analysis. While generally requiring less processing time than the Comet search, depending on the size and number of files to be analyzed, as well as hardware specifications, this step may take a few minutes to a few hours. Using the example data, this analysis required approximately 40 min to complete.\nOutput files\n22. After completion of the analysis, a new pep.xml file (or files) will be produced in the designated output directory. The results can be viewed from the File option on the TPP menu bar.",
    "23. Selecting the results will open the PepXML viewer (Fig. 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0007]). Export the results data in .xls format by selecting “Other Actions” > “Export Spreadsheet”. This will produce a new .xls file in the same directory as the pep.XML file.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/3e4dc06a-e5f1-494b-8a4d-223393789d9c/cpz1506-fig-0007-m.jpg</p>\nFigure 7\nScreenshot of example PeptideProphet results in pepXML viewer. Several different functionalities can be accessed from the menu bar. (A) Provides general information about the results including total number of spectra, unique peptides, and single hits. (B) Alters the data displayed, including number of rows shown per page, sorting of data (e.g., by spectrum, probability, expect score), and highlighting of specific peptide/protein/spectrum text. (C) Alters information to be displayed—the default columns shown are probability, spectrum, expect score, ions, peptide sequence, parent protein, number of parent proteins (num_prots), and calculated mass (calc_neutral_pep_mass). (D) Options to apply filter criteria to result data. (E) Perform additional actions including export of data in .xls format, statistical data, and viewing of FDR estimates.\n24. In order to determine the FDR for a specified PeptideProphet probability score, navigate to “Other Actions” > “Additional Analysis Info”. This will open a new window containing sensitivity and error rate modeling (Fig. 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0008]). Selecting the Sens/Error Tables tab will show the corresponding predicted FDR (labeled as “Error_Rate”) for a given PeptideProphet probability score.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/fba54aab-f514-42a7-8abb-9d8dbca685c5/cpz1506-fig-0008-m.jpg</p>\nFigure 8",
    "Screenshot of sensitivity/error tables from pepXML viewer. (A) Additional tabs to show predicted sensitivity and error rate graphs (Models Charts), information regarding these models (Learned Models), details of the MS run and search parameters (MS Runs), sensitivity and error tables (Sens/Error Tables; currently pictured), and details of the PeptideProphet run (Run Options). (B) Predicted sensitivity and FDR (Error_Rate) based on PeptideProphet probability scores, including breakdown of specific charge state species, allowing FDR to be determined based on a probability score cut-off. (C) PeptideProphet probability scores based on FDR, allowing selection of a minimum probability score at a specified FDR.\nProtein inference using ProteinProphet\nProteinProphet can also be run immediately after PeptideProphet by selecting “Run ProteinProphet afterwards” under “PeptideProphet” options.\n25. From the TPP home page menu bar, select “TPP Tools” > “Analyze Proteins” to access the ProteinProphet module (Fig. 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0009]; Nesvizhskii, Keller, Kolker, & Aebersold, 2003[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0025]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/8f985f13-1010-4cdd-8cc8-122c3833897e/cpz1506-fig-0009-m.jpg</p>\nFigure 9\nScreenshot of ProteinProphet module. (A) Input field: files added to the analysis (done by selecting “Add Files”) will appear here. (B) Output options to determine output directory and filename. (C) ProteinProphet options that can be enabled if specific quantitation software modules were used in upstream analyses (QUANTIC, XPRESS, ASAPRatio, and Libra). Additional options can be enabled to exclude results with probability scores of 0 and to report protein length and calculated protein molecular weight. (D) Initiate ProteinProphet analysis: the “Run ProteinProphet” button will appear once input files have been added.\n26. Multiple fields will be visible, corresponding to file input, output options, and the ProteinProphet run options.\n27. In the “Files to analyze” section, selecting “Add Files” will again open up the directory navigator. Locate and select the pep.xml file outputs from the PeptideProphet analysis for input into ProteinProphet.",
    "28. Under “Specify output file name and location”, the output directory can be changed, as well as the output filename. When changing the output filename, ensure it ends with the .prot.xml extension.\n29. Under “ProteinProphet options”, several settings can be adjusted if specific quantitation software applications were used during upstream analysis. By default these parameters are unused.\n30. Once input files have been selected, the “Run ProteinProphet” button will appear at the bottom under “Run protein analysis!”. Clicking this will initiate the analysis and bring the user to a new screen showing job progress. While generally requiring less processing time than the Comet search, again depending on the size and number of files to be analyzed, as well as hardware specifications, this step may take a few minutes to a few hours. Using the example data, this analysis step required approximately 2 min to complete.\nOutput files\n31. After completion of the analysis, a new prot.xml file (or files) will be produced in the designated output directory. The results can be viewed from the File option on the TPP menu bar.\n32. Selecting the results will open the ProtXML viewer (Fig. 10[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0010]). The results data can be exported in .tsv format by selecting “File & Info” > “Export TSV”. This will produce a new .tsv file in the same directory as the prot.xml file. The ProteinProphet output of example data using a peptide mass tolerance of 5 ppm can be viewed in Supplementary File 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#support-information-section] (see Supporting Information[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#support-information-section]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/7ea30b02-ba97-4302-9907-6ec663881d14/cpz1506-fig-0010-m.jpg</p>\nFigure 10",
    "Screenshot of example results in ProtXML viewer. (A) Results for all proteins, showing ProteinProphet probability scores, number of peptide-spectrum matches (PSMs), sequence coverage, and spectrum ID percentage. (B) After selecting an entry in (A) by clicking the corresponding number in the “#” column, the user will be brought to this tab, which will display more detailed peptide information. (C) Displays general file information and also presents options for export of data in .tsv format and visualization using ProteoGrapher with .json files and PloTPP using .data files. (D) Options to apply filters based on probability, protein name, etc., and also to sort data according to various parameters. (E) Opens up the results analysis and modeling in a new window, showing information for predicted sensitivity and error rates.\n33. In order to determine the FDR for a specified ProteinProphet probability score, click “Models” in the ProtXML viewer menu bar. This will open a new window containing sensitivity and error rate modeling (Fig. 11[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0011]). Selecting the Sens/Error Tables tab will show the corresponding predicted FDR (labeled as “Error_Rate”) for a given ProteinProphet probability score.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/eb0bf39e-a683-4f48-b124-a95e629bd8ee/cpz1506-fig-0011-m.jpg</p>\nFigure 11\nScreenshot of sensitivity/error tables from protXML viewer. (A) Additional tabs to show predicted sensitivity and error rate graphs (Models Charts), information regarding these models (Learned Models), sensitivity and error tables (Sens/Error Tables; currently pictured), and details of the ProteinProphet run (Run Options). (B) Predicted sensitivity and FDR (Error_Rate) based on ProteinProphet probability score for FDR determination based on a probability score cut-off. (C) ProteinProphet probability scores based on FDR, allowing selection of a minimum probability score at a specified FDR.",
    "R, commonly used with the RStudio Integrated Development Environment (IDE), is a powerful programming language developed primarily for statistical computing and has many applications in the sciences for data analysis and visualization (Ihaka & Gentleman, 1996[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0016]). This protocol describes the secondary analysis of the TPP ProteinProphet .tsv outputs using RStudio for data processing, and visualization of taxonomic information from inferred microbial species using sunburst plots. The following protocol will be broken up into the following sections:\n         \n1.Creating a project directory\n2.Import and filtering of data\n3.Appending taxonomy information and visualization using sunburst plot.\nNecessary Resources\nHardware\nSee Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-prot-0001]\nSoftware\nBoth R and RStudio are software are required, and can be freely downloaded from https://cran.rstudio.com/[href=https://cran.rstudio.com/] and https://www.rstudio.com/products/rstudio/download/[href=https://www.rstudio.com/products/rstudio/download/], respectively.\nIn addition, several R packages are required for this protocol: ‘tidyverse’, ‘plotly’, and ‘data.table’. These can be installed from the console window in RStudio using the “install.packages()” function. Alternatively these can be installed from the menu bar by navigating to “Tools” > “Install Packages…”. R version 4.1.2. and RStudio (ver. 2021.09.01+372) have been used for the current protocol.\nCreating a project directory\n1. Open RStudio and use the menu bar to navigate to “File” > “New Project”. From here you will have the option of creating a new project from a new or from an existing directory.\n2. After creating the new project, move the ProteinProphet .tsv output files (from Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-prot-0003]) into the new project directory. This allows for ease of access, as opening the project automatically defines the working directory as the same as the project directory. An .Rproj-format file should be present in this folder.",
    "3. With the project open, from the menu bar, navigate to “File” > “New File” > “R Script”, or use the shortcut Ctrl+Shift+N to create a new script for subsequent coding. With a new script open, four panes should be visible (see Fig. 12[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0012]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/24ef0de8-2044-4201-ab53-79715ba707dd/cpz1506-fig-0012-m.jpg</p>\nFigure 12\nScreenshot of RStudio interface (Dark mode) with project directory set up. The pane layout can be adjusted through the menu bar by navigating to “Tools” > “Global Options” > “Pane Layout”. (A) RStudio menu bar for access to various functions and options. (B) Project bar for opening and closing projects. If using a project directory, the directory name will be displayed here. (C) Source editor: scripts will be displayed here as well as certain objects that can be displayed such as data frames. (D) Workspace browser: displays variables and objects in the current workspace environment. History can also be viewed from here. (E) Console window: will show commands as they are run as well as their outputs, if any. Commands can be run from the source editor or typed directly into the console. (F) Files window: shows the files in the current project directory. Additional displays include plot outputs, packages, and help vignettes.\nImport and filtering of data\n4. Load the packages required in the protocol (tidyverse, data.table, plotly) using the function “library()”.\n5. Import the .tsv ProteinProphet outputs and assign them to a variable (e.g., “raw_data”) using the “read_tsv()” function. The imported data can be viewed in the source editor by selecting it from the workspace browse (Fig. 12D[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0012]) or by using the function “View()”.",
    "6. Once the data has been imported, filtering criteria can be applied in the source editor to remove human entries, protein groups, and low-probability proteins. This can be performed using the filter() function. Example code is shown below, assuming the reference database used was created using UniProt reference data, with ‘#’ denoting comments:\n         \n`filter_data` <- `raw_data` %>%\n#remove HUMAN entries\nfilter(!str_detect(protein, \"HUMAN\")) %>%\n#remove protein groups\nfilter(!str_detect(protein, \"tr\\\\|.*tr\\\\|.*\")) %>%\nfilter(!str_detect(protein, \"sp\\\\|.*tr\\\\|.*\")) %>%\nfilter(!str_detect(protein, \"tr\\\\|.*sp\\\\|.*\")) %>%\nfilter(!str_detect(protein, \"sp\\\\|.*sp\\\\|.*\")) %>%\n#remove low prob proteins\nfilter(`protein probability`>= 0.95)\nWhile the ProteinProphet probability threshold shown here is 0.95, this value will differ for each analysis and should be based on the desired FDR threshold in consultation with the ProteinProphet sensitivity and error tables. For the analysis of the example data using a 5 ppm peptide mass tolerance, a probability threshold of 0.96 was used to give FDR = 0.01.\n7. New separate columns with species id information can then also be created in this output using the “str_extract()” function. The example code shown below assumes that UniProt sequences were used to construct the reference database, giving the new columns “tax_name” and “tax_id”:\n         \n`filter_data`$tax_name <- str_extract(`filter_data`$\"protein description\", \"(?<=OS\\\\=).*(?= OX\\\\=)\")\n`filter_data`$tax_id <- str_extract(`5ppm_filter`$\"protein description\",\"(?<=OX\\\\=)[:digit:]*\")\n`filter_data`$tax_id <- as.numeric(`filter_data`$tax_id)\n8. This analyzed data of high-probability proteins and their associated organism can then be exported using the “write.csv()” function for further interpretation.\nAppending of taxonomy information and visualization using sunburst plot\n9. Move the “rankedlineage.dmp” file (here, downloaded 4th Feb 2022), extracted in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-prot-0001], to the R project directory.\n10. Import the .dmp file into R using the “read_tsv()” function. Additional cleaning of the database is performed using the “select()” and “colnames()” functions before mapping the output from step 7 above to the taxonomy database with the “merge()” function:\n         \n# import data and remove unused columns",
    "db <- read_tsv(\"rankedlineage.dmp\") %>% select(seq(1,by=2,len=10))\n# label database column names with corresponding taxon level\ncolnames(db) <- c(\"tax_id\", \"tax_name\", \"species\", \"genus\", \"family\", \"order\", \"class\", \"phylum\", \"kingdom\", \"superkingdom\")\n# map output data to taxonomy database\n`taxa_data` <- merge(db,`filter_data`, by=\"tax_id\")\nThe examples here assume that a UniProt reference database was used in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-prot-0003]. Taxon identifiers (tax_id) remain mostly consistent between Uniprot and NCBI; however, there may be occasional inconsistencies. Make sure to compare the number of rows for consistency following the merge; in this example we compare “filter_data” and “taxa_data” and find that 17 out of 2098 entries (<1% of total data) are not successfully merged. Using the “anti_join()” function can allow for unmerged rows to be identified, after which they can be manually checked based on “tax_name” and other info. Following manual checking, only two rows were unable to be merged.\nIf an alternate reference database was used for the primary MS analysis (i.e., not from Uniprot), an alternative taxonomy database other than NCBI may be required, and differences in the data structure may require modifications to the example code shown here. For example, the expanded Human Oral Microbiome Database has its own taxon database (https://www.homd.org/download#taxon[href=https://www.homd.org/download#taxon]) that contains additional columns not present in the NCBI taxonomy database.\n11. Convert the data frame into a data table using the “as.data.table()” function. Additionally, assign N/A values as “Unclassified” (i.e., no classification present in the taxonomy database being used) and create a “count” column to view how many protein hits map back to a particular organism. Using this “counts” column, a threshold can be implemented to increase confidence in species inference based on desired acceptance criteria (i.e., only keep species identified by two or more proteins).\n         \n`data_table` <- as.data.table(`taxa_data`)\n`data_table`[,count := .N, by = . (tax_name)]\n`data_table` <- `data_table` %>% filter(count >= 2)",
    "`data_table`[is.na(`data_table`)] <- \"Unclassified\"\n`taxa_data` <- taxa_data[,c(\"kingdom\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"tax_name\")]\n12. Prior to sunburst plot visualization using the plotly package, the data has to be converted into an amenable format. This can be performed by defining the function “as.sunburst”, shown in the code below. Execute the function on the output from step 12 above, and then use the “plot_ly()” function to visualize the data:\n         \n#Define function as.sunburst\nas.sunburst <- function(dataframe, value_column = NULL, add_root = FALSE){\nrequire(data.table)\nnames_dataframe <- names(dataframe)\nif(is.data.table(dataframe)){\ndatatable <- copy(dataframe)\n} else {\ndatatable <- data.table(dataframe, stringsAsFactors = FALSE)\n}\nif(add_root){\ndatatable[, root := \"Total\"]\n}\nnames_datatable <- names(datatable)\nhierarchy_cols <- setdiff(names_datatable, value_column)\ndatatable[, (hierarchy_cols) := lapply(.SD, as.factor), .SDcols = hierarchy_cols]\nif(is.null(value_column) && add_root){\nsetcolorder(datatable, c(\"root\", names_dataframe))\n} else if(!is.null(value_column) && !add_root) {\nsetnames(datatable, value_column, \"values\", skip_absent=TRUE)\nsetcolorder(datatable, c(setdiff(names_dataframe, value_column), \"values\"))\n} else if(!is.null(value_column) && add_root) {\nsetnames(datatable, value_column, \"values\", skip_absent=TRUE)\nsetcolorder(datatable, c(\"root\", setdiff(names_dataframe, value_column), \"values\"))\n}\nhierarchy_list <- list()\nfor(i in seq_along(hierarchy_cols)){\ncurrent_cols <- names_datatable[1:i]\nif(is.null(value_column)){\ncurrent_datatable <- unique(datatable[, ..current_cols][, values := .N, by = current_cols], by = current_cols)\n} else {\ncurrent_datatable <- datatable[, lapply(.SD, sum, na.rm = TRUE), by=current_cols, .SDcols = \"values\"]\n}\nsetnames(current_datatable, length(current_cols), \"labels\")\nhierarchy_list[[i]] <- current_datatable\n}\nhierarchy_datatable <- rbindlist(hierarchy_list, use.names = TRUE, fill = TRUE)\nparent_columns <- setdiff(names(hierarchy_datatable), c(\"labels\", \"values\", value_column))\nhierarchy_datatable[, parents := apply(.SD, 1, function(x){fifelse(all(is.na(x)), yes = NA_character_, no = paste(x[!is.na(x)], sep = \":\", collapse = \" - \"))}), .SDcols = parent_columns]\nhierarchy_datatable[, ids := apply(.SD, 1, function(x){paste(x[!is.na(x)], collapse = \" - \")}), .SDcols = c(\"parents\", \"labels\")]\nhierarchy_datatable[, c(parent_columns) := NULL]\nreturn(hierarchy_datatable)\n}\n#Execute function on data\n`plot_data`<- as.sunburst(`data_table`)\n#Create sunburst plot\nplot_ly(data=`plot_data`, ids=∼ids, labels=∼labels, parents=∼parents, values=∼values, type='sunburst', branchvalues='total', textinfo='label+percent root', maxdepth=5)",
    "Multiple arguments in the function can be modified to change the appearance of the plot, the most common being the “maxdepth” argument to determine the maximum taxon levels shown at a given time, as well as the “textinfo” argument, which display any combination of “label”, “value”, “percent root”, “percent parent”, and “current path,” among others.\nOutput files\n13. Executing the above commands will generate a sunburst plot in the Viewer pane (Fig. 12F[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0012]) with the taxonomic information of species inferred from the MS data. The sunburst plot is interactive, where specific section selections magnify them and their lower levels (Fig. 13[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0013]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/11af8ec3-1afa-4889-b53d-cddbab00fb62/cpz1506-fig-0013-m.jpg</p>\nFigure 13\nSunburst plots of viral taxonomy inferred by two or more proteins (5 ppm peptide mass tolerance, FDR = 0.01) in oral cancer tissue stroma from the invasive tumor front using example data (Carnielli et al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-bib-0006]). From inner to outermost radials: Kingdom, Phylum, Class, Order, Family. Colors represent species’ Kingdom while percentage values represent taxa proportion within a given classification level. (A) Whole taxonomy of inferred species. (B) Magnified view of Kingdom Orthornavirae inferred species by selecting the “Orthornavirae” section from (A) in the RStudio Viewer pane.\n14. A static image of the sunburst plot can be exported by selecting “Export” > “Save as Image” from the Viewer pane toolbar. Alternatively, an image can be created using the bmp(), png(), jpg(), or tiff() functions, depending on the desired image format. The exported image will appear in the project working directory.\n15. In addition to creating a static image, an interactive figure can be exported as a .html document by assigning the plot to a variable and using the function “htmlwidgets::saveWidget()”. Example code is shown below:\n         \nsunburst <- plot_ly(data=plot_data, ids=∼ids, labels=∼labels, parents=∼parents, values=∼values, type='sunburst', branchvalues='total', textinfo='label+percent root', maxdepth=5)\nhtmlwidgets::saveWidget(as_widget(sunburst),\"supp_file4_sunburst.html\")",
    "The R script used here for the analysis of the example data can be viewed in Supplementary File 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#support-information-section] (see Supporting Information[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#support-information-section]). Additionally, an interactive version of Figure 13[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#cpz1506-fig-0013] can also be viewed in Supplementary File 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.506#support-information-section]."
  ],
  "subjectAreas": [
    "Bioinformatics"
  ],
  "bigAreas": [
    "Bioinformatics & Computational Biology"
  ]
}