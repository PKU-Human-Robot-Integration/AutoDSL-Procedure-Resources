{
  "id": 4162,
  "origin_website": "Cell",
  "title": "Uncovering structured responses of neural populations recorded from macaque monkeys with linear support vector machines",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nPreparation of the data\nTiming: 4 h\nAs a first step, we choose the classification method and prepare the data for classification. Correct pre-processing and the choice of the classification method have an important influence on the results and the quality of analysis. These choices should be led by the scientific question to be answered, and by considering the dataset constraints.\nNote that the indicated Timings are a rough estimation of the computational time needed to perform computations on a server with 10 cores. Timings do not include the time needed for implementation of the source code.\nChoose the classification method.\nOur choice of the classifier is the linear Support Vector Machine (SVM). The SVM is used for binary classification and regression problems on multivariate data (i.e., data with multiple variables). The SVM is particularly efficient on datasets with many variables and a limited number of samples, since it has optimal generalization performance, and achieves good performance also on non-Gaussian distributions (Belousov et al., 2002[href=https://www.wicell.org#bib3]; Meyer et al., 2003[href=https://www.wicell.org#bib20]). These are precisely the properties of neural data that we typically obtain from in-vivo experiments. In in-vivo experiments, the number of samples (corresponding to the number of trials) in a recording session is typically low (<100) and the number of variables (corresponding to the number of neurons recorded in parallel) can be moderate to high, typically on the order of tens to hundreds of units. Moreover, distributions of spike counts across neurons are often not Gaussian. Another reason why the linear SVM is an attractive choice for the analysis of electrophysiological data is its ability to associate the activity of each neuron with its weight in the classification task in a straightforward manner.",
    "Note, however, that the choice of a linear model is not always appropriate. A linear classification model is searching for a linear separation of data points belonging to classes A and B, and a linear separation might not always be optimal. Troubleshooting 1[href=https://www.wicell.org#sec6.1]\nChoose which conditions to decode.\nWe decode conditions match vs. non-match, using only trials with correct behavioral performance. Since these conditions differ in both the stimulus presented and the subsequent choice of the animal, we are decoding a mixed variable “stimulus+choice” (see Koren et al., 2020a[href=https://www.wicell.org#bib15]).\nChoose a relevant time window for averaging neural responses.\nIn the dataset considered here, the information about matching and non-matching of the target and the test stimuli only becomes available when the test stimulus is shown. We therefore expect that the neural activity contains class-related information during and after the presentation of the test stimulus, and not before. We use a time window of 0–500 ms after the onset of the test stimulus. This time window covers the entire presentation of the test stimulus (0–300 ms), and, in addition, 200 ms after the offset of the test stimulus, when potential choice-related information is expected to unfold. Troubleshooting 2[href=https://www.wicell.org#sec6.3]\nSet binary labels for the two classes.\nWe can choose arbitrary labels, for example yj = 1 for trials in condition match and yj = −1 for trials in condition non-match.\nDecide for a cross-validation (CV) method and split the data samples and the class labels into training and validation sets. We use Monte-Carlo CV, where the data is randomly split into non-overlapping training and test sets. Troubleshooting 3[href=https://www.wicell.org#sec6.5]\nFor each Monte-Carlo CV set, the order of trials is randomly permuted, without repetition. The new trial order is applied to data samples    x j    and to labels    y j   .",
    "Trials are then split into the training set (80%) and the test set (the remaining 20%). This is again relevant to both data samples and labels.\nSteps in 6.a and 6.b are iterated NCV-times. We use NCV=100 cross-validations in every recording session.\nCompute z-scored spike counts in the chosen time window. Troubleshooting 4[href=https://www.wicell.org#sec6.7]\nCompute the spike count in the chosen time window. For the neuron   n   in trial   j  , the spike count is the following:\n   S  n j   =  ∑ k    f  n j    (  t k  )    \nwhere    f  n j    (  t k  )    is a binary vector of zeros and ones (the spike train) of the neuron n in trial j.\nCompute the Z-scored spike counts for each neuron individually. For each neuron, compute the mean and the standard deviation of the spike count across trials, using only trials from the training set. Then, z-score the spike counts in the training as well as in the test set as follows:\n   x  n j   =    S  n j   −   S ¯  n    S T D  (  S n  )     \nIf N is the number of neurons in the recording session, the vector of N z-scored spike counts in trial   j   is one sample for the classifier (Figure 1[href=https://www.wicell.org#fig1]A) and can be written as follows:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/925-Fig1.jpg\nFigure 1. Preparing the data for binary classification\n(A) Schema of a dataset from one recording session. We measure the z-scored spike count of multiple neurons in many trials. Each trial belongs to one of the two binary classes, “match” or “non-match”. For the classifier, trials are samples, and neurons are features.",
    "(B) Schema of the division of the dataset in the training set and the test set (gray rectangles). Within the training set, another division of the data is done for the selection of the C-parameter (red rectangles).\n   x j  =  [   x  1 j   ,  x  2 j   , …  x  N j    ]   \nwhere   T  denotes the transpose.\nCritical: For a control, it is important to repeat the analysis for additional time windows where we expect that no significant effect occurs, for example the target and/or the delay period. This increases the validity of the chosen method and the credibility of results.\nNote: The purpose of the decoding model here is to extract from the data information that is useful for classification, and not to build a generative model of the neural activity. See Part 4 and Troubleshooting 6[href=https://www.wicell.org#sec6.11] for further information and discussion on this topic.\nNote: As we use all the available trials in the experiment, the number of trials in each condition is imbalanced. The cross-validation does not change this since it merely permutes the order of trials. However, the imbalance in the number of trials, unless extreme, is not a problem if we use a performance measure that takes into account this bias (see Part 2).\nAlternatives: There are many alternative classification models that can be applied to neuroscience data, such as logistic regression (Alkan et al., 2005[href=https://www.wicell.org#bib1]) or Generalized Linear Models (GLMs, see Pillow et al., 2011[href=https://www.wicell.org#bib26]). For an introduction into explainable machine learning methods and some more examples, see Molnar (2019)[href=https://www.wicell.org#bib21].\nComputation of the classification model and its performance\nTiming: 6 h",
    "When working with linear SVMs, we utilize neurons as features and trials as samples for the classifier (Figure 1[href=https://www.wicell.org#fig1]A). While features can be correlated, samples are required to be independent and therefore uncorrelated. Troubleshooting 5[href=https://www.wicell.org#sec6.9]\nCompute the parameters of the classification model on the training set.\nThe linear SVM has one hyperparameter, the regularizer, also commonly referred to as the C-parameter. The C-parameter must be optimized, utilizing exclusively the training set (Figure 1[href=https://www.wicell.org#fig1]B). First, define a range of C parameters to be tested, for example, C = [0.001, 0.01, 0.1, 1, 10, 100, 1000]. If most of the data is best fit with one of suggested C-parameters, we may refine the range around plausible values. However, once the range is defined, it cannot be changed anymore. In particular, the same range should be used for all recording sessions.\nChoose the optimal C-parameter with 10-fold cross-validation (CV). Split the training data into 10 folds. 9 folds are used to compute the classification model while the remaining fold (the validation set for the C-parameter) is used to compute the performance of the model (Figure 1[href=https://www.wicell.org#fig1]B). Iterate the procedure such that every fold is the validation set, and average across iterations. By testing the entire range of C-parameters, we can choose the C-parameter that gives the highest performance (optimal C-parameter).\nNow, all the training data is gathered to compute a new classification model, utilizing the optimal C-parameter.\nTest the performance of the model on the test set.\nAfter the classification model has been trained, we assess its performance on yet unseen samples, the test set. As a performance measure, we use the balanced accuracy (BAC), which corrects for the imbalance in the number of trials (samples) across the two categories (“match” and “non-match”). The balanced accuracy is defined as follows:",
    "B A C =  1 2   (    T P   T P + F N   +   T N   T N + F P    )   \n where TP, TN, FP and FN are the number of true positive, true negative, false positive and false negative test samples, respectively. Let us think of the condition “match” as “positive” and “non-match” as a “negative”. We can see that the balanced accuracy computes the proportion of correct classifications of the condition “match” among all classifications that are “match” (first term in the parenthesis on the right-hand-side), and the proportion of correct classifications of “non-match” among all classifications that are “non-match” (second term on the right-hand side).\nBalanced accuracy, which is a single number between 0 and 1, is calculated in each cross-validation run. After collecting results across cross-validation runs, we report the balanced accuracy that is averaged across cross-validations.\nCritical: It is crucial that the model is tested on yet unseen data. Testing the performance on the training set will usually give much higher performance than on a held-out test set. Testing on a held-out test set is putting to the test the capability of the classification model to generalize to yet unseen samples.\nNote: In a binary classification task, the balanced accuracy of 0.5 is the chance level performance, meaning that test samples have been classified correctly as often as incorrectly. The balanced accuracy of 1, on the other hand, indicates that all test samples have been classified correctly.\nAlternatives: We opted for the 10-fold CV method for determining the C-parameter because it is faster than the Monte-Carlo CV. Computation of the optimal C-parameter is nested in the training set/validation set CV and is for this reason computationally expensive. With small datasets, however, Monte-Carlo CV is an alternative method for the selection of the C-parameter.",
    "Alternatives: We chose to measure the classification performance with balanced accuracy, a measure that accounts for the imbalance for the data classes. As an alternative, one can balance the classes with stratified k-fold cross-validation, a method that ensures equal participation of classes in each fold (Zeng and Martinez, 2000[href=https://www.wicell.org#bib33]).\nAssessment of significance of classification performance\nTiming: 24 h\nSignificance of classification performance is assessed with a permutation test. The permutation test is a non-parametric test that does not make any assumption on data, but instead creates the distribution of results of random models from the data itself, against which the true result is tested.\nCompute the performance of models trained on randomly permuted class labels.\nGather class labels of the training data in a vector   y  . Randomly permute the entries of the vector   y  , without repetition.\nTrain and test the classification model as in steps 7 and 8 but utilizing permuted class labels. The permutation of class labels is iterated Nperm-times, giving Nperm values for the balanced accuracy (BACperm). We used Nperm=1000.\nCompute the p-value by ranking the balanced accuracy among the distribution of balanced accuracies of models with permuted class labels.\nThe p-value is the proportion of the BACperm that are bigger than the BAC. Note that this is a one-sided test. If the p-value is smaller than the significance level α, the BAC is significant. We used a significance level α=0.05.\nWhen performing more than one permutation test, the significance level must be corrected for multiple testing. We used the Bonferroni correction, where the significance level is divided by the number of tests, α corrected=α/Ntest.",
    "Critical: The precision of the p-value depends on the number of permutations we use. If we use Nperm=1000 permutations, and if one instance of BACperm is larger than the BAC, our result is significant with the p-value of p=0.001. If no instance of BACperm is larger than the BAC, our p-value is p<0.001. The precision of the p-value is therefore limited by the number of permutations, more precisely, with the p-value of p<1/ Nperm. Even if all BACperm are larger than BAC, stating that the p-value is zero is incorrect.\nNote: As we randomly permute class labels, the classification model cannot find the underlying structure of the data, since the association between data samples and class label has been randomized by the permutation. However, due to the limited number of samples, the BACperm will not be exactly 0.5, but will take values around 0.5 in different permutation cycles (Figure 2[href=https://www.wicell.org#fig2]A).\nAlternatives: The permutation test is a very rigorous way of testing the significance of an effect. On the down side, the permutation test takes a long time to run, since it requires that the entire model is re-computed and re-evaluated Nperm - times. An alternative significance test can be performed with a parametric test, where one needs to be careful that the criteria of the parametric test are met.\nDecoding weights and functionally relevant subgroups\nTiming: 6 h",
    "If the balanced accuracy is significant, we can proceed to study the role of single neurons in the classification task. With the linear SVM, neurons are features of the model (Figure 1[href=https://www.wicell.org#fig1]A), and the activity of each neuron is associated with one feature weight. Considering feature weights of N neurons observed in parallel, we define a weight vector   w  . The weight vector determines the orientation of the separating boundary that separates data points in conditions A and B (Figure 3[href=https://www.wicell.org#fig3]). Troubleshooting 6[href=https://www.wicell.org#sec6.11]\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/925-Fig2.jpg\nFigure 2. Balanced accuracy of models with permuted class labels\n(A) Left: BAC of models with permutation of class labels (BACperm) in one recording session. We used 1000 random permutations. Right: Distribution of BAC shown on the left. The magenta line marks the mean of the distribution.\n(B) Left: Distribution of BACperm (black) and the BAC of the regular model (red) during target. The p-value is p=0.418. Right: Same as on the left, but during test. The p-value is p<0.001, since none of the BACperm is bigger than the BAC.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/925-Fig3.jpg\nFigure 3. Schema of the separating boundary in a toy model with two neurons\nThe separating boundary is an (N-1)-dimensional plane (a hyperplane) in the space of inputs of N neurons. In case of N=2 neurons, the separating boundary is a line. The separating boundary optimally divides the space of inputs from category A and B (yellow and blue circles). The separating boundary is fully determined by the offset from the origin (here the offset is 0), and the weight vector w that determines its orientation. The weight vector has N entries, one for each neuron. The change in sign of the weight of a particular neuron changes the orientation of the separating boundary.",
    "The weight vector can easily be calculated numerically from the classification model. To calculate the weight vector, we utilize a subset of data points that are the most informative for the classification task, the so-called “support vectors” (hence the name of the classifier). A support vector is a data point in the N-dimensional data space (in a particular trial) that the classifier has used for determining the separation boundary. Support vectors are data points that lie on or close to the separation boundary.\nGather all data and compute the classification model.\nWhen estimating feature weights of the model, no training/validation is required. Apply steps 6 and 7 on the complete dataset.\nCompute feature weights of the model.\nAs we compute the classification model, the classification function has several available outputs. We call the following outputs from the classification function: support vectors    α j   , indices of support vectors, and Lagrange multipliers    λ j   .\nCompute weights according to the following expression:\n  w =  ∑  j = 1  Q    λ j   y j   x j    \nwhere    y j    is the class label in trial   j   and    x j    is the vector of z-scored spike counts in trial   j  .   Q   is the number of support vectors, and it is smaller than the number of trials.\nRepeat the procedure for every cross-validation run. Average weights across cross-validations.\nNormalize weight vectors and gather results across recording sessions.\nNormalize the weight vector in every recording session as follows:\n   w ˜  =  w /  ‖ w ‖    \nwhere    ‖ w ‖  =    w 1 2  +  w 2 2  + … ,  w N 2      is the L2 norm.\nGather results across recording sessions.\nUsing properties of weights, define functional subgroups.",
    "An important property of the weight is its sign. Neurons with positive and negative weights have the opposite effect on the separation boundary, as they are pulling the separation boundary in opposite directions (Figure 3[href=https://www.wicell.org#fig3]). Another important property is the amplitude of the weight. The larger the amplitude of the weight of a particular neuron, the higher the importance of the neuron for classification is.\nNote: The range of weights depends on the C-parameter, and the C-parameter differs across recording sessions. If we want to compare the amplitude of weights across neurons from different recording sessions, we are required to normalize the weight vector.\nNote: If all samples are correctly separated by the separation boundary, we say that the data is linearly separable. However, the model can function even in the absence of linear separability. In neural recordings, linear separability is unlikely. even samples in the training data can sometimes lie on the wrong side of the separating boundary. Such samples are called “slack points.\" The C-parameter determines how strongly slack points are considered in the computation of the separating boundary (see Belousov et al., 2002[href=https://www.wicell.org#bib3]; Vapnik and Vapnik, 1998[href=https://www.wicell.org#bib27]).",
    "Alternatives: There are alternative ways of determining effects of a particular feature, one example of which is a Shapely value (see Molnar, 2019[href=https://www.wicell.org#bib21]). The Shapley value is the average marginal contribution of a feature (or set of features) across all possible sets of features. Unfortunately, the computation of the Shapley value is computationally expensive, to the point that it is only feasible if the number of simultaneously recorded units is small. We need to account for all possible subpopulations of neurons, which quickly leads to a combinatorial explosion. In the case of our data with up to 17 parallel units, systematic computation of Shapley values for single neurons was not feasible. However, computing the Shapley value might be feasible in datasets with a small number of parallel units (e.g., 5 units).\nThe effect of heterogeneity across neurons on performance\nTiming: 12 h\nIf all neurons contribute the same quantity of information to the classifier and respond to the stimulus class the same way (e.g., by increasing in the firing rate for the stimulus “match”), we can call the observed population homogeneous. Within such a population, and assuming no correlations in the data, all neurons would have the same decoding weight. A homogeneous network is an abstract concept that would hardly ever happen in the real-world scenarios, where we rather expect that neurons differ in the way they activate for a given class (e.g., some neurons increase the firing rate for the class “match” while others decrease the firing rate for the class “non-match”). We also expect that neurons differ in the quantity of information that they convey to the classifier. In such a case, every neuron has a different weight, and we call the population heterogeneous.",
    "Here, we inquire how the heterogeneity across neurons contributes to the performance of the classification model. To this end, we compute the performance of the model that is homogeneous across neurons (Figure 4[href=https://www.wicell.org#fig4]A) and compare it to the performance of the regular model. We remove the heterogeneity across neurons by permuting the activity across neurons, independently in each trial. Such a procedure destroys the activation patterns that are a source of information for the classifier, and we therefore expect a decrease in classification performance.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/925-Fig4.jpg\nFigure 4. Creating homogeneous neural ensembles\n(A) By swapping the activity across all the neurons from the population (left) or between neurons with the same sign of the weight (right), we create homogeneous neural ensembles. Figure reprinted with permission from Koren et al., 2020a[href=https://www.wicell.org#bib15].\n(B) Homogeneous ensembles are created by randomly permuting the neural activity across neurons in the same group, independently in each trial.\nAssess the effect of heterogeneity across neurons.\nRandomly permute, without repetition, the elements of the vector of activations    x j    across all simultaneously recorded neurons. The vector of activations is permuted independently in every trial (Figure 4[href=https://www.wicell.org#fig4]B), so that the identity of neurons is mixed differently across trials. Note that the class labels do not change.\nRepeat steps 5–8. In step 8, we get the balanced accuracy of the homogeneous model, BACh.\nCompare BACh with BAC by computing the difference, Δh = BAC-BACh, in every recording session.\nAssess the effect of heterogeneity within functional subpopulations. A population is defined by the same sign of the weight.\nRandomly permute, without repetition, the vector of activations    x j   , but only across units with the same sign of the weight. As in step 15.a, the permutation is done on every trial independently, while labels stay untouched.",
    "Repeat steps 5–8. In step 8, we get the balanced accuracy of the model that is homogeneous within groups, BACh.groups.\nCompare BACh,groups with BAC by computing the difference,\nΔh, groups = BAC-BACh, groups, in every recording session.\nCritical: It is crucial that all the steps are performed on the data from the same recording session. Gathering data from different recording sessions before we start with the step 14 will create artificial assemblies with artificial covariance and will give misleading results. However, we can evaluate the general effect of heterogeneity by gathering across recording sessions final results, Δh and Δh, groups..\nNote: Comparing the performance of the homogeneous models with the regular model, we assess how much the heterogeneity across neurons contributes to the performance of the regular model. Note that removing heterogeneity is always expected to decrease the performance of the model. However, it is interesting to consider how much of the prediction accuracy of the regular model is due to heterogeneity across neurons, and to compare results from steps 15 and 16 (see Expected Outcomes)."
  ],
  "subjectAreas": [
    "Neuroscience",
    "Model Organisms",
    "Bioinformatics"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}