{
  "id": 2092,
  "origin_website": "Cell",
  "title": "Protocol for non-invasive assessment of spontaneous movements of group-housed animals using remote video monitoring",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nHere, we describe the implementation of this protocol for remote video monitoring and non-invasive assessment of spontaneous movement, and demonstrate its use with the example of an acute chlorine exposure model. All procedures were approved by Duke University’s Institutional Animal Care and Use Committee and Division of Laboratory Animal Resources.",
    "Our research group has a long-standing interest in the development of countermeasures for treatment of chemical inhalation injuries, including pulmonary injuries caused by exposures to chlorine gas. Chlorine gas inhalation represents a major chemical threat in domestic, occupational, and transportation accidents. Chlorine gas has also been used as a chemical weapon beginning in World War I and was most recently used by the Syrian regime in the country’s civil war (Schneider and Lütkefend, 2019[href=https://www.wicell.org#bib9]). Chlorine gas also represents a terrorism threat due to its wide availability and ease of manufacture. Exposure to chlorine gas causes acute respiratory distress syndrome and mortality, depending on the exposure conditions (i.e., concentration and duration of exposure). No specific antidote is available for treating chlorine inhalation injuries and symptomatic treatment is considered the standard of care (Achanta and Jordt, 2021[href=https://www.wicell.org#bib1]; Achanta and Jordt, 2020[href=https://www.wicell.org#bib2]). Rodent models are widely used for studying the pathophysiology of chlorine inhalation injuries and for developing medical countermeasures (Achanta and Jordt, 2021[href=https://www.wicell.org#bib1]; Balakrishna et al., 2014[href=https://www.wicell.org#bib3]). Chlorine exposure of rodents reduces their subsequent mobility (Filippidis et al., 2012[href=https://www.wicell.org#bib4]) and results in decreased feed and water intake. Increased mobility in response to administration of an experimental chlorine countermeasure may be interpreted as evidence for efficacy, deserving more detailed analysis of the countermeasure response. This protocol was developed to remotely and accurately monitor the mobility of cohorts of chlorine-exposed mice and air-controls, to be used in the future to assess the efficacy of countermeasures. Remote monitoring of animals enables investigators to collect experimental data without the adverse and confounding effects of human presence, whilst ensuring animal welfare and facilitating timely intervention before any individuals reach their a priori humane endpoints.",
    "In our example study, 7-week-old male C57BL/6 mice were housed in two groups of five in cages on our HCCS-equipped rack and maintained in climate-controlled facilities under a 12:12 h light:dark cycle (‘daytime’: 07:00–19:00). Food (PicoLab Rodent Diet 20, #5053, LabDiet, MO, USA) was provided on the cage floor, instead of the standard overhead feeding baskets, so that the camera’s field of view was not obstructed. Water was available ad lib via nipple drinkers and supplemental hydrogel (ClearH2O, ME, USA) was provided in trays. Nesting material and infrared transmittable shelters (Safe Harbor Mouse Retreat, Bio-Serv, NJ, USA) were provided as additional enrichment items.\nFollowing acclimatization to their home-cage environment, the mice were exposed to either 400 ppm chlorine gas or medical-grade air for 30 min in a whole-body exposure chamber within a chemical fume hood, as per our previously published model with some minor modifications (Balakrishna et al., 2014[href=https://www.wicell.org#bib3]). Mice were then transferred to an open-topped enclosure within the fume hood for 5 min, before being returned to their group home-cages. At 24 h post-exposure, mice were terminally anesthetized for sample collection for studying other injury parameters, as previously described (Balakrishna et al., 2014[href=https://www.wicell.org#bib3]).\nNote: Although the mice were individually identified for longitudinal body weight monitoring using tail ink marks (VWR Lab Markers, Avantor, PA, USA), as the ink used transmits infrared light, marks were not resolved by the camera under the infrared illumination and hence could not be used to identify specific mice in the recorded videos. If the identification of specific individuals within a group is important for your video analysis, ensure that they are marked in such a way so as to be distinctly identifiable in the recorded videos (e.g., with unique fur cuts or marking using non-infrared-transmitting dyes/inks).\nVideo monitoring and capture",
    "Timing: Specific to the intended duration of the experiment\nIn this section, we describe the use of the HCCS to monitor and record animals during an experiment. During the pre-experimental acclimatization period, the HCCS should be turned on to acclimatize the animals to the LED lights. Animals may also be monitored during this period using either the preview or the recording scripts.\nCheck the camera focus.\nBefore each experiment, the camera’s position and focus should be checked and adjusted as required.\nVerify that the RPi has the correct date and time.\nStart recording.\nStart the video recording script manually, if not already initiated with an automatic script.\nMonitor the animals and cage environment remotely via VNC.\nCage removal and experimental interventions.\nRecordings can be stopped by terminating the camera script or simply by shutting down the RPi.\nUnless the cage is to be removed for an extended period of time, we tend to continue the recording and discard/disregard the intervening videos.\nRemove the LEDs and remove the cage.\nAfter the experimental intervention and cage replacement, insert the LEDs into their holders and, if stopped, restart the video recording manually or automatically by turning the RPi back on. Verify and adjust the camera’s position if necessary.\nEnd recording.\nAfter your experiment, terminate the camera script and transfer the video files to another device for storage, backup, and analysis.\nDelete the video files from the RPi’s memory card and shut down the RPi.\nData preprocessing\nTiming: <1 day\nAlthough it is possible to analyze the entirety of the recorded experimental period, this may be unnecessary for many scientific applications and impractical if implementing manual proofreading as a data quality assurance step. In this section, we describe the preparation of a representative sample of recorded videos for input into the SLEAP framework.",
    "Choose the time points of interest.\nThe duration and number of time points to analyze will be dependent upon the outcome of interest and design of each specific experiment. In our example study, we chose to analyze a 5-min period every hour, starting 5 min after the mice were returned to their cage and replaced on the rack. For the analysis of spontaneous movement, it is important to consider and minimize potential sources of disturbance within the animal housing area. On the other hand, the response to a stimulus challenge may reveal informative differences between groups.\nConcatenate time point H264 files and convert to MP4.\nUse FFmpeg_concatenate-convert-to-mp4.bat to concatenate and convert the two (or more) files encompassing the selected time points. If your time point duration is shorter than the length of the recorded segment, and entirely encompassed within one recording, then use FFmpeg_convert-to-mp4.bat to just convert the files to MP4.\nTrim concatenated files.\nUse LosslessCut to trim each MP4 file to the period of interest plus at least one keyframe before and after. Whilst optional, trimming the files will reduce the required processing time and storage space.\nRename trimmed files.\nUse PowerRename to batch rename the trimmed files. For example, in our study with recordings from two groups (air- and chlorine-exposed) with the time point of interest at 35 min past the hour, every hour, we renamed our files to fit the format “RPiHostname-Group_YYYYMMDD_HH-35.mp4”:\nSelect all video files, right-click, and select “PowerRename”.\nTick “Use regular expressions”.\nSearch for “h.∗” and replace with “-35.mp4”. Click “Apply”.\nSearch for “Cam01” and replace with “Cam01-Air”. Click “Apply”.\nSearch for “Cam02” and replace with “Cam02-Cl2”. Click “Apply and close”.\nResult: PiCam01-Air_20210122_09-35.mp4, PiCam01-Air_20210122_10-35.mp4, etc.; and PiCam02-Cl2_20210122_09-35.mp4, PiCam02-Cl2_20210122_10-35.mp4, etc.\nMove the renamed video files to a project folder called “videos”.",
    "Multi-animal tracking with SLEAP for movement analysis\nTiming: Varies with the number of video files to analyze and number/type of GPUs available\nIn this section, we describe the use of SLEAP to train a deep-learning neural network to track the movements of individual animals in the videos. Whilst we strongly recommend the use of a GPU computing cluster to considerably reduce the amount of time required by taking advantage of parallel processing, in order to maximize the accessibility of this protocol, we provide directions here for use with Colab (a freely accessible cloud notebook environment with a single GPU). The steps in this section entail iteratively training a model with prediction-assisted labeling, using the model to predict (infer) the position of all animals across all videos, proofreading the model predictions, and then calculating the “Total Distance Moved” and “Time Mobile” for each animal at each time point. Note, in this protocol we demonstrate the tracking and analysis of only a single point on each mouse – the centroid – however, more complex pose estimation is easily implemented with SLEAP and can facilitate more complex analysis and feed into other analytical pipelines (e.g., SIMBA; Nilsson et al., 2020[href=https://www.wicell.org#bib6]). We recommend reviewing the tutorial available at https://sleap.ai[href=https://sleap.ai] for more information and advice about using the SLEAP interface, beyond our specifications and brief description here.\nInitial labeling.\nOpen SLEAP and save a new project (File > Save As; e.g., HCCS_example_project.slp). Intermittently save the project as you are working.\nIn the “Skeleton” panel, click “New Node” and rename it “centroid”.\nIn the “Video” panel, click “Add Videos” and select your recorded videos.\nSelect the first video. In the “Instances” panel, click “New Instance”. A labeled dot will appear in the video – move this “centroid” to the appropriate position on one of your animals.",
    "Add more instances until you have labeled all of the animals in the frame.\nIf the centroid position of an individual is not visible, but its position can be reasonably inferred, position the centroid as appropriate – this helps the network to learn about the likely positions of occluded parts.\nIf you cannot reasonably determine the position of an occluded centroid, leave it unlabeled.\nLabel approximately 10–20 frames across several different videos, ideally representing individuals in different positions and postures.\nMove to other frames by using the keyboard shortcuts or clicking along the timeline at the bottom of the screen.\nYou can also use the “Labeling Suggestions” panel to generate a list of frames to label.\nSpecify training parameters and export labeled frames.\nOpen the training pipeline dialog (Predict > Run Training) and select “multi-animal top-down” as the pipeline type.\nSelect “centroid” as the anchor part.\nChange “Predict On” to “nothing”.\nCustomize the hyperparameters to suit your data set in the centroid and centered instance model configuration tabs, as necessary.\nNote, although the default hyperparameter values may be appropriate, particular for initial training, it is recommended to adjust the receptive field size to suit the size of your animals and turn on contrast and brightness augmentation; see https://sleap.ai/guides/choosing-models.html[href=https://sleap.ai/guides/choosing-models.html] for more details.\nClick “Save configuration files” and select an output folder (e.g., create a folder called “config”).\nThis will export the model specification files (centered_instance.json and centroid.json) as well as two scripts (inference-script.sh and train-script.sh); you can delete the inference and training scripts, which are not used in this protocol.\nClick “Cancel” to close the dialog box. Note, clicking “Run” will attempt to train the model locally.\nExport the labeled frames to the config folder (Predict > Export Labels Package > Labeled frames; e.g., HCCS_example_project.pkg.slp).",
    "Upload files to Google drive to train and use an initial model.\nCreate a Google Drive account or use an existing account (https://google.com/drive[href=https://google.com/drive]).\nDrive is used to store and transfer the data used for training and inference with Colab; any data present within Colab is otherwise lost at the end of each session.\nCreate a new folder “SLEAP” and within this folder create new folders called “analysis”, “config”, “models”, and “videos”. Within the “analysis” folder, create the folder “plots”.\nUpload your configuration and video files to their respective folders.\nOpen our Colab notebook SLEAP_Training_&_Inference and follow the instructions within.\nThis notebook will guide you through training an initial top-down model to identify and track the centroids on all recorded animals. No programming skills are required; you simply need to input details where prompted and click the “play” buttons on each cell (each section of code) to run the code.\nOnly one cell will run at a time; by clicking on each cell in order whilst a prior one is running, you can queue them to play automatically.\nUse HCCS_SLEAP-create-training-videos-list.bat to create the list of videos for input into step 3 of the notebook.\nThe time to train and infer is dependent upon the type of GPU allocated to you, the size of your training file, and the number of videos and frames selected for inference.\nNow is a good time to enjoy a cup of tea.",
    "Optional: Depending on your study design and the characteristics of your videos, you may want to edit some of the settings for inference. In our example study, we culled the number of individuals to track to a maximum of five, used the “simple” tracker with Hungarian matching, a tracking window of 10, and connected short breaks in tracking. The culling and tracking window settings may be readily adjusted using the notebook inputs. Other settings may be changed by editing the notebook code; a reference of options is available: https://sleap.ai/guides/cli.html[href=https://sleap.ai/guides/cli.html].\nAssess initial model and use predictions for further labeling.\nUnzip the downloaded predictions into your videos project folder.\nOpen each predictions file in SLEAP (File > Open Project) and scroll through the video, taking note of the “Score” value in the “Instances” panel for each predicted instance.\nCorrect the predictions.\nMake the predictions editable for the current frame (Labels > Add Instances from All Predictions on Current Frame).\nMove, add, and delete instances as required.\nRepeat for as many frames as desired, particularly for egregious predictions.\nWhen done, delete the remaining predictions (Labels > Delete All Predictions), and save the file.\nOpen the main project file and add in the new labeled frames (File > Merge into Project; select all corrected prediction files).\nA dialog box will provide a summary of merged changes and options to manage any conflicts.\nExport the labeled frames file again and upload to Drive. You do not need to change the model specification configuration files (although you can, if desired).\nRetrain your model using the new labeled frames file to generate a (hopefully) better model.",
    "Continue this iterative prediction-assisted labeling and training process until you are satisfied with the performance of your model. Metrics are available to assist with model assessment (Predict > Evaluation Metrics for Trained Models).\nOnce you have decided on a “final” model, perform inference across all frames for all of your videos.\nProofread predictions/tracks and export data.\nOpen in SLEAP the predictions file from your “final” model for the first video and “Add Instances from all Predictions on Current Frame” for the first and last frames in the video. Adjust the instance positions if necessary.\nOptional: Remove all predictions with a low score (Labels > Delete Predictions with Low Score) to potentially improve the quality of your tracking. We deleted all predictions with a score below 0.8, however, the nature of your videos and the performance of your trained model will influence what is an appropriate threshold for your study.\nScroll through the video to review and correct the positions of the predicted instances and their temporal association into individual tracks (Figure 5[href=https://www.wicell.org#fig5]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1576-Fig5.jpg\nFigure 5. A cropped screenshot from SLEAP showing a representative night-time infrared-illuminated camera view with five mice and their centroid tracks\nColor each individual (View > Color Predicted Instances) and adjust the trail length to suit the speed of review and nature of the animal movements (View > Trail Length).\nCorrect or delete erroneous predicted instances and delete, combine, or correct the identities of tracks, as necessary. There should be one track for each animal present in the video.\nTemporal gaps in the tracks are okay (see analysis below) although missing data should be minimized where possible. Files with many deleted points may indicate that further labeling and training is required.",
    "Save the corrected prediction file with a new name (e.g., we replace the suffix “predictions” with “tracked”).\nExport the analysis file (File > Export Analysis HDF5).\nVerify using the terminal output that the correct number of tracks are present.\nRecord the following details in the “video_details” tab of HCCS_example_project_analysis_template.xlsx (this is required for the movement analysis step below):\nh5_filename (you can use HCCS_SLEAP-create-analysis-h5-list.bat to create a list of all H5 files to copy into this column, rather than copying each manually).\ndate (of the recorded video; e.g., 22-Jan-2021).\ntime_point (e.g., 9:35).\ngroup (e.g., Air).\nstart_frame (this is the frame number that first shows the embedded time annotation for the start of the time point; e.g., first frame with time 09:35:00 is frame 8. Note, SLEAP shows the frame number in the bottom left of the screen).\nend_frame (this is the frame number that last shows the time for the end of the time point; e.g., last frame with time 09:39:59 is frame 9012).\npixels_per_cm (see below for calculation).\nfps (e.g., 30).\nRepeat this step for all predictions files.\nCalculate the real-world video scale (pixels_per_cm).\nTo determine the scale for each camera, use a video recording of an object of known size in the same position in each cage, e.g., we used the width of the feeding tray in the first recorded video for each group. Alternatively, you may print and use a checkerboard pattern of known size (e.g., HCCS_1cm_checkerboard.png; Figure 6[href=https://www.wicell.org#fig6]). This pattern may also be used for assessing and correcting image distortion (see discussion in the limitations[href=https://www.wicell.org#limitations] section).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1576-Fig6.jpg",
    "Figure 6. A frame from a day-time video showing a 1 cm checkerboard pattern on the bottom of an empty home-cageHCCS_1cm_checkerboard.png was printed, trimmed to size, and taped in position. This pattern may be used to determine the real-world scale of your video and assess the need for image distortion correction.\nYou only need to determine this scale once for each camera, unless the position of the camera changes during the study.\nOpen SLEAP and save a new project (e.g., HCCS_example_project-scale.slp).\nIn the “Skeleton” panel, click “New Node” and rename it “scale”.\nAdd the videos for measurement.\nAdd two instances and position them at either end of your object of known size.\nAdd a separate track to each instance (select instance; Tracks > Set Instance Track > New Track) and give them unique names (e.g., PiCam01_S1; PiCam01_S2).\nRepeat for all cameras used in the study.\nExport the analysis file and upload it to the “analysis” folder in Drive.\nOpen our Colab notebook SLEAP_Scale and follow the instructions within.\nThis notebook will extract the instance coordinates from an H5 analysis file and save them in a CSV file for input into the analysis template.\nOpen the downloaded coordinates file and copy the data into the respective fields in the “scale” tab of HCCS_example_project_analysis_template.xlsx.\nCopy the pixels_per_cm value for each camera to the appropriate rows in the “video_details” tab.\nEnsure that you paste the value and not the underlying formula.\nCalculate movement parameters.\nExport the “video_details” tab as a CSV file and upload it along with the video H5 analysis files to the “analysis” folder in Drive.\nOpen our Colab notebook SLEAP_Movement_Analysis and follow the instructions within.",
    "This notebook will calculate the Total Distance Moved and Time Mobile for each animal at each time point and save them in a CSV file. The raw centroid movements of each animal can also be plotted.\nThe method of missing data imputation can be chosen (in our example study, we used the previous available value to more accurately estimate the Total Distance Moved, however, this will underestimate the Time Mobile).\nTo reduce the impact of any variability in frame-to-frame tracking, you can apply a movement threshold that zeros movements equal to or less than the set value (for our dataset, we used a value of 0.2 cm).\nStatistical analysis\nTiming: 5–10 min\nIn this section, we describe how to calculate the area under the curve (AUC) for the movement parameters and statistically compare between two groups. We have provided an example GraphPad Prism project file (HCCS_example_project_AUC_template.pzfx) to help demonstrate the analysis.\nImport the Total Distance Moved data into GraphPad Prism.\nCreate a grouped table with replicate subcolumns for the number of animals tracked.\nPerform an AUC analysis and compare differences in mean AUCs with an unpaired t-test.\nRun an AUC analysis (XY analyses).\nCopy the “Total Peak Area” and the “Std. Error” values into a new grouped table formatted for entry of “Mean, SEM, N”.\nThe “N” column is calculated for each group as the degrees of freedom plus one (calculated as the number of data points minus the number of time points plus 1). In our example study, the “N” values were 73 and 87 for the Air and Chlorine groups, respectively.\nRun an unpaired t-test (Column analyses).\nCreate an AUC graph using the data entered in step 16.\nCreate an XY graph with area fill, plotting the mean and SEM.\nRepeat the analysis with the Time Mobile data."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Neuroscience",
    "Model Organisms",
    "Behavior"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Molecular Biology & Genetics",
    "Ecology & Environmental Biology"
  ]
}