{
  "id": 2246,
  "origin_website": "Cell",
  "title": "EpiTopics: A dynamic machine learning model to predict and inform non-pharmacological public health interventions from global news reports",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nData preprocessing\nTiming: 10 min\nThis section describes 1) The removal of white spaces, special characters and non-English words 2) The removal of stop words as in (Dieng et al., 2020[href=https://www.wicell.org#bib1]) 3) The extraction of information that is relevant to us from AYLIEN and WHO datasets 4) The removal from WHO dataset of documents whose country or source are not observed in the AYLIEN data.\nPreprocess AYLIEN data.\nmodify the script run_data_process.sh to include the correct path to the AYLIEN dataset, stop words file, and country NPIs file.\nset ‘aylien_flag’ to 1 and ‘label_harm’ to 1.\nexecute ‘run_data_process.sh’ from the command line.\nPreprocess WHO data.\nmodify the script run_data_process.sh to include the correct path to the WHO dataset, stop words file, and country NPIs file.\nset ‘label_harm’ to 1.\nexecute ‘run_data_process.sh’ from the command line.\nThe program will store the processed data (e.g., bag-of-words) in the output directory specified by save_dir.\nTake note that this should also be the input directory for running MixMedia (Li et al., 2020[href=https://www.wicell.org#bib2]) (see below).\nMore specifically, check that the output directory contains:\nText file that contains the mappings between labels and their ids.\nText file that contains countries and their assigned ids.\nTime_stamps and their ids.\n43 pickle (.pkl) files that mainly feature the pickled vocabulary and embeddings and bag-of-word representations of tokens.\nRunning MixMedia\nTiming: 5 h\nPretraining of the MixMedia (Li et al., 2020[href=https://www.wicell.org#bib2]) framework on the larger AYLIEN dataset as part of our transfer learning scheme.\nModify the script run_MixMedia.sh.\nset K = 25 (the number of desired topics).\nset cuda = {the indices to the GPUs that are available to you}.\nset dataset = “AYLIEN”.\nset datadir = path to your AYLIEN files.\nset outdir = path to the output directory of your choice.",
    "set wemb = path to the output directory of your choice, this contains the embeddings that are needed for stage 3.\nset mode = “train”.\nset batch_size = “128”.\nset lr = “1e-3”.\nset epochs = “400”.\nset min_df = “10”.\nset train_embeddings = “1”.\nset eval_batch_size = “128”.\nset time_prior = “1”.\nset source_prior = “1”.\nExecute ‘./run_MixMedia.sh’ from the command line.\nThe program will save the outputs to a folder under save_path: save_path/<timestamp>, and\nThe timestamp records the time this script starts to run, and is in the format of {month}-{day}-{hour}-{minute}.\nThe program saves the trained model.\nThe program saves the learned topics (e.g., the topic embedding α, the word embedding ρ, LSTM weights for topic prior η, etc).\nMonitor the progress with Tensorboard or Weights & Biases by setting “logger”.\nTransfer learning for NPI prediction\nTiming: 1 h\nAfter MixMedia is trained on AYLIEN, we can use the learned topics for NPI prediction via transfer learning. This consists of three consecutive stages: inferring WHO documents’ topic mixtures, training a classifier on document-NPI prediction, transferring the classifier to country-NPI prediction.\nInfer WHO documents’ topic mixtures.\nPopulate the ‘save_dir’, ‘data_dir’ and ‘model_dir’ entries of the ‘infer_theta.sh’ file according to the instructions within the file.\nThe program saves the output to a folder under save_dir: save_dir/{timestamp}, where the timestamp records the time this script starts to run, and is in the format of {month}-{day}-{hour}-{minute}.\nThe program also saves the document topic mixtures theta.\nTrain a classifier on document-NPI prediction.\nWithin classify_npi.sh, set ‘mode’ to zero-shot or finetune or from-scratch based on the type of result that currently needs to be reproduced.\nFor document-level NPI prediction, set mode to “doc” and provide who_label_dir and theta_dir.\nFor zero-shot transfer, set mode to zero_shot and provide cnpi_dir and ckpt_dir;",
    "For fine-tuning, set mode to finetune and provide cnpi_dir and ckpt_dir.\nSet eta_dir to the directory where you saved your outputs in step 5.\nSpecify save_ckpt. When set, the program saves the results reported in Wen et al. (2022)[href=https://www.wicell.org#bib3] to a subfolder under save_dir: save_dir/mode/{timestamp}\nThe timestamp records the time this script starts to run, and is in the format of {month}-{day}-{hour}-{minute}.\nFor each random seed, the program saves a trained linear classifier and the corresponding test predictions, with suffixes in filenames that specify the seed.\nThe program also saves the aggregated results in AUPRC into a json file.\nRepeat the above steps for the other modes."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Health Sciences"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}