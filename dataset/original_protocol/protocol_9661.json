{
  "id": 10072,
  "origin_website": "Jove",
  "title": "A Step-by-Step Implementation of DeepBehavior, Deep Learning Toolbox for Automated Behavior Analysis",
  "procedures": [
    "1. GPU and Python Setup\nGPU Software\n\tWhen the computer is first setup for deep learning applications, GPU-appropriate software and drivers should be installed which can be found on the GPU's respective website. (see the Table of Materials for those used in this study).\nPython 2.7 Installation\n\tOpen a command line prompt on your machine.\nCommand line: sudo apt-get install python-pip python-dev python-virtualenv\n2. TENSORBOX\nTensorbox Setup\nCreate Virtual Environment for Tensorbox\nCommand line: cd ~\nCommand line: virtualenv --system-site-packages ~/tensorflow\n\t\tNOTE: '~/tensorflow' is the name of the environment and is arbitrary\nActivate environment\nCommand line: source ~/tensorflow/bin/activate\nTensorbox Installation\n\tWe will be using GitHub to clone TensorBox from http://github.com/aarac/TensorBox[href=http://github.com/aarac/TensorBox] and install it on our machine as well as installing additional dependencies.\nCommand line: cd ~\nCommand line: git clone http://github.com/aarac/TensorBox[href=http://github.com/aarac/TensorBox]\nCommand line: cd TensorBox\nCommand line: pip install -r requirements.txt\nLabel Data\nCreate a folder of images of behavior\n\t\tOpen source tools such as ffmpeg are useful to accomplish converting videos to individual frames We recommend labeling at least 600 images from a wide-distribution of behavior frames for training. Put these images in a folder.\nLaunch labeling graphical user interface\nCommand line: python make_json.py <path to image folder> labels.json\n\t\tTo label an image, click the top left corner of the object of interest (i.e. paw) first and then click the bottom right corner of the object of interest (Figure 4). Inspect that the bounding box captures the entire object of interest. Press 'undo' to re-label the same image or press 'next' to move onto the next frame.\nTrain TensorBox\nLink training images to network hyperparameters file\n\t\tWithin the tensorbox folder, open the following folder in a text editor:",
    "/TensorBox/hypes/overfeat_rezoom.json. Navigate to the attribute under data named train_idl and replace the file path from ./data/brainwash/train_boxes.json to the labels.json filepath. Save the changes to file.\nBegin training script\nCommand line: cd ~/TensorBox\nCommand line: python train.py --hypes hypes/overfeat_rezoom.json --gpu 0 --logdir output\n\t\tThe network will then begin training for 600,000 iterations. In the output folder, the resulting trained weights of the convolutional neural network will be generated.\nPredict on New Images\n\tFor image labeling:\nCommand line: cd ~/TensorBox\nCommand line: python label_images.py --folder <path to image folder> --weights output/overfeat_rezoom_<timestamp>/save.ckpt-600000 --hypes /hypes/overfeat_rezoom.json --gpu 0\n\tTo get coordinates of bounding boxes:\nCommand line: cd ~/TensorBox\nCommand line: python predict_images_to_json.py --folder <path to image folder> --weights\n\toutput/overfeat_rezoom_<timestamp>/save.ckpt-600000 --hypes\n\t/hypes/overfeat_rezoom.json --gpu 0\nMATLAB Post-Processing for TensorBox\n\tAdditional MATLAB code has been provided to extract kinematics and visualizations of the coordinates using the resulting JSON coordinate file from the model\n\tRun the \"Process_files_3Dreaching_mouse.m\" script for 3D kinematic analysis of single food pellet reaching task.\n3. YOLOv3\nInstall YOLOv3\nCommand line: cd ~\nCommand line: git clone cd darknet\n\tFor GPU usage, open 'Makefile' and change the following lines: GPU=1; CUDNN=1.\nCommand line: make\nLabeling Training Data using Yolo_mark\nCommand line: cd ~\nCommand line: git clone cd ~/Yolo_Mark\nCommand line: cmake .\nCommand line: make\n\tPlace the training images in ~/Yolo_mark/data/obj folder\nCommand line: chmod +x ./linux_mark.sh\nCommand line: ./linux_mark.sh\n\tLabel the images one by one in the graphical user interface (Figure 5). The recommended amount of images is approximately 200.\nTraining YOLOv3\nSetup configuration file\nCommand line: cd ~/Yolo_mark\nCommand line: scp -r ./data ~/darknet\nCommand line: cd ~/darknet/cfg\nCommand line: cp yolov3.cfg yolo-obj.cfg\nModify the configuration file",
    "Open the yolo-obj.cfg folder and modify the following lines: batch=64, subdivision=8, classes=(# of class to detect), and for each convolutional layer before a yolo layer change the filter=(classes+5)x3. Details on these changes can be found at https://github.com/aarac/darknet/blob/master/README.md[href=https://github.com/aarac/darknet/blob/master/README.md]\nDownload network weights\n\t\tDownload the network weights from https://www.dropbox.com/s/613n2hwm5ztbtuf/darknet53.conv.74?dl=0[href=https://www.dropbox.com/s/613n2hwm5ztbtuf/darknet53.conv.74?dl=0]\n\t\tPlace the downloaded weight file into ~/darknet/build/darknet/x64\nRun training algorithm\nCommand line: cd ~/darknet\nCommand line: ./darknet detector train data/obj.data cfg/yolo-obj.cfg darknet53.conv.74\nYOLOv3 Evaluation\n\t\tAfter the training is complete based on a set number of iterations (ITERATIONNUMBER), you can view them by\nCommand line: ./darknet detector test data/obj.data cfg/yolo-obj.cfg backup/yolo-obj_ITERATIONNUMBER.weights <IMAGE>.jpg\nPredict on new videos and get coordinates\n\tThis command can be run to obtain the coordinates of the labels in the new video:\nCommand line: ./darknet detector demo data/obj.data cfg/yolo-obj.cfg backup/yolo-obj_ITERATIONNUMBER.weights VIDEO.avi -ext_output <VIDEO.avi> FILENAME.txt\nYOLOv3 PostProcessing in MATLAB\n\tTake the FILENAME.txt file to MATLAB, and run the \"Process_socialtest_mini.m\" script for two mice social interaction test. See results in Figure 2\n4. OpenPose\nOpenPose is ideal to track multiple body parts in a human subject. The setup and installation processes are very similar to the previous two frameworks. However, there is no training step as the network is already trained on human data.\nOpenPose Installation\n\tNavigate to https://github.com/aarac/openpose[href=https://github.com/aarac/openpose] and follow the installation instructions.\nProcess Video\n\t./build/examples/openpose/openpose.bin --video VIDEONAME.avi --net_resolution \"1312x736\" --scale_number 4 --scale_gap 0.25 --hand --hand_scale_number 6 --hand_scale_range 0.4 --write_json JSONFOLDERNAME --write_video RESULTINGVIDEONAME.avi\n\tHere the --net_resolution, --scale_number, --scale_gap, --hand_scale_number and --hand_scale_range handles can be omitted if a high precision detection is not needed (this would decrease the processing time).\nOpenPose Post-Processing",
    "In MATLAB folder, please use 'process_files_human3D.m' script to run the code after adding the appropriate folder containing json files from cameras 1 and 2, as well as the calibration file. This will create a \"cell\" file with all the 3D poses of the joints. It will also make a movie of the 3D skeletal view. For camera calibration, please follow the instructions at this link: http://www.vision.caltech.edu/bouguetj/calib_doc/[href=http://www.vision.caltech.edu/bouguetj/calib_doc/]\nSubscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Behavior"
  ],
  "bigAreas": [
    "Ecology & Environmental Biology"
  ]
}