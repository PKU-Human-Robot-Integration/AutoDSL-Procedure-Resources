{
  "id": 12888,
  "origin_website": "Jove",
  "title": "A Method to Quantify Visual Information Processing in Children Using Eye Tracking",
  "procedures": [
    "The protocol described here was approved by the Medical Ethical Research Committee of the Erasmus Medical Center, Rotterdam, the Netherlands (MEC 2012-097). The procedures adhered to the tenets of the Declaration of Helsinki (2013) for research involving human subjects.\n1. Visual Stimuli\nSelect a set of visual stimuli, i.e., images and movies, to target the processing of basic oculomotor functions and visual processing functions.\nUse images and movies to evaluate basic oculomotor functions such as fixation, saccades, smooth pursuit, and optokinetic nystagmus. When abnormalities in oculomotor function are detected, take this into account in data analysis and interpretation.\n\t\nUse an image to assess fixation and saccades. The present paradigm contains smiley pictures with a radius of 3º of visual angle, which are presented in the left, right, upper and lower half of the monitor.\nUse a slowly moving image to assess smooth pursuit. The present paradigm contains movies of smileys which move 16º in sinusoidal horizontal and vertical direction across the monitor, with a velocity of 4º/sec.\nUse a movie to assess optokinetic nystagmus reflexes. The present paradigm contains movies of black-and-white sinusoidal gratings that move in leftward and rightward direction.\nUse images and movies to assess visual processing functions, e.g., contrast, color, form or motion.\nUse a set of visual stimuli that are based on a 4-alternative forced choice preferential looking paradigm (4-AFC PL19). In the present paradigm, the 4 stimulus corners (i.e., upper left and right quadrant, lower left and right quadrant) each represent an alternative choice, i.e., a target area. Each target area has a radius of 6º and differs from the other 3 quadrants with respect to specific visual information, e.g., based on contrast, color, form or motion. The following visual stimuli can be used as an example:",
    "Use an image to assess Form Coherence processing: an image with an array of randomly oriented short white lines (0.2º x 0.6º; density 4.3 lines/degree2) against a black background. In the target area all lines are arranged in the shape of a circle.\nUse a movie to assess Local Motion processing: a movie with a black/white patterned square target, with a visual angle of 2.3º, against an equally patterned background, moving 2.5º to the left and to the right in one quadrant at 2.5º/sec.\nUse a movie to assess Global Motion processing: an image with an array of white dots (diameter 0.25º, density 2.6 dots/degree2) expanding from the center of the target area towards the borders of the monitor. The dots move over a black background with a velocity of 11.8º/sec and a limited lifetime of 0.4 sec.\nUse an image to assess Contrast Detection: an image with a 0% brightness (black) Hiding Heidi picture in the target area, against a 75% (light gray) brightness background.\nUse an image to assess Color Detection: an image with a green number 17 in the target area, against a red-yellow background.\nUse a movie to assess simultaneous visual processing, e.g., a Cartoon: a colorful, high contrast picture (reproduced with permission from Dick Bruna, Mercis BV, Amsterdam, The Netherlands) with a visual angle of 4.5º x 9.0º (width x height) moving 1.5º up and down at a speed of 3º/sec in the target area, against a black background.\n\t\tNOTE: For the purpose of clarity, the representative results of this paper will focus on the highly salient cartoon stimulus that contains various types of visual information (Figure 1). For pictures of the other visual stimuli, please consult a previous study20.\nimgsrc://cloudfront.jove.com/files/ftp_upload/54031/54031fig1.jpg",
    "Figure 1. Cartoon stimulus. The cartoon stimulus contains various visual modalities (form, motion, color and contrast). This stimulus triggers visual attention, and gives fastest response times in children. Superimposed is an eye movement (gray), going from the lower left corner of the monitor into the target area in the upper right corner (i.e., a reflexive response to the stimulus). Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/54031/54031fig1large.jpg]\n2. Eye Tracking-based Test Paradigm\nChoose an eye tracking system suitable for pediatric populations (e.g., non-invasive, tolerance of head movements, and ease of use)12. This generally entails remote infrared eye trackers (e.g., Tobii T60XL, SMI RED)10,11.\nChoose a wide angle size computer monitor to fully display each stimulus (i.e., minimum visual angle of 24º x 30º at 60 cm viewing distance). The remote eye tracker is either integrated with the monitor, or can be attached separately to a monitor.\n\tNOTE: Remote eye trackers emit infrared light that is sampled using cornea reflection. An eye tracking sampling rate of ~60 Hz is generally sufficient to study patterns of gaze behavior in children.\nAssemble a mobile measurement set-up by connecting a monitor and the remote eye tracking system to a laptop or desktop PC.\nInstall a compatible software program on the PC (e.g., Tobii Studio, iView) for the presentation of visual stimuli and the recording of eye movements.\nDesign a test sequence containing all stimulus types that are required to test oculomotor functions and/ or visual processing functions (see protocol step 1: visual stimuli). The present example contains all stimulus types that are described in step 1, i.e., 9 in total.",
    "Place the different types of visual stimuli in random order in the test sequence, but make sure that the position of the target area alternates from trial to trial. This ensures the need for making reflex eye movements to the target.\nPresent each stimulus at least 4 times (i.e.,with the target area at least once in every quadrant), and for at least 4 sec, to allow sufficient time to make an eye movement response. In the present example, the Cartoon stimuli are shown 16 times whereas all other stimuli are shown 4 times. This adds up to a total of 48 stimulus presentations and a total testing time of ~3.5 min.\n\t\tNOTE: Repeated presentations increase the chance of sampling sufficient gaze points for each stimulus and each target area in the child's visual field. In general, the availability of gaze data for at least 25% of stimulus presentations is needed to ensure reliable results21.\nMake sure testing time per sequence is not longer than ~5 min, because once a test sequence is running, it cannot be paused. It is preferable to make two sequences that can be run in succession, to provide a rest period halfway.\n\t\tNOTE: To maximize attention during the test, present audio or audiovisual cues near the monitor in between, but not simultaneously with, the presentation of visual stimuli. Children with visual impairments are particularly more sensitive and responsive to audio cues. Such cues might enhance test attentiveness in this population.\nApply the test sequence(s) in the eye tracker software. First, select the type of stimulus to be added to the timeline of the eye tracker software: image or movie. Next, select the desired stimulus from the folder in which it is located and click 'Add'. Repeat these steps until all stimuli have been added.",
    "3. Running the Eye Tracking Experiment\nAttach the eye tracker monitor with a flexible LCD arm to a solid table or wall. Choose an arm that can move in 3 dimensions (i.e., 3 translations, 3 rotations).\nPosition children at a short distance (generally ~60 cm) from the monitor to ensure efficient pupil tracking of both eyes.\nAdjust the monitor position to be perfectly perpendicular to the child's eyes. With an LCD arm this is possible even when the child is lying or sitting in a pram or in a wheelchair.\n\tNOTE: This set-up allows the assessment of very young and intellectually disabled children, since it does not require a particular body posture, verbal communication or active participation. Certain oculomotor impairments (e.g. nystagmus) are characterized by preference positions of the head in order to compensate for deviant eye positions (e.g., torticollis). The ability to adjust the eye tracker monitor to individual head position enables accurate pupil tracking in this group of children.\nCheck the quality of pupil reception. This is generally indicated by the presence of two markers representing the child's eyes (e.g., white dots). If the two markers are clearly visible and do not regularly disappear, quality is sufficient. In a separate display, check the distance of the eyes to the monitor (preferably ~60 cm).\n\tNOTE: Most eye trackers record the gaze position of each eye separately and compensate for free head movements. Pupil signal reception is in general not compromised in children who wear glasses or contact lenses, in children with one or two functioning eyes, or in children with strabismus.",
    "Start the eye tracker software calibration procedure to align the gaze positions with predefined positions on the monitor, prior to start of the measurement. In most eye tracker software packages this calibration procedure consists of the presentation of moving dots in predefined areas of the monitor, which have to be fixated. For children, a version with cartoons or looming dots can be used to improve visual attention.\n\tNOTE: Although calibration procedures for children have improved significantly, they can still be challenging to perform in young children and children with certain eye- or behavioral disorders.\nCheck the quality of the pre-set calibration. When the quality of calibration is poor, (e.g., due to excessive head movements, lack of proper fixations, deviant gaze position or deviant head position), no recording can be made. To circumvent this, apply a post-calibration procedure after the recording has been finished, prior to further data analysis (see Discussion section).\nBefore starting the test recording, activate the 'live viewer': a separate window that shows the child's eye movement responses to the test stimuli by superimposing the gaze signal on the video recording.\nActivate a web cam that is directed at the child, to observe and record the child's general behavior during the test. Such a recording provides an overview of the child's visual attention, behavior, fatigue, and environmental conditions.\nPrior to starting the test, tell the child he or she will be 'watching television'. No specific instructions are necessary during the test.\nDuring test execution, observe the child's physical behavior and eye movement responses. This can be done by observing behaviorally in real-time, or by observing the recordings made with the web cam.\n\t\nWhen the pupil signal disappears during test execution, reposition either the child or the monitor to resume proper pupil detection.",
    "When a child is not paying attention to the monitor, verbally encourage the child to watch the monitor. Do not direct the attention of the child directly to the target area; direct the child's gaze solely to the general location of the eye tracker monitor.\nAfter test execution, replay the gaze recording off-line to observe the gaze responses to the presented stimuli. This is a first step in characterizing the child's visual orienting behavior.\n\tNOTE: A multitude of parameters are recorded continuously by the eye tracker software during total testing time. Essential parameters that need to be exported to perform the data analysis for the present paradigm are: time stamps, viewing distance between both eyes and the monitor, the position of the left and right eye on the monitor (in x- and y-coordinates), validity of the gaze data, and the timing and position of presented stimuli (i.e., events).\nPer subject, export and store the recorded time-based data on eye movement characteristics (gaze data such as viewing distance and gaze positions), and separately the time-based list of presented visual stimuli (event data such as stimulus positions). Make sure to export the two data files as text files and convert them into a data spreadsheet (e.g., save as an Excel file).\n\tNOTE: The two text files (event data and gaze data) are combined using their corresponding time stamps, and are converted into a set of quantitative parameter values with a self-written software program (see next section). Compared to standard eye tracker analysis software, such parameters provide a more precise and quantitative eye movement analysis, to aim at detailed visual and cognitive processes.\n4. Quantitative Analysis of Eye Movements",
    "NOTE: The present protocol is specific to a self-written software program. In order to replicate it, one should write such a software program, e.g., in MATLAB or Python, to quantify the child's visual orienting behavior. In the software program, the following steps are performed for every stimulus type. The present example is focused on Cartoon; the same protocol is applicable to other stimulus types.\nPost-calibrate the Gaze Data\n\t\nOpen MATLAB. Select the stimulus to analyze the gaze data, by typing in '1' next to the stimulus of choice.\nPress Run. In the appearing pop-up menu, select the option 'Post-calibrate the data'. A list with gaze data files per subject appears. Select gaze data of one subject and press 'Open'.\nFrom the next pop-up menu, select which eye(s) to analyze: Left, Right, or Both. The program now generates a scatter plot of all recorded gaze positions and target positions, over the total stimulus presentation time.\nCheck whether gaze positions correctly overlap with the corresponding target positions. If this calibration is correct, press 'Yes'. Otherwise, press 'No'. This will start the option to perform a post-calibration.\nTranslate the center of gaze points to the center of the monitor, by clicking once on the center of gaze points. This center point is located exactly in the middle of the vertical- and horizontal axes.\nScale the gaze positions to the corresponding target positions by clicking the center of gaze points in each of the four target areas once (i.e., the 4 quadrants).\nCheck again whether gaze positions correctly overlap with the corresponding target positions. If this is the case, indicate in the next pop-up menu that calibration has been performed correctly, by pressing 'Yes', after which the calibrated gaze data is saved. Otherwise, press 'No', after which post-calibration starts again from step 4.1.5.",
    "NOTE: After post-calibration, multiple gaze responses are available per stimulus type and per subject. These can be used to calculate quantitative parameters of visual processing. Prior to calculation of these parameters, verify that the gaze responses were made to the target area (i.e., that the specific stimulus has been seen by the child).\nDetermine Whether the Stimulus has been Seen\n\t\nPer stimulus presentation of each subject, the corresponding gaze data that was recorded during total presentation time is visualized in a graph (Figure 2). Verify whether this stimulus has been seen, by checking the criteria that are stated in Table 1, and that are visualized in Figure 2. If the eye movement response adheres to the criteria, i.e., the stimulus can be classified as seen, click 'Accept' in the pop-up menu. If the eye movement response is not in accordance with the criteria, click 'Reject'.\nSimultaneously, plot all fixation points belonging to the presented stimulus and the corresponding target area (i.e., quadrant) in a second graph. Inspect visually whether the fixation points are located in the correct quadrant.\nContinue with the subsequent stimulus presentation, and perform steps 4.2.1 and 4.2.2 for all available eye movement responses. After manually checking the eye movement responses, the software program calculates three outcome parameters: RTF, FD, and GFA (Figure 3).\nimgsrc://cloudfront.jove.com/files/ftp_upload/54031/54031fig2.jpg",
    "Figure 2. Eye movement response to the target area of a stimulus. One eye movement trace (horizontal and vertical directions combined) in distance from the center of the target area (in degrees, y-axis) over stimulus presentation time (in ms, x-axis). The dotted line represents the border of the target area (6° radius). Letters indicate criteria to establish whether the stimulus has been seen: (A) Gaze signal in the first 500 msec; (B) Gaze was not in the target area before 120 msec; (C) Gaze inside the target area for ≥200 msec. Note that in this figure, the depicted presentation time is max 2,000 msec to visualize the first, reflexive response. During testing, total presentation time of all stimuli was 4,000 msec. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/54031/54031fig2large.jpg]\ntable:\n﻿0,1,2\nCriterium (Figure 2),Verify that the gaze signal:,Rationale:\nA,Has been recorded for ≥500 msec after stimulus onset,Capture reflex orienting responses\nB,\"Did not enter the target area <120 msec after stimulus onset, and was not already inside the target at the start of stimulus presentation\",Exclude correct performance based on chance\nC,Was in the target area for ≥200 msec,Ensure fixation on the target\nD,\"Entered the target area within a time window of 1,500 msec, and less than 4 saccades were made\",Exclude visual search behavior\nTable 1: Criteria to establish whether a stimulus has been seen. Criteria A, B, and C are visualized in Figure 2.\nimgsrc://cloudfront.jove.com/files/ftp_upload/54031/54031fig3.jpg",
    "Figure 3. Visualization of the quantitative parameters RTF, FD, and GFA. One eye movement trace in distance from the center of the target area (in degrees, y-axis) over stimulus presentation time (in msec, x-axis). The vertical red line represents the time at which gaze entered the target area; i.e., Reaction Time to Fixation (RTF). The horizontal red line represents the total time gaze was fixated on the target area; i.e., Fixation Duration (FD). The vertical red arrow represents the width of the fixation trace, in degrees of visual angle, i.e., Gaze Fixation Area (GFA). Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/54031/54031fig3large.jpg]"
  ],
  "subjectAreas": [
    "Behavior"
  ],
  "bigAreas": [
    "Ecology & Environmental Biology"
  ]
}