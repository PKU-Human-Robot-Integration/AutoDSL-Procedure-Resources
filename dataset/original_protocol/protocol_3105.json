{
  "id": 3288,
  "origin_website": "Cell",
  "title": "A knowledge-integrated deep learning framework for cellular image analysis in parasite microbiology",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nIn this section, detailed step-by-step instructions for this protocol are provided, including computation environment setup, knowledge representation, data pre-processing, training and tuning, and evaluation and visualization, as exhibited in Figure 3[href=https://www.wicell.org#fig3].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig3.jpg\nFigure 3. The guideline of the knowledge-integrated deep learning framework for cellular image analysis in microbiology",
    "Steps 1–4. Computing environment setup. Steps 5–7. Human expert knowledge representation. Quantitative knowledge refers to knowledge that can be represented in a quantitative form, such as the microscopic and macroscopic correlations and geometric spectra. Qualitative knowledge refers to knowledge that cannot be represented quantitatively, such as knowledge from natural images. Steps 8–14. Data pre-processing. These steps ensure the suitability of the input data for model training and testing. The process begins by reading the images and their corresponding ground truths and then splitting them into training, testing, and validation data. For consistency, the images are reshaped into a unified shape as the original sizes of the image may vary. Finally, data normalization is performed to scale the value within a specific range. Depending on the size of the training data, some data augmentation techniques such as scaling, cropping, and color changes can be applied to augment the training data. Steps 15–16. Loss function and gradient descent algorithm selection. Tuning the possible combinations of different loss functions and gradient descent algorithms as much as possible for optimal training efficiency. Steps 17–18. Hyperparameter tuning. There are many hyperparameter settings such as learning rate, number of iterations, etc. They could be tested to help the model find the optimal convergence route. Steps 19–20. Performance evaluation. Different task-specific evaluation metrics can be used to evaluate the performance of the trained model. Steps 21–22. Visualization analysis. Different visualization techniques can be employed for intuitively evaluating the performance of the trained model. Results visualization reveals the effectiveness of the trained model used, while model visualization provides an understanding of the overall performance of the trained model. Steps 23–25. Ablation study is a set of experiments by removing or replacing different components of the DL model to monitor their impacts on the prediction of the model.",
    "Occlusion experiment involves blocking certain parts of the input image and analyzing the performance changes in model outputs and evaluation metrics. This reveals whether the trained model is using the relevant part of the image for prediction. Generalization analysis aims to test the generalizability of the trained model. It can be done by testing the trained model on some unseen datasets.",
    "Part 1: Computing environment setup\nTiming: 1 h (depending on the network speed)\nInstalling Anaconda.\nDownload Anaconda from “https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Windows-x86_64.exe[href=https://repo.anaconda.com/archive/Anaconda3-2023.03-1-Windows-x86_64.exe]”.\nInstall the Anaconda Navigator following the default settings.\nLaunch the Anaconda Navigator.\nInstall the “CMD.exe Prompt” from the “Home” page (Figure 4[href=https://www.wicell.org#fig4]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig4.jpg\nFigure 4. Anaconda Navigator\n(A) “Home” page illustration. The “CMD.exe Prompt” is used to execute the commands and the “Spyder IDE” is used to execute the codes.\n(B) “Environments” page illustration. It is used to manage the virtual environments.\nNote: As shown in Figure 4[href=https://www.wicell.org#fig4]A, on the left navigation bar, the “Home” page button is used to install the Spyder IDE and “CMD.exe Prompt”. And the “Environments” page, as shown in Figure 4[href=https://www.wicell.org#fig4]B is used to manage the created virtual environments. Since this is the first installation, only one virtual environment named “base (root)” will be listed under the environments list.\nVirtual environment setup (Figure 5[href=https://www.wicell.org#fig5]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig5.jpg\nFigure 5. Virtual environment setup\n(A) Launch the “CMD.exe Prompt”.\n(B) Enter the command in the command prompt to create the corresponding virtual environment.\n(C) After successful installation, a new virtual environment named “cellular_image_analysis” will be displayed on the “Environments” page.\nLaunch the “CMD.exe Prompt” on the “Home” page (Figure 5[href=https://www.wicell.org#fig5]A).\nEnter different commands based on the type of hardware being used.\nIf the CUDA-compatible GPU is being used, use the following command:\n> conda create -n cellular_image_analysis cudatoolkit=10.0.130 cudnn=7.6.5 python=3.7.16\nIf the CPU is being used, enter the command below:\n> conda create -n cellular_image_analysis python=3.7.16\nWhen prompted, type in “y” to confirm the installation process for the virtual environment (Figure 5[href=https://www.wicell.org#fig5]B).\nAfter successful installation, a virtual environment named “cellular_image_analysis” will be created and can be accessed in the environments list (Figure 5[href=https://www.wicell.org#fig5]C).\nNote: For more detailed information on managing virtual environments, please refer to the following link: “https://docs.anaconda.com/anaconda/navigator/tutorials/manage-environments/[href=https://docs.anaconda.com/anaconda/navigator/tutorials/manage-environments/]”.",
    "Package installation (Figure 6[href=https://www.wicell.org#fig6]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig6.jpg\nFigure 6. Package installation\n(A) Install “CMD.exe Prompt” in the newly installed “cellular_image_analysis” virtual environment.\n(B) Launch the “CMD.exe Prompt” and enter the commands to install the packages.\nNavigate to the “Home” page.\nSwitch to the newly installed “cellular_image_analysis” environment.\nInstall “CMD.exe Prompt” within this new virtual environment and launch it.\nInstall pytorch.\nIf the CUDA-compatible GPU is being used, run the following command to install the GPU-compatible version of pytorch.\n> pip install https://download.pytorch.org/whl/cu100/torch-1.2.0-cp37-cp37m-win_amd64.whl https://download.pytorch.org/whl/cu100/torchvision-0.4.0-cp37-cp37m-win_amd64.whl\nIf the CPU is being used, use the following command to install the CPU-only version of pytorch.\n> pip install https://download.pytorch.org/whl/cpu/torch-1.2.0%2Bcpu-cp37-cp37m-win_amd64.whl https://download.pytorch.org/whl/cpu/torchvision-0.4.0%2Bcpu-cp37-cp37m-win_amd64.whl\nInstall the tensorflow and keras.\nIf the CUDA-compatible GPU is being used, use the following command to install the GPU-compatible versions of tensorflow and keras.\n> pip install tensorflow-gpu==1.15.0 tensorboard==1.15.0 tensorflow-estimator==1.15.1 keras==2.2.4 h5py==2.10.0 git+https://www.github.com/keras-team/keras-contrib.git\nIf the CPU is being used, use the following command to install the CPU-only versions of tensorflow and keras.\n> pip install tensorflow==1.15.0 tensorboard==1.15.0 tensorflow-estimator==1.15.1 keras==2.2.4 h5py==2.10.0 git+https://www.github.com/keras-team/keras-contrib.git\nInstall the remaining necessary Python packages using the command provided below.\n> pip install numpy==1.21.6 pandas==1.3.5 opencv-python==4.6.0.66 matplotlib==3.5.3 scikit-learn==1.0.2 scikit-image==0.17.2 tqdm==4.64.1 pycocotools==2.0.5 protobuf==3.19.0\nNote: If no error (error messages are usually indicated by red text) is displayed in the command prompt, it signifies that the installation has been successful. Otherwise, please retype the command to initiate the installation process again.\nCritical: Because the compatibility of these installation packages has been tested for this step, the “pip install” command is used for optimal installation efficiency. However, in the future, if readers need to install other packages in this virtual environment, it is recommended to use the “conda install” command whenever possible. This is because “conda install” will check for compatibility between the new and existing packages, which can help to avoid package conflict automatically.",
    "Alternatives: We offer an alternative one-step installation method using two “.txt” files named “packages_gpu.txt” and “packages_cpu.txt”, which can be found on our Github repository. To use this method, simply download the “.txt” file and place it in the C drive, then use the following command for installation:\n> cd C:\\\n> pip install -r <\npackages_gpu.txt/packages_cpu.txt>\nSpyder IDE installation.\nNavigate to the “Home” page.\nSwitch to the “cellular_image_analysis” virtual environment.\nClick “install” to start the installation of Spyder IDE.\nNote: A quick guide on Spyder IDE is available at the following source: “https://docs.spyder-ide.org/current/quickstart.html[href=https://docs.spyder-ide.org/current/quickstart.html]”.\nPart 2: Knowledge representation\nTiming: More than 2 hours or days for each model (if considering data annotation and which human expert knowledge to use, the time will be extended to several days)\nThe provided templates represent human expert knowledge from multiple perspectives, as mentioned in the “description of the methods” section. In summary, DCTL uses selected morphologically similar macroscopic objects for model training. As for GFS-ExtremeNet, it incorporates the geometric spectrum into the post-processing process of the backbone network. In terms of COMI, pixel-level annotations provided by human experts and two VGGNets pre-trained on large-scale datasets are used for knowledge representation and integration. The codes have already incorporated this human expert knowledge. When using a new dataset, the human expert knowledge will be automatically integrated into the model. The following are the specific steps to follow when using a new dataset.\nPreparing new dataset in DCTL (Figure 7[href=https://www.wicell.org#fig7]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig7.jpg\nFigure 7. Example of naming for DCTL when using a new dataset\n(A–D) Naming and placement of folders and files when using a new dataset.\nCollect images of the objects to be identified (Figure 7[href=https://www.wicell.org#fig7]A).\nDivide the collected images into two folders named “train” and “test” (Figure 7[href=https://www.wicell.org#fig7]B) in a certain proportion (e.g., 8:2, 9:1, etc).",
    "Create a folder named with a numeric code (e.g., 0, 1, 2, etc) (Figure 7[href=https://www.wicell.org#fig7]C) and place the “train” and “test” folders inside it.\nPlace the folder in the “Y” folder under the path “DCTL/dataset/” (Figure 7[href=https://www.wicell.org#fig7]D).\nCollect extra images of objects with similar shapes to assist the model training. For example, Toxoplasma has a similar shape to a banana, so banana images can be collected from the internet as additional training samples. The steps for placing and naming the files are the same as the above steps, except that the “Y” folder in step c should be replaced with the “X” folder.\nNote: The number of images in the folders “X” and “Y” does not have to be equal. When collecting images, it is recommended to use a clean background that only includes the object itself, as shown in Figure 2[href=https://www.wicell.org#fig2]A.\nPreparing new dataset in GFS-ExtremeNet (Figure 8[href=https://www.wicell.org#fig8]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig8.jpg\nFigure 8. Example of naming for GFS-ExtremeNet when using a new dataset\n(A–D) Naming and placement of folders and files when using a new dataset.\nDivide the collected images into training, validation, and testing data, usually with a ratio of 8:1:1.\nPlace the training data, validation data, and testing data in folders named “train2017”, “val2017”, and “test2017” respectively (Figure 8[href=https://www.wicell.org#fig8]A).\nName the corresponding ground truths as “instances_extreme_train2017.json”, “instances_extreme_val2017.json”, and “image_info_test-dev2017.json” (Figure 8[href=https://www.wicell.org#fig8]B).\nPlace the image and annotation files in folders named “images” and “annotations” respectively (Figure 8[href=https://www.wicell.org#fig8]C).\nCreate a folder named after the object to be recognized (Figure 8[href=https://www.wicell.org#fig8]D).\nPlace the “images” folder and the “annotations” folder inside it.\nPut this folder under the path “GFS-ExtremeNet/dataset/”.",
    "Note: In GFS-ExtremeNet, each image sample may contain multiple objects, but all of these objects should belong to the same category. For example, as shown in Figure 2[href=https://www.wicell.org#fig2]B, the image of Toxoplasma should only contain Toxoplasma. Other parasites such as Babesia or Trypanosoma should not be included as they belong to different categories.\nPreparing new dataset in COMI (Figure 9[href=https://www.wicell.org#fig9]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig9.jpg\nFigure 9. Example of naming for COMI when using a new dataset\n(A–D) Naming and placement of folders and files when using a new dataset.\nName the collected images. For example, the first image collected under the z = 7 setting should be named “Z7_0” (Figure 9[href=https://www.wicell.org#fig9]A).\nPlace the newly captured in-focus images in a folder named “Z007” (Figure 9[href=https://www.wicell.org#fig9]B).\nPlace the newly captured out-of-focus images in the folder named based on the used z-axis settings during collection, such as “Z004” for z = 4 (Figure 9[href=https://www.wicell.org#fig9]B).\nStore the dataset of in-focus and out-of-focus images in the same folder, named based on the category of the objects being captured (Figure 9[href=https://www.wicell.org#fig9]C).\nPlace this folder in the “COMI/dataset/BPAEC” directory (Figure 9[href=https://www.wicell.org#fig9]D).\nNote: It is important to pair each collected in-focus image with its corresponding out-of-focus image. Furthermore, it is recommended that the images only capture the target object without any other objects in the background, as shown in Figure 2[href=https://www.wicell.org#fig2]C. These factors will help to ensure the reliability of the dataset.\nPart 3: Data pre-processing\nTiming: 5–20 mins for each model (depending on the hardware and datasets)\nThis part explains the procedures for reading and pre-processing images. The time required for these processes generally varies from 5 to 20 min, depending on the size of the dataset and CPU performance.",
    "Optional: To include additional data pre-processing steps, please refer to the relevant code files for DCTL at “lib/reader_image.py” and “lib/utils.py”, or the “.py” files located in the “db” and “sample” folders for GFS-ExtremeNet. For COMI, refer to the “utils/read_image.py” file. Details for configuring all three models can be found in their respective “train.py” and “test.py” files. Additionally, GFS-ExtremeNet utilizes a separate configuration file at “config/GFS-ExtremeNet.json”.\nFolder structure check (Figure 10[href=https://www.wicell.org#fig10]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig10.jpg\nFigure 10. File structure consistency check\n(A and B) File structure examination. Switch to the “cellular_image_analysis” virtual environment and launch the Spyder IDE. Switch to the directory of the code repository to be run, and the File pane will display the file structure of the code and dataset.\n(C) File structure of different code repositories. The downloaded code repositories need to keep an identical file structure in order to get the entire pipeline running.\n(D) File structure of different datasets. Three different tasks in cellular image analysis were shown from left to right, including classification (DCTL), detection (GFS-ExtremeNet), and reconstruction (COMI). Apart from the dataset of COMI, the datasets for DCTL and GFS-ExtremeNet have been split into training data and testing data.\nSwitch to the “cellular_image_analysis” virtual environment (Figure 10[href=https://www.wicell.org#fig10]A).\nLaunch Spyder IDE (Figure 10[href=https://www.wicell.org#fig10]A).\nChoose the directory where the code repository is located (Figure 10[href=https://www.wicell.org#fig10]B).\nCheck whether the folder structure in the Files pane is consistent with Figure 10[href=https://www.wicell.org#fig10]C.\nCheck whether the folder structure of the downloaded datasets is consistent with Figure 10[href=https://www.wicell.org#fig10]D.\nDataset specification and reading.\nSpecify the dataset used in GFS-ExtremeNet.\nOpen the “config/GFS-ExtremeNet.json” file.\nSpecify a variable named “dataset” (i.e., Babesia, Toxoplasma, and Trypanosoma), which is stored in a dictionary variable named “db”.\n>{\n> \"sytem\":{\n> \"data_dir\": \"./dataset\",\n> },\n> \"db\":{\n> \"dataset\": \"Babesia\",\n> }\n>}\nSpecify the dataset used in COMI.",
    "Open the “train.py” file.\nSpecify a variable named “cell_type” that determines the image category to be used (i.e., actin, mitochondria, and nucleus).\nSpecify a variable named “z_depth” that determines the out-of-focus image datasets to be used (i.e., Z004, Z005, etc).\n> cell_type = 'actin'\n> z_depth = 'Z005'\nNote: Once the code starts running, the images (.png or .jpg) and their corresponding ground truths (.jpg or .json) are read sequentially from the “dataset” folders and converted into numpy arrays. Since DCTL utilizes the entire image datasets for training, there is no need to specify dataset parameters.\nDataset splitting.\nSpecify the ratio of training data and testing data in COMI.\nOpen the “train.py” file.\nSpecify the variable “ratio” as 0.8 to split the dataset into training data and testing data at a proportion of 8:2.\n> ratio = 0.8\nNote: The loaded images are typically divided into different subsets for the purpose of training and testing. In most cases, the ratio of training to testing data is set between 9:1 and 7:3.\nNote: In the case of DCTL and GFS-ExtremeNet, the datasets provided have already been split into training and testing data with a proportion of 8:2. Therefore, they do not need to specify the ratio of training data and testing data.\nData reshaping.\nSpecify the input image size used by DCTL.\nOpen the “train.py” file.\nSpecify the “image_size” variable as follows:\n> tf.flags.DEFINE_integer('image_size', 256, 'image size, default: 256')\nSpecify the input image size used by GFS-ExtremeNet.\nOpen the “config/GFS-ExtremeNet.json” file.\nSpecify the variable “input_size” as follows:\n>\n{\n> \"db\":{\n> \"input_size\": [511, 511],\n> }\n>}\nSpecify the input image size used by COMI.\nOpen the “train.py” file.\nSpecify the variable “input_shape” as follows:\n> input_shape = (128, 128, 3)",
    "Note: It is necessary to reshape the image into a uniform size to ensure compatibility with the network’s input size. For DCTL, GFS-ExtremeNet, and COMI, the default input image sizes are (3, 256, 256), (3, 511, 511), and (3, 128, 128) respectively.\nCritical: It should be noted that changing the input image size would require adjusting the layer settings of the model accordingly. Hence, if it is not necessary, please do not modify this setting.\nData normalization.\nData normalization process used in DCTL.\nOpen the “lib/reader_image.py” file.\nThe corresponding code segment is as follows:\n> image = np.array(image) / 127.5 - 1.\nData normalization process used in GFS-ExtremeNet.\nOpen the “utils/image.py” file.\nThe corresponding code segment is as follows:\n>def normalize(image, mean, std):\n> image -= mean\n> image /= std\nData normalization process used in COMI.\nOpen the “lib/read_image.py” file.\nThe corresponding code segment is as follows:\n>hrhq_train = np.array(hrhq_train)\n>hrhq_train = hrhq_train.astype('float32') /127.5 - 1.\n>hrhq_test = np.array(hrhq_test)\n>hrhq_test = hrhq_test.astype('float32') /127.5 - 1.\nNote: Data normalization is a technique used to scale the value of each tensor within a specific range, ensuring a similar data distribution among input tensors. The provided templates use two data normalization methods. DCTL and COMI use a simple method, as shown in Equation 1[href=https://www.wicell.org#fd1], which scales the values between -1 and 1 by first dividing the image tensor x by 127.5 and then subtracting by 1. GFS-ExtremeNet uses a more formal z-score normalization, as shown in Equation 2[href=https://www.wicell.org#fd2], which scales the values between 0 and 1 by dividing the image tensor x by its mean value μ and then subtracting the standard deviation σ.\n(Equation 1)\nx\n′\n=\nx\n127.5\n−\n1\n(Equation 2)\nx\n′\n=\nx\n−\nμ\nσ",
    "Note: Different normalization methods have a limited impact on performance, but they are important for the backbone network to learn more general knowledge.\nData augmentation.\nSpecify the data augmentation operations used in GFS-ExtremeNet.\nOpen the “config/GFS-ExtremeNet.json” file.\nSet the variables “rand_scale_min”, “rand_scale_max”, “rand_scale_step”, and “rand_scales” to 0.6, 1.4, 0.1, and null respectively.\nSet the variable “rand_crop” to true to enable the cropping augmentation.\nSet the variable “rand_color” to true to enable the color jittering augmentation.\n>{\n> \"db\":{\n> \"rand_scale_min\": 0.6,\n> \"rand_scale_max\": 1.4,\n> \"rand_scale_step\": 0.1,\n> \"rand_scales\": null,\n> \"rand_crop\": true,\n> \"rand_color\": true,\n> }\n>}\nNote: Data augmentation is a technique that expands training data without the need for manual collection and labeling of additional images. In GFS-ExtremeNet, due to the sparsity of objects within each image, multiple data augmentation techniques are supported.\nBatch size configuration.\nSpecify the batch size setting used in DCTL.\nOpen the “train.py” file.\nSpecify the variable “batch_size” based on the hardware resources.\n> tf.flags.DEFINE_integer('batch_size', 1, 'batch size, default: 1')\nSpecify the batch size setting used in GFS-ExtremeNet.\nOpen the “config/GFS-ExtremeNet.json” file.\nSpecify the variable “batch_size” based on the hardware resources.\n>{\n> \"system\":{\n> \"batch_size\": 1,\n> }\n>}\nSpecify the batch size setting used in COMI.\nOpen the “train.py” file.\nSpecify the variable “batch_size” based on the hardware resources.\n> batch_size = 1\nNote: In most scenarios, increasing the batch size can improve GPU utilization and accelerate model training. However, it is important to note that if model training is performed on a CPU, increasing the batch size may not significantly speed up the training process. Nevertheless, it is always advisable to set the batch size to the maximum possible value based on the available hardware resources.\nPart 4: Training and tuning",
    "Timing: More than 2 days or more for each model (depending on the hardware, initial setting, and datasets)\nRepeatedly fine-tuning a DL model is crucial for achieving its optimal performance and generalization. The duration of model training and tuning can vary depending on various factors (e.g., the hardware used, initial hyperparameter setting, etc). It may take several hours or even days to make delicate adjustments.\nOptional: Customization of the training pipeline can be done by editing the “train.py” file in each code repository.\nCritical: The model should be fine-tuned based on the results from validation data, as tuning on the testing data may result in model overfitting and reduced generalization performance.\nCode execution for model training and training visualization (Figure 11[href=https://www.wicell.org#fig11]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig11.jpg\nFigure 11. Code execution for model training and training visualization\n(A) Code execution procedure. Open the “train.py” file in the Spyder IDE and run it. If successful, the IPython Console will display the training information. After training, the trained model weights will be saved in the “checkpoint” folder and the loss curve during the training process will be displayed in the Plots pane.\n(B) Visualization by loss curves under different learning rate settings. An appropriate learning rate setting can help the model find the global optimal solution while a lower or higher learning rate setting will lead to the suboptimal solution.\nOpen the “train.py” file within each code repository.\nRun the selected “train.py” file.\nDisplay the training information in the IPython Console.\nSave the trained model weights for DCTL (.index, .meta, and .data-00000-of-00001), GFS-ExtremeNet (.pkl), and COMI (.h5) in their corresponding “checkpoints” folder.\nVisualize the loss curve of the training process in the Plots pane.",
    "Note: Using the trend of the loss curve to guide the tuning process can help the DL model find a more optimal convergence path. To demonstrate how to tune the model based on the loss curve, the following steps will use Figure 11[href=https://www.wicell.org#fig11]B as an illustrative example.\nFine-tune the loss functions and gradient descent algorithms (optional). The default gradient descent algorithm used in the provided templates is the adaptive moment estimation (Adam),21[href=https://www.wicell.org#bib21] and the loss functions employed are consistent with those shown in Figure 2[href=https://www.wicell.org#fig2].\nOptional: The choice of loss functions and gradient descent algorithms can impact whether the model can escape local optima and converge to a better location. Customization of the loss functions and gradient descent algorithms can be done by modifying the related code files. For DCTL, the changes can be made in the “train.py” and “models/model.py” files, while for GFS-ExtremeNet, it can be done in the “models/py_utils/exkp.py” and “nnet/py_factor.py” files. As for COMI, the customization can be implemented in the “train.py” file.\nFine-tune the learning rate configuration. Decrease the learning rate when the model converges too rapidly and increase it when the model converges too slowly.\nTune the learning rate configuration used in DCTL.\nOpen the “train.py” file.\nTune the variable “learning_rate” to regulate the learning rate configuration of CycleGAN.\nTune the variable “feature_learning_rate” to regulate the learning rate configuration of feature extractor.\n>tf.flags.DEFINE_float('learning_rate', 2e-4, 'initial learning rate for CycleGAN, default: 0.0002') >tf.flags.DEFINE_float('feature_learning_rate', 2e-6, 'initial learning rate for feature extractor, default: 0.000002')\nTune the learning rate configuration used in GFS-ExtremeNet.\nOpen the “config/GFS-ExtremeNet.json” file.\nTune the variable “learning_rate” to regulate the learning rate configuration of GFS-ExtremeNet.\n>{\n> \"system\":{\n> \"learning_rate\": 0.0025,\n> }\n>}\nTune the learning rate configuration used in COMI.\nOpen the “train.py” file.",
    "Tune the variable “learning_rate” to regulate the learning rate configuration of COMI.\n> learning_rate = 1e-4\nRetrain the model based on step 15.\nNote: Learning rate is a positive floating value between 0 and 1 that controls how much the model parameters should be adjusted in response to the loss values. Figure 11[href=https://www.wicell.org#fig11]B illustrates some typical loss curves under different learning rate settings. If the learning rate is set too high (as shown by the red curve), the model may converge too rapidly and miss better solutions. Conversely, if the learning rate is set too small (as shown by the blue curve), the convergence may be slow and lead to a suboptimal solution.\nNote: When conducting a learning rate experiment, it is generally recommended to commence with a high learning rate and reduce it by a factor of 10 each time until the optimal range is found. Once the optimal range is identified, further fine-tuning can be performed within this range to identify the optimal setting.\nFine-tune the iteration setting by selecting the value at which the loss values stabilize and cease decreases.\nTune the iteration setting used in DCTL.\nOpen the “train.py” file.\nTune the variable “max_iter” to set the maximum number of iterations.\n> tf.flags.DEFINE_integer('max_iter', 100000, 'max_iter, default: 100000')\nTune the iteration setting used in GFS-ExtremeNet.\nOpen the “config/GFS-ExtremeNet.json” file.\nTune the variable “max_iter” to control the maximum number of iterations.\n>{\n> \"system\":{\n> \"max_iter\": 100000,\n> }\n>}\nTune the iteration setting used in COMI.\nOpen the “train.py” file.\nTune the variable “max_iter” to regulate the maximum number of iterations.\n> max_iter = 25000\nRetrain the model based on step 15.",
    "Note: The setting of iteration is another important factor in determining the convergence of DL models. This parameter is an integer value starting from 1 and can be set to an infinite positive value. As shown in Figure 11[href=https://www.wicell.org#fig11]B if the number of iterations is set too small (e.g., 5000), the model will stop training before reaching its optimal convergence point. Conversely, if the number of iterations is set too large (e.g., 35000), the model will be updated too many times, resulting in overfitting to the training data.\nNote: An epoch is a pass-through of the entire dataset to the DL model. However, passing the entire dataset at once will generate too much updated information, which is likely to exceed the hardware capacity. Therefore, the dataset is generally partitioned into mini-batches and passed through the DL model several times, and each pass-through process of a mini-batch is an iteration. In most cases, the iteration is set based on the number of epochs to ensure that the entire dataset can be passed through the model multiple times. For consistency, the provided codes use iteration as the configurable hyperparameter, and the model is saved in the last iteration.\nPart 5: Evaluation and visualization\nTiming: 5–30 min for each model (depending on the hardware, models, and datasets)\nThis part explains how to use testing data to evaluate the overall performance of the trained DL model after fine-tuning. Due to differences between target tasks and DL frameworks being used, the supported functionalities between different codes may vary slightly.\nSpecify the directory of the trained model to be evaluated.\nSpecify the model weights that are to be evaluated for DCTL.\nOpen the “test.py” file.\nSet the variable “FLAGS.meta_dir” to the path of the “.meta” file.",
    "Set the variable “FLAGS.saved_weights_dir” as the parent directory for storing the “.meta” file.\n>tf.flags.DEFINE_string('meta_dir', 'checkpoints/20230504-155358/our_model.ckpt-49.meta','directory of the saved .meta file of the saved model weights that you wish to testify (e.g. checkpoints/20221119-2211/bestmodel/our_model.ckpt-10.meta), default: None') >tf.flags.DEFINE_string('saved_weights_dir', 'checkpoints/20230504-155358', 'direcotry of the saved model weights folder that you wish to testify (e.g. checkpoints/20221119-2211/bestmodel), default: None')\nSpecify the model weights to be evaluated for GFS-ExtremeNet.\nOpen the “test.py” file.\nSet the variable “args.weights_dir” to the directory of the model weights.\nSet the variable “args.split” to the proportion of the dataset used for evaluation (i.e., training, testing, validation).\n>args.weights_dir = \"checkpoints/20221127-165855/GFS-ExtremeNet_best.pkl\"\n>args.split = \"validation\"\nSpecify the model weights that are to be evaluated in COMI.\nOpen the “test.py” file.\nSet the variable “weights_dir” to the path of the “.h5” file.\n> weights_dir = 'checkpoints/20221129-014343/deblursrgan4_actin_Z005_weights/iteration_240'\nResult of the evaluation metrics (Figure 12[href=https://www.wicell.org#fig12]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig12.jpg\nFigure 12. Code execution for testing and evaluation\nOpen the “test.py” file within each code repository.\nRun the selected “test.py” file.\nDisplay the results of the evaluation metrics in the IPython Console.\nNote: Details about the evaluation metrics can be found in the literature of DCTL,1[href=https://www.wicell.org#bib1] GFS-ExtremeNet,2[href=https://www.wicell.org#bib2] and COMI.3[href=https://www.wicell.org#bib3]\nResult visualization (Figures 12[href=https://www.wicell.org#fig12] and 13[href=https://www.wicell.org#fig13]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig13.jpg\nFigure 13. Visualization methods for evaluating the deep learning models\n(A) Storage path for visualization results. (i)-(iii) display the storage paths for the visualization results of DCTL, GFS-ExtremeNet, and COMI, located in the “results” folder respectively.",
    "(B) Visualization by receiver operating characteristic curve and t-distributed stochastic neighbor embedding. (i) The area under the curve reflects the classification performance of the trained model. The larger the covered area the higher the classification performance of the trained model. (ii) The distribution of different color points reflects whether the model learns the decisive features for classification. If the model does not learn the decisive features, different color points will be mixed without a clear boundary. While if the model does learn the decisive features, a relatively clear boundary can be observed between different color points.\n(C) Visualization by extreme-point heatmap. (i)-(iv) Extreme-point heatmaps in four different orientations. (v) Center-point heatmap is obtained by calculating the center point of the extreme points in four orientations.\nOpen the “test.py” file within each code repository.\nRun the selected “test.py” file.\nDisplay the visualization results on the Plots pane.\nSave the visualization results for each model in their corresponding “results” folder (Figure 13[href=https://www.wicell.org#fig13]A).",
    "Note: Result visualization for DCTL includes the receiver operating characteristic (ROC) curve and the t-distributed stochastic neighbor embedding (t-SNE) visualization. The ROC curve area reflects the classification performance of the model. As displayed on the left of Figure 13[href=https://www.wicell.org#fig13]B, the diagonal blue line represents random classification. If the ROC curve is below the diagonal blue line, like the red curve, it means that the trained model is worse than random classification. Otherwise, if the ROC curve is above the diagonal blue line, it means the training is effective and the trained model is better than randomly classifying the samples. The right of Figure 13[href=https://www.wicell.org#fig13]B is the t-SNE visualization, where each color denotes a parasite category, and the points with the same color form a cluster. If different clusters are well separated, and the inner points with the same color are tightly clustered, then the model is deemed to have learned the decisive features to identify different parasites.\nNote: Result visualization of GFS-ExtremeNet includes bounding boxes visualization and extreme-point heatmaps visualization. The bounding box visualization is similar to Figure 2[href=https://www.wicell.org#fig2]B and the extreme-point heatmaps are shown in Figure 13[href=https://www.wicell.org#fig13]C. These visualizations demonstrate the detection capability of the trained DL model.\nNote: Result visualization of COMI is the restored in-focus images. The restored in-focus images can be compared with the corresponding input images and ground truth images to assess whether the trained DL model has learned useful features for image reconstruction.\nCritical: The following steps 22 to 25 require proficiency in DL and programming skills. Readers without a background in these areas can choose to skip them.\nClass activation maps (CAM) (optional).\nLoad the trained model weights.\nSelect a convolutional layer to generate feature maps.\nExtract a weight vector from the feature maps.",
    "Transform the feature maps into a single activation map which indicates the region of focus.\nNote: For professionals who want to explore different visualization techniques, there are several options available, such as CAM and their variants.22[href=https://www.wicell.org#bib22] Figure 14[href=https://www.wicell.org#fig14] illustrates the use of CAM for model visualization. If the visualization outcomes focus on the target to be identified, it indicates that the trained model is using the correct part of the images for prediction. Otherwise, some adjustments and retraining of the model may be necessary.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig14.jpg\nFigure 14. Visualizations by class activation map\nA well-trained DL model should focus on the parasite itself whereas a poorly trained model will focus more on some unnecessary regions like the external cell or only part of the parasite.\nAblation study (optional).\nAblation study for DCTL.\nRandomize the order of the macroscopic and microscopic object pair.\nRetrain the model\nMonitor the changes in the model classification performance.\nNote: A decrease in the performance after disruption indicates that the use of morphologically similar objects is effective during model training. Other components, such as feature extractors outlined in Figure 1[href=https://www.wicell.org#fig1]B, can also be tested to verify their validity.\nAblation study for GFS-ExtremeNet.\nChange the cluster radius Rs into a negative value to disable the use of geometric spectrum.\n>{\n> \"db\":{\n> \"cluster_radius\": 500,\n> }\n>}\nObserve the changes in the model detection performance.\nAblation study for COMI.\nRemove the use of pre-trained VGGNet.\nRetrain the model.\nMonitor the changes in model restoration performance.\nNote: Ablation study is a set of experiments used to test whether the used components in the DL models are necessary or not.",
    "Critical: It is worth noting that an ablation study should be conducted only after the model tuning process is completed. Otherwise, the results might not reflect the validity of the components used. Moreover, further adjustments on hyperparameters may be needed to fully exploit the potential of each component.\nOcclusion experiment (optional).\nOcclusion experiment for DCTL.\nBlock a portion of parasites and their external parasitized cells in the image.\nMonitor the classification performance changes.\nNote: The occlusion experiment of DCTL reveals whether the use of macroscopic images is helping the model to learn the essential morphological features for parasite classification.\nOcclusion experiment for GFS-ExtremeNet.\nBlock a small part of the target to be detected in the image.\nMonitor the changes in the bounding box and confidence score.\nNote: The occlusion experiment of GFS-ExtremeNet indicates which parts of the image the trained model is using for detection.\nOcclusion experiment for COMI.\nBlock parts of the out-of-focus image.\nMonitor the changes in the restored in-focus image.\nNote: The occlusion experiment of COMI allows us to see whether the trained model uses global information for deblurring.\nNote: Occlusion experiment is used to determine whether the trained model is utilizing the appropriate parts of the image for prediction.23[href=https://www.wicell.org#bib23],24[href=https://www.wicell.org#bib24] This is done by blocking certain parts of the input image and analyzing the changes in model outputs and evaluation metrics, as demonstrated in Figure 15[href=https://www.wicell.org#fig15]. In practical applications, Photoshop can be used to block a portion of an image with black color.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2879-Fig15.jpg\nFigure 15. Visualizations by occlusion experiment\nAreas of interest can be tested by blocking different parts of the images. Then, changes in the model outputs and evaluation metrics can be measured to highlight the areas that the DL models consider important in prediction.\nGeneralization analysis (optional).",
    "Prepare a new dataset (steps 5 to 7).\nEvaluate the model performance on the new dataset.\nNote: The generalization performance of a model refers to how well the trained model performs on datasets that it has not been trained on. It can also be assessed by comparing its predicted results with those of human experts. By identifying the samples on which the model performs poorly, it is possible to use these challenging examples to fine-tune the model and enhance its capabilities."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Microscopy",
    "Bioinformatics"
  ],
  "bigAreas": [
    "Bioengineering & Technology",
    "Bioinformatics & Computational Biology"
  ]
}