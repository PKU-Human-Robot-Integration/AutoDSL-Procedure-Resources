{
  "id": 2259,
  "origin_website": "Cell",
  "title": "Protocol for live cell image segmentation to profile cellular morphodynamics using MARS-Net",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nPart 1. Installing MARS-Net\nTiming: 45 min\nFull installation of MARS-Net includes downloading the MARS-Net package from GitHub and installing its software requirements.\nSetup the Anaconda environment by running the following commands.\nIn the command prompt in Windows 10/11:\n>conda env create --name marsnet –file environment_windows.yml\nIn the Linux Terminal in Ubuntu:\n>conda env create –name marsnet –file environment_linux.yml\n>conda activate marsnet\nTo download the MARS-Net pipeline from GitHub Repository, click the link and navigate to the green “Code” button on the page. Click this, and then select “Download ZIP”. Be sure to also install the requirements listed in the key resources table[href=https://www.wicell.org#key-resources-table] above (also located on the MARS-Net pipeline GitHub page).\nUnzip the downloaded zip file from the GitHub and open the unzipped folder.\nOpen “UserParams.py” file with a text editor or IDE such as PyCharm.\nNote: UserParams.py is a file that contains all the configurations necessary for running Mars-Net.\nEdit the following parameters inside __init__() function within UserParams class. These specify the type of the deep learning model to use, location of datasets, and training configurations. The parameters for training step and prediction step are independent and must be set separately. For example, if statement block from line 62 to 81 specify parameters for training the ‘multi_micro’ model and the if statement block from line 422 to 441 specify parameters for segmenting live cell movies using the trained ‘multi_micro’ model.\n“strategy_type”.\nSet as the type of deep learning model to use (ex. VGG19_dropout, unet).\nNote: We recommend users to always use the VGG19_dropout model since we experimentally showed that the VGG19_dropout model segmented all live cell movies in our dataset with the highest accuracy. Other options are U-Net and plain VGG19 which are the baseline models.\n“dataset_folders”.",
    "Set as the location where your images and masks are stored (ex. [“..\\assets\\”, “..\\assets\\second_dataset\\”].\nNote: The sample images and labels are in the “..\\assets\\040119_PtK1_S01_01_phase_ROI2\\” directory so that users can practice the written procedures.\n“img_type”.\nSet as the type of file your image is (ex. “.png\").\n“img_folders”.\nSet as a list of folders where images are located (ex. [\"/img/\",\"/img/\"]).\n“mask_folders”.\nSet as a list of folders where masks are located (ex. [\"/mask/\",\" mask\"]).\nNote: These are the folders for ground truth masks of the images.\n“frame_list”.\nSet as a list of the number of images to train the model (ex. [1,2,6,10]).\nNote: [1,2,6,10] means that the first model will be trained on one of the randomly selected images in the dataset folder. Then the second model will be trained on two images, and so on. Model can train on as few as just one image and its corresponding ground truth mask or as many frames as possible from the live cell movie but more training frames require more labeling efforts.\n“dataset_names”.\nSet as a list of the dataset names, which will be parent folders containing img and mask folders. (ex. [\"dataset1\",\" dataset2\"]).",
    "Note: For training, more than one dataset name should be specified in the list because our pipeline performs a leave-one-movie-out cross validation. For instance, given the list of datasets m1, m2, m3 and m4, first three datasets (m1, m2 and m3) will be used for training and m4 will be used as a test set for evaluating the segmentation accuracy. Then, other combination of three datasets such as (m1, m3, m4) will be used for training and m2 will be used as a test set for evaluating the segmentation accuracy. For prediction, only one dataset can be specified in the list, which would segment one live cell movie using a trained model. See part 4. Training MARS-Net and segmenting movies[href=https://www.wicell.org#sec4.4] for more details.\n“model_names”.\nSet as a list of the model names, these names can be any distinguishing characters (ex. [\"A\",\" B\"]).\nNote: As described in the note in step g, each model name in the list represents a different model trained during the leave-one-movie-out cross validation. These models use the same deep learning architecture specified in the “strategy_type” but are trained on different combinations of datasets.\n“REPEAT_MAX”.\nSet as the number of times to repeat cross validation (ex. 1 or 5).\nNote: Repeating the cross validation is for evaluating the deep learning model robustly. The same deep learning model trained with different “random seed” can yield slightly different performances. Therefore, in each repetition, we vary the random seed and train the same model repetitively to see the variance of our models’ performance. 5 is preferred for robust evaluation of the model but it also takes five times longer to train and evaluate. So, in practice, it is set to 1.\nNote: Make sure the list length is equal for “model_names”, “img_folders”, “mask_folders”, “dataset_folders”, and “dataset_names”.\nPart 2. Data organization",
    "Timing: 3 min per movie\nOrganizing the data with the correct labels, in the correct folders, etc. is a key component in insuring MARS-Net is run effectively.\nCreate a folder with a unique name, and inside that folder, create subfolders with names “img” and “mask”.\nIf the dataset is a movie, separate the movie into images (we used PNG format) with ImageJ.\nOpen ImageJ, load in the movie, click “File”, “Save As” then “Image Sequence”.\nMake sure that the images are generated in order with filenames having the four-digit frame id with leading zeros at the end, starting with ‘0001’.\nFor example, given a movie with 200 frames, the first frame’s filename is cell_0001.png, the second frame’s filename is cell_0002.png, and the last frame’s filename is cell_0200.png.\nPut images into the “img” folder and leave the mask folder empty. The mask folder will be filled with labeled images later.\nRepeat previous steps for each additional movie.\nPart 3. Labeling images\nTiming: 3 min per image\nThe labeling tool (Figure 1[href=https://www.wicell.org#fig1]) facilitates labeling raw images semi-automatically for training MARS-Net. Gaussian, bilateral, and guided blurring operations denoise the raw images, followed by the Canny edge detector (Canny, 1986[href=https://www.wicell.org#bib3]) extracting edges from three blurred images. And then, they are combined into one preliminary edge image. Human annotators should fix any errors of the preliminary edges manually (Figure 2[href=https://www.wicell.org#fig2]). All files for labeling tools are found in the “label_tool” folder. The timing is from labeling the image of size 392 × 474 by an expert.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1729-Fig1.jpg\nFigure 1. Labeling procedure",
    "Simplified diagram of four steps involved in semi-automatically labeling live cell movie. In the first step, prepare the raw image and set the parameters of the label tool. In the second step, extract the edge from the raw image. It is inaccurate since edge extraction is performed by a traditional algorithm. Therefore, in the third step, a user needs to fix extracted edge image by connecting fragmented edges or removing incorrect edges. A manually corrected image is shown as an example. Then in the fourth step, a simple flood fill algorithm can segment the cell body region in the fully connected edge image. Scale Bar: 32.5 μm.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1729-Fig2.jpg\nFigure 2. Manual correction of labels\nThe image on the left contains fragmented edges or isolated edges that are extracted due to noise. The red circles are used to indicate some of those regions. The image on the right shows the result after all fragmented or isolated edges that are corrected manually in GIMP. Scale bars: 32.5 μm.\nNote: This Part can be skipped if no movie needs to be labeled for training and evaluation and a user just wants to use our pretrained model to segment live cell movies. If a user chooses to label a dataset, we recommend labeling about 1% of the data for training. For a robust evaluation of the trained model, we recommend labeling about 10% of the data. Throughout this Part, it is recommended to use a similar naming scheme for folders and images as the example in MARS-Net Github.\nSpecify the location of image files to the label in the “user_params.py”:\nSet variable “a_dataset” to the name of the folder with original images.\nSet variable “img_root_path” to the path where original images are located.\nSave “user_params.py”.",
    "Note: “user_params.py” is not the same as “UserParams.py” used in previous steps. The “user_params.py” file is specifically for the labeling tool.\nDetermine the optimal hysteresis thresholding for canny detector and kernel size for blurring (parameters located in “user_params.py”):\nRun the following code:\n>python explore_edge_extraction_user_params.py\nWhen “explore_edge_extraction_user_params.py” completes all edge extractions, results will be located in the “generated_explore_edge” folder.\nSelect each generated image and determine which parameter values are best for optimizing hysteresis thresholding.\nSet the selected best thresholding values in “user_params.py” for “canny_std_multiplier” and “denoise_kernel_size” parameters located at around line 11 and 12.\nSave “user_params.py”.\nExtract edges with optimized parameters:\nRun the following code:\n>python extract_edge.py\nWhen “extract_edge.py” completes all generated edge images, results will be located in the “generated_edge” folder.\nManually fix the generated edge images:\nThe image edges will not always be correctly connected, even with the optimized parameters. Hence, it is important to manually connect any fragmented edges and remove the wrong edges in the image. Download ImageJ or GIMP to manually fix after overlaying the edge on the original image∗\nReplace any generated edge images in the “generated_edge” folder with manually fixed edge images.\nNote: ∗When using ImageJ for manual edge fixing, use the freehand tool to obtain the true edges of the cells. When using GIMP, overlay the image with a mask that is 50% transparent. From here, draw the true edges of the cell with the paintbrush tool.\nPost-process the edge images to fill the edge images:\nRun the following code:\n>python segment_edge.py",
    "The “segment_edge.py” will ask for how many backgrounds to fill in your image, and one pair of (x, y) coordinates in each background. The total number of backgrounds will be determined based on the image of the cell. Once you locate all backgrounds in the image, provide any (x, y) coordinates in each respective background.\nWhen “segment_edge.py” completes all segmentations, results will be located in the “generated_segmentation” folder.\nMove these labeled images to the “assets” folder to begin training the model.\nRepeat previous steps in this Part on all cell movies before moving to the next Part. All movies are labeled for training multiple models for cross-validation in the evaluation Part. Labeling all live cell movies is only necessary for training multiple models and evaluating them by leave-one-movie-out cross validation.\nPart 4. Training MARS-Net and segmenting movies\nTiming: 4 h",
    "MARS-Net (Jang et al., 2021[href=https://www.wicell.org#bib7]) takes a transfer learning approach (Bertasius et al., 2015[href=https://www.wicell.org#bib23]; Iglovikov et al., 2018[href=https://www.wicell.org#bib24]; Kim et al., 2018[href=https://www.wicell.org#bib8]; Long et al., 2015[href=https://www.wicell.org#bib25]; Vaidyanathan et al., 2021[href=https://www.wicell.org#bib18]) by integrating ImageNet-pretrained VGG19 encoder and U-Net decoder with additional dropout layers (Deng et al., 2009[href=https://www.wicell.org#bib5]; Ronneberger et al., 2015[href=https://www.wicell.org#bib15]; Simonyan and Zisserman, 2015[href=https://www.wicell.org#bib26]; Srivastavanitish et al., 2014[href=https://www.wicell.org#bib17]), trained on the datasets from multiple types of microscopy. MARS-Net accepts the segmented images generated in Part 3 of this protocol. The code described in the following steps crops each image of the dataset into 200 of 128 × 128 patches, trains the deep learning model, and predicts the segmentation of the images in the test set, which is defined in UserParams.py in Part 1. The cropping code samples 60% of the patches from the cell edges, 20% of the patches from the cell interior and the other 20% from the background outside the cell. Titan RTX took about 4 h, but training time can vary based on which GPU is used.\nIn the terminal, navigate to the “crop” folder and run the “crop_augment_split.py” file to begin cropping patches for training:\n>cd “location of crop folder”\n>python crop_augment_split.py\nWhen “crop_augment_split.py” finishes cropping the images, navigate to the “models” folder in the terminal and run the following code to begin training MARS-Net:\n>cd “location of model folder”\n>python train_mars.py\nDuring training which might take a few hours, the terminal will show the training loss and dice coefficient on the training set and the validation loss and dice coefficient on the validation set for each epoch. For metrics on both training and validation sets, usually high dice coefficient at around 0.98 and low loss at around 0.01 indicates the successful training.",
    "While the model is training, set up parameters for prediction in UserParams.py as described in the step 5 in Part 1 to specify which live cell movie to segment.\nWhen “train_mars.py” finishes training the model, navigate to the “models” folder and run the following code to segment the live cell movies:\n>python predict.py\nThe segmented live cell movie will be generated in the “models/results/predict_wholeframe_round1_∗“directory where ∗ represents the model name specified in the “strategy_type” parameter.\nNote: Please remember this directory of segmented result since it will be used in Part 6 for quantification of morphodynamics.\nCritical: MARS-Net will over-write previous cropped files, trained models and predicted results saved with the same “strategy_type”, so please use a unique “strategy_type” in UserParams.py before cropping, training and prediction.\nNote: A user can predict segmentation of our sample live cell movie with pretrained U-Net and VGG19D models without going through the cropping and training steps above. Follow the instructions below if you want to use our pretrained model without cropping and training. Also, follow the instructions below if you don’t want to perform leave-one-movie-out cross validation but segment a live cell movie using one of the trained models. The instructions below are a more user-friendly way to use our pipeline that only requires four parameters from the user, without the need to specify any parameters in UserParams.py:\nTo use our pretrained U-Net or VGG19D models, download them from the following google drive as a zip file: https://drive.google.com/drive/folders/1FLP0D-Y9-DHQmhC-LBZChdUSe6W5zyPw?usp=sharing[href=https://drive.google.com/drive/folders/1FLP0D-Y9-DHQmhC-LBZChdUSe6W5zyPw?usp=sharing].\nNote: The sample images and labels are in the “..\\assets\\040119_PtK1_S01_01_phase_ROI2\\” directory so that users can practice the written procedures.\nCreate the new folder named “results” under the “models” folder.\nMove the downloaded zip file from the google drive into the “results” folder and unzip it. Then, there should be “model_round1_Multi_VGG19D” folder “model_round1_Single_Unet” folder inside the “results” folder.",
    "Navigate to the “models” folder in the terminal and run the following code to segment the live cell movie. Please specify trained_model_path, live_cell_images_path, save_path, and img_format as input arguments in the command line as follows:\n> python predict_simple.py --trained_model_path ./results/model_round1_Multi_VGG19D/model_frame2_D_repeat0.hdf5 --live_cell_images_path ../assets/040119_PtK1_S01_01_phase_2_DMSO_nd_02/img_all/ --save_path ./results/ --img_format .png\nTo use one of the trained models from leave-one-movie-out cross validation scheme above, navigate to the folder in the directory “models/results/model_round1_∗/” where ∗ stands for the name of the model specified in the “strategy_type” parameter. Inside that folder, there should be one or more trained models with the name “model_∗_repeat0.hdf5”. Choose one of those models for prediction in step 26 based on the evaluation of each cross validation in Part 5.\nPart 5. Evaluation of the segmentation results\nTiming: 30 min",
    "Now that MARS-Net model training and prediction are complete, we can evaluate and visualize the segmentation results. Segmentation results are evaluated by Precision, Recall, and F1 score (Arbelaez et al., 2010[href=https://www.wicell.org#bib1]). To briefly explain these criteria; precision measures how well the model identified only the real cell boundaries compared to non-cell boundaries, recall measures if the model found all the real cell boundaries and F1 measures a harmonic mean of precision and recall. For comparison of the model against other models, the evaluated results from multiple models are shown together in a bar graph, edge evolution diagram, violin plot, line graph, and bubble plot. All evaluations follow the leave-one-movie-out cross-validation scheme, which means that given ‘n’ number of movies, ‘n-1’ movies are used for training the model, and one movie is segmented, and segmentation results are evaluated. Cross validation repeats such training and evaluation until each movie is used for validation once. The visualization codes in this Part show the cross-validation results by averaging the evaluation results from the segmentation of each movie. For more details on what each plot describes, refer to the MARS-Net (Jang et al., 2021[href=https://www.wicell.org#bib7]).\nCritical: Correspondence Algorithm(Arbelaez et al., 2010[href=https://www.wicell.org#bib1]) necessary for calculating F1, precision and recall is only supported in Linux. Windows 10 user still can perform steps 35, 36 and 40 in this Part or go to the Part 6.\nNavigate to the “evaluation” folder in the MARS-Net folder and open the “GlobalConfig.m” file in MATLAB.\nEdit the following parameters that specify the location of the segmentation results, name of the deep learning architecture to evaluate, the number of training frames used to train the model, the location to save the evaluated results and the location of the input images.:\n“prediction_path_list”.",
    "Set as folder path to a newly made file in “results” folder called “predict_wholeframe_round{#}_{project name}”.\n“display_names”.\nSet as the project name, or a name that is to be displayed on results.\n“frame_list”.\nSet as the number of frames that was used to train the model.\nNote: This is the list of the number of training frames used in the training dataset. Since we train multiple models with different number of training frames, we specify which models to evaluate here. This is useful to see the how the segmentation accuracy changes as the size of training dataset increases.\n“root_path”.\nSet as a folder path to the “evaluation” folder.\nNote: This is a location to save the evaluation results.\n“img_root_path”.\nSet as a folder path to the “assets” folder.\nNote: This is a location of input images which are necessary for overlaying segmentation results on top of input images.\nNote: Below is the example parameters for comparing the segmentation results from Single_Unet and Multi_VGG19D models in the location described in step 22 in Part 4. User can copy parameters below and paste to line 244 in GlobalConfig.m to set the parameters for evaluation. Instead of using the example parameters below as is, please change the part ‘C:\\Users\\JunbongJang\\PycharmProjects’ to a folder path where MARS-Net is installed in the user’s computer.\nprediction_path_list = {'C:\\Users\\JunbongJang\\PycharmProjects\\MARS-Net\\models\\results\\predict_wholeframe_round1_ Single_Unet\\'; 'C:\\Users\\JunbongJang\\PycharmProjects\\MARS-Net\\models\\results\\predict_wholeframe_round1_ Multi_VGG19D\\'};\ndisplay_names = {‘Single Unet’;’Multi VGG19D’};\nframe_list = [2;2];\nroot_path = 'C:\\Users\\JunbongJang\\PycharmProjects\\MARS-Net\\evaluation\\';\nimg_root_path = 'C:\\Users\\JunbongJang\\PycharmProjects\\MARS-Net\\assets\\';\nOptionally in GlobalConfig.m, update_config function can be edited to process a new result with the user’s project name. Currently, update_config function handles results from phase contrast, spinning disk confocal, total internal reflection fluorescence (TIRF), single-microscopy, and multi-microscopy models.",
    "For instance, add another else if statement at around line 308 in GlobalConfig.m and specify parameters such as: img_root_path, mask_type, img_type, dataset_list, and fold_name_list. They are similar to the parameters specified in UserParams.py in Part 1.\nDownload NPC Reader and Correspondence Algorithm (Arbelaez et al., 2010[href=https://www.wicell.org#bib1]). Add the NPC Reader folder to the “evaluation” folder, and add the Correspondence Algorithm folder to the “evaluation_f1” folder located in the “evaluation” folder.\nCalculate F1, precision and recall from the segmented movies by running the following code in the Terminal after going to the “evalutation_f1” folder:\n>cd “location of evaluation_f1 folder”\n>matlab -nodisplay -nosplash -nodesktop -r “run(‘run_overlap_mask_prediction.m’);exit;”\nAlternatively, this can be run in the MATLAB command window by typing “run_overlap_mask_prediction”.\nOpen the generated MATLAB data files with the following name “Recall_Precision_F_score_frame#.mat” located in the “results” folder. # in the name can be any number.\nDraw the violin plot of F1, precision, and recall of the opened MATLAB data file by running the following code in the terminal:\n>cd “evalutation_f1 folder”\n>matlab -nodisplay -nosplash -nodesktop -r “run(‘visualize_a_model.m’);exit;”\nAlternatively, this can be run in MATLAB by typing “visualize_a_model”.\nPause point: Instructions below are for comparing the multiple models’ segmentation results against the ground truth segmentation labeled in Part 3, so make sure to train and predict segmentation of movies and run 'run_overlap_mask_prediction.m' for each model before comparing them. Some of the models can be our VGG19_dropout (VGG19D) or U-Net for instance and other new models that users developed.\nDraw the learning curves by running the following code located in the “evaluation” folder:\n>cd “location of evaluation folder”\n>python draw_learning_curve.py\nDraw bubble plots by running the following code located in the “evaluation” folder:\n>cd “location of evaluation folder”\n>python bubble_training_curve.ipynb",
    "Draw bar graphs and line graphs across different training frames by running the following code in the terminal located in the “evalutation_f1” folder:\n>cd “location of evaluation_f1 folder”\n>matlab -nodisplay -nosplash -nodesktop -r “run(‘visualize_across_frames_datasets.m’);exit;”\nDraw edge evolution by running the following code in the terminal located in the “evalutation_f1” folder:\n>cd “location of evaluation_f1 folder”\n>matlab -nodisplay -nosplash -nodesktop -r “run(‘run_overlap_compare.m’);exit;”\nDraw violin plots by running the following code in the terminal located in the “evaluation” folder:\n>cd “location of evaluation folder”\n>matlab -nodisplay -nosplash -nodesktop -r “run(‘run_violin_compare.m‘);exit;”\nIndependent to above MATLAB evaluation codes for segmentation accuracy, trained models’ weights can be visualized by drawing activation maps on each live cell image using SEG-GRAD-CAM (Vinogradova et al., 2020[href=https://www.wicell.org#bib19]). Visualizing trained model’s weights is helpful for understanding how each layer in the model uses the information of the cell images to segment the cell in the image. Run the following code in the terminal:\n>python SegGradCAM/main.py\nView evaluation results in the “results” or “generated” folder located in the directory where the corresponding code was run.\nPart 6. Quantification of morphodynamics of the single cell\nTiming: 1 h",
    "This Part can be performed right after Part 4, but we recommend first evaluating the segmentation results in Part 5. This Part uses the segmentation results in the location described in step 22 in Part 4. By verifying the accuracy of the segmentation results, quantification of morphodynamics is more reliable. Window and Protrusion package is developed by Danuser Lab at UT Southwestern (Machacek and Danuser, 2006[href=https://www.wicell.org#bib12]; Machacek et al., 2009[href=https://www.wicell.org#bib13]) to quantify morphodynamics of the single cell (Lee et al., 2015[href=https://www.wicell.org#bib9]; Machacek et al., 2009[href=https://www.wicell.org#bib13]; Wang et al., 2018[href=https://www.wicell.org#bib20]). The quantified morphodynamics can be further processed to discover morphodynamics phenotypes (Choi et al., 2021[href=https://www.wicell.org#bib4]; Ma et al., 2018[href=https://www.wicell.org#bib11]; Wang et al., 2018[href=https://www.wicell.org#bib20], 2021[href=https://www.wicell.org#bib21]) and causal relationships without molecular perturbations (Noh et al., 2021[href=https://www.wicell.org#bib14]). For more details on Window and Protrusion package, refer to its GitHub page.\nFor single cell cropping the segmented movie, navigate to the “img_proc” folder and run the following code in the terminal:\n>cd “location of img_proc folder”\n>python rotate_crop_img.py\nDownload the Window and Protrusion package and in MATLAB, add the downloaded package’s folder as a directory, including its subdirectories.\nType the following code in the command window in MATLAB:\n>movieSelectorGUI\nWhen the movieSelectorGUI pops up, click the button “New” to add segmented images as a movie and specify the dataset’s details.\nWe specified details of our live cell movies as follows: pixel size is 72 mm, time interval is 5 s, numerical aperture is 1.4, and camera bit depth is 16.\nAfter clicking “Save” button to save the movie’s information, select the radio button next to Windowing and click “continue”.\nRun the following operations in order: Thresholding, Mask Refinement, Protrusion, Windowing, Protrusion Sampling, Window Sampling.",
    "Thresholding: choose “External Segmentation” among segmentation methods and specify the location of the folder described in step 22 in Part 4 to load the segmented images.\nMask Refinement: Use the default settings which are 10 pixels for minimum size, 3 pixels for closure radius, 1 object number, and check marks for fill holes and fill boundary holes.\nProtrusion: Choose Mask Refinement for mask process, 50 for down sampling parameter and 30 for spline tolerance value.\nWindowing: Use the default settings, which are 10 for minimum size of objects to window, 7 pixels for window size parallel and perpendicular to the mask edge, constant number for the number of windows, and 2 for StartContour number.\nProtrusion Sampling: There are no parameters to set.\nWindow Sampling: Choose raw images with 1 channel for images to sample."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Cell Biology",
    "Bioinformatics",
    "Microscopy"
  ],
  "bigAreas": [
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology",
    "Bioengineering & Technology"
  ]
}