{
  "id": 10493,
  "origin_website": "Jove",
  "title": "Estimation of Contact Regions Between Hands and Objects During Human Multi-Digit Grasping",
  "procedures": [
    "Prior to beginning an experiment, the participants must provide informed consent in accordance with the institutional guidelines and the Declaration of Helsinki. All the protocols described here have been approved by the local ethics committee of Justus Liebig University Giessen (LEK-FB06).\n1. Installation of all the necessary software\nDownload the project repository at Data and Code Repository.\nInstall the software listed in the Table of Materials (note the software versions and follow the links for purchase options and instructions).\nWithin the Data and Code Repository, open a command window, and run the following command:\nconda env create -f environment.yml\nDownload and install the pretrained DeepHandMesh29 instantiation following the instructions provided at https://github.com/facebookresearch/DeepHandMesh.\n\t\nPlace DeepHandMesh in the folder \"deephandmesh\" of the Data and Code Repository. Replace the file \"main/model.py\" with the model.py file contained in the Data and Code Repository.\n2. Preparing the motion capture system\nPosition a workbench within a tracking volume imaged from multiple angles by motion-tracking cameras arranged on a frame surrounding the workspace (Figure 1A). Prepare reflective markers by attaching double-sided adhesive tape to the base of each marker.\nExecute Qualisys Track Manager (QTM) as an Administrator.\n\tNOTE: Executing QTM as an Administrator is necessary for the Python SDK to take control of the QTM interface. We advise always running QTM as an Administrator.\n3. Calibrating the cameras\nPlace the L-shaped calibration object within the tracking volume.\nWithin the QTM, click on Calibrate in the Capture menu, or press the wand icon in the Capture toolbar. Wait for a calibration window to open. Select the duration of the calibration, and press OK.",
    "Wave the calibration wand across the tracking volume for the duration of the calibration. Press the Export button, and specify a file path in which to export the calibration as a text file. Accept the calibration by pressing OK.\n4. Creating a stimulus object\nConstruct a virtual 3D object model in the form of a polygon mesh. Use a 3D printer to construct a physical replica of the object model.\n\t​NOTE: The data repository in step 1.1 provides example objects in STL and Wavefront OBJ file formats. Objects in STL format are manifold and ready for 3D printing.\n5. Preparing the stimulus object\nAttach four non-planar reflective markers to the surface of the real object. Place the object within the tracking volume.\nIn the project repository, execute the Python script \"Acquire_Object.py\". Follow the instructions provided by the script to perform a 1 s capture of the 3D position of the object markers.\nSelect all the markers of the rigid body. Right-click on and select Define Rigid Body (6DOF) | Current Frame. Enter the name of the rigid body, and press OK.\nIn the File menu, select Export | To TSV. In the new window, check the boxes 3D, 6D, and Skeleton in the Data Type settings. Check all the boxes in the General settings. Press OK and then Save.\n6. Co-registering real and mesh model versions of the stimulus object\nOpen Blender, and navigate to the Scripting workspace. Open the file \"Object_CoRegistration.py\", and press Run. Navigate to the Layout workspace, and press n to toggle the sidebar. Within the sidebar, navigate to the Custom tab.\nSelect the .obj file to be co-registered, and press the Load Object button.",
    "Select the trajectory file that was exported in step 3.3, and specify the names of the markers attached to the rigid object separated by semicolons. In the Marker header, specify the line in the trajectory file that contains the column names of the data (counting starts at 0).\nSelect the corresponding rigid body file with the 6D suffix, and specify the name of the rigid body defined in step 4.1. In the 6D header, specify the line in the rigid body file that contains the column names of the data.\nPress Load Markers. Translate and rotate the Markers object and/or the Object object to align them. Specify a mesh output file, and press Run Coregistration. This will output an .obj file that contains the co-registered stimulus mesh.\n7. Setting up markers on the hands\nAttach 24 spherical reflective markers on different landmarks of a participant's hand using double-sided tape.\n\tNOTE: The specific positioning of the markers is demonstrated in Figure 2.\n\t\nPosition the markers centrally on top of the respective fingertips, as well as the distal interphalangeal joints, proximal interphalangeal joints, and metacarpophalangeal joints of the index finger, middle finger, ring finger, and small finger.\nFor the thumb, position one marker each on the fingertip and the basal carpometacarpal joint, as well as a pair of markers each on the metacarpophalangeal and the interphalangeal joints.\n\t\t​NOTE: These marker pairs need to be displaced in opposite directions perpendicular to the thumb's main axis and are necessary to estimate the thumb's orientation.\nFinally, place markers at the center of the wrist and on the scaphotrapeziotrapezoidal joint.\nimgsrc://cloudfront.jove.com/files/ftp_upload/64877/64877fig2v2.jpg\nFigure 2: Marker placement on a participant's hand. Abbreviation: RH = right hand. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64877/64877fig02large.jpg]\n8. Acquiring a single trial",
    "Ask the participant to place their hand flat on the workbench with the palm facing downward and to close their eyes. Place the stimulus object on the workbench in front of the participant.\nWhile the QTM is running, execute the Python script \"Single_Trial_Acquisition.py\" in the project repository. Follow the instructions provided by the script to capture a single trial of the participant grasping the stimulus object.\n\tNOTE: The script will produce an auditory cue. This will signal to the participant to open their eyes and execute the grasp. In our demonstrations, the task is to reach and grasp the target object, lift it vertically by approximately 10 cm, set it down, and return the hand to its starting position.\n9. Labeling the markers\nWithin the QTM, drag and drop the individual marker trajectories from Unidentified trajectories to Labeled trajectories, and label them according to the naming convention in Figure 2.\nSelect all the markers attached to the hand, and right-click on and select Generate AIM model from selection. In the new window, select Create new model based on Marker connections from existing AIM model and press the Next button.\nSelect the RH_FH model definition, and press Open. Press Next, enter a name for the AIM model, and press OK. Finally, press Finish to create an AIM model for the participant's hand, which will be used to automatically identify markers in successive trials from the same participant.\n10. Creating a personalized skeleton definition for the participant\nIn the QTM, navigate to the Play menu, and select Play with Real-Time Output.\nOpen Maya. Navigate to the QTM Connect shelf, and press the Connect to QTM icon. In the new window, check Markers, and press Connect. Now, press the Play icon in the QTM Connect shelf.",
    "Shift-select all the hand markers and press the Wash Locators icon. Select the washed hand markers, and press Ctrl + G. This will create a group node. Name the group Markers.\nSelect all the hand markers. In the Modify menu, click Search and Replace Names. Search for the RH_ prefix, and delete the prefix for the markers.\nPress the Import Solver icon in the QTM Connect shelf. Load the skeleton definition \"RH_FH.xml\".\nIn the Windows menu, navigate to General Editors | Namespace Editor. Within the new window, click on :(root), and press New to create a new namespace, RH. Click on the RH namespace, press New, and name the new namespace ModelPose.\nSelect all the markers, click on the RH namespace, and press Add Selected to add the markers to the RH namespace.\nSelect the skeleton bones, click on the ModelPose namespace, and press Add Selected to add the skeleton bones to the ModelPose namespace.\nRotate, translate, and scale the skeleton to fit the marker data. Next, for each skeleton joint individually, Shift + Select the skeleton joint and its associated markers, and press the Add Attachments icon. Finally, press the Export Solver icon to export the new skeleton definition to an XML file that can be loaded in the QTM (see next step).\n\t​NOTE: This step is not strictly necessary, but it is useful to increase the accuracy of the skeleton fit to the marker data. Read the QSolverQuickstartGuide on https://github.com/qualisys/QTM-Connect-For-Maya for more information.\n11. Reconstruct the joint skeletal joint poses\nWithin the QTM, open the project settings by pressing the gearwheel icon. In the sidebar, navigate to Skeleton Solver, and press Load to select a skeleton definition file. Adjust the Scale Factor to 100%, and press Apply.",
    "Navigate to TSV Export, and check the boxes 3D, 6D, and Skeleton in the Data Type settings. Check all the boxes in the General settings. Press Apply, and close the project settings.\nPress the Reprocess icon, check the boxes Solve Skeletons and Export to TSV file, and press OK.\n12. Generating hand mesh reconstructions\nOpen a command window in the project repository, and activate the conda environment by executing the command:\nconda activate contact-regions\nThen, execute the following command, and follow the instructions provided by the script to generate, for each frame of the trial, a hand mesh reconstructing the current hand pose.\npython Hand_Mesh_Reconstruction.py --gpu 0 --test_epoch 4\n\t​NOTE: These mesh reconstructions are generated automatically using a modified version of the open-source and pretrained hand mesh generation tool, DeepHandMesh29.\n13. Generating hand-object contact region estimates\nOpen a command window in the project repository, execute the following command, and follow the instructions provided by the script to generate hand and object contact region estimates by computing the intersection between the hand and object meshes.\nblender --background --python \"Contact_Region_Estimation.py\""
  ],
  "subjectAreas": [
    "Behavior"
  ],
  "bigAreas": [
    "Ecology & Environmental Biology"
  ]
}