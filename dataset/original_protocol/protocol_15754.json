{
  "id": 19580,
  "origin_website": "Jove",
  "title": "A Flexible Platform for Monitoring Cerebellum-Dependent Sensory Associative Learning",
  "procedures": [
    "The animal protocols described here have been approved by the Animal Care and Use Committees of Princeton University.\n1. Setting up the SBC\nConnect the camera serial interface (CSI) cable to the Raspberry NoIR V2 camera and the camera port on the SBC.\nDownload the operating system for the SBC onto the host computer. Write the operating system image to a micro secure digital (microSD) card.\n\tNOTE: Detailed instructions for these procedures for a Raspberry Pi SBC can be found elsewhere23. The system has been tested using the following operating systems: Stretch, Buster, Bullseye.\nTo enable secure shell communication, create an extensionless file called \"ssh\" in the boot partition of the microSD card. Once this is done, eject the microSD card from the host machine and insert it into the SBC microSD card slot. Power the SBC by plugging in its power supply.\nPrepare the SBC to accept a wired connection to the host.\n\t\nAttach a monitor with an appropriate cable to the SBC. Open a terminal, type the command ifconfig and record the ethernet IP address of the SBC.\n\t\tNOTE: Raspberry Pi model 3B+ has an HDMI display port, while model 4B has a micro-HDMI port.\nGo to the Interface tab of the Raspberry Pi configuration setting and enable the options for Camera, secure shell network protocol (SSH), and Virtual Network Computing (VNC).\nEstablish a wired connection between the host computer and the SBC.\n\t\nConnect an ethernet cable to the ethernet port on the SBC and a host computer. Attach the other end of these cables to an ethernet switch.\nUse a virtual network computing client such as VNC viewer24 and access the desktop using the SBC IP address and the default authentication (user = \"pi\", password = \"raspberry\").\nDownload required software included in the protocol steps.",
    "CAUTION: Change the default username and password to prevent unauthorized access to the SBC.\n\t\nEnter the following command in the SBC terminal to download the rig software:\ngit clone --depth=1 https://github.com/gerardjb/assocLearnRig\nEnter the following commands to download the necessary python libraries.\ncd assocLearnRig\n\t\tpython3 setup.py\nTo allow direct control over the microcontroller, connect to the SBC and download the microcontroller integrated development environment (IDE) following steps 1.6.4-1.6.7.\nOpen the web browser on the SBC desktop and navigate to https://arduino.cc/en/software. Download the latest Linux ARM 32 bit version of the IDE.\nOpen a terminal window on the SBC desktop and navigate to the downloads directory by typing cd Downloads/\nTo install the IDE, type the following commands in the terminal:\ntar -xf arduino-<version>-linuxarm.tar.xz\n\t\tsudo mv arduino-<version> /opt\n\t\tsudo /opt/arduino-<version>/install.sh\n\t\t(here <version> is the version of the downloaded IDE)\nOpen an instance of the microcontroller IDE on the SBC desktop. Select menu option Tools > Manage Libraries. Install the \"Encoder\" library from Paul Stoffregen.\nExpand SBC onboard memory with a USB thumb drive.\n\t\nInsert a thumb drive to a USB port on the SBC. Use a USB 3.0 port if available.\nType in the terminal ls -l /dev/disk/by-uuid/ to find the thumb drive and its unique reference (UUID). Record the UUID.\nTo allow the pi user to write to the USB device, type the following commands one by one into the terminal:\nsudo mkdir /media/usb\nsudo chown -R pi:pi /media/usb\nsudo mount /dev/sda1 /media/usb -o uid=pi,gid=pi\n\t\tNOTE: The thumb drive can be added as a device that will auto-mount when the SBC restarts by adding the following line to the end of the fstab file at /etc/fstab:\n\t\t​UUID=<UUID from step 1.7.2> /media/usb vfat auto,nofail,noatime,users,rw,uid=pi,gid=pi 0 0\n2. Wiring stimulus hardware and assembling stage\nConnect and prepare microcontrollers.",
    "Connect the SBC to the programming port of the microcontroller (Arduino Due) with a USB2 type A to USB2 micro cable.\n\t\t​NOTE: Use a high-quality cable such as the product in the Table of Materials to ensure proper operation.\nLocate \"dueAssocLearn.ino\" in the downloaded project repository. Open the sketch with the microcontroller IDE and upload it to the microcontroller connected to the SBC.\nDownload and install the appropriate version of the Arduino IDE on the host computer.\nConnect the host computer to the microcontroller (Arduino Uno) with a USB2 type B to USB2 type A cable.\nGo to the GitHub repository (https://github.com/gerardjb/assocLearnRig) and download the \"DTSC_US.ino\" sketch to the host computer.\nOn the host computer, run the microcontroller IDE and open the \"DTSC_US.ino\" sketch, then upload it to the microcontroller.\nAttach wires to the microcontrollers, breadboard, LEDs, rotary encoder, stepper motor with driver, and solenoid valve with driver as indicated in the Fritzing diagram in Figure 2.\nPower the stepper motor and solenoid valve.\n\t\nProperly wire one channel of a power supply to the +V and GND pins of the stepper motor driver.\nTurn on the power supply and set the attached channel voltage to 25 V.\n\t\t​NOTE: If the connections between the stepper motor, driver, and power supply are correctly configured, a green indicator LED on the stepper motor driver will turn on.\nProperly wire the positive lead of a power supply to the solenoid valve driver hold voltage pin and the other positive lead to the spike voltage pin.\nAttach the negative leads to a ground shared with the control signal.\nTurn on the power supply and set the channel connected to the hold voltage to about 2.5 V and the channel connected to spike voltage to about 12 V.",
    "Connect an air source regulated to a pressure of ~20 PSI to the solenoid valve using the luer adapter.\nTest that all stimulus components and camera are functioning properly.\n\t\nOpen a terminal on the SBC and type cd ~/assocLearnRig to navigate to the cloned GitHub repository.\nIn the terminal, type python3 assocLearnRig_app.py to start the control graphical user interface.\nStart the camera stream by hitting the Stream button.\nSelect the DEC Radio button, upload to the microcontroller, and start a session with default parameters by hitting the Start Session button.\n\t\t​NOTE: After this step, a printout of the data log should appear in the terminal, the message on the camera stream should disappear, and the LED CS and solenoid valve US should turn on and off at appropriate times during each trial.\nAfter the session ends, repeat the previous steps with the DTSC Radio button selected.\n\t\tNOTE: Sketches in the GitHub repository (\"testStepper.ino\", \"testRotary.ino\", and \"testSolenoid.ino\") can be used to test individual components if the above steps do not provide satisfactory results.\nMake the running wheel.\n\t\nCut a 3\" wheel from a foam roller. Drill a 1/4\" hole in the exact wheel center so that the wheel will not wobble when turned by the mouse's locomotion.\nInsert a 1/4\" shaft into the wheel and fix it in place using clamping hubs placed on each side of the wheel.\nAffix the rotary encoder to a 4.5\" aluminum channel using an M3 bolt. Stabilize the aluminum channel on the aluminum breadboard using a right angle bracket with a 1/4\" bolt, nut, and washer as shown.\nAttach the wheel and rotary encoder using a shaft-coupling sleeve.\nStabilize the free side of the wheel shaft with a bearing inserted in a right-angle end clamp installed on a breadboard-mounted optical post.",
    "​NOTE: Ensure the wheel spins freely without wobbling when rotated by hand.\nPosition the stimulus hardware, head restraint, infrared light array, and picamera around the assembled wheel.\n\t\nPosition the head restraints using optical posts and right angle post clamps so that the head posts are 1.5 cm in front of the wheel axle and 2 cm above the wheel surface. (Values are for a 20 g mouse).\nPosition the CS LED and solenoid valve outlet used for the DEC US less than 1 cm from the eye used for DEC.\nMount the stepper motor used for the DTSC US\nMount the picamera on an optical post ~10 cm from where the animal will be.\n\t\t​NOTE: The design for the picamera mount can be made on a 3D printer from the file in \"RaspPiCamMount1_1.stl\" in the GitHub repository.\nPlace the infrared light array slightly above and directly facing the position of the face on the same side as the picamera.\nMake a tactile stimulus for DTSC by taping foam to the edge of a piece of acrylic mounted to a 1/4\" shaft using a clamping hub. Attach the tactile stimulus to the stepper motor shaft.\n\t\t​NOTE: The design for the acrylic piece can be laser cut following the pattern in \"TactileStimDesign.pdf\" in the GitHub repository.\n3. Preparing and running behavior experiments\nImplanting mouse headplate.\n\t\nAnesthetize a mouse using 2% isoflurane and head fix in a stereotactic frame.\nApply an ophthalmic ointment to the eyes.\nShave the scalp using soapy water and a sterile scalpel. Inject lidocaine directly underneath the skin of the incision site and clean the surgical site with povidone.",
    "Make an incision with a scalpel along the midline of the scalp from the back edge of the eyes to the back edge of the skull, being careful not to press too hard on the skull.\nSpread the incision open and clamp both sides with sterile hemostats to hold it open. Gently remove the periosteum using a cotton swab dipped with ethanol and allow the surface of the exposed skull to dry.\nPosition the headplate level on the skull, making sure to position the front of the headplate posterior to the eyes. Use cyanoacrylate glue to attach the headplate to the skull and allow the glue to dry fully.\nMix the dental cement powder (1 scoop), solvent (2 drops), and catalyst (1 drop) in a mixing dish and apply to all areas of exposed bone. Add layers until the surface is flush with the top edge of the headplate, making sure the headplate is securely attached to the skull.\nSuture the skin closed behind and in front of the headplate if necessary.\nInject post-operative analgesia such as carprofen per institutional guidelines while allowing the animal to recover for at least 5 days.\nPreparing for behavior sessions.\n\t\nAllow the test animals to habituate to the platform by mounting them in the head restraint for 30-min sessions for 5 days preceding experiments.\n\t\tNOTE: By the end of the habituation sessions, animals should run comfortably on the wheel.\n(DEC only) Prior to sessions, ensure that the solenoid valve outlet is centered on the target eye positioned >1 cm away.\n(DEC only) Manually actuate an air puff using the push button. Ensure that the mouse promptly produces a blink without showing overt signs of stress such as adopting a hunched posture or grabbing the affected periocular region with the ipsilateral forepaw.",
    "(DTSC only) Prior to sessions, ensure that the tactile stimulus is centered on the animal's nose positioned ~1.5 cm away.\n\t\tNOTE: When a DTSC behavioral session is not running, the stepper motor is automatically inactivated to allow manual repositioning.\n(DTSC only) In the SBC terminal, type python3 assocLearnRig_app.py to start the GUI.\n(DTSC only) Run a test session of three trials with the default parameters by hitting the Start Session button in the GUI.\n(DTSC only) Ensure that the logged data that prints to the terminal show a deflection of greater than 20 but less than 100 steps logged on the rotary encoder following the US on each trial.\n\t\t​CAUTION: To avoid harm and reduce stress to the animal, start the stimulus farther from the animal and move it closer until the required conditions are met.\nRunning behavioral sessions with data logging.\n\t\nMount a mouse to the head restraint.\nIn the terminal of the SBC, type python3 assocLearnRig_app.py to start the GUI.\nTo allow camera recordings during the behavioral trials, hit the Stream button.\n\t\tNOTE: Sessions can be run without a camera. In this case, only data from the rotary encoder and stimulus presentation timestamps are logged.\nInput identifying information for the animal into the Animal ID field and hit the Set button.\nSelect either the DEC or DTSC from the radio button under the Session Type heading depending on which behavioral paradigm is desired.\nInput the desired experiment parameters to the fields below the Animal ID field and hit the Upload to Arduino button.\n\t\t​NOTE: Details of the experiment parameters can be found in the GitHub repository README section.\nHit the Start Session button to begin the session.",
    "When a session is initialized, data will begin logging in a new directory created in \"/media/usb\" in the SBC thumb drive mount point.\n4. Exporting and analyzing data\nTo export all the recorded sessions to the host computer, open a command prompt and input the command pscp -r pi@Pi_IP_address:/media/usb* host_computer_destination, then authenticate with the SBC password.\n\tNOTE: The above command is for a Windows machine. On Mac and Linux machines, use terminal and replace \"pscp\" with \"scp\".\nInstall Anaconda25 or another python package manager (PPM) on the host computer.\nGo to the GitHub repository and download \"analyzeSession.py\", \"summarizeSessions.py\", \"session2mp4s.py\", and \"requirementsHost.txt\".\nOpen a PPM prompt and type conda install --file directory_containing_requirementsHostrequirements Host.txt to ensure that the Python package installation has the required python libraries.\nIn the prompt, type cd directory_containing_analyzeData to navigate to the directory containing \"analyzeData.py\" and \"session2mp4s.py\". Run the analysis program by typing python analyzeSession.py\n\tNOTE: An error message will be generated if using a Python 2 version as python. To check the version, type python -V in the prompt.\nSelect the directory containing the data when prompted. Directories with multiple subdirectories will be analyzed sequentially.\nFor DEC sessions, for each session directory analyzed, select a region of interest (ROI) containing the mouse's eye from a trial average image.\n\tNOTE: Final analysis data files and summary graphs will populate to a subdirectory of each analyzed session directory.\nType python summarizeSessions.py to generate summary data across multiple sessions.\nType in the prompt python session2mp4s.py to convert imaging data files into viewable .mp4 files.\nSubscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Neuroscience"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}