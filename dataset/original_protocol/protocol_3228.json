{
  "id": 3419,
  "origin_website": "Cell",
  "title": "Protocol to analyze fundus images for multidimensional quality grading and real-time guidance using deep learning techniques",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nModel training\nTiming: 120 min\nThese steps show details of training the Deep-Fundus quality detection model.\nChange directory to the ‘Deep-fundus’ folder.\nTrain model.\nRun ‘train.py’. If you have a GPU available for model training, assign ‘os.environ[‘‘CUDA_-VISIBLE_DEVICES’’]’ as the serial number of the GPU.\nRun the ‘train.py’ and set the arguments simultaneously. For example:\n>python train.py --aspect acceptable\nNote: There are 4 arguments: aspect, image_dir, train_file_path, val_file_path. The ‘aspect’ argument indicates the quality aspect that you want to train. If you intended to use your own data, you should specify other three arguments, meanwhile, the content format of the file must be the same as our files in the demo.\nCritical: The hyperparameters, such as the data augmentation settings, ‘epochs’, ‘IMG_SIZE, etc., have been fine-tuned and established as the default values, reflecting the optimal choices. However, you have the flexibility to customize these values according to your preferences.\nThe following codes help you load the csv file.\n>df_train = pd.read_csv(args.train_file_path).dropna(subset=[y_col])[:100]\n>df_train[y_col] = df_train[y_col].astype('str')[:100]\n>df_val = pd.read_csv(args.val_file_path).dropna(subset=[y_col])[:100]\n>df_val[y_col] = df_val[y_col].astype('str')[:100]\nThe following codes can augment data when training, you can reset the arguments to customize your data augmentation way.\n>idg = ImageDataGenerator(\n>rescale=1./255,\n>rotation_range=90,\n>horizontal_flip=True,\n>vertical_flip=True,\n>height_shift_range=0.02,\n>width_shift_range=0.02,\n>#brightness_range=(0.5, 1.5),\n>#channel_shift_range=0.5,\n)\n>tdg = ImageDataGenerator(rescale=1./255)\nThe following codes create model and train the model. You can use other model or different training method by modifying the codes.\n> base = InceptionResNetV2(input_shape=(IMG_SIZE, IMG_SIZE, 3), include_top=False, weights=\"imagenet\", pooling=\"avg\")\n>x = base.output\n>x = Dense(1024, activation=\"relu\")(x)\n>x = Dropout(0.5)(x)\n>x = Dense(1, activation=\"sigmoid\")(x)\n>model = Model(inputs=base.input, outputs=x)\n>model.compile(\"adam\", loss=focal_loss(alpha=0.5), metrics=[\"accuracy\"])\n>model.fit_generator(\n>idg.flow_from_dataframe(df_train,\n>args.image_dir,\n>y_col=y_col,\n>class_mode=\"binary\",\n  seed=42,\n  target_size=(IMG_SIZE, IMG_SIZE),\n  batch_size=10,\n),\nsteps_per_epoch=890,\nepochs=500,\nvalidation_data=tdg.flow_from_dataframe(df_val,\n  args.image_dir,\n  y_col=y_col,\n  class_mode=\"binary\",\n  seed=42,\n  target_size=(IMG_SIZE, IMG_SIZE),\n  batch_size=10,\ntarget_size=(IMG_SIZE, IMG_SIZE),\n  batch_size=10,\n),\nModel inference\nTiming: 20 min (Figure 2[href=https://www.wicell.org#fig2])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2998-Fig2.jpg\nFigure 2. Prediction modes of DeepFundus",
    "These steps demonstrate how to make predictions using a trained model.\nChange directory to the ‘Deep-fundus’ folder.\nMake prediction.\nRun ‘inference.py’. If you have a GPU available for model training, assign ‘os.environ[‘‘CUDA_-VISIBLE_DEVICES’’]’ as the serial number of the GPU.\nRun the ‘inference.py’ and set the arguments simultaneously. For example:\n>python inference.py --mode 1\nNote: There are 3 arguments: mode, image_dir, image_size. The ‘mode’ argument indicates the mode of making prediction, where 1 indicates quality classification and 2 indicates real-time guidance. The program generates result files in ‘results’ directory which will be used in evaluation step.\nThe following code can generate results of different aspect quality (mode 1)\nif args.mode == 1:\n  res_dict = dict([(k, []) for k in aspect_to_model.keys()])\n  for aspect in list(aspect_to_model.keys()):\n    clear_session()\n    model = load_model(os.path.join('models/', aspect_to_model[aspect]), compile=False)\n    all_fn = sorted(os.listdir(args.image_dir))\n    for fn in all_fn:\n      try:\n        image = Image.open(os.path.join(args.image_dir, fn)).convert('RGB')\n        image = image.resize((512, 512))\n        image = np.array(image) / 255.\n        image = image.reshape(1, 512, 512, -1)\n        y_pred_prob = float(np.squeeze(model.predict(image)))\n        res_dict[aspect].append(y_pred_prob)\n      except:\n        print('error when opening '.format(fn))\n        continue\n    df_proba = pd.DataFrame(res_dict, index=all_fn)\n    df_pred = df_proba.applymap(lambda x: 1 if x > 0.5 else 0)\n    df_pred['illuminate_od'] = df_proba['illuminate_od'].map(lambda x: 1 if x > 0.2 else 0)\n    df_pred['illuminate_macula'] = df_proba['illuminate_macula'].map(lambda x: 1 if x > 0.39 else 0)\n    df_pred = df_pred.drop(['cataract'], axis=1)\n    df_proba = df_proba.drop(['cataract'], axis=1)\n    df_proba.to_excel('results/quality_proba.xlsx')\nThe following code can provide advice based on the input images (mode 2)\nelse:\n  all_fn = sorted(os.listdir(args.image_dir))\n  predictions = []\n  for fn in all_fn:\n    result = ''\n    for aspect in ['overall', 'position', 'illuminate', 'clarity', 'cataract']:\n      clear_session()\n      model = load_model(os.path.join('/data/yellowcard/llx/models/', aspect_to_model[aspect]), compile=False)\n      try:\n        image = Image.open(os.path.join(args.image_dir, fn)).convert('RGB')\n        image = image.resize((512, 512))\n        image = np.array(image) / 255.\n        image = image.reshape(1, 512, 512, -1)\n        y_pred_prob = float(np.squeeze(model.predict(image)))\n      except:\n        print('error when opening {}'.format(fn))\n        continue\n      if aspect == 'overall' and y_pred_prob <= 0.5:\n        result = 'finish'\n        break",
    "if aspect == 'position' and y_pred_prob > 0.5:\n        result = 'recapture'\n        break\n      if aspect == 'illuminate' and y_pred_prob > 0.5:\n        result = 'recapture'\n        break\n      if aspect == 'clarity' and y_pred_prob <= 0.5:\n        result = 'finish'\n        break\n      if aspect == 'cataract':\n        if y_pred_prob > 0.5:\n          result = 'referral'\n        else:\n          result = 'recapture'\n  predictions.append(result)\ndf = pd.DataFrame({'prediction': predictions}, index=all_fn)\ndf.to_excel('results/advice_pred.xlsx')\nModel evaluation\nTiming: 20 min (Figure 3[href=https://www.wicell.org#fig3])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2998-Fig3.jpg\nFigure 3. Model evaluation for quality classification and real-time guidance\nFor quality classification mode of DeepFundus, the common classification metrics, as well as ROC (Receiver Operating Characteristic) will be generated (A). For real-time guidance mode, the common classification metrics and confusion matrix will be generated (B).\nThese steps describe how to get the corresponding metrics.\nChange directory to the ‘Deep-fundus’ folder.\nGet metrics.\nRun ‘evaluation.py’. If you use your own files, you need to change the excel path in the code.\nRun the ‘evaluation.py’ and set the arguments simultaneously. For example:\n>python evaluation.py --mode 1\nNote: The definition of ‘mode’ argument is the same as above. For mode 1, the common classification metrics, as well as ROC will be generated in ‘results’ directory, and for mode 2, the common classification metrics and confusion matrix will be generated.\nResults display through heatmaps\nTiming: 60 min (Figure 4[href=https://www.wicell.org#fig4])\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2998-Fig4.jpg\nFigure 4. Demonstration of original input images and corresponding heatmaps\nWe presented fundus images of 3 different types of quality defects, along with the respective category prediction probability scores. Furthermore, the algorithm identifies and displays the high-risk areas through Grad-CAM heatmaps.",
    "The next steps elucidate the procedure of generating the areas where the prediction of DeepFundus was based through Grad-CAM. The Grad-CAM algorithm facilitates the production of a class-specific activation heatmap, where the significance of classification towards a particular class is denoted by each activation value. More important features are indicated by redder regions.\nDepict the heatmap through Grad-CAM.\nRun ‘grad_cam.py’. If you use your own files and have a GPU available for model training, assign ‘os.environ[‘‘CUDA_-VISIBLE_DEVICES’’]’ as the serial number of the GPU.\nRun the ‘grad.py’ and set the arguments simultaneously. For example:\n>python grad.py --aspect acceptable\nNote: The definition of the ‘mode’ argument is the same as above. If you use your own model, maybe you should change the convolution layer name specified in ‘heatmap, heatmap = grad_cam(model, image, pred, 'conv_7b')’"
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Clinical Protocol",
    "Bioinformatics"
  ],
  "bigAreas": [
    "Bioinformatics & Computational Biology"
  ]
}