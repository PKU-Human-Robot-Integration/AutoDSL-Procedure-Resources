{
  "id": 15998,
  "origin_website": "Jove",
  "title": "Real-Time Proxy-Control of Re-Parameterized Peripheral Signals using a Close-Loop Interface",
  "procedures": [
    "Study was approved by the Rutgers Institutional Study Board (IRB) in compliance with the declaration of Helsinki.\n1. Participants\nDefine the population to be studied and invite them to participate in the study. The present interface can be used in various populations. This protocol and the examples used here to provide proof of concept are not limited to a specific group.\nObtain written informed consent of the IRB approved protocol in compliance with the Declaration of Helsinki.\nAsk the participant or guardian to sign the form before the beginning of the experiment.\n2. Setup of the Close-Loop Interface\nSetup of kinematic equipment-PNS\n\t\nHelp the participant to carefully wear the LED-based motion-capture costume (body and head, shown in Figure 3, step 1 and 5) accompanying the motion-capture system used. The LED markers of the costume will be tracked by the cameras of the system to estimate the location of the moving body in space.\nConnect the wireless LED controller (also known as LED driver unit) of the system with the LED cables of the costume by plugging it into the proper port. Turn the device on and set it on the streaming mode.\nTurn on the server of the motion-capture system.\nOpen a web-browser, visit the server address, and sign-in (sign-in info must be provided by the company upon purchase of the product).\nCalibrate the system as needed (for example, calibrate the system if this is the first time to use the equipment, otherwise move to step 2.1.17).\nOpen the calibration tool of the motion-capture system and select Calibration Wizard.\nMake sure that the entry of the server number in the text-field on the left-upper side of the interface is correct and click Continue.",
    "Connect the wand to the first port of the LED controller and turn ON the controller and click Continue. Once the wand is connected, its LED markers will be turned on and will appear on the display, in the camera views.\nPlace the wand in the center of the camera view-field, confirm that it can be recorded by the cameras, and click Continue.\nMove the wand throughout the space by keeping it vertical and drawing cylinders. Make sure that the motion is captured by at least 3 cameras every time and is registered on the view field of each camera making it green. Do this for all cameras.\nOnce the view-field of each camera has been fully registered (it is all green), click Continue and wait for calibration computations to be executed.\n\t\t​NOTE: Once calibration is completed, the camera location along with the LED markers will be seen on the display, as they are physically placed in the room. At this point, the user may resume calibration because it is done, or continue aligning the system.\nHold the wand vertically and place the side with the LED closer to the end of the wand on the ground, where the origin of the 3D space must be set (point (0,0,0)).\nHold the wand stable until registered. Once registered, the screen flashes green. A point indicating the origin of the reference frame on the space will appear on the interface and the next alignment axis, x-axis, will be highlighted green.\nMove the wand, maintaining the same orientation (vertically), at the point of the x-axis and hold it stable until registered.\nRepeat for the z-axis. Once the point of the z-axis is registered, the calibration is complete.\nClick Finish to exit calibration.",
    "Open the interface of the motion-capture system and click Connect to start streaming the data from the LED markers. Once the connection is established, the position of the markers will be displayed on the virtual world of the interface.\nCreate the virtual skeleton (automatically estimate the bone positions of the body from the position data collected from the LED markers of the costume, as shown Figure 8 step2).\nRight click on Skeletons on the right side of the window and select New skeleton.\nChoose Marker Mapping and then select the proper file (provided by the company based on the interface version that is used). Then, click OK.\nAsk participant to stay stable on the T-pose (straight up posture with arms open on the sides).\nRight click on skeleton and select Generate skeleton without training.\nIf all steps are correctly performed the skeleton will be generated. Ask participant to move and check how accurately the virtual skeleton follows participant's movements.\nTo stream the skeleton data to LSL, select Settings and Options from the main menu.\nOpen Owl emulator and click \"start\" Live streaming.\nSetup of EEG equipment - CNS\n\t\nHelp the same participant to wear the EEG head-cap.\nPlace the gel electrodes (the traditional gel-based electrodes used with the EEG head-cap) on the head-cap and 2 sticky electrodes (electrodes that work like stickers) on the back side of the right ear for the CMS and DRL sensors.\nFill electrodes with high-conductive gel, as needed, to improve conductivity between the sensor and the scalp.\nConnect the electrode-cables on the gel-trodes and the two sticky electrodes.\nStick the wireless monitor on the back of the head-cap and plug in the electrode cables.\nTurn on the monitor.\nOpen the interface of the EEG system.\nSelect Use Wi-Fi device and click Scan for devices.",
    "Select NE Wi-Fi and Use this device.\nClick on the head icon, select a protocol that allows the recording of all 32 sensors, and click Load.\nMake sure that the streamed data of each channel are displayed on the interface.\nSetup of ECG equipment- ANS\n\t\nFollow the exact steps presented in 2.2 but use channel O1 to connect on the heart rate (HR) extension.\nUse a sticky electrode to stick the other end of the extension right below the left ribcage.\nPreparation of LSL for synchronized recording and streaming of kinematic data.\n\t\nRun the LSL application for the motion-capture system by double-clicking on the corresponding icon. Locate the application on the following path of the LSL folder, LSL\\labstreaminglayer-master\\Apps\\PhaseSpace.\nOn the interface, set the proper server address.\nThen, select File and Load configuration.\nSelect the proper configuration file (it must be provided by the company based on the product version that is used)\nClick Link. If no mistakes are made, then no error message will be displayed.\nPrepare LSL for synchronized recording and streaming of EEG and ECG data. No extra steps are required for this equipment.\nSetup of LSL\n\t\nRun LabRecorder application by double clicking on the file located in the LSL\\labstreaminglayer-master\\Apps\\LabRecorder path of the LSL folder.\nClick Update. If all instructions are correctly executed, all data types of the motion-capture and EEG system will be seen on the panel Record for streams.\nSelect directory and name for the data on Storage location panel.\nClick Start. The data collection of the motion-capture and EEG system will begin synchronously.\nAt the end of the recording click Stop. If recording was successful, the data will be located on the directory previously selected. Open the files to confirm that they include the recorded information.\nReal-time analyses and monitoring of the human system.",
    "Execute the MATLAB, Python, or other code that receives, processes, and augments the streamed data. Example codes corresponding to the representative examples described in the following sections can be found here: https://github.com/VilelminiKala/CloseLoopInterfaceJOVE\nGeneration of the augmented sensory feedback\n\t\nProduce the sensory output using the proper device (e.g., speakers, monitor, among others).\n3. Experimental procedure\nFollow the experimental procedure that is defined by the setup, if any.\n\tNOTE: The close-loop interfaces are designed to be intuitively explored and learned. Thus, most of the times no instructions are needed."
  ],
  "subjectAreas": [
    "Behavior"
  ],
  "bigAreas": [
    "Ecology & Environmental Biology"
  ]
}