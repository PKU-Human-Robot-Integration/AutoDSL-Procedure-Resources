{
  "id": 3082,
  "origin_website": "Cell",
  "title": "Protocol for vision transformer-based evaluation of drug potency using images processed by an optimized Sobel operator",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nCell culture\nTiming: 24 h\n      This describes the detailed protocol for preparing the cells before the\n      drug treatment.\n    \n        Take a vial of cancer cells from the liquid nitrogen tank, thaw it in a\n        water bath at 37°C, and centrifuge the vial at 800 g for 4 min.\n      \n        Discard the supernatant and resuspend the cell pellet in 10 mL of DMEM\n        complete culture medium.\n      \n        Culture the cells in a T75 flask in an incubator at 37°C and 5% CO2 for\n        24 h.\n      \n        Collect the cells after removal of culture medium, rinse the flask with\n        3 mL PBS, digest the cells with the TrypLe Select enzyme, followed by\n        centrifugation at 800 g for 4 min.\n      \nResuspend and count the cells.\n        Choose the plate (e.g., 96-well plate, 384-well plate, 1536-well plate,\n        Figure 1[href=https://www.wicell.org#fig1]A) based on the number of drugs that need to\n        be tested.\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fig1.jpg\n              Figure 1. Cell culture and seeding cells in microplates\n            \n(A) Settings for seeding cells.\n              (B) Images of cells from an overpopulated well. Scale bars are\n              30 μm.\n            \n        Seed the cells at a concentration of 6, 000 cells per well for 96-well\n        plates.\n      \nNote: If the concentration of the cells is\n      too high at the beginning, the control groups will have too many cells\n      48 h later, presenting challenges in accurately counting cells stained\n      with Hoechst, as demonstrated in Figure 1[href=https://www.wicell.org#fig1]B.\n    \n      Treating cells with different drugs at various concentrations\n    \nTiming: 25 h\n      The cells are incubated with different drugs. Detailed steps are described\n      below.\n    \nPrepare stock solutions by dissolving drugs in DMSO.\n        Dilute the stock solutions with DMEM complete culture medium and prepare\n        the first concentration for the drug (e.g., 200 μM Cephalotaxine and\n        Fasudil).\n      \n        Prepare a serial dilution by mixing 1 volume of the previous solution",
    "with 2 volumes of the DMEM complete culture medium, i.e., 1:3 dilution.\n      \n        Remove the old media and replace them with 200 μL solution containing\n        drugs and fresh culture media.\n      \n        Keep the microplates in the incubator at 37°C and 5% CO2 for\n        24 h.\n      \nNote: If bubbles were created during\n      pipetting, they will show up in the final images, creating new shapes and\n      introducing artifacts. To prevent bubbles, new pipette tips should be used\n      for every well. Do not push the plunger of the pipette over the first\n      stop. Otherwise, air will be injected into the solution, forming bubbles.\n    \nImaging the cells treated with drugs using Pico\nTiming: 25 h\n      Microscopic images are collected using the ImageXpress Pico imaging\n      system. The procedures are described below in detail.\n    \n        To image multi-well plates, turn on the ImageXpress Pico imaging system\n        installed in Windows 10 (Note) and double-click click CRX App\n        (CRX-2.6.130 for Windows 10) icon\n        imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fx3.jpg\n        to open the application.\n      \n        Click “ACQUISITION” on the home page (Figure 2[href=https://www.wicell.org#fig2]A),\n        and click “ADD PROTOCOL” (Figure 2[href=https://www.wicell.org#fig2]B).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fig2.jpg\n              Figure 2. Pico interface with a full pipeline\n            \n(A) Pico CRX home screen.\n(B–I) Creation of a new protocol (B–H) and start imaging (I).\n(J) Analyze the pictures.\n(K) Select images and export.\n        Click the second icon below “STEPS”\n        imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fx4.jpg, and select “96 Corning 3603” (Figure 2[href=https://www.wicell.org#fig2]C).\n      \n        Click the second icon under “TOOLS”\n        imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fx5.jpg, turn off all the “STAINS” and set “TL” to “Capture first” (Figure 2[href=https://www.wicell.org#fig2]D).\n      \n        Click the third icon under “TOOLS”\n        imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fx6.jpg, and select 4× objective (3.464 mm × 3.464 mm, PL FLUOTAR 4 × 0.13\n        objective, Figure 2[href=https://www.wicell.org#fig2]E).\n      \n        Click the third icon under “STEPS”\n        imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fx7.jpg, and select 29.8% (Figure 2[href=https://www.wicell.org#fig2]F).\n      \n        Click the sixth icon under “STEPS”\n        imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fx8.jpg, and select the wells of interest (Figure 2[href=https://www.wicell.org#fig2]G).\n      \n        Click the ninth icon under “STEPS”",
    "imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fx9.jpg, and fill in the protocol name and save it (Figure 2[href=https://www.wicell.org#fig2]H).\n      \n        Click the tenth icon under “STEPS”\n        imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fx10.jpg, and fill in the experiment name and start imaging (Figure 2[href=https://www.wicell.org#fig2]I).\n      \n        Return to the home page and click “MONITOR”, if the imaging is\n        progressing, the experiment will show under “IN PROGRESS”; if the\n        imaging is completed, it will show under “SUCCEEDED”. If the experiment\n        is completed, click on it, and then click “Thumbnail View” (Figure 2[href=https://www.wicell.org#fig2]J).\n      \n        Select the wells, click the fifth icon on the right\n        imgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fx11.jpg, and select “Export full size” and export the images (Figure 2[href=https://www.wicell.org#fig2]K).\n      \nNote: The ImageXpress Pico is an\n      all-in-one imaging system and a complete solution containing all the\n      hardware (camera, stage, monitor, etc.) and software (CRX App) needed for\n      our experiments. After purchasing this imaging system from Molecular\n      Devices, the company helped set up the CRX App and provided on-site\n      training.\n    \nhttps://www.moleculardevices.com/products/cellular-imaging-systems/high-content-imaging/imagexpress-pico[href=https://www.moleculardevices.com/products/cellular-imaging-systems/high-content-imaging/imagexpress-pico].\n    \n      For other general questions related to Pico, please refer to the\n      “ImageXpress Pico Automated Cell Imaging System User Guide” (https://www.moleculardevices.com/sites/default/files/en/assets/user-guide/dd/img/imagexpress-pico-automated-cell-imaging-system-with-cellreporterxpress-software-v2-5.pdf[href=https://www.moleculardevices.com/sites/default/files/en/assets/user-guide/dd/img/imagexpress-pico-automated-cell-imaging-system-with-cellreporterxpress-software-v2-5.pdf]).\n    \nModify the file formats using ImageJ\nTiming: 10 min\n      This describes the detailed procedures for changing the format of the\n      images.\n    \n        Create two new folders and name them “8-bit TIFF” and “8-bit PNG”,\n        respectively.\n      \n        Open ImageJ (bundled with 64-bit Java 8 for Windows 10 can be downloaded\n        from\n        https://imagej.nih.gov/ij/download.html[href=https://imagej.nih.gov/ij/download.html]) and select “Process > Batch > Convert” (Figure 3[href=https://www.wicell.org#fig3]A).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fig3.jpg\n              Figure 3. Batch-convert files with ImageJ\n            \n(A–C) Procedures to change the format of images.\n(D) Output folder contains PNG files.\n        Select the input and output folders, change the output format to “8-bit\n        TIFF,” and click “convert” (Figure 3[href=https://www.wicell.org#fig3]B).\n      \n        Select the input and output folders, change the output format to “8-bit\n        PNG,” and click “Convert” (Figure 3[href=https://www.wicell.org#fig3]C).\n      \n        Check the format of the files in the final output folder (Figure 3[href=https://www.wicell.org#fig3]D).",
    "Process the images with Jupyter Notebook (6.4.8 for Windows 10)\n    \nTiming: 30 min\n      The images are prepared using Jupyter Notebook. Detailed steps and codes\n      are provided below.\n    \n        Download Anaconda 3-2022.10 for Windows 10, open Anaconda from “Anaconda\n        Navigator”.\n      \nClick “Environments” on the left and “Create” on the bottom.\nName the environment, select Python 3.7.13 and click “Create”.\nAfter the environment is set up, select “Not installed”.\n        Search and select TensorFlow (2.9.1), Keras (2.9.0), NumPy (1.22.4), and\n        matplotlib(3.5.1). Click “Apply” on the bottom right.\n      \n        Return to the Anaconda home page and select “All applications” “on” to\n        the environment.\n      \n        Install Jupyter Notebook. The codes for image processing and training\n        can be found on Zenodo:https://doi.org/10.5281/zenodo.7509014[href=https://doi.org/10.5281/zenodo.7509014].\n      \n        Launch Jupyter Notebook from Anaconda, open the file\n        “High_pass_code.ipynb”, and run the following codes to import packages\n        for array calculation, data visualization, and operating system\n        interface.\n      \n>%matplotlib inline\n>import numpy as np\n>import matplotlib.pyplot as plt\n>import matplotlib.image as mpimg\n>import os\nLoad the 8-bit PNG images.\n>#8bit png dirctory\n>RAW_PNG_DIR = ('E:\\\\raw')\n        Create a folder for the output files and set the directory by running\n        the next cell.\n      \n>#find edge png save dirctory\n>EDGE_PNG_SAVE_DIR = ('E:\\\\Highpass')\nThe following codes help locate the files.\n>def prepend(list, str):\n>    str += '{0}'\n>    list = [str.format(i) for i in\n          list]\n>    return(list)\n>png_files = os.listdir(RAW_PNG_DIR)\n>png_prepend = RAW_PNG_DIR+\"\\\\\"\n>png_dir = prepend(png_files,png_prepend)\nDefine the vertical filter and horizontal filter as arrays.\n>vertical_filter = [[-1,-2,-1], [0,0,0], [1,2,1]]\n>horizontal_filter = [[-1,0,1], [-2,0,2], [-1,0,1]]\n>file_count = 0\nRun the following codes to apply a Sobel operator.\n>for i in png_dir:\n>  img = mpimg.imread(i)\n>  n,m = img.shape # n=number of pixels in the row of\n          the image\n            #\n          m=number of pixels in the column of the image\n>  edges_img = np.zeros_like(img)\n>  for row in range(3,n-2):\n>    for col in range(3,m-2):",
    ">      local_pixels =\n          img[row-1:row+2, col-1:col+2]\n>      vertical_transformed_pixels =\n          vertical_filter∗local_pixels\n>      vertical_score =\n          vertical_transformed_pixels.sum()\n>      horizontal_transformed_pixels =\n          horizontal_filter∗local_pixels\n>      horizontal_score =\n          horizontal_transformed_pixels.sum()\n>      edge_score =\n          (vertical_score∗∗2 + horizontal_score∗∗2)∗∗.5\n>      #print(\"edge\n          score\",(edge_score)∗2)\n>      #print(\"edge row\n          col\",edges_img[row,col] )\n>      edge_score =\n          (edge_score)∗∗0.8\n>      if edge_score >=\n          0.2:\n>        edge_score=edge_score∗∗0.6\n>      edges_img[row,col]=\n          edge_score\n>    plt.imsave(EDGE_PNG_SAVE_DIR +\n          \"\\\\edge_\"\n>        +\n          png_files[file_count],\nedges_img, cmap = 'gray')\n>    file_count = file_count+1\n>    print(file_count, \"/\",\n          len(png_dir))\n>print(\"Done\")\n      Split each image into 100 patches of sub-images with PhotoScape X\n    \nTiming: 10 min\n      This describes the detailed steps for splitting images using PhotoScape X.\n    \n        In PhotoScape X (Windows 10), open the file under “Viewer” (Figure 4[href=https://www.wicell.org#fig4]A).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fig4.jpg\n              Figure 4. Split the images with Photoscape X\n            \n(A) Home screen of Photoscape X.\n(B) Specify the number of columns and rows.\n        Right-click the image and select “Split” (Figure 4[href=https://www.wicell.org#fig4]A).\n      \n        Change the numbers of “Columns” and “Rows” and click “Split” (Figure 4[href=https://www.wicell.org#fig4]B).\n      \nTrain a Conv2D model\nTiming: 30 min\n      A convolutional neural network model is trained in this step. Detailed\n      procedures are provided below.\n    \n        To randomly split the data into training and testing sets, we run the\n        following script and assign the files into corresponding subfolders (Figure 5[href=https://www.wicell.org#fig5]).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fig5.jpg\n              Figure 5. Create folders for training and testing\n            \n>import pandas as pd\n>import numpy as np\n># set up random state seed for reproducibility.\n>seed = 12\n>training_frac = 0.9\n># load meta data\n>meta_data = pd.read_csv('meta_data.csv',\n          index_col=0)\n>meta_data[\"ids\"] = list(meta_data.index)\n># the splitting is based on\n          'drug','concentration','treatment'\n># training set\n>meta_train =\n          meta_data.groupby(['drug','concentration','treatment']).apply(\nlambda x: x.sample(frac=training_frac, random_state=seed))\n>meta_train.index = meta_train[\"ids\"]\n>meta_train.sort_index(inplace=True)\n>meta_train.to_csv('meta_data_training.csv')\n># testing set\n>meta_test = meta_data.loc[set(meta_data.index) -\n          set(meta_train.index)]\n>meta_test.sort_index(inplace=True)\n>meta_test.to_csv('meta_data_testing.csv')\n        Launch Jupyter Notebook from Anaconda and open the following file\n        “Conv2D_code.ipynb”.\n      \n>import tensorflow as tf\n>import keras_preprocessing\n>from keras_preprocessing import image\n>from keras_preprocessing.image import ImageDataGenerator\nData augmentation.10[href=https://www.wicell.org#bib10]\n>TRAINING_DIR = ('D:\\\\Fasudil\\\\con1ctrl\\\\train')",
    ">training_datagen = ImageDataGenerator(\n>    rescale = 1./255,\n>    rotation_range=40,\n>    width_shift_range=0.2,\n>    height_shift_range=0.2,\n>    shear_range=0.2,\n>    zoom_range=0.2,\n>    horizontal_flip=True,\n>    fill_mode='nearest')\nLoad the test set and rescale the data.\n>VALIDATION_DIR = ('D:\\\\Fasudil\\\\con1ctrl\\\\test')\n>validation_datagen = ImageDataGenerator(rescale = 1./255)\n        Set the image size in the train to “(201, 201)”, choose “binary\n        classification” and set the batch size for training.\n      \n>train_generator = training_datagen.flow_from_directory(\n>    TRAINING_DIR,\n>    target_size=(201,201),\n>    class_mode='binary',\n>    batch_size=10)\n        Resize the images in the test set, choose “binary classification” and\n        set the batch size for testing.\n      \n>validation_generator =\n          validation_datagen.flow_from_directory(\n>    VALIDATION_DIR,\n>    target_size=(201,201),\n>    class_mode='binary',\n>    batch_size=10)\nTrain Conv2D.\n>model = tf.keras.models.Sequential([\n>    # This is the first convolution\n>    tf.keras.layers.Conv2D(64, (3,3),\n          activation='relu',\n>    input_shape=(196, 196, 3)),\n>    tf.keras.layers.MaxPooling2D(2, 2),\n>    # The second convolution\n>    tf.keras.layers.Conv2D(64, (3,3),\n          activation='relu'),\n>    tf.keras.layers.MaxPooling2D(2,2),\n>    # The third convolution\n>    tf.keras.layers.Conv2D(128, (3,3),\n          activation='relu'),\n>    tf.keras.layers.MaxPooling2D(2,2),\n>    # The fourth convolution\n>    tf.keras.layers.Conv2D(128, (3,3),\n          activation='relu'),\n>    tf.keras.layers.MaxPooling2D(2,2),\n>    # Flatten the results to feed into a\n          DNN\n>    tf.keras.layers.Flatten(),\n>    tf.keras.layers.Dropout(0.5),\n>    # 512 neuron hidden layer\n>    tf.keras.layers.Dense(512,\n          activation='relu'),\n>    tf.keras.layers.Dense(1,\n          activation='sigmoid')])\n>model.summary()\nGenerate a confusion matrix.\n>model.compile(loss = tf.keras.losses.BinaryCrossentropy(),\n>        optimizer =\n          tf.keras.optimizers.Adam(learning_rate=1e-3),\n>        metrics =\n          [tf.keras.metrics.BinaryAccuracy(),\n>              tf.keras.metrics.TruePositives(),\n>              tf.keras.metrics.TrueNegatives(),\n>              tf.keras.metrics.FalsePositives(),\n>              tf.keras.metrics.FalseNegatives()])\nTest the model.\n>history = model.fit(train_generator,\n>          epochs=4,\n>          steps_per_epoch=504,\n>          validation_data =\n          validation_generator,\n>          verbose =\n          1,\n>          validation_steps=56)\n        After the training is completed, use “val_accuracy” to plot the curve.\n      \nTrain a vision transformer model\nTiming: 30 min\n      A vision transformer model is trained in this step. Details are provided\n      below.\n    \n        Launch Jupyter Notebook from Anaconda and open the following\n        file“Vision_Transformer_code.ipynb”. Import packages.\n      \n>import numpy as np\n>import tensorflow as tf\n>from tensorflow import keras\n>from tensorflow.keras import layers\n>import tensorflow_addons as tfa\n>from keras_preprocessing import image\n>from keras_preprocessing.image import ImageDataGenerator\nLoad the images.\n>num_classes = 2",
    ">input_shape = (201, 201, 3)\n>TRAINING_DIR =\n          ('D:\\\\all-con1ctrl\\\\c-con1ctrl\\\\train')\n>training_datagen = ImageDataGenerator(rescale = 1./255)\n>VALIDATION_DIR =\n          ('D:\\\\all-con1ctrl\\\\c-con1ctrl\\\\test')\n>validation_datagen = ImageDataGenerator(rescale = 1./255)\nChoose binary classification and specify batch sizes.\n>train_generator = training_datagen.flow_from_directory(\n>    TRAINING_DIR,\n>    target_size=(201,201),\n>    class_mode='binary',\n>    batch_size=200)\n>validation_generator =\n          validation_datagen.flow_from_directory(\n>    VALIDATION_DIR,\n>    target_size=(201,201),\n>    class_mode='binary',\n>    batch_size=200)\nCheck matrix shape.\n>x_train, y_train = train_generator.next()\n>x_test, y_test = validation_generator.next()\n>print(f\"x_train shape: {x_train.shape} - y_train shape:\n          {y_train.shape}\")\n>print(f\"x_test shape: {x_test.shape} - y_test shape:\n          {y_test.shape}\")\nSet training parameters.\n>learning_rate = 0.001\n>weight_decay = 0.0001\n>batch_size = 200\n>num_epochs = 100\n>image_size = 72\n>patch_size = 6\n>num_patches = (image_size // patch_size) ∗∗ 2\n>projection_dim = 64\n>num_heads = 4\n>transformer_units = [\n>    projection_dim ∗ 2,\n>    projection_dim,]\n>transformer_layers = 4\n>mlp_head_units = [2048, 1024]\nData augmentation.\n>data_augmentation = keras.Sequential([\n>          layers.Normalization(),\n>          layers.Resizing(image_size,\n          image_size),\n>          layers.RandomFlip(\"horizontal\"),\n>          layers.RandomRotation(factor=0.02),\n>          layers.RandomZoom(\n>            height_factor=0.2,\n          width_factor=0.2),],\n>  name=\"data_augmentation\",)\n>data_augmentation.layers[0].adapt(x_train)\nDefine multilayer perceptron and patches.\n>def mlp(x, hidden_units, dropout_rate):\n>    for units in hidden_units:\n>      x = layers.Dense(units,\n          activation=tf.nn.gelu)(x)\n>      x =\n          layers.Dropout(dropout_rate)(x)\n>    return x\n>class Patches(layers.Layer):\n>    def __init__(self, patch_size):\n>      super(Patches,\n          self).__init__()\n>      self.patch_size =\n          patch_size\n>    def call(self, images):\n>      batch_size =\n          tf.shape(images)[0]\n>      patches =\n          tf.image.extract_patches(\n>          images=images,\n>          sizes=[1,\n          self.patch_size, self.patch_size, 1],\n>          strides=[1,\n          self.patch_size, self.patch_size, 1],\n>          rates=[1,\n          1, 1, 1],\n>          padding=\"VALID\",)\n>      patch_dims =\n          patches.shape[-1]\n>      patches = tf.reshape(patches,\n          [batch_size, -1, patch_dims])\n>      return patches\nResize images.\n>import matplotlib.pyplot as plt\n>plt.figure(figsize=(4, 4))\n>image = x_train[np.random.choice(range(x_train.shape[0]))]\n>plt.imshow(image.astype(\"uint8\"))\n>plt.axis(\"off\")\n>resized_image = tf.image.resize(\n      tf.convert_to_tensor([image]),\n          size=(image_size, image_size))\n>patches = Patches(patch_size)(resized_image)\n>print(f\"Image size: {image_size} X {image_size}\")\n>print(f\"Patch size: {patch_size} X {patch_size}\")\n>print(f\"Patches per image: {patches.shape[1]}\")\n>print(f\"Elements per patch: {patches.shape[-1]}\")\n>n = int(np.sqrt(patches.shape[1]))\n>plt.figure(figsize=(4, 4))\n>for i, patch in enumerate(patches[0]):\n>    ax = plt.subplot(n, n, i + 1)\n>    patch_img = tf.reshape(patch,\n          (patch_size, patch_size, 3))\n>    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n>    plt.axis(\"off\")",
    "Encode patches and positions.\n>class PatchEncoder(layers.Layer):\n>    def __init__(self, num_patches,\n          projection_dim):\n>        super(PatchEncoder,\n          self).__init__()\n>        self.num_patches =\n          num_patches\n>            self.projection =\n          layers.Dense(units=projection_dim)\n>        self.position_embedding =\n          layers.Embedding(\n>            input_dim=num_patches,\n          output_dim=projection_dim)\n>    def call(self, patch):\n>        positions =\n          tf.range(start=0, limit=self.num_patches, delta=1)\n>        encoded =\n          self.projection(patch) +\n          self.position_embedding(pos > itions)\n>        return\n          encoded\nSet the parameters of the model.\n>def create_vit_classifier():\n>    inputs =\n          layers.Input(shape=input_shape)\n>    # Augment data.\n>    augmented =\n          data_augmentation(inputs)\n>    # Create patches.\n>    patches =\n          Patches(patch_size)(augmented)\n>    # Encode patches.\n>    encoded_patches =\n          PatchEncoder(num_patches, projection_dim)(patches)\n>    # Create multiple layers of the\n          Transformer block.\n>    for _ in range(transformer_layers):\n>            #\n          Layer normalization 1.\n>        x1 =\n          layers.LayerNormalization(epsilon=1e-6)(encoded_patches)\n>        # Create a\n          multi-head attention layer.\n>        attention_output =\n          layers.MultiHeadAttention(\n>        num_heads=num_heads,\n          key_dim=projection_dim, dropout=0.1)\n>        (x1, x1)\n>        # Skip connection\n          1.\n>        x2 =\n          layers.Add()([attention_output, encoded_patches])\n>        # Layer\n          normalization 2.\n>        x3 =\n          layers.LayerNormalization(epsilon=1e-6)(x2)\n>        # MLP.\n>        x3 = mlp(x3,\n          hidden_units=transformer_units, dropout_rate=0.1)\n>        # Skip connection\n          2.\n>        encoded_patches =\n          layers.Add()([x3, x2])\n>    # Create a [batch_size, projection_dim]\n          tensor.\n>    representation =\n          layers.LayerNormalization(epsilon=1e- > 6)(encoded_patches)\n>    representation =\n          layers.Flatten()(representation)\n>    representation =\n          layers.Dropout(0.5)(representation)\n>    # Add MLP.\n>    features = mlp(representation,\n          hidden_units=mlp_head_units, dropout_rat > e=0.5)\n>    # Classify outputs.\n>    logits =\n          layers.Dense(1,activation='sigmoid')(features)\n>    # Create the Keras model.\n>    model = keras.Model(inputs=inputs,\n          outputs=logits)\n>    return model\nTrain the transformer model.\n>def run_experiment(model):\n>    optimizer = tfa.optimizers.AdamW(\n>        learning_rate=learning_rate,\n          weight_decay=weight_decay)\n>    model.compile(\n>        optimizer=optimizer,\n>        loss=keras.losses.BinaryCrossentropy(from_logits=True),\n>        metrics = [\n          tf.keras.metrics.BinaryAccuracy(),\n>              tf.keras.metrics.TruePositives(),\n>              tf.keras.metrics.TrueNegatives(),\n>              tf.keras.metrics.FalsePositives(),\n>              tf.keras.metrics.FalseNegatives()])\n>    history = model.fit(\n>        x=x_train,\n>        y=y_train,\n>        batch_size=batch_size,\n>        epochs=num_epochs,\n>        validation_split=0.1,)\n>    return history\n>vit_classifier = create_vit_classifier()\n>history = run_experiment(vit_classifier)\nCheck the accuracy of the model using the whole test set.\n>score = vit_classifier.evaluate(x_test, y_test, verbose =\n          0)\nQuantify cell numbers using ImageJ",
    "Timing: 30 min\n      This describes the detailed procedures for the quantifications of cells\n      with ImageJ.\n    \n        Open the file (Figure 6[href=https://www.wicell.org#fig6]A).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fig6.jpg\n              Figure 6. Count nuclei with ImageJ\n(A–C) Open the file and modify the image.\n(D) Set the parameters for “Analyze Particles”.\n        Select “Process > Subtract Background”, adjust the parameters in the\n        popup window, and click “OK” (Figure 6[href=https://www.wicell.org#fig6]B).\n      \n        Select “Image > Adjust > Threshold”, adjust the parameters in the\n        popup window, and click “Apply” (Figure 6[href=https://www.wicell.org#fig6]B).\n      \n        Select “Process > Binary > Fill Holes” (Figure 6[href=https://www.wicell.org#fig6]C).\n      \n        Select “Process > Binary > Convert to Mask” (Figure 6[href=https://www.wicell.org#fig6]C).\n      \n        Select “Process > Binary > Watershed” (Figure 6[href=https://www.wicell.org#fig6]C).\n      \n        Select “Analyze > Analyze Particles”, adjust the parameters in the\n        popup window, and click “OK” (Figure 6[href=https://www.wicell.org#fig6]D).\n      \nCheck the result (Figure 6[href=https://www.wicell.org#fig6]D).\n      Calculate IC50s using GraphPad Prism 9.3.1 for Windows 10.\n    \nTiming: 10 min\n      The IC50s can be determined using GraphPad. Detailed steps are described\n      below.\n    \n        Select “File > New Project File”, select “XY”, “enter 3 replicates\n        values in side-by-side subcolumns” and click “Create” (Figure 7[href=https://www.wicell.org#fig7]A).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2657-Fig7.jpg\n              Figure 7. Calculate IC50s using GraphPad Prism\n            \n              (A–D) Create a new project and fill in the table. Select the graph\n              type and settings for the x-axis.\n            \n              (E–G) Select analysis and equation to generate the IC50 values.\n            \n        Paste data and click “Data 1” under “Graphs” (Figure 7[href=https://www.wicell.org#fig7]B).\n      \n        Select “Graph Type” in the popup window and click “OK” (Figure 7[href=https://www.wicell.org#fig7]C).\n      \n        Double-click the X-axis on the graph, select “Log 10” for “Scale”,\n        “Short” for “Ticks length”, “Power of 10” for “Format” and click “OK”\n        (Figure 7[href=https://www.wicell.org#fig7]D).\n      \n        Click “Analyze”, select “Nonlinear regression (curve fit)” under “XY\n        analyses” and click “OK” (Figure 7[href=https://www.wicell.org#fig7]E).\n      \n        Select “[Inhibitor] vs. response – Variable slope (four parameters) [2]”\n        and click “OK” (Figure 7[href=https://www.wicell.org#fig7]F).\n      \nCheck the IC50 value (Figure 7[href=https://www.wicell.org#fig7]G)."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Biotechnology And Bioengineering",
    "Microscopy",
    "Cancer",
    "High Throughput Screening"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Bioengineering & Technology"
  ]
}