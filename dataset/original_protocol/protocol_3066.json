{
  "id": 3244,
  "origin_website": "Cell",
  "title": "Non-negative tensor factorization workflow for time series biomedical data",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nData preprocessing\nTiming: 30 min\nIn this section, we describe how to preprocess the demonstration data.\nFirst, we show the procedure for constructing tensor data. A tensor can be considered a generalization of a matrix. For example, a third-order tensor is a three-dimensional array that stores values in the depth way in addition to the vertical and horizontal ways. When saving such high-dimensional data (e.g., 3D) as a 2D Excel spreadsheet, several data types can be considered. Here, we introduce the procedures for constructing tensor data from three data types. In subsequent demonstrations, we will use only a portion of the data taken from Ikeda K. et al.1[href=https://www.wicell.org#bib1] The following code lines in this section are in R language, to be inputted into R-console window.\nStart R and load the following packages:\n> library(\"readxl\")\n> library(\"writexl\")\n> library(\"tidyverse\")\n> library(\"einsum\")\n> library(\"abind\")\n> library(\"reticulate\")\n> library(\"rTensor\")\nTo set up the subsequent analysis, type as follows:\n> metadata_name <- c(\"ID\", \"Sex\", \"AgeCategory\", \"BMICategory\", \"ShotSite\", \"ShotInterval\", \"IntervalToTest\", \"PostTiterLog2\", \"Antiinflammatory\")\n> symptoms <- c(\"joint_pain\", \"fatigue\", \"fever\", \"cold\",\n  \"headache\", \"muscle_pain\", \"nausea\", \"flare\",\n  \"swelling\", \"pain\", \"use_painkiller\", \"medical_checkup\")\n> days <- c(paste(1, 1:7, sep=\"_\"), paste(2, 1:7, sep=\"_\"))\n> thr <- length(symptoms) ∗ length(days) ∗ 0.3\nConverting 2D data to 3D.\nType 1: Wide short data: The first type is wide short matrix data, with rows for subjects and columns for combinations of days and symptoms (https://figshare.com/ndownloader/files/38365868[href=https://figshare.com/ndownloader/files/38365868], Figure 1[href=https://www.wicell.org#fig1]). To load this Excel file in R, use read_excel22[href=https://www.wicell.org#bib22] function as follows:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2810-Fig1.jpg\nFigure 1. Type 1: Wide short data\nRows indicate subjects and columns indicate symptoms and days.\n> read_excel(\"type1_wide.xlsx\") -> data1",
    "Then, a set of matrices stratified by days is created with the map22[href=https://www.wicell.org#bib22] function, and they are stacked in the depth direction with the abind23[href=https://www.wicell.org#bib23] function to construct a third-order tensor called vaccine_tensor.\n> map(days, function(x){data1 %>% select(contains(paste0(x, symptoms)))}) %>% abind(along=3) -> vaccine_tensor\nType 2: Tall narrow data: The second type is tall narrow data also known as \"tidy\" data22[href=https://www.wicell.org#bib22] (https://figshare.com/ndownloader/files/38362235[href=https://figshare.com/ndownloader/files/38362235] Figure 2[href=https://www.wicell.org#fig2]). To load this Excel file in R, use read_excel function as follows:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2810-Fig2.jpg\nFigure 2. Type 2: Tall narrow data (also known as \"tidy\" data)\nRows indicate subjects and columns indicate all the variables corresponding to each subject.\n> read_excel(\"type2_long.xlsx\") -> data2\nThen, a tidy dataset stratified by days is created with the map function, reshaped as a matrix form by pivot_wider22[href=https://www.wicell.org#bib22] and as.matrix, and they are stacked in the depth direction with the abind function to construct a third-order tensor called vaccine_tensor.\n> subjects <- as.character(sort(unique(data2$ID)))\ntemplate <- matrix(NA, nrow=length(subjects), ncol=length(symptoms))\n> dimnames(template) <- list(subjects, symptoms)\n> map(days, function(x){data2 %>% filter(., days==x) %>% pivot_wider(., names_from=\"symptoms\") -> tmp\n  template[as.character(tmp$ID), symptoms] <- as.matrix(tmp[, symptoms])\n  template\n  }) %>% abind(along=3) -> vaccine_tensor\nType 3: Multiple sheets data: The third type is one in which Type 1 data is pre-stratified by days and stored as separate sheets (https://figshare.com/ndownloader/files/38362238[href=https://figshare.com/ndownloader/files/38362238], Figure 3[href=https://www.wicell.org#fig3]). This format may be the most straightforward format to create tensor data; this can omit the stratification part in the procedure for Type 1 data above and each sheet is directly stored as a frontal slice of vaccine_tensor as follows:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2810-Fig3.jpg\nFigure 3. Type 3: Multiple sheets data\nAfter stratifying Type 1 data by days, multiple matrices are saved in each sheet.\n> map(seq_along(days), function(x){\n  read_excel(\"type3_multisheets.xlsx\", sheet=x, range=cell_cols(\"B:M\"))\n}) %>% abind(along=3) -> vaccine_tensor\nFilter low-quality data.",
    "Note: In Ikeda, K. et al.'s study,1[href=https://www.wicell.org#bib1] we added the process for filtering the subjects with few observations (i.e., subjects whose observed elements are under 30%). This process is the summation of a 3D tensor for a given dimension and transforming it to 1D and can be easily described by using the einsum24[href=https://www.wicell.org#bib24] function, which is inspired by Einstein's summation.\n> vaccine_tensor %>% is.na %>% einsum('ijk->i', .) %>% `<`(thr) %>% which -> subjects\nSave tensor data as a NumPy binary file.\n> np <- import(\"numpy\")\n> np$save(\"demo_data.npy\", r_to_py(vaccine_tensor))\nFinally, convert the vaccine_tensor to NumPy’s binary file.\nNote: This conversion can be performed by reticulate25[href=https://www.wicell.org#bib25] package. The third-order tensor data (1516 subjects × 12 symptoms × 14 days) for all subjects can be obtained from Figshare (see key resources table[href=https://www.wicell.org#key-resources-table]) and be used in the next section.\nNon-negative tensor factorization\nTiming: two days\nIn this section, we describe how to perform TensorLyCV.\nTensorLyCV consists of 14 rules, and once a rule is successfully executed, the downstream rule is then executed, and this procedure is repeated to ensure that all calculations are properly finished (Figures 4[href=https://www.wicell.org#fig4] and 5[href=https://www.wicell.org#fig5]). The most time-consuming one of these rules is \"tensorly_w_mask\", which performs the rank estimation with masking approach (Figure 6[href=https://www.wicell.org#fig6]). To accelerate this step, user has some options to perform TensorLyCV like below. The following code lines in this section are in Bash script, to be inputted into terminal window.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2810-Fig4.jpg\nFigure 4. Rules and the descriptions in TensorLyCV\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2810-Fig5.jpg\nFigure 5. Dependency graph between rules in TensorLyCV\nIf a rule on the graph is successfully executed, its downstream rule is executed. This figure is generated by Snakemake’s --rulegraph option (cf. https://github.com/kokitsuyuzaki/TensorLyCV/blob/main/src/dag.sh[href=https://github.com/kokitsuyuzaki/TensorLyCV/blob/main/src/dag.sh]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2810-Fig6.jpg\nFigure 6. Computational time of each rule in TensorLyCV",
    "The x-axis indicates the calculation time of each rule and the y-axis indicates rules in TensorLyCV. This figure is generated by Snakemake’s --report option (cf. https://github.com/kokitsuyuzaki/TensorLyCV/blob/main/src/report.sh[href=https://github.com/kokitsuyuzaki/TensorLyCV/blob/main/src/report.sh]).\nExecute the following Snakemake workflow.\nOn a local machine such as a laptop, run TensorLyCV as follows:\n> snakemake -j 5 --config input=data/vaccine_tensor.npy outdir=output rank_min=1 rank_max=10 trials=50 n_iter_max=1000 ratio=30 --resources mem_gb=10 --use-singularity\nCritical: The above is a code that performs a series of NTF analyses at once, including rank estimation using masking approach, decomposition at the optimal rank, and visualization of the decomposition results. The argument -j is the number of CPU cores to be used in Snakemake. The arguments input and outdir are the input file and output directory, respectively. The arguments associated with rank estimation are rank_min, rank_max, trials, and n_iter_max, meaning a decomposition from rank 1 to 10 with 50 random trials using different initial values for each rank, up to 1,000 iterations before convergence on each random trial, and ratio is an argument related to the masking approach, meaning the percentage of elements to be masked as noise. Previous research1[href=https://www.wicell.org#bib1] has shown that varying the percentage of noise by 5%, 10%, 20%, and 30% does not affect the rank estimation, but noise above 40% makes the decomposition unstable and should be avoided. The argument mem_gb is the memory usage. --use-singularity is the argument to use Docker container via Singularity.\nOn a distributed environment with GridEngine, run TensorLyCV as follows:\n> snakemake -j 32 --config input=data/vaccine_tensor.npy outdir=output rank_min=1 rank_max=10 trials=50 n_iter_max=1000 ratio=30 --resources mem_gb=10 --use-singularity --cluster \"qsub -l nc=4 -p -50 -r yes\"\nCritical: Here, the argument --cluster is added to use a distributed environment, and the command \"qsub -l nc = 4 -p -50 -r yes\" is given when submitting jobs to GridEngine.",
    "On a distributed environment with Slurm, run TensorLyCV as follows:\n> snakemake -j 32 --config input=data/vaccine_tensor.npy outdir=output rank_min=1 rank_max=10 trials=50 n_iter_max=1000 ratio=30 --resources mem_gb=10 --use-singularity --cluster \"sbatch -n 4 --nice=50 --requeue\"\nCritical: The arguments other than --cluster are the same as for GridEngine, but the command enclosed in \"\" after -cluster has been changed for Slurm.\nBecause TensorLyCV itself is also dockerized, if Docker is available, user can perform TensorLyCV by docker command as follows:\n> docker run --rm -v $(pwd):/work ghcr.io/kokitsuyuzaki/tensorlycv:main -i /work/data/vaccine_tensor.npy -o /work/output --cores=5 –-rank_min=1 --rank_max=10 --trials=50 --n_iter_max=1000 --ratio=30 --memgb=100\nCritical: In this case, the installation of Snakemake and Singularity can be omitted.\nOptional: If a user predetermines the optimal rank, either empirically or by using the elbow method, etc., the user can skip the rank estimation step and just decompose and visualize the decomposition results at the specified rank.\n> snakemake -j 32 --config input=data/vaccine_tensor.npy outdir=output rank_min=4 rank_max=4 trials=50 n_iter_max=1000 ratio=30 --resources mem_gb=10 --use-singularity\nCritical: By setting rank_min and rank_max to the same value (e.g., 4), no rank estimation is performed, and the decomposition is directly performed at the specified rank."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Health Sciences",
    "Bioinformatics"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Bioinformatics & Computational Biology"
  ]
}