{
  "id": 1706,
  "origin_website": "Cell",
  "title": "Protocol to assess performance of crisis standards of care guidelines for clinical triage",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nStudy design\nTiming: 3 days for protocol development. IRB/ethics review and revision can take 4–8 weeks.\nEmpirical testing of CSC triage algorithms starts with careful consideration of the clinical data needed to capture the nature of the crisis. In depth review of existing algorithms, their applicability toward the specific crisis, and whether adequate patient data is available in health records are important steps when designing and testing triage algorithms. In addition, developing study parameters and selecting patient cohorts are necessary before beginning the data acquisition stage.\nDefine the crisis situation (e.g., mass casualty event, COVID-19 pandemic).\nSelect existing algorithms for study, such as those issued by U.S. states: Select representative approaches that test hypotheses and areas of interest, like comorbidities.\nDefine hypothetical triage algorithms for study:\nDesign de novo algorithms that test hypotheses and areas of interest, such as disease severity scoring systems, comorbidity scoring indices, or tie-breaker criteria.\nCritical: Selection of an initial triage algorithm should reflect the clinical nature of the crisis. We recommend having physician specialists with direct experience in this area to guide this process.\nSelect primary and secondary clinical endpoints—both the type of outcome (e.g., mortality, length of stay) and time point (e.g., 28 days).\nDetermine the clinical data needed to calculate triage algorithm scores and clinical endpoints.\nSelect the patient cohort for study: Common approaches are a retrospective cohort or cross-sectional analysis of a prospective cohort.\nAssess whether the necessary clinical data are available in this patient cohort.\nCritical: To make the study feasible, the clinical endpoint(s) or choice of patient cohort may need to be changed or modified. A multi-disciplinary team of domain experts (e.g., clinician-scientists, data scientists, and biostatisticians) is essential to ensure the proposed study has clinical relevance and statistical rigor.",
    "Adapt the triage algorithm scoring scheme to data available in the selected patient cohort.\nSubmit the proposed study to the IRB or ethics panel.\nCritical: IRB or ethics approval is required prior to further work.\nCalculating priority scores\nTiming: 3 weeks to 3 months (varies by method of data acquisition)\nAcquiring and recording patient data needed for triage scoring is an important process when testing algorithms. Viewing and recording health information requires a secure management system (i.e., software) that will protect patients privacy as well as maintain data integrity. The following section outlines the steps involved in developing and recording data into a case report form, searching data from health records, and using the case report form to calculate triage and priority scores from the designed algorithm.\nCreate a case report form with fields for all data required to calculate triage algorithm scores and clinical endpoints, as discussed in our prior STAR Protocol (Crowley et al., 2021[href=https://www.wicell.org#bib10]).\nSelect a data management system. Key features include collaborative tools, security to protect patient health information, and back-up methods. Examples approved at our institution include REDCap (www.project-redcap.org[href=http://www.project-redcap.org]), enterprise Dropbox, or enterprise Microsoft Teams.\nCritical: Data management systems must be approved by your institution’s human subject research policies.\nSelect the method of data entry (e.g., manual chart review, electronic queries) and establish protocols for data quality checks and pre-processing.\nPerform pilot data acquisition with the case report form. In an iterative fashion, refine protocols for data entry and pre-processing. Confirm the study’s feasibility in this cohort.\nAfter finalizing the data acquisition protocol, complete the case report form for all patients.\nApply the triage algorithms and calculate priority scores for each patient in the cohort.",
    "Critical: Case report forms should anticipate and include additional data fields that may be needed for sensitivity testing and algorithm adaptation.\nTesting algorithm accuracy\nTiming: 1 day\nAfter the design, data acquisition, and calculation of priority scores are completed, the next step is to test the algorithm's accuracy in predicting the defined outcome(s). Utilizing each patient's priority score, the study parameters, and the known outcomes obtained from the health record, the number of true and false positives within the patient cohort can be calculated. Using this data, accuracy can be calculated. We recommend using the AUROC method by DeLong et al. (1988)[href=https://www.wicell.org#bib12].\nDetermine the accuracy of the priority scores for clinical endpoints that are binary values (e.g., survival) using the area under the receiver operating characteristic (AUROC) curve.\nSimulation of clinical decision-making: Selection from a smaller group\nTiming: 1 day\nClinicians will make triage decisions in small groups of patients (e.g., 2 to 5 patients), rather than across a large population. To quantify performance in a simulation of this scenario, this protocol section uses a bootstrap method and then iterates this sampling to generate summary statistics for performance (Figure 1[href=https://www.wicell.org#fig1]). The code and an example using sample data is provided at https://github.com/maheetha/csc[href=https://github.com/maheetha/csc].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1164-Fig1.jpg\nFigure 1. Simulation of small patient group clinical decision-making (steps 17–19)\nIn this illustration, investigators selected groups of two patients (n=2); generation of 100 patient groups per simulation run; and iteration of the simulation for 1,000 runs to generate summary statistics. The simulation code is open-access and can be modified as needed (e.g., to model patient groups of different sizes within the same simulation run). Reproduced from Jezmir et al. (2021)[href=https://www.wicell.org#bib21]\nFormat input files by assigning a patient to each row and assigning these columns:\nOutcomes: 0 or 1\nPriority score calculated by triage algorithm 1",
    "Priority score calculated by triage algorithm 2\nPriority score calculated by triage algorithm 3 Continue for all triage algorithms tested\nSensitivity analyses (e.g., race)\nRun CSC script: Rscript Groups_Analysis.R [input file] [group size] [number of iterations] [size of each bootstrap] [final output filename]:\nThe [group size] refers to the number of patients in each small patient group that one patient is selected from (e.g., 5 patients). The [size of each bootstrap] refers to the number of patient small groups randomly selected in each iteration.\nCritical: Ensure input file is formatted correctly in step 17.\nAssess bootstrap analysis output, with sequential columns reporting:\nTriage algorithm\nMean percentage of patient selections made without tied priority scores\n95% confidence interval (CI)\nMean percentage of decisions that selected the patient with the better outcome (among non-tied decisions)\n95% CI for non-tied decisions that selected for the better outcome.\nMean percentage of all decisions (i.e., tied or non-tied) that chose a surviving patient\n95% CI for all decisions that chose a surviving patient.\nSensitivity analyses\nTiming: 1 week\nTest the sensitivity of triage algorithm performance to key factors by repeating the analysis in steps 16–19 with different inputs.\nTest the effect of data processing methods by repeating step 16–19 with different data protocols (e.g., different methods of imputing missing data values).\nTest the effect of patient characteristics by repeating steps 16–19 in patient sub cohorts (e.g., subdivided by race),\nTest the effect of triage algorithm components by repeating steps 16–19 with modified triage algorithms (e.g., separately scoring the disease severity and comorbidity components)."
  ],
  "subjectAreas": [
    "Bioinformatics",
    "Systems Biology",
    "Computer Sciences",
    "Health Sciences",
    "Clinical Protocol"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}