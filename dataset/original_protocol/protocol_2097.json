{
  "id": 2211,
  "origin_website": "Cell",
  "title": "Protocol for unbiased, consolidated variant calling from whole exome sequencing data",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nIn all the subsequent steps, the paths to the required software tools are the same as the “exported” paths in the respective command boxes under the “before you begin[href=https://www.wicell.org#before-you-begin]” section.\nQuality control and filtering\nTiming: 2 h 15 min\nQuality control of the generated data is a crucial step in every Next Generation Sequencing protocol, let alone in the case of processes related also to the clinic, such as exome sequencing and variant calling. Quality control in exomes becomes even more critical, as in the case of detecting variants on a large scale, it is not straightforward to distinguish between sequencing errors and actual variations in the human genome. Therefore, quality control procedures are often lenient and total quality assessment is a combination of various factors. In this section we outline a typical pre-alignment quality control procedure for whole exome sequencing data. In the end, quality controlled FASTQ files ready for alignment will be acquired.\nQuality control with FastQC and MultiQC.\nPre-alignment QC using FastQC to determine if any raw data corrective actions need to be taken. Default FastQC reports are not interactive and not aggregated.\nUse MultiQC to create a more user-friendly and complete report.\nThe following bash script can be used as a template:\n#!/bin/bash\nHOME_PATH=/home/user/analysis\nFASTQ_PATH=$HOME_PATH/fastq\nFASTQ_PATTERN=∗.fastq.gz\nFASTQC_COMMAND=$FASTQC_PATH/fastqc\nMULTIQC_COMMAND=$MULTIQC_PATH/multiqc\nFASTQC_OUTPUT=$HOME_PATH/fastqc\nCORES=8\nif [ ! -d $FASTQC_OUTPUT ]\nthen\n  mkdir -p $FASTQC_OUTPUT\nfi\n$FASTQC_COMMAND --outdir $FASTQC_OUTPUT --threads $CORES\n$FASTQ_PATH/$FASTQ_PATTERN\n$MULTIQC_COMMAND $FASTQC_OUTPUT -o $FASTQC_OUTPUT\nThe results of MultiQC can be viewed by opening the file $FASTQC_OUTPUT/multiqc_report.html in a web browser.",
    "Note: From the results of FastQC and MultiQC, a lot of useful information may be revealed. Some examples include the presence of adapters, the presence of bias in the 3′/5′ end of reads, poor quality in the 3′/5′ end of reads, poor quality for certain samples and sequence over-representation other than the one expected from adapter contamination. After a first round of inspection, we may have to improve the quality of the overall dataset prior to continuing with other actions regarding alignment to the reference genome and the subsequent variant calling. Trim Galore is a good option for this as it automates many processes, including standard adapter automated removal and maintaining paired-end read integrity. In the case of the data presented in this protocol, the quality of the dataset is acceptable and none of the above points apply. No further further action is needed. Therefore, the following section is not required. It is only mentioned here for reference purposes and protocol completeness.\nAdapter and poor-quality base trimming (optional). A template bash script to wrap Trim Galore follows. With comments, below the main command, a stricter alternative filtering approach:\n#!/bin/bash\nHOME_PATH=/PATH/TO/ANALYSIS/DIRECTORY\nFASTQ_PATH=$HOME_PATH/fastq\nTRIMGALORE_COMMAND=$TRIMGALORE_PATH/trim_galore\nCUTADAPT_COMMAND=$CUTADAPT_PATH/cutadapt\nTRIMGALORE_OUTPUT=$HOME_PATH/fastq_qual\nCORES=4\nif [ ! -d $TRIMGALORE_OUTPUT ]\nthen\n  mkdir -p $TRIMGALORE_OUTPUT\nfi\nfor FILE in $FASTQ_PATH/∗_1.fastq.gz\ndo\n  BASE=`basename $FILE | sed s/_1\\.fastq\\.gz//`\n  echo \"Processing $BASE\"\n  mkdir -p $TRIMGALORE_OUTPUT\n  F1=$FASTQ_PATH/$BASE\"_1.fastq.gz\"\n  F2=$FASTQ_PATH/$BASE\"_2.fastq.gz\"\n    $TRIMGALORE_COMMAND \\\n    --quality 30 \\\n    --length 50 \\\n    --output_dir $TRIMGALORE_OUTPUT/$BASE \\\n    --path_to_cutadapt $CUTADAPT_COMMAND \\\n    --cores 4 \\\n    --paired \\\n    --fastqc \\\n    --trim-n $F1 $F2\n  mv $TRIMGALORE_OUTPUT/$BASE\"_1_val_1.fq.gz\" \\\n      $TRIMGALORE_OUTPUT/$BASE\"_1.fastq.gz\"\n  mv $TRIMGALORE_OUTPUT/$BASE\"_2_val_2.fq.gz\" \\\n      $TRIMGALORE_OUTPUT/$BASE\"_2.fastq.gz\"\n  mv $TRIMGALORE_OUTPUT/$BASE\"_1_val_1_fastqc.html\" \\\n      $TRIMGALORE_OUTPUT/$BASE\"_1_fastqc.html\"\n  mv $TRIMGALORE_OUTPUT/$BASE\"_1_val_1_fastqc.zip\" \\\n      $TRIMGALORE_OUTPUT/$BASE\"_1_fastqc.zip\"\n  mv $TRIMGALORE_OUTPUT/$BASE\"_2_val_2_fastqc.html\" \\\n      $TRIMGALORE_OUTPUT/$BASE\"_2_fastqc.html\"\n  mv $TRIMGALORE_OUTPUT/$BASE\"_2_val_2_fastqc.zip\" \\\n      $TRIMGALORE_OUTPUT/$BASE\"_2_fastqc.zip\"\ndone",
    "For paired-end reads, Trim Galore! produces four outputs and specifically, mate 1 reads passing QC, mate 2 reads passing QC (and matched to mate 1), mate 1 failed reads (optional, not chosen above), mate 2 failed reads (optional, not chosen above).\nInspection of the outcome. Trim Galore also runs FastQC again. From its output we may be able to see that:\nThe problematic points identified above are remedied and brought to acceptable states and error rates.\nThe number of filtered reads remains at acceptable amounts.\nAlignment to the reference genome and alignment statistics\nTiming: 4 h 30 min\nThis section describes the process of aligning the FASTQ pairs to the reference genome and collecting alignment statistics for quality control purposes. In the end of the step, BAM files and a report of read alignment statistics are generated.\nIndex the reference genome.\nThis step is needed only once and does not have to be repeated for the application of the protocol to new data, unless the index and/or reference genomes are deleted by the user. When this process is completed, we need to create an additional file called hs37d5.dict expected by GATK tools for variant calling and other processing. We use samtools for this.\ncd $RESOURCES_PATH/hs37d5\n$BWA_PATH/bwa index hs37d5.fa\n$SAMTOOLS_PATH/samtools faidx hs37d5.fa\n$SAMTOOLS_PATH/samtools dict hs37d5.fa > hs37d5.dict\ncd $CWD\nAlignment to the reference genome.\nAfter the index building is finished, the alignment process can be initiated for each FASTQ file.",
    "Critical: The downstream variant calling analysis requires read group information. Read groups are added to each alignment resulting in a BAM file in order to separate different individuals as well as samples resulting from different lanes and libraries. Read groups are required as variant callers pool samples to estimate the models behind variant discovery. Read groups (the RG tag) can be added during alignment with bwa using the -R option. The following shell script can be used to accomplish the alignment and read group addition procedure. Furthermore, as BAM files need further processing, the file extension of the aligned files is .uns.\n#!/bin/bash\nHOME_PATH=/home/user/analysis\n#  Change the path below with the quality-controlled data directory\n#  if trimming performed (see commented line below)\nFASTQ_PATH=$HOME_PATH/fastq\n#FASTQ_PATH=$HOME_PATH/fastq_qual\nBAM_PATH=$HOME_PATH/bam\nTHREADS=24\nBWA_INDEX=$RESOURCES_PATH/hs37d5/hs37d5.fa\nif  [  -d $BAM_PATH ]\nthen\n  mkdir -p $BAM_PATH\nfi\nfor FILE in `ls $FASTQ_PATH/∗_1.fastq.gz`\ndo\n    BASE=`basename $FILE | sed s/_1\\.fastq\\.gz//`\n    F1=$FASTQ_PATH/$BASE\"_1.fastq.gz\"\n    F2=$FASTQ_PATH/$BASE\"_2.fastq.gz\"\n    RG=\"@RG\\tID:\"$BASE\"\\tSM:\"$BASE\"\\tLB:WES\\tPL:ILLUMINA\"\n    $BWA_PATH/bwa mem -t $THREADS -R $RG $BWA_INDEX $F1 $F2 | \\\n          $SAMTOOLS_PATH/samtools view -bS -o $BAM_PATH/$BASE\".uns\" -\ndone\nPreparation of BAM files\nTiming: 3 h\nIn this section we describe the steps taken to prepare the BAM files for the subsequent variant calling and discovery. The output of this part comprises BAM files suitable for the subsequent variant calling. The vast majority of variant callers require these preparation steps and the major steps taken (in slightly different flavors according to the tools used to make them so) are the following:\nMerging of BAM files from different lanes. This is an optional step according to the instrument and sequencing protocol used (the files used in this protocol do not require this step).\nThen, if the sequencing is paired-end:\nSort the reads in the BAM file according to their names so that pairs are placed one below the other.",
    "Fix mates so that they both have the same sets of attributes for the subsequent preprocessing.\nRe-sort the reads according to their genomic coordinates this time.\nMark the duplicate reads as variant callers take this information into account.\nIf the sequencing is single-end:\nSort the reads according to their genomic coordinates.\nMark the duplicate reads as variant callers take this information into account.\nIn our case, we have paired-end sequencing, so we are following the first set of steps above.\nSort the reads in the BAM file according to their names so that pairs are placed one below the other and fix read-mates so that they both have the same sets of attributes for the subsequent preprocessing.\n#!/bin/bash\nBAM_PATH=$HOME_PATH/bam\nCORES=16\nfor FILE in `ls $BAM_PATH/∗.uns`\ndo\n  SAMPLE=`basename $FILE | sed s/\\.uns//`\n  echo \"Processing $SAMPLE\"\n  $SAMTOOLS_PATH/samtools sort -n -@ $CORES -m 4G \\\n      $BAM_PATH/$SAMPLE\".uns\" | \\\n      $SAMTOOLS_PATH/samtools fixmate -m -\n      $BAM_PATH/$SAMPLE\"_fixmate.bam\"\ndone\nrm $BAM_PATH/∗.uns\nRe-sort the reads according to their genomic coordinates and mark the duplicate reads as variant callers take this information into account.\n#!/bin/bash\nBAM_PATH=$HOME_PATH/bam\nCORES=16\nfor FILE in `ls $BAM_PATH/∗_fixmate.bam`\ndo\n  SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n  echo \"Processing $SAMPLE\"\n  $SAMTOOLS_PATH/samtools sort -@ $CORES -m 4G \\\n      $BAM_PATH/$SAMPLE\"_fixmate.bam\" | \\\n      $SAMTOOLS_PATH/samtools markdup - $BAM_PATH/$SAMPLE\".bam\"\n  echo \"Indexing $SAMPLE\"\n  $SAMTOOLS_PATH/samtools index $BAM_PATH/$SAMPLE\".bam\"\ndone\nCollection of alignment statistics\nTiming: 3 h 15 min\nIn this section, several statistics related the quality control of the alignment process are collected. At the end of the process, a text file with statistics should be produced.\nCollect alignment statistics for quality control.\nTotal sequenced reads.\nAligned reads.\nUniquely aligned reads (q>20).\nChimeric reads (for paired-end sequencing).\nReads overlapping targets.\nTotal sequenced bases.\nAligned bases.\nUniquely aligned bases.\nBases overlapping targets. Furthermore, for paired-end sequencing, we collect:\nTotal sequenced read pairs.\nProperly aligned read pairs.",
    "Properly paired uniquely aligned reads.\nThe following shell script template can be used for this purpose:\n#!/bin/bash\nCAPTURE_KIT=$HOME_PATH/resources/panel/Agilent_SureSelect_All_Exon_V2.bed\nBAM_PATH=$HOME_PATH/bam\nREPORT=$HOME_PATH/reports/finalbamstats.txt\nmkdir $HOME_PATH/reports\nprintf \"%s\\t%s\\t%s\\t%s\\t%s\\t%s%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" \"name\" \\\n  \"total reads\" \"total reads pairs\" \"aligned reads\" \\\n    \"properly paired aligned pairs\" \"uniquely aligned reads (q>20)\" \\\n    \"properly paired uniquely aligned reads\" \"chimeric reads\" \\\n        \"reads overlapping targets\" \"total bases\" \"aligned bases\" \\\n        \"uniquely aligned bases\" \"bases overlapping targets\" > $REPORT\nfor FILE in `ls $BAM_PATH/∗_fixmate.bam`\ndo\n  SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n  echo \"Processing $SAMPLE\"\n  BAM=$BAM_PATH/$SAMPLE\".bam\"\n  printf \"%s\\t\" $SAMPLE >> $REPORT\n  echo \" total reads\"\n  printf \"%d\\t\" `$SAMTOOLS_PATH/samtools view -c -F2048 $BAM` >> $REPORT\n  echo \" total read pairs\"\n  printf \"%d\\t\" `$SAMTOOLS_PATH/samtools view -c -F2048 $BAM | awk '{print $1/2}'` \\\n    >> $REPORT\n  echo \" aligned reads\"\n  printf \"%d\\t\" `$SAMTOOLS_PATH/samtools view -c -F2052 $BAM` >> $REPORT\n  echo \" properly paired aligned pairs\"\n  printf \"%d\\t\" `$SAMTOOLS_PATH/samtools view -c -f66 -F2048 $BAM` \\\n    >> $REPORT\n  echo \" uniquely aligned reads (q>20)\"\n  printf \"%d\\t\" `$SAMTOOLS_PATH/samtools view -c -F2052 -q20 $BAM` >> \\\n    $REPORT\n  echo \" properly paired uniquely aligned reads\"\n  printf \"%d\\t\" `$SAMTOOLS_PATH/samtools view -c -f66 -F2048 -q20 $BAM` \\\n    >> $REPORT\n  echo \" chimeric reads\"\n  printf \"%d\\t\" `\n    $SAMTOOLS_PATH/samtools flagstat $BAM | \\\n    perl -e 'my @in;' \\\n      -e 'while(<>) { chomp $_; push(@in,$_); }' \\\n      -e 'my @tmp = split(\"\\\\\\+\",pop(@in));' \\\n      -e '$tmp[0] =∼ s/\\s+$//;' \\\n      -e 'print STDOUT $tmp[0];'\n    ` >> $REPORT\n  echo \" reads overlapping targets\"\n  printf \"%d\\t\" `\n    $BEDTOOLS_PATH/bedtools intersect -a $CAPTURE_KIT -b $BAM -c | \\\n        awk 'BEGIN {tot=0}{tot+=$4} END {print tot}'\n    ` >> $REPORT\n  echo \" total bases\"\n  printf \"%d\\t\" `\n    $SAMTOOLS_PATH/samtools view $BAM | cut -f10 | \\\n        awk 'BEGIN {tr=0}{tr+=length($0)} END {print tr}'\n    ` >> $REPORT\n  echo \" aligned bases\"\n  printf \"%d\\t\" `\n    $SAMTOOLS_PATH/samtools view -F2052 $BAM | cut -f10 | \\\n        awk 'BEGIN {tr=0}{tr+=length($0)} END {print tr}'",
    "` >> $REPORT\n  echo \" uniquely aligned bases\"\n  printf \"%d\\t\" `\n    $SAMTOOLS_PATH/samtools view -F2052 -q20 $BAM | cut -f10 | \\\n        awk 'BEGIN {tr=0}{tr+=length($0)} END {print tr}'\n    ` >> $REPORT\n  echo \" bases overlapping targets\"\n  printf \"%d\\n\" `\n    $BEDTOOLS_PATH/bedtools coverage -a $CAPTURE_KIT -b $BAM -d | \\\n        awk 'BEGIN {tr=0} {tr+=$5} END {print tr}'\n    ` >> $REPORT\ndone\nNote: This section describes the steps taken to collect some useful alignment statistics and prepare the BAM files for the subsequent variant calling and discovery. The former may further help identify poor quality samples that should not be used for variant calling. While such samples may have passed the QC process applied on raw data, it is possible that they may present low alignment rates or low coverage over the target areas (exome capture kit), as for example a result of possible contamination.\nSignal visualization\nTiming: 30 min\nAnother level of quality control as well as supporting evidence for discovered variants is the actual inspection of the sequencing signal or coverage (i.e., the histogram created by the short reads pileup in a specific locus). This can be accomplished by uploading, opening or linking signal files created from BAM files, to a genome browser such as the UCSC Genome Browser or the IGV (Robinson et al., 2011[href=https://www.wicell.org#bib18]). Signal tracks in BigWig format can be created using the following shell script as a template. In this case, we note the addition of the “chr” short string before the chromosome names. This is required for viewing in the UCSC Genome Browser. For other browsers such as IGV, this addition depends on the reference genome loaded. The latter can be controlled in IGV but not in the UCSC Genome Browser. The output of this part is BigWig files suitable for visualization in a genome browser.\n#!/bin/bash",
    "BAM_PATH=$HOME_PATH/bam\nTRACKS_PATH=$HOME_PATH/tracks\nGENOME_SIZE=$BEDTOOLS_PATH/../genomes/human.hg19.genome\nif [ -d $TRACKS_PATH ]\nthen\n  mkdir -p $TRACKS_PATH\nfi\nfor FILE in `ls $BAM_PATH/∗_fixmate.bam`\ndo\n  SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n  echo \"Processing $SAMPLE\"\n  $BEDTOOLS_PATH/bedtools genomecov -bg \\\n      -ibam $BAM_PATH/$SAMPLE/$SAMPLE\".bam\" | \\\n      grep -vP 'chrU|rand|hap|loc|cox|GL|NC|hs37d5' | \\\n      awk '{print \"chr\"$1\"\\t\"$2\"\\t\"$3\"\\t\"$4}' | \\\n      sed s/chrMT/chrM/g | \\\n      sort -k1,1 -k2g,2 > $TRACKS_PATH/$SAMPLE\".bedGraph\" &\ndone\nwait\nfor FILE in `ls $TRACKS_PATH/∗.bedGraph`\ndo\n  echo \"Processing $FILE\"\n  SAMPLE=`basename $FILE | sed s/\\.bedGraph//`\n  $UCSCTOOLS_PATH/bedGraphToBigWig $FILE $GENOME_SIZE\n$TRACKS_PATH/$SAMPLE\".bigWig\" &\ndone\nwait\nrm $TRACKS_PATH/∗.bedGraph\nThe produced BigWig files must then either be put in a directory served by a web browser such as Apache in order to be viewed by a web-based genome browser (such as UCSC) or be opened directly in a local genome browser such as IGV.\nVariant calling with GATK HaplotypeCaller\nTiming: 6 h\nThis section describes the variant calling procedure using GATK HaplotypeCaller and its output is a VCF file with filtered variants after the application of basic filters. Each caller accepts the BAM files as main inputs but in order to be as efficient as possible, different pre-calling procedures are required. Examples of such procedures are:\nExample 1: The GATK HaplotypeCaller requires a procedure called Base Quality Score Recalibration (BQSR) in order for its underlying model to work as best as possible.\nExample 2: For parallel execution, HaplotypeCaller and FreeBayes require the splitting of the capture kit target genomic intervals so that the algorithm operates on different intervals in parallel. However, the capture kit should be split using different strategies for each caller.\nExample 3: DeepVariant on the other hand does the splitting of the capture kit regions automatically.",
    "In addition, there is nowadays some debate on whether BQSR is needed prior to variant calling or not, as this process was initially developed for older sequencers that produced poorer results than modern ones. We choose to apply BQSR for protocol completeness purposes. More info on the debate can be found in the official GATK community forums and other bioinformatics communities such as Biostars.\nThe calling process with GATK HaplotypeCaller has several steps and substeps. Below we outline the process and provide template scripts.\nBase Quality Score Recalibration and application on BAM files.\nSplit the capture kit to as many intervals as the cores we wish to use.\nCalculate separate BQSR reports.\nGather these reports to a joint model.\nApply the model to existing BAM files.\nKeep the original BAM files as they are required unchanged by the other variant callers.\nThe following shell script template can be used for BQSR:\n#!/bin/bash\nBAM_PATH=$HOME_PATH/bam\nCAPTURE_KIT=$RESOURCES_PATH/panel/Agilent_SureSelect_All_Exon_V2.bed\nINTERVAL_LIST_PATH=$HOME_PATH/resources/interval_scatter\nBWA_INDEX=$RESOURCES_PATH/hs37d5/hs37d5.fa\nDBSNP=$RESOURCES_PATH/dbSNP151.vcf\nGNOMAD=$RESOURCES_PATH/gnomad.exomes.r2.1.1.sites.vcf.bgz\nCORES=16\nPADDING=50\n# Process dbSNP\n$HTSLIB_PATH/bgzip $DBSNP\n$HTSLIB_PATH/tabix $DBSNP”.gz”\nDBSNP=$RESOURCES_PATH/dbSNP151.vcf.gz\nmkdir -p $HOME_PATH/reports\nMETA_REPORT=$HOME_PATH/reports/bsqr_current.log\necho \"=== Splitting intervals\" > $META_REPORT\nif [ -d $INTERVAL_LIST_PATH ]\nthen\n  echo \"  Cleaning previous intervals\" >> $META_REPORT\n  rm -r $INTERVAL_LIST_PATH\nfi\nmkdir -p $INTERVAL_LIST_PATH\n# Firstly split exome intervals for parallel BSQR\n$GATK_PATH/gatk SplitIntervals \\\n      --reference $BWA_INDEX \\\n      --intervals $CAPTURE_KIT \\\n      --interval-padding $PADDING \\\n      --scatter-count $CORES \\\n      --output $INTERVAL_LIST_PATH \\\n      --QUIET\necho \"=== Calculating BQSR tables\" >> $META_REPORT\nfor FILE in `ls $BAM_PATH/∗_fixmate.bam`\ndo\n      SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n      echo \"Processing $SAMPLE\" >> $META_REPORT\n      BAM=$BAM_PATH/$SAMPLE/$SAMPLE\".bam\"\n      mkdir -p $BAM_PATH/$SAMPLE\n      BQSR_PART_OUT=$BAM_PATH/$SAMPLE/bqsr_parts\n      if [ -d $BQSR_PART_OUT ]\n      then\n        echo \" Cleaning previous tables\" >> $META_REPORT\n        rm -r $BQSR_PART_OUT\n      fi\n      mkdir -p $BQSR_PART_OUT\n      # Calculate BQSR over intervals\n      for INTERVAL in `readlink -f $INTERVAL_LIST_PATH/∗`\n      do\n        BQSR_NAME=`basename $INTERVAL | sed s/\\-scattered\\.interval_list//`\n        echo \" Processing $BQSR_NAME\" >> $META_REPORT\n        $GATK_PATH/gatk BaseRecalibrator \\",
    "--input $BAM \\\n            --reference $BWA_INDEX \\\n            --output $BQSR_PART_OUT/$BQSR_NAME\".tab\" \\\n            --known-sites $DBSNP \\\n            --known-sites $GNOMAD \\\n            --intervals $INTERVAL \\\n            --interval-padding $PADDING \\\n            --QUIET &\n      done\n      # Wait for individuals to complete before moving to the next thread\n      wait\ndone\necho \"=== Gathering BQSR reports\" >> $META_REPORT\nfor FILE in `ls $BAM_PATH/∗_fixmate.bam`\ndo\n      SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n      echo \"Processing reports for $SAMPLE\" >> $META_REPORT\n      BQSR_PART_OUT=$BAM_PATH/$SAMPLE/bqsr_parts\n      for TAB in `readlink -f $BQSR_PART_OUT/∗`\n      do\n        echo \"--input $TAB\" >> $BAM_PATH/$SAMPLE/gather_bqsr.arg\n      done\n      # Gather reports\n      $GATK_PATH/gatk GatherBQSRReports \\\n        --arguments_file $BAM_PATH/$SAMPLE/gather_bqsr.arg \\\n        --output $BAM_PATH/$SAMPLE/bqsr.tab \\\n        --QUIET &\ndone\n# Wait for BQSR tables to be merged for each sample\nwait\necho \"=== Applying BQSR to BAM files\" >> $META_REPORT\nfor FILE in `ls $BAM_PATH`\ndo\n      SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n      echo \"Processing BAM file $SAMPLE\" >> $META_REPORT\n      BAM=$BAM_PATH/$SAMPLE/$SAMPLE\".bam\"\n      BQSR_TABLE=$BAM_PATH/$SAMPLE/bqsr.tab\n      # Apply BQSR to BAM files\n      $GATK_PATH/gatk ApplyBQSR \\\n        --input $BAM \\\n        --reference $BWA_INDEX \\\n        --bqsr-recal-file $BQSR_TABLE \\\n        --output $BAM_PATH/$SAMPLE/$SAMPLE\"_bqsr.bam\" \\\n        --QUIET &\ndone\n# Wait for new BAM files to be created before reporting finished\nwait\necho \"=== Finished!\" >> $META_REPORT\nNote: BQSR is a relatively lengthy process and can be executed in parallel if we split the capture kit genomic regions. The GATK toolkit has tools for this. The main inputs for BQSR in exome sequencing are, the exome capture kit, the reference genome and a list of known variant locations (e.g., dbSNP, gnomAD) used to provide the algorithm with a list of ground truth sites used to recalibrate scores.",
    "After the BQSR process, we are ready to proceed with variant calling for each sample separately. Although there are many alternatives to run exome analysis with HaplotypeCaller in an efficient way (e.g., parallelization of capture intervals or running each sample on the background or even using GNU parallel), we propose the following sub-protocol (“intervals” are the capture kit genomic intervals created during the BQSR process).\nBase Quality Score Recalibration and application on BAM files.\nFor each sample.\nFor each genomic interval use GATK HaplotypeCaller to create a gVCF callset file. The files for each interval are written in a sample-specific directory.\nFor each sample.\nLoop through created gVCFs and create a list file.\nMerge gVCFs by placing one below the other and create one unique gVCF file.\nFor each sample, sort the consolidated gVCF using GATK SortVcf.\nCreate a list file for input to GATK GenotypeGVCFs.\nCall GATK GenotypeGVCFs to create the final callset in VCF format.\nUsing bcftools.\nApply the GATK best hard filtering practices for SNPs and create a filtered SNP VCF.\nApply the GATK best hard filtering practices for INDELs and create a filtered INDEL VCF while at the same time normalizing the INDELs.\nWith the SNP and INDEL filtered VCFs, use GATK MergeVcfs to merge the separate filtered VCF files.\nCleanup.",
    "Critical: At this point and with respect to step 2e above, it should be noted that the best filtering practices suggested by the GATK community comprise only basic variant filters in order to reduce noise. As with the rest of the variant callers, more elaborate filtering should follow, which is not generic as these filters but it is application dependent. For example, a user investigating rare disease should look for damaging variants (e.g., frameshift, splicing, missense) after variant annotation while a user interested in conducting a population study with many samples should focus on filtering variants with low frequencies as those would not characterize a population cohort. Finally, under different clinical settings, a user would possibly combine various filters, for example restrict damaging variants to certain virtual gene panels of interest.\nThe suggested hard filters by the GATK community for multiple samples are:\nFor SNPs: QD < 2.0, MQ < 40.0, FS > 60.0, SOR > 3.0, MQRankSum < -12.5, ReadPosRankSum < -8.0.\nFor Indels: QD < 2.0, ReadPosRankSum < -20.0, InbreedingCoeff < -0.8, FS > 200.0, SOR > 10.0.\nSummaries for all steps (including background processes) are recorded in a “report” file for general supervision. The following shell script template can be used to run the above steps:\n#!/bin/bash\nexport VCF_PATH=$HOME_PATH/vcf\nBAM_PATH=$HOME_PATH/bam\nINTERVAL_LIST_PATH=$RESOURCES_PATH/panel/interval_scatter\nBWA_INDEX=$RESOURCES_PATH/hs37d5/hs37d5.fa\nCORES=16\nPADDING=50\nMETA_REPORT=$HOME_PATH/reports/haca_current.log\necho \"=== Calling variants\" > $META_REPORT\nfor SAMPLE in `ls $BAM_PATH`\ndo\n  echo \"Processing $SAMPLE\" >> $META_REPORT\n  BAM=$BAM_PATH/$SAMPLE/$SAMPLE\"_bqsr.bam\"\n  GVCF_PART_OUT=$BAM_PATH/$SAMPLE/gvcf_parts\n  if [ -d $GVCF_PART_OUT ]\n  then\n    echo \" Cleaning previous gVCFs\" >> $META_REPORT\n    rm -r $GVCF_PART_OUT\n  fi\n  mkdir -p $GVCF_PART_OUT\n  # Call variants over intervals\n  for INTERVAL in `readlink -f $INTERVAL_LIST_PATH/∗`\n  do\n    GVCF_NAME=`basename $INTERVAL | sed s/\\-scattered\\.interval_list//`\n    echo \" Processing $GVCF_NAME\" >> $META_REPORT\n    $GATK_PATH/gatk HaplotypeCaller \\\n        --input $BAM \\\n        --reference $BWA_INDEX \\\n        --intervals $INTERVAL \\\n        --interval-padding $PADDING \\\n        --output $GVCF_PART_OUT/$GVCF_NAME\".g.vcf\" \\",
    "--emit-ref-confidence GVCF \\\n        --create-output-variant-index false \\\n        --QUIET &\n  done\n  # Wait for individuals to complete before moving to the next thread\n  wait\ndone\n# Then GVCFs must be consolidated\necho \"=== Merging gVCFs\" >> $META_REPORT\nfor SAMPLE in `ls $BAM_PATH`\ndo\n  echo \"Processing interval gVCFs for $SAMPLE\"\n  GVCF_PART_OUT=$BAM_PATH/$SAMPLE/gvcf_parts\n  if [ -f $BAM_PATH/$SAMPLE/interval_gvcfs.txt ]\n  then\n    echo \" Cleaning previous gVCFs input file\" >> $META_REPORT\n    rm $BAM_PATH/$SAMPLE/interval_gvcfs.txt\n  fi\n  for GVCF in `readlink -f $GVCF_PART_OUT/∗.g.vcf`\n  do\n    echo \"$GVCF\" >> $BAM_PATH/$SAMPLE/interval_gvcfs.txt\n  done\n  # Get the gVCF header and strip the GATK command\n  GVFH=`readlink -f $GVCF_PART_OUT/∗.g.vcf | head -1`\n  grep \"ˆ#\" $GVFH | grep -v \"ˆ##GATKCommand\" > $BAM_PATH/$SAMPLE/gvcf.header\n  # Cat the gVCFs\n  for GVCF in `readlink -f $GVCF_PART_OUT/∗.g.vcf`\n  do\n    echo \" Concatenating $GVCF\"\n    #echo \" Concatenating $GVCF\" >> $META_REPORT\n    grep -v \"ˆ#\" $GVCF >> $BAM_PATH/$SAMPLE/gvcf.tmp\n  done\n  # Place the header\n  echo \" Creating final gVCF\"\n  #echo \" Creating final gVCF\" >> $META_REPORT\n  cat $BAM_PATH/$SAMPLE/gvcf.header $BAM_PATH/$SAMPLE/gvcf.tmp > \\\n      $BAM_PATH/$SAMPLE/$SAMPLE\".u.g.vcf\"\n  rm $BAM_PATH/$SAMPLE/gvcf.tmp $BAM_PATH/$SAMPLE/gvcf.header\ndone\n# Sort gVCFs\necho \"=== Sorting gVCFs\" >> $META_REPORT\nfor SAMPLE in `ls $BAM_PATH`\ndo\n  echo \"Sorting gVCF for $SAMPLE\" >> $META_REPORT\n  $GATK_PATH/gatk SortVcf \\\n      --INPUT $BAM_PATH/$SAMPLE/$SAMPLE\".u.g.vcf\" \\\n      --OUTPUT $BAM_PATH/$SAMPLE/$SAMPLE\".g.vcf.gz\" \\\n      --QUIET &\ndone\n# Wait for sorting to finish before cleaning unsorted\nwait\n# Some cleanup\necho \"=== Deleting unsorted gVCFs\" >> $META_REPORT\nfor FILE in `ls $BAM_PATH/∗_fixmate.bam`\ndo\n  SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n  echo \"Deleting unsorted gVCF for $SAMPLE\" >> $META_REPORT\n  rm $BAM_PATH/$SAMPLE/$SAMPLE\".u.g.vcf\"\n  echo \"Compression gVCF parts for $SAMPLE\" >> $META_REPORT\n  pigz $BAM_PATH/$SAMPLE/gvcf_parts/∗\n  echo \"Compression BQSR reports for $SAMPLE\" >> $META_REPORT\n  pigz $BAM_PATH/$SAMPLE/bqsr_parts/∗\ndone\n# Gather VCFs\necho \"=== Combining sorted population gVCFs\" >> $META_REPORT\nif [ ! -d $VCF_PATH ]\nthen\n  mkdir $VCF_PATH\nfi\n# Delete the .arg file as it will get multiple entries\nif [ -f $VCF_PATH/combine_gvcf.arg ]\nthen\n  rm $VCF_PATH/combine_gvcf.arg\nfi\nfor FILE in `ls $BAM_PATH/∗_fixmate.bam`\ndo",
    "SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n  GVCF=`readlink -f $BAM_PATH/$SAMPLE/$SAMPLE\".g.vcf.gz\"`\n  echo \"--variant $GVCF\" >> $VCF_PATH/combine_gvcf.arg\ndone\n# Combine gVCFs\n$GATK_PATH/gatk CombineGVCFs \\\n  --reference $BWA_INDEX \\\n  --arguments_file $VCF_PATH/combine_gvcf.arg \\\n  --output $VCF_PATH/haplotypecaller_full.g.vcf.gz\n# Genotype VCFs\necho \"=== Genotyping gVCFs\" >> $META_REPORT\n$GATK_PATH/gatk GenotypeGVCFs \\\n    --reference $BWA_INDEX \\\n    --variant $VCF_PATH/haplotypecaller_full.g.vcf.gz \\\n    --output $VCF_PATH/haplotypecaller_full.vcf.gz\n# Apply basic GATK hard filters\necho \"=== Applying GATK hard filters\" >> $META_REPORT\n$BCFTOOLS_PATH/bcftools view \\\n    --include 'QUAL>20 & INFO/QD>2 & INFO/MQ>40 & INFO/FS<60 & INFO/SOR<3\n& INFO/MQRankSum>-12.5 & INFO/ReadPosRankSum>-8 & TYPE=\"snp\"' \\\n    --output-type z \\\n    --output-file $VCF_PATH/haplotypecaller_filtered_snp.vcf.gz \\\n    $VCF_PATH/haplotypecaller_full.vcf.gz &\n# The normalization step is potentially not required but it is harmless\n$BCFTOOLS_PATH/bcftools view \\\n    --include 'QUAL>20 & INFO/QD>2 & INFO/ReadPosRankSum>-20 & INFO/InbreedingCoeff>-0.8 &\nINFO/FS<200 & INFO/SOR<10 & TYPE∼\"indel\"' \\\n    $VCF_PATH/haplotypecaller_full.vcf.gz | \\\n    $BCFTOOLS_PATH/bcftools norm \\\n    --fasta-ref $BWA_INDEX \\\n    --output-type z \\\n    --output $VCF_PATH/haplotypecaller_filtered_norm_indel.vcf.gz &\nwait\necho \"=== Merging GATK filtered SNPs and INDELs\" >> $META_REPORT\n$GATK_PATH/gatk MergeVcfs \\\n    --INPUT $VCF_PATH/haplotypecaller_filtered_snp.vcf.gz \\\n    --INPUT $VCF_PATH/haplotypecaller_filtered_norm_indel.vcf.gz \\\n    --OUTPUT $VCF_PATH/haplotypecaller_filtered_norm.vcf.gz \\\n    --QUIET\nrm $VCF_PATH/haplotypecaller_filtered_snp.vcf.gz \\\n  $VCF_PATH/haplotypecaller_filtered_norm_indel.vcf.gz\n#echo \"=== Finished!\"\necho \"=== Finished!\" >> $META_REPORT\nVariant calling with FreeBayes\nTiming: 5 h\nThis section presents the variant calling and filtering steps with FreeBayes. Its output is a VCF file with filtered (basic filters) variants called with FreeBayes.\nIn comparison with GATK HaplotypeCaller, the model behind FreeBayes does not require BQSR (therefore it is faster), requires all samples processed altogether and at once in the same command (using read groups and the RG tag to distinguish them) and does not operate directly on HaplotypeCaller genomic intervals. These have to be recalculated and reformatted to the BED format for FreeBayes parallelization.",
    "Although there are many alternatives to run exome analysis with FreeBayes in an efficient way (e.g., parallelization of exome kit capture intervals or running each sample on the background or even using GNU parallel), we propose the following protocol (“intervals” are the capture kit genomic intervals recreated with GATK SplitIntervals for FreeBayes):\nRerun GATK SplitIntervals to create FreeBayes specific intervals for parallelization.\nCreate a list file with the individual interval filenames.\nCreate a list file with the individual BAM filenames.\nFor each interval, run FreeBayes jointly for all samples to create a VCF file for that interval.\nMerge the produced multi-sample VCFs for each interval into one multi-sample VCF file using vcflib.\nUsing bcftools and R, determine upper quality (QUAL) and depth (DP) cutoffs based on the respective distributions (assuming initial QUAL>20).\nUsing bcftools apply the filters of (6).\nUsing vcflib decompose the complex variants.\nUsing bcftools normalize INDELs and produce the final VCF.\nCleanup the computation environment.\nSummaries for all steps (including background processes) are recorded in a “report” file for general supervision. The following shell script template can be used to run the above protocol:\n#!/bin/bash\nexport VCF_PATH=$HOME_PATH/vcf\nBAM_PATH=$HOME_PATH/bam\nCAPTURE_KIT=$RESOURCES_PATH/panel/Agilent_SureSelect_All_Exon_V2.bed\nINTERVAL_LIST_PATH=$RESOURCES_PATH/resources/interval_scatter_bed\nBWA_INDEX=$RESOURCES_PATH/hs37d5/hs37d5.fa\nCORES=32\nPADDING=50\nMETA_REPORT=$HOME_PATH/reports/freebayes_current.log\necho \"=== Splitting intervals\" > $META_REPORT\nif [ -d $INTERVAL_LIST_PATH ]\nthen\n  echo \" Cleaning previous intervals\" >> $META_REPORT\n  rm -r $INTERVAL_LIST_PATH\nfi\nmkdir -p $INTERVAL_LIST_PATH\n# Firstly split exome intervals for parallel freebayes\n$GATK_PATH/gatk SplitIntervals \\\n    --reference $BWA_INDEX \\\n    --intervals $CAPTURE_KIT \\\n    --interval-padding $PADDING \\\n    --scatter-count $CORES \\\n    --extension .pre \\\n    --output $INTERVAL_LIST_PATH \\\n    --QUIET\necho \"=== Converting intervals\" >> $META_REPORT\nfor INTERVAL in `ls $INTERVAL_LIST_PATH`\ndo\n  BED=`basename $INTERVAL | sed s/\\.pre//`\n  INTERVAL_FILE=$INTERVAL_LIST_PATH/$INTERVAL\n  grep -vP \"ˆ@\" $INTERVAL_FILE | awk '{print $1\"\\t\"$2\"\\t\"$3}' > \\\n    $INTERVAL_LIST_PATH/$BED\".bed\" &\ndone\n# Wait and clear intermediate intervals\nwait\nrm $INTERVAL_LIST_PATH/∗.pre\n# Prepare BAM file list for freebayes",
    "echo \"=== Preparing BAM file list\" >> $META_REPORT\nBAMLIST=/media/raid/tmp/tmp/medex/scripts/bamlist.txt\nif [ -f $BAMLIST ]\nthen\n  rm $BAMLIST\nfi\nfor FILE in `ls $BAM_PATH/∗_fixmate\\.bam`\ndo\n  SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n  BAM=$BAM_PATH/$SAMPLE/$SAMPLE\".bam\"\n  echo \"$BAM\" >> $BAMLIST\ndone\necho \"=== Calling variants with FreeBayes\" >> $META_REPORT\nif [ -d $VCF_PATH/fb_parts ]\nthen\n  rm -r $VCF_PATH/fb_parts\nfi\nmkdir -p $VCF_PATH/fb_parts\nfor TARGET in `ls $INTERVAL_LIST_PATH`\ndo\n  NAME=`basename $TARGET | sed s/\\.bed//`\n  echo \"Processing interval list $NAME\" >> $META_REPORT\n  INTERVAL=$INTERVAL_LIST_PATH/$TARGET\n  $FREEBAYES_PATH/freebayes \\\n    --fasta-reference $BWA_INDEX \\\n    --bam-list $BAMLIST \\\n    --targets $INTERVAL \\\n    --vcf $VCF_PATH/fb_parts/$NAME\".vcf\" &\ndone\n# Wait before gathering the results\nwait\necho \"=== Merging VCFs\" >> $META_REPORT\ncat $VCF_PATH/∗.vcf | \\\n  $VCFLIB_PATH/scripts/vcffirstheader | \\\n  $VCFLIB_PATH/bin/vcfstreamsort -w 1000 | \\\n  $VCFLIB_PATH/bin/vcfuniq > \\\n  $VCF_PATH/all_samples_freebayes.vcf\necho \"=== Compressing and indexing final VCF\" >> $META_REPORT\n$HTSLIB_PATH/bgzip $VCF_PATH/freebayes_full.vcf\n$HTSLIB_PATH/tabix $VCF_PATH/freebayes_full.vcf.gz\n### Basic filtering before decomposing and normalization\n# Determine a quality and depth cutoff pre-filter based on 99th percentile of\n# the respective distributions\necho \"=== Determining QUAL and DP hard pre-filters\" >> $META_REPORT\n$BCFTOOLS_PATH/bcftools query \\\n    --include 'QUAL>20' \\\n    --format '%QUAL\\n' $VCF_PATH/freebayes_full.vcf.gz > quals.tmp &\n$BCFTOOLS_PATH/bcftools query \\\n    --include 'QUAL>20' \\\n    --format '%INFO/DP\\n' $VCF_PATH/freebayes_full.vcf.gz | \\\n    awk -F \",\" '{print $1}' > $VCF_PATH/dps.tmp &\nwait\nRscript -e '\n    vp <- Sys.getenv(\"VCF_PATH\")\n    dps <- as.numeric(readLines(file.path(vp,\"dps.tmp\")));\n    quals <- as.numeric(readLines(file.path(vp,\"quals.tmp\")));\n    qudp <- unname(round(quantile(dps,0.99)));\n    ququ <- unname(quantile(quals,0.99));\n    write(qudp,file.path(vp,\"dpt.tmp\"));\n    write(ququ,file.path(vp,\"qut.tmp\"));\n'\nQUALUP=`cat $VCF_PATH/qut.tmp`\nDPUP=`cat $VCF_PATH/dpt.tmp`\nrm $VCF_PATH/qut.tmp $VCF_PATH/dpt.tmp $VCF_PATH/dps.tmp $VCF_PATH/quals.tmp\n# Apply the filters, decompose complex variants and normalize\necho \"=== Applying filters and normalizing\" >> $META_REPORT\n$BCFTOOLS_PATH/bcftools view \\\n    --include 'QUAL>20 & INFO/DP>10 & QUAL<'$QUALUP' & INFO/DP<'$DPUP' &\n(QUAL/(INFO/DP))>2' $VCF_PATH/freebayes_full.vcf.gz | \\\n    $VCFLIB_PATH/bin/vcfallelicprimitives -kg | \\\n    $BCFTOOLS_PATH/bcftools norm \\\n    --fasta-ref $BWA_INDEX \\\n    --output-type z \\\n    --output $VCF_PATH/freebayes_filtered_norm.vcf.gz\n$HTSLIB_PATH/tabix $VCF_PATH/freebayes_filtered_norm.vcf.gz\necho \"=== Finished!\" >> $META_REPORT\nVariant calling with DeepVariant\nTiming: 1 h",
    "The model behind DeepVariant is similar to FreeBayes regarding BQSR, therefore not needing it. DeepVariant requires the RG tag (read groups) and splits the capture kit in BED format. The splitting is done internally, so no manual split required from the user for parallelization based on the capture kit genomic intervals. The output of this part is a VCF file with filtered variants called with DeepVariant. Based on DeepVariant authors, we propose the following protocol:\nFor each sample run DeepVariant and create a gVCF and a VCF file.\nCreate a list file with gVCF outputs of DeepVariant for input to DNA Nexus GLnexus.\nRun GLnexus on the DeepVariant gVCFs to consolidate the gVCFs into one final population VCF file.\nUsing bcftools filter the variants with QUAL<20 and normalize.\nThe DeepVariant pipeline is pretty well-defined and quite automated, leaving few steps for the user which essentially come down to variant filtering (which again is not complex). Summaries for all steps (including background processes) are recorded in a “report” file for general supervision. The following shell script template can be used to run the above protocol:\n#!/bin/bash\nexport VCF_PATH=$HOME_PATH/vcf\nBAM_PATH=$HOME_PATH/bam\nCAPTURE_KIT_DIR=$RESOURCES_PATH/resources/panel\nCAPTURE_KIT=$RESOURCES_PATH/panel/Agilent_SureSelect_All_Exon_V2.bed\nDV_VERSION=0.9.0\nBWA_INDEX_DIR=$RESOURCES_PATH/hs37d5\nBWA_INDEX=$RESOURCES_PATH/hs37d5/hs37d5.fa\nCORES=32\nMETA_REPORT=$HOME_PATH/reports/deepvariant_current.log\necho \"=== Calling variants\" > $META_REPORT\nfor FILE in `ls $BAM_PATH/∗_fixmate.bam`\ndo\n  SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n  echo \"Processing $SAMPLE\" >> $META_REPORT\n  BAM=$BAM_PATH/$SAMPLE\".bam\"\n  docker run \\\n      -v \"$BAM_PATH\":\"/data\" \\\n      -v \"$BWA_INDEX_DIR\":\"/reference\" \\\n      -v \"$CAPTURE_KIT_DIR\":\"/capture_kit\" \\\n      google/deepvariant:$DV_VERSION \\\n      /opt/deepvariant/bin/run_deepvariant \\\n      --model_type=WES \\\n      --ref=\"/reference/hs37d5.fa\" \\\n      --reads=\"/data/$SAMPLE.bam\" \\\n      --regions=\"/capture_kit/Agilent_SureSelect_All_Exon_V2.bed\" \\\n      --output_vcf=\"/data/$SAMPLE/$SAMPLE'_DV.vcf'\" \\\n      --output_gvcf=\"/data/$SAMPLE/$SAMPLE'_DV.g.vcf'\" \\\n      --num_shards=$CORES\ndone\necho \"=== Creating list of gVCF files\" >> $META_REPORT\nfor FILE in `ls $BAM_PATH/∗_fixmate.bam`\ndo\n  SAMPLE=`basename $FILE | sed s/_fixmate\\.bam//`\n  GVCF=`readlink -f $BAM_PATH/$SAMPLE/$SAMPLE\"_DV.g.vcf\"`\n  echo \"$GVCF\" >> $VCF_PATH/deepvariant_gvcf_list.txt\ndone\necho \"=== Gathering gVCFs\" >> $META_REPORT\nrm -r GLnexus.DB\n$GLNEXUS_PATH/glnexus_cli \\\n    --config DeepVariantWES \\\n    --bed $CAPTURE_KIT \\\n    --list $VCF_PATH/deepvariant_gvcf_list.txt \\\n    --threads $CORES | \\",
    "$BCFTOOLS_PATH/bcftools view --include 'QUAL>=20' - | \\\n    $BCFTOOLS_PATH/bcftools norm \\\n    --fasta-ref $BWA_INDEX \\\n    --output-type z \\\n    --output $VCF_PATH/deepvariant_filtered_norm.vcf.gz\n$HTSLIB_PATH/tabix $VCF_PATH/deepvariant_filtered_norm.vcf.gz\necho \"=== Finished!\" >> $META_REPORT\nVariant annotation\nTiming: 9 h 30 min\nIn this section the output of each variant caller is annotated with additional elements such as variant impacts and frequencies of known variants in major population studies. The output of this part is one annotated VCF file for each variant caller.\nUsing SnpEff and SnpSift, annotate the findings with basic information including:\nGenomic location (gene, exon, etc.).\nImpact prediction based on the Sequence Ontology[href=http://www.sequenceontology.org/] and the Sequence Variant Nomenclature[href=https://varnomen.hgvs.org/].\nKnown variant IDs from dbSNP.\nVarious pathogenicity prediction scores and other SNP metrics from dbNSFP.\nPopulation study variant frequencies from gnomAD.\nCritical: It is assumed that the required resources for SnpEff and SnpSift are in place (see also the “before you begin[href=https://www.wicell.org#before-you-begin]” section). Prior to using SnpEff and SnpSift, a SnpEff database for our genome of interest must be downloaded (see script below).\nThe following shell script template can be used for annotation of the final (filtered) outputs from each variant caller:\n#!/bin/bash\nexport VCF_PATH=$HOME_PATH/vcf\nDBSNP_FILE=$RESOURCES_PATH/dbSNP/dbSNP151.vcf.gz\nDBNSFP_FILE=$RESOURCES_PATH/dbNSFP/dbNSFP2.9.3.txt.gz\nGNOMAD_FILE=$RESOURCES_PATH/gnomAD/gnomad.exomes.r2.1.1.sites.vcf.bgz\nif [ ! -d $SNPEFF_PATH/data ]\nthen\n  java -jar $SNPEFF_PATH/snpEff.jar download GRCh37.75\nfi\n## Haplotype Caller\n# Variant effect annotation\njava -Xmx4096m -jar $SNPEFF_PATH/snpEff.jar ann \\\n  -v -noLog -noStats -noLof GRCh37.75 \\\n  $VCF_PATH/haplotypecaller_filtered_norm.vcf.gz >\n$VCF_PATH/haplotypecaller_filtered_norm_eff.vcf\n$HTSLIB_PATH/bgzip $VCF_PATH/haplotypecaller_filtered_norm_eff.vcf\n$HTSLIB_PATH/tabix $VCF_PATH/haplotypecaller_filtered_norm_eff.vcf.gz\n# Annotation with dbSNP\njava -Xmx4096m -jar $SNPEFF_PATH/SnpSift.jar annotate \\\n  -v -id $DBSNP_FILE \\\n  $VCF_PATH/haplotypecaller_filtered_norm_eff.vcf.gz >\n$VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp.vcf\n$HTSLIB_PATH/bgzip\n  $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp.vcf.gz\n$HTSLIB_PATH/tabix $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp.vcf.gz\n# Annotation with dbNSFP\njava -Xmx4096m -jar $SNPEFF_PATH/SnpSift.jar dbnsfp \\\n  -v -m -db $DBNSFP_FILE \\\n  $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp.vcf.gz >\n$VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf\n  $HTSLIB_PATH/bgzip $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf\n$HTSLIB_PATH/tabix\n$VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz\n# Annotation with gnomAD\njava -Xmx4096m -jar $SNPEFF_PATH/SnpSift.jar annotate \\\n  -v $GNOMAD_FILE \\\n  $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz >\n$VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp_gnomad.vcf\n  $HTSLIB_PATH/bgzip\n$VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp_gnomad.vcf\n$HTSLIB_PATH/tabix $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp_gnomad.vcf.gz\n# FreeBayes\n# Variant effect annotation",
    "java -Xmx4096m -jar $SNPEFF_PATH/snpEff.jar ann \\\n  -v -noLog -noStats -noLof GRCh37.75 \\\n  $VCF_PATH/freebayes_filtered_norm.vcf.gz > $VCF_PATH/freebayes_filtered_norm_eff.vcf\n  $HTSLIB_PATH/bgzip $VCF_PATH/freebayes_filtered_norm_eff.vcf\n  $HTSLIB_PATH/tabix \\\n  $VCF_PATH/freebayes_filtered_norm_eff.vcf.gz\n# Annotation with dbSNP\njava -Xmx4096m -jar $SNPEFF_PATH/SnpSift.jar annotate \\\n  -v -id $DBSNP_FILE \\\n  $VCF_PATH/freebayes_filtered_norm_eff.vcf.gz > $VCF_PATH/freebayes_filtered_norm_eff_dbsnp.vcf\n  $HTSLIB_PATH/bgzip\\\n  $VCF_PATH/freebayes_filtered_norm_eff_dbsnp.vcf.gz\n  $HTSLIB_PATH/tabix \\\n  $VCF_PATH/freebayes_filtered_norm_eff_dbsnp.vcf.gz\n# Annotation with dbNSFP\njava -Xmx4096m -jar $SNPEFF_PATH/SnpSift.jar dbnsfp \\\n  -v -m -db $DBNSFP_FILE \\\n  $VCF_PATH/freebayes_filtered_norm_eff_dbsnp.vcf.gz >\n$VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf\n  $HTSLIB_PATH/bgzip\\\n  $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf \\\n  $HTSLIB_PATH/tabix \\\n  $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz\n# Annotation with gnomAD\njava -Xmx4096m -jar $SNPEFF_PATH/SnpSift.jar annotate \\\n  -v $GNOMAD_FILE \\\n  $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz >\n$VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp_gnomad.vcf\n  $HTSLIB_PATH/bgzip\\\n  $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp_gnomad.vcf\n  $HTSLIB_PATH/tabix \\\n  $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp_gnomad.vcf.gz\n## Deep Variant\n# Variant effect annotation\njava -Xmx4096m -jar $SNPEFF_PATH/snpEff.jar ann \\\n  -v -noLog -noStats -noLof GRCh37.75 \\\n  $VCF_PATH/deepvariant_filtered_norm.vcf.gz >\n$VCF_PATH/deepvariant_filtered_norm_eff.vcf.gz\n  $HTSLIB_PATH/bgzip \\\n  $VCF_PATH/deepvariant_filtered_norm_eff.vcf\n  $HTSLIB_PATH/tabix \\\n  $VCF_PATH/deepvariant_filtered_norm_eff.vcf.gz\n# Annotation with dbSNP\njava -Xmx4096m -jar $SNPEFF_PATH/SnpSift.jar annotate \\\n  -v -id $DBSNP_FILE \\\n  $VCF_PATH/deepvariant_filtered_norm_eff.vcf.gz >\n$VCF_PATH/deepvariant_filtered_norm_eff_dbsnp.vcf\n  $HTSLIB_PATH/bgzip\\\n  $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp.vcf\n  $HTSLIB_PATH/tabix \\\n  $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp.vcf.gz\n# Annotation with dbNSFP\njava -Xmx4096m -jar $SNPEFF_PATH/SnpSift.jar dbnsfp \\\n  -v -m -db $DBNSFP_FILE \\\n  $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp.vcf.gz >\n$VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf\n  $HTSLIB_PATH/bgzip\\\n  $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz\n  $HTSLIB_PATH/tabix \\\n  $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz\n# Annotation with gnomAD\njava -Xmx4096m -jar $SNPEFF_PATH/SnpSift.jar annotate \\\n  -v $GNOMAD_FILE \\\n  $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz >\n$VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp_gnomad.vcf.gz\n  $HTSLIB_PATH/bgzip\\\n  $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp_gnomad.vcf\n  $HTSLIB_PATH/tabix \\\n  $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp_gnomad.vcf.gz\n# Remove intermediate files\nrm $VCF_PATH/haplotypecaller_filtered_norm_eff.vcf.gz \\\n  $VCF_PATH/haplotypecaller_filtered_norm_eff.vcf.gz.tbi \\\n  $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp.vcf.gz \\\n  $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp.vcf.gz.tbi \\\n  $VCF_PATH/freebayes_filtered_norm_eff.vcf.gz \\\n  $VCF_PATH/freebayes_filtered_norm_eff.vcf.gz.tbi \\\n  $VCF_PATH/freebayes_filtered_norm_eff_dbsnp.vcf.gz \\\n  $VCF_PATH/freebayes_filtered_norm_eff_dbsnp.vcf.gz.tbi \\\n  $VCF_PATH/deepvariant_filtered_norm_eff.vcf.gz \\\n  $VCF_PATH/deepvariant_filtered_norm_eff.vcf.gz.tbi \\\n  $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp.vcf.gz \\\n  $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp.vcf.gz.tbi\nVariant callset consolidation\nTiming: 20 min\nIn this section, we consolidate the variant calls from the three different callers. The output consists of several VCF files with unique and combined annotated variants for each caller as well as common variants between all callers and between pairs of callers. The output also contains the genotypes returned by each caller.\nConsolidate variant calls using bcftools.\n#!/bin/bash\nexport VCF_PATH=$HOME_PATH/vcf\n# 1\n$BCFTOOLS_PATH/bcftools isec \\\n    --prefix 1 \\\n    --output-type z \\\n    --nfiles ∼100 \\\n    --collapse none \\\n    $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz",
    "AREA1=`cat ./1/sites.txt | wc -l`\necho $AREA1\n# 2\n$BCFTOOLS_PATH/bcftools isec \\\n    --prefix 2 \\\n    --output-type z \\\n    --nfiles ∼010 \\\n    --collapse none \\\n    $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz\nAREA2=`cat ./2/sites.txt | wc -l`\necho $AREA2\n# 3\n$BCFTOOLS_PATH/bcftools isec \\\n    --prefix 3 \\\n    --output-type z \\\n    --nfiles ∼001 \\\n    --collapse none \\\n    $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz\nAREA3=`cat ./3/sites.txt | wc -l`\necho $AREA3\n# 4\n$BCFTOOLS_PATH/bcftools isec \\\n    --prefix 4 \\\n    --output-type z \\\n    --nfiles ∼110 \\\n    --collapse none \\\n    $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz\nAREA4=`cat ./4/sites.txt | wc -l`\necho $AREA4\n# 5\n$BCFTOOLS_PATH/bcftools isec \\\n    --prefix 5 \\\n    --output-type z \\\n    --nfiles ∼011 \\\n    --collapse none \\\n    $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz\nAREA5=`cat ./5/sites.txt | wc -l`\necho $AREA5\n# 6\n$BCFTOOLS_PATH/bcftools isec \\\n    --prefix 6 \\\n    --output-type z \\\n    --nfiles ∼101 \\\n    --collapse none \\\n    $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz\nAREA6=`cat ./6/sites.txt | wc -l`\necho $AREA6\n# 7\n$BCFTOOLS_PATH/bcftools isec \\\n    --prefix 7 \\\n    --output-type z \\\n    --nfiles ∼111 \\\n    --collapse none \\\n    $VCF_PATH/haplotypecaller_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/freebayes_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz \\\n    $VCF_PATH/deepvariant_filtered_norm_eff_dbsnp_dbnsfp.vcf.gz\nAREA7=`cat ./7/sites.txt | wc -l`\necho $AREA7\nNote: A challenging issue when using variant callers is how to summarize and consolidate different DNA variant callsets from different callers into one summarized result. Major challenges for consolidation include the decision on which of the reported variant call metrics reported in VCF file(s) from each caller will be included in the final callset (e.g., which QUAL, which DP etc.) and the level at which the variants from different callers should be considered identical or nearly identical. Regarding the latter, two common questions are should they be considered identical if they share the same genomic coordinates or start position, or, should they be considered identical if they share both positions and alleles?",
    "Fortunately, bcftools offer functions to experiment with the many options that exist to consolidate the callsets. We have chosen to intersect the callsets and consider the overlapping variants identical if they share both genomic position and alleles. We perform the various intersections using bcftools and for each intersection we perform three operations in order to retain all the metrics for each caller but on the intersected (shared) variants. From the produced callsets, the most interesting one to begin the exploration should be the #7 which corresponds to the common variants between all the three callers we have used.\nVisualization and further post-processing\nTiming: 1 h\nAs with most high-throughput techniques, the final processed data cannot be fully denoised, and some false positives and artifacts are always to be expected. One popular way of further assessing the quality of the produced data is visualization. In this section we describe how the variant callsets can be visualized in two ways. Firstly, by simultaneous loading and visualization of the results (VCF files) and raw data (BAM files) in a genome browser such as IGV and secondly, with a Venn diagram to qualitatively visualize overlaps between callsets. In addition, we briefly discuss the need for additional filtering steps according to the application of the WES experiment, for example clinical settings or population studies.\nThis protocol step comprises two substeps:\nGeneration of a Venn diagram to depict common and unique variants across the three callsets.",
    "A 3-way Venn diagram contains seven areas (Figure 1[href=https://www.wicell.org#fig1]A). Each area is numbered according to the number in the comment section directly above each $BCFTOOLS_PATH/bcftools isecin the commands presented in step 7. For example, the number of variants in area 1, is given by the $BCFTOOLS_PATH/bcftools isec command below the line containing #1. By using the outcome of echoing variable X in the same command-line set, the user can fill the numbers required for the completion of the Venn diagram (Figure 1[href=https://www.wicell.org#fig1]B).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1686-Fig1.jpg\nFigure 1. Venn diagram visualization of the three callsets\n(A) Numbering of the Venn overlapping and unique areas which correspond to specific callsets created with bcftools.\n(B) The same Venn diagram where the areas depicted in (A) have been filled with the actual number of variants resulting from the application of the protocol to the data presented in the article, accompanied by the shell variable names presented in the code in part “variant callset consolidation[href=https://www.wicell.org#sec3.10]” and step 29.\nVisualization of the callsets in the IGV genome browser. At the end of parts “preparation of BAM files[href=https://www.wicell.org#sec3.3]”, “signal visualization[href=https://www.wicell.org#sec3.5]”, “variant calling with GATK HaplotypeCaller[href=https://www.wicell.org#sec3.6]”, “variant calling with FreeBayes[href=https://www.wicell.org#sec3.7]”, “variant calling with DeepVariant[href=https://www.wicell.org#sec3.8]” the following files were produced respectively:\nAt the end of part “preparation of BAM files[href=https://www.wicell.org#sec3.3]”, read alignment files in BAM format.\nAt the end of step “signal visualization[href=https://www.wicell.org#sec3.5]”, WES signal visualization files in BigWig format.\nAt the end of step “variant calling with GATK HaplotypeCaller[href=https://www.wicell.org#sec3.6]”, a filtered VCF file with GATK Haplotype Caller results.\nAt the end of step “variant calling with FreeBayes[href=https://www.wicell.org#sec3.7]”, a filtered VCF file with FreeBayes results.\nAt the end of step “variant calling with DeepVariant[href=https://www.wicell.org#sec3.8]”, a filtered VCF file with DeepVariant results.\nThese files can be loaded in the IGV genome browser for visualization of the results with the following steps.",
    "Download and install the IGV[href=https://software.broadinstitute.org/software/igv/download] genome browser.\nOpen the IGV genome browser.\nFrom the Genomes menu select Load Genome From Server.\nSelect Human (1 kg, b37 + decoy) or Human (b37) and click OK.\nFrom the File menu select Load from File.\nSelect one or more from the BAM files created at the end of the part “preparation of BAM files[href=https://www.wicell.org#sec3.3]”.\nFrom the File menu select Load from File.\nSelect one or more from the BigWig files created at the end of the part “signal visualization[href=https://www.wicell.org#sec3.5]”.\nFrom the File menu select Load from File.\nSelect the three variant callsets generated at the end of the parts “variant calling with GATK HaplotypeCaller[href=https://www.wicell.org#sec3.6]”, “variant calling with FreeBayes[href=https://www.wicell.org#sec3.7]” and “variant calling with DeepVariant[href=https://www.wicell.org#sec3.8]”.\nFollowing the steps (32)–(41), the user should be able to visualize variant callsets and supporting information such as overall signal and reads supporting each variant call. The user can navigate through the callsets using the respective IGV controls (zoom in and out, navigate to specific areas by chromosomal coordinates etc.)."
  ],
  "subjectAreas": [
    "Genomics",
    "Bioinformatics",
    "Genetics"
  ],
  "bigAreas": [
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}