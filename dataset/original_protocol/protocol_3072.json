{
  "id": 3250,
  "origin_website": "Cell",
  "title": "A protocol for applying low-coverage whole-genome sequencing data in structural variation studies",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nPart 1: SV detection from the short-read sequences with low coverage\nTiming: 1 day (12 threads, variable depending on the number of threads used)\nThe discovery of reliable structure variation (SV), including deletion (DEL), duplication (DUP), insertion (INS), and inversion (INV), in each sample is the first and most important step in SV studies. There are four basic short-read sequencing-based methods: read pairs (RP), read depth (RD), split read (SR), assembly (AS), as well as a combination of the aforementioned ones (CB). Kosugi et al. (2019) demonstrated that the combination of specific algorithms (SV detection tools using one or more aforementioned basic methods) effectively improves SV detection accuracy.21[href=https://www.wicell.org#bib21] Then, we first employ three algorithms, including Manta (RP-SR-AS), Lumpy (RP-SR-RD), and SVseq2 (SR), to discover reliable SVs from the short-read sequences with low coverage (4–10×). The testing is only depicted in the following code shows on one individual (AHRm.725), but the same procedure can be applied to all samples as well.\nPrepare the config file to serve as the input file of our protocol.\n# The config file contains five parts\n# Please don’t change the variable name in the config file. The variable name will be used in our pipelines.\n# The following path of software are paths in our Docker image.\n# Owing to failed to install pacakge 'pysam', we do not install HTSeq and lumpy-sv in our docker image\n#======== Please Copy the following information into a config file ========\n# Part 1: The path of executive program to be used\n#∗∗∗∗ common executive program\nbgzip=/root/software/samtools-1.17/htslib-1.17/bgzip\ntabix=/root/software/samtools-1.17/htslib-1.17/tabix\nbcftools=/usr/local/bin/bcftools\nvcftools=/usr/local/bin/vcftools\nsamtools=/usr/local/bin/samtools\nperl=/usr/bin/perl\njava=/usr/bin/java\npicard=/root/software/picard_2.26.11/picard.jar\npython3_9=/usr/bin/python3\n#∗∗∗∗ add path of bgzip into PATH\nexport PATH=${PATH}:/root/software/samtools-1.17/htslib-1.17\n# Python2 be used to run Manta and Lumpy\npython2=/usr/bin/python2",
    "# Part 2: The path of executive program to be used in SV calling\n#∗∗∗∗ 110.raw.reads.mapping\nbwa=/usr/bin/bwa\ngatk=/root/software/gatk-4.4.0.0/gatk\n#∗∗∗∗ 130.sv.detection\nmanta_install_path=/root/software/manta-1.6.0.centos6_x86_64\n# User can install lumpy-sv using conda with command: /path miniconda3/bin/conda install lumpy-sv=0.2.13\nlumpy_install_path=/your_software_path/lumpy-sv-0.2.13\n#! The format of inversion detected by Manta should be converted to another format using the following script\n#https://github.com/Illumina/manta/tree/master/src/python/libexec/convertInversion.py[href=https://github.com/Illumina/manta/tree/master/src/python/libexec/convertInversion.py]\ncode_convertInversion=/root/software/manta-1.6.0.centos6_x86_64/libexec/convertInversion.py\nsvseq2=/root/software/SVseq2/SVseq2_2\nsvimmer=/root/software/svimmer-master/svimmer\n#∗∗∗∗ 140.sv.genotyping\ngraphtyper2=/root/software/graphtyper_2.7.4/graphtyper\n#∗∗∗∗ 150.sv.filtering\nvcffilter=/usr/bin/vcffilter\n#∗∗∗∗ downstream analysis\nbedtools=/root/software/bedtools_v2.30.0/bedtools\nplink19=/root/software/plink_1.9/plink\nplink2=/root/software/plink_2/plink2\n# Part 3: The path of executive program to be used in gene expression\n#∗∗∗∗ 110.hisat2.index_genome\nhisat2_build=/root/software/hisat2-master/hisat2-build\n#∗∗∗∗ 120.hisat2.genome_out\nhisat2=/root/software/hisat2-master/hisat2\n#∗∗∗∗ 130.htseq.genome_out\n# User can install htseq using conda with command: /path/miniconda3/bin/conda install htseq=2.0.2\nhtseq_count=/your_software_path/python-3.7.13/bin/htseq-count\n# R version 3.6.3\nRscript=/usr/bin/Rscript\n# We have install the following packages in Docker: dplyr, edgeR\n# Make sure you have installed these packages in yourself system before testing this protocol.\n# Part 4: The path of input data to be used in SV analysis\n#∗∗∗∗/ The directory of scripts in our protocol\nthis_protocol_script_path=/your_software_path/SVprotocol\n#∗∗∗∗/ The directory of input and output data to be used\nyour_analysis_dir=/your_data_path/SVCalling\n#∗∗ In your_analysis_dir (/your_data_path/SVCalling), you must have a subdirectory named \"fastq/<sample_name>\" including two FASTQ files (<sample_name>_R1.fastq.gz and <sample_name>_R2.fastq.gz)\n# For example, for sample AHRm.725, we required two files in the following directory\n# $your_analysis_dir/fastq/AHRm.725/AHRm.725_R1.fastq.gz\n# $your_analysis_dir/fastq/AHRm.725/AHRm.725_R2.fastq.gz\n#∗∗ When you source this config, this config will create some subdirectory in your_analysis_dir, which will be the directory of some output data.\n#∗∗∗∗/\nmkdir -p $your_analysis_dir/110.raw.reads.mapping 2>/dev/null\nmkdir -p $your_analysis_dir/120.remove.duplicates 2>/dev/null\nmkdir -p $your_analysis_dir/130.sv.detection 2>/dev/null\nmkdir -p $your_analysis_dir/140.sv.genotyping 2>/dev/null\nmkdir -p $your_analysis_dir/150.sv.filtering 2>/dev/null\n#∗∗∗∗/ The full path of reference genome sequence data (FASTA)\n#∗∗ Please just include chromosome-level sequence\n#∗∗ Reference genome data must end with \"fasta\"\n#∗∗ Please put all reference data into one directory\n#∗∗∗∗/\nref_genome_fasta=/your_bundle_path/reference/Rhipicephalus_microplus.chromosome.fasta\n# Part 5: The path of input data to be used in gene expression analysis",
    "#∗∗∗∗/ The directory of input and output data to be used\n#∗∗ In your_gene_expression_analysis_dir, you must have a subdirectory named \"fastq/<sample_name>\" including two FASTQ files (<sample_name>_1.fastq.gz and <sample_name>_2.fastq.gz) for pair-end sequencing data\n#∗∗∗∗/\nyour_gene_expression_analysis_dir=/ your_gene_expression_path/gene_express\n# Files used in script “SVprotocol_part5.sh”\nref_genome_gtf=/your_bundle_path/reference/Rhipicephalus_microplus.NCBI.TIGMIC.sorted.updPos.gtf\nref_genome_gff=/your_bundle_path/reference/Rhipicephalus_microplus.NCBI.TIGMIC.sorted.updPos.gff.gz\n# A variable used in script “SVprotocol_part5.sh” as the prefix of index files of the reference genome, which index file is used in hisat2\nref_genome_prefix_name=Rhipicephalus_microplus\nPrepare pair-end sequencing data as the input file of our protocol “SVprotocol_part1.sh”.\n#∗∗∗∗/ The directory of input and output data to be used\nyour_analysis_dir=/your_data_path/SVCalling\n# In the config file, we have added the above information. So, in your_analysis_dir, the user must make a subdirectory named \"fastq/<sample_name>\" including two FASTQ files (<sample_name>_R1.fastq.gz and <sample_name>_R2.fastq.gz). For example, for sample AHRm.725, we required two files in the following directory\n$your_analysis_dir/fastq/AHRm.725/AHRm.725_R1.fastq.gz\n$your_analysis_dir/fastq/AHRm.725/AHRm.725_R2.fastq.gz\nIndex the reference genome file.\n#∗∗∗∗/ The full path of reference genome sequence data (FASTA)\n#∗∗ Please just include chromosome-level sequence\n#∗∗ Reference genome data must end with “fasta”\n#∗∗∗∗/\nref_genome_fasta=/your_bundle_path/reference/Rhipicephalus_microplus.chromosome.fasta\n# In the config file, we have added the above information, So, the user can index the reference genome fasta as follows:\n>cd /your_bundle/path/reference\n>bwa index Rhipicephalus_microplus.chromosome.fasta\nMap the pair-end short-read sequence to the reference genome and remove duplicated reads.\nNote: Make sure that two FASTQ files for the same sample (∗_R1.fastq.gz and ∗_R2.fastq.gz) are in the same directory named “fastq/∗sample_name”. It is recommended that the genome sequence file (FASTA) only contain the sequence at the chromosome level, i.e., chromosomes 1–11.\nDetect SVs in each sample using three algorithms (Manta, Lumpy, and SVseq2).\nDetect SVs using Manta (troubleshooting 1[href=https://www.wicell.org#troubleshooting]).\nDetect SVs using Lumpy (troubleshooting 1[href=https://www.wicell.org#troubleshooting]).\nDetect SVs using SVseq2.\nDivide an individual’s bam file by chromosome and separately detect deletions in each chromosome using SVseq2 (troubleshooting 2[href=https://www.wicell.org#troubleshooting] and 3[href=https://www.wicell.org#troubleshooting])",
    "Critical: SVseq2 can only use an uncompressed reference genome fasta file with the suffix “fasta” as input (troubleshooting 4[href=https://www.wicell.org#troubleshooting]).\nMerge the results throughout all chromosomes and save the outputs as a VCF file.\nUse the svimmer software to merge the SVs discovered by Manta, Lumpy, and SVseq2 in each sample and retain the SVs with a size range between [50 bp, 2 Mb].\nNote: Manta can discover DEL, INS, INV, and DUP. Lumpy is capable of finding DEL, INV, and DUP. SVseq2 can discover DEL. Only variants that were found by at least two tools were retained, except for insertion, which was found by Manta. The users can modify the filtering criteria in our pipeline (code_SV_filtering.pl) with the parameters “--len” and “--xlen”.\n# We have generated a pipeline including all necessary processes (steps 4–6) to detect SVs for one sample\n# Please read the usage before you run this pipeline\n# Use the above config file following the argument “--config”\n>sh /this_protocol_script_path/SVprotocol_part1.sh --sample AHRm.725 –config /this_protocol_script_path/example.config --thread 12\nPart 2: SV genotyping and filtering\nTiming: 6 h (12 threads, variable depending on the number of threads and samples used)\nPopulation-scale genotyping is conducted by Graphtyper2. Next, we filter out SVs that match the criteria suggested by Graphtyper2. Additionally, we eliminate SVs that overlap with low-complexity regions, simple repeats, DNA satellites, or regions of segmental duplications, as well as those with large genotyping missing rates.\nFor all samples, we first merge SVs detected through the above steps using the svimmer software while keeping SVs with a size range between [50 bp, 2 Mb].\nNote: The users can modify the filtering criteria in our pipeline (code_SV_filtering.pl) with the parameters “--len” and “--xlen”.",
    "To improve the accuracy of SV genotyping, we first use SAMtools to generate a file containing the average coverage divided by the read length for each bam/sample (one value per line), and then subsample reads in such regions using argument “avg_cov_by_readlen” in Graphtyper2.\nNote: When reads are mapped to the reference genome, some genomic regions have abnormally high read coverage (e.g., 10 times the average coverage), which will have an impact on the accuracy of genotyping.\nGenotype SVs for each chromosome using Graphtyper2.\nConcatenate SV genotyping results throughout all chromosomes and save outputs as a VCF file.\nFilter SVs.\nFilter out SVs followed by the recommendation suggested by Graphtyper2 with the command “( SVTYPE = DEL & QD > 12 & ( ABHet > 0.30 | ABHet < 0 ) & ( AC / NUM_MERGED_SVS ) < 25 ) | ( SVTYPE = DUP & QD > 5 & ( AC / NUM_MERGED_SVS ) < 25 ) | ( SVTYPE = INS & ( AC / NUM_MERGED_SVS ) < 25 & ( ABHet > 0.25 | ABHet < 0 ) & MaxAAS > 4 ) | ( SVTYPE = INV & ( AC / NUM_MERGED_SVS ) < 25 & ( ABHet > 0.25 | ABHet < 0 ) & MaxAAS > 4 )”.\nNote: Graphtyper2 classifies SV genotype into four categories, i.e., “PASS” with genotype quality (GQ) ≥ 30, “FAIL1” with GQ ≥ 20, “FAIL2” with GQ ≥ 10, and “FAIL3” with GQ < 10. “PASS_AC” means the total number of alternate alleles in the called genotype, and “PASS_ratio” means the ratio of genotype with the “PASS” flag. Considering the sequencing data with low coverage, we remove the filtering argument “PASS_AC” and “PASS_ratio” in our protocol.",
    "Set SV genotype as missing genotype if the read depth (DP) < 1 or genotype quality (GQ) < 13. GQ is larger or equal to 13 means that the error rate of genotype calling is less than 5% (95% confidence). Considering the low coverage of the sequencing data, we relax the DP and GQ criteria in our protocol.\nNote: The users can modify the filtering criteria in our pipeline (code_convert_GT_FT.pl) with the parameters “--dp” and “--gq”. For example, GQ ≥ 10 (90% confidence), GQ ≥ 15 (97% confidence), GQ ≥ 20 (99% confidence), and GQ ≥ 30 (99.9% confidence).\nFilter out SVs with genotyping missing rates larger than 50%.",
    "Note: Considering the low coverage of the sequencing data, we relax the genotyping missing rate criteria in our protocol. Users can use other criteria in the initial filtration based on the data quality, e.g., 40%.22[href=https://www.wicell.org#bib22] If the NGS data have read coverage larger than 10, users can skip steps “b” and “c” and modify the command in our pipeline to the following command: “( SVTYPE = DEL & QD > 12 & ( ABHet > 0.30 | ABHet < 0 ) & ( AC / NUM_MERGED_SVS ) < 25 & PASS_AC > 0 & PASS_ratio > 0.1 ) | ( SVTYPE = DUP & QD > 5 & PASS_AC > 0 & ( AC / NUM_MERGED_SVS ) < 25 ) | ( SVTYPE = INS & PASS_AC > 0 & ( AC / NUM_MERGED_SVS ) < 25 & PASS_ratio > 0.1 & ( ABHet > 0.25 | ABHet < 0 ) & MaxAAS > 4 ) | ( SVTYPE = INV & PASS_AC > 0 & ( AC / NUM_MERGED_SVS ) < 25 & PASS_ratio > 0.1 & ( ABHet > 0.25 | ABHet < 0 ) & MaxAAS > 4 )”.\nFilter out SVs that overlap with low-complexity regions, simple repeats, DNA satellites, or regions of segmental duplications. Firstly, we calculate the cumulative size of overlapped regions between a SV with 1 or N repeat regions (1-bp overlap threshold). Then the ratio of the cumulative size of overlapped regions is calculated. Finally, SVs with a ratio larger or equal to 0.5 are filtered out.22[href=https://www.wicell.org#bib22]\nCreate a de novo repeat database using RepeatModeler software. The following code only shows one chromosome, but the same process can be used for other chromosomes as well.",
    "Note: Make sure that the sequence file for each chromosome has been prepared individually before analysis (troubleshooting 5[href=https://www.wicell.org#troubleshooting]).\n# For one chromosome\n# BuildDatabase -name <species>_<chr> <species>.<chr>.unmasked.fasta\n# -database <species>_<chr>\n# You can change the threads through the argument “-pa”\n# Chromosome: GWHAMMN00000001 == 1\n>BuildDatabase -name Rhipicephalus_microplus_1 Rhipicephalus_microplus.1.unmasked.fasta\n>RepeatModeler -pa 10 -database Rhipicephalus_microplus_1 -LTRStruct\n# After completing analysis for all chromosomes\n>cat Rhipicephalus_microplus_∗-families.fa > Rhipicephalus_microplus.genome.denovo.repeat.lib\nIdentify repeat sequences using the RepeatMasker. Only one chromosome is displayed in the code, but the procedure is the same step for all chromosomes.\n# For one chromosome\n# <species>.<chr>.unmasked.fasta\n# <species>.genome.denovo.repeat.lib\n>RepeatMasker -xsmall -lib Rhipicephalus_microplus.genome.denovo.repeat.lib -pa 10 Rhipicephalus_microplus.1.unmasked.fasta\n# After completing analysis for all chromosomes\n# Merge all masked fasta to one fasta\n>cat ∗fasta.masked > Rhipicephalus_microplus.gwd.repeatmasker.masked.fasta\n# Extract the position and sequence type for each repeat region\n>for j in `ls ∗unmasked.fasta.out`\ndo\nsed '1,3d' $j | awk 'BEGIN{OFS=\"\\t\"}{print $5,$6,$7,$10,$11}' >> Rhipicephalus_microplus.gwd.repeatmasker.bed\ndone\n# Extract simple repeat or low complexity region for the downstream analysis\n>awk '{if($5==\"Simple_repeat\" || $5==\"Low_complexity\")print $0}' Rhipicephalus_microplus.gwd.repeatmasker.bed > Rhipicephalus_microplus.gwd.repeatmasker.simpleRepeat_lowComplex.bed\nIdentify segment duplication regions using sedef software.\n# You can change the threads through the argument “-j”\n>sedef.sh -o Rhipicephalus_microplus -j 20 Rhipicephalus_microplus.chromosome.fasta\nCombine the bed files generated in the above steps, then use this file as an input file for our pipeline with the argument “--repeat”.\n# Prepare a file named “sample.list” in the following format:\nAHRm.725\nHnRm.648\nYNRm.729\n# We have generated a pipeline including all necessary processes (steps 7–11) to genotype and filter SVs for all samples\n# Please read the usage before you run this pipeline\n# Use the above config file following the argument “--config”\n>sh /this_protocol_script_path/SVprotocol_part2.sh --samplelist sample.list --config /this_protocol_script_path/example.config --thread 12 --repeat Rhipicephalus_microplus.SR.LC.SD.mask.0based.bed\nPart 3: SV annotation\nTiming: 5 min",
    "In this part, we will annotate SVs to gene regions using GFF3 or GTF files.\nConvert GFF3 or GTF to a 0-based bed-format file (Figure 1[href=https://www.wicell.org#fig1], columns 11–15) including five columns: <chromosome><start><end><geneName_functionalRegion><functionalRegion>. The functional region includes the coding region (CDS), promoter region, intronic region, intergenic region, and so on.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2816-Fig1.jpg\nFigure 1. Screenshot of the output files in SV annotation analysis using BEDTools software\nThe information above the dashed line represents the meaning of each column.\nNote: The user can extend the definition of functional regions, e.g., upstream and downstream regions.\nScreen functional regions existed in the above bed-format file for overlap with SVs using the BEDTools software. A file generated by our protocol with the suffix “perSite.SVcount” can be used as the input file of BEDTools with the argument “-a” (Figure 1[href=https://www.wicell.org#fig1]).\n# Convert GFF to 0-based bed file\n>perl /this_protocol_script_path/code_gff2SVAnnBed.pl Rhipicephalus_microplus.NCBI.TIGMIC.sorted.updPos.gff.gz out_prefix\n# In the subdirectory “140.sv.genotyping”, you can find a 1-based bed file named “population.graphtyper2.aggregated.PASS.GQ13.DP1.GENO0.5.rmRepeat.perSite.SVcount”\n# Tab-separated Format: <chr><start><end><ID_in_VCF><SV_type><SV_size><num_of_individual_haveSV><SV_count_in_population><num_of_individual_haveGenotypeCall><num_of_individual_in_population>\n# Before running BEDTools, convert 1-based bed file to 0-based bed file\n>input_sv_bed= population.graphtyper2.aggregated.PASS.GQ13.DP1.GENO0.5.rmRepeat.perSite.SVcount\n>awk 'BEGIN{OFS=\"\\t\"}{$2=$2-1;print $0}' $input_sv_bed > ${input_sv_bed}.bed\n>bedtools intersect -a ${input_sv_bed}.bed -b out_prefix.gff.cds.bed -wa -wb > sv.annotation.bed\nPart 4: Population structure and natural selection analysis\nTiming: 10 min\nTo understand the fine-scale genetic structure of a specie, population structure analysis based on SVs is conducted. Natural selection analysis facilitates comprehension of adaptation to the environment in a population or specie. Here, we only use FST statistics to evaluate the population differentiation between populations.\nConduct the principal component analysis (PCA).\n# Create a file including reference allele\n# VCF_file = population.graphtyper2.aggregated.PASS.GQ13.DP1.GENO0.5.rmRepeat.vcf.gz\n>zcat VCF_file | perl -alne 'if(/ˆ#/){next;}print \"$F[2]\\t$F[3]\"' > VCF_ref\n# In the output VCF file of graphtyper2, ALT allele of some SVs contain character “+”, remove this character",
    ">zcat VCF_file | perl -alne 'if(/ˆ#/){print \"$_\";next;}$F[4]=∼s/\\+//;$out=join(\"\\t\",@F);print \"$out\"' | bgzip -c > VCF_file_changeALT.vcf.gz && tabix -p vcf -f VCF_file_changeALT.vcf.gz\n# Convert VCF format to PLINK format\n>plink1.9 --vcf VCF_file_changeALT --a2-allele VCF_ref --allow-extra-chr --make-bed --out plink_prefix\n# Conduct PC analysis using plink software\n>plink1.9 --bfile plink_prefix --pca 150 --allow-extra-chr --keep-allele-order --out plink_prefix\nEvaluate population differentiation between two or more populations using FST statistics (Figure 2[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2816-Fig2.jpg\nFigure 2. Screenshot of the output files from the population structure and FST analysis using PLINK software\n(A) The eigenvalue file.\n(B) The eigenvector file. The meaning of each column is given below the dashed line. FID: family identifier; IID, individual identifier; PC: principal component.\n(C) The result of FST analysis between three populations in R. microplus. Identifier “SNP” is the default output of PLINK software.\n# Conduct Fst analysis using plink software\n# The format of cluster_file: <FID><IID><population>, tab-separated\n>plink2 --bfile plink_prefix --fst CATPHENO method='wc' report-variants --allow-extra-chr --keep-allele-order --within cluster_file --out plink_prefix\nPart 5: Differential gene expression analysis\nTiming: 1 day (4 threads, variable depending on the number of threads and samples used)\nHigh-differentiated SVs between populations that locates in the CDS region of a gene may have an important functional influence on the development of a species. Differential gene expression (DGE) analysis will help us to better understand the specific function of genes contributing to local adaptation at the transcript level and tissue level.\nPrepare pair-end RNA sequencing data as the input file of our protocol “SVprotocol_part5.sh”.\n# Part 5: The path of input data to be used in gene expression analysis\n#∗∗∗∗/ The directory of input and output data to be used\n#∗∗ In your_gene_expression_analysis_dir, you must have a subdirectory named as \"fastq/<sample_name>\" including two FASTQ file (<sample_name>_1.fastq.gz and <sample_name>_2.fastq.gz) for pair-end sequencing data\n#∗∗∗∗/\nyour_gene_expression_analysis_dir=/your_gene_expression_path/gene_express",
    "# In the config file, we have added the above information. So, in your_gene_expression_analysis_dir, the user must make a subdirectory named as \"fastq/<sample_name>\" including two FASTQ file (<sample_name>_1.fastq.gz and <sample_name>_2.fastq.gz). For example, for sample SRR1187017, we required two files in the following directory\n$your_gene_expression_analysis_dir/fastq/SRR1187017/SRR1187017_1.fastq.gz\n$your_gene_expression_analysis_dir/fastq/SRR1187017/SRR1187017_2.fastq.gz\nQuality control (QC) for reads must be done before conducting RNA read mapping. FastQC and TrimGalore are widely used and effective software to check read quality, filter out the low-quality reads and remove adaptors. Here, we just give an example of how to run the above two software.\n# How to run FastQC\n>fastqc -q -t 4 -o <OUTDIR> <sample.fastq.gz>\n# How to run TrimGalore\n>trim_galore --output_dir <OUTDIR> --paired --length <Read_length_cutoff> --quality <Read_quality_cutoff> --fastqc --gzip --cores <thread> --path_to_cutadapt <cutadapt_software_path> --basename <outfile_prefix> <SAMPLE_1.fastq.gz> <SAMPLE_2.fastq.gz>\nMap short-read sequence to the reference genome using HISAT2.\nThe R packages (dplyr, edgeR) need to be installed by the user before running the pipeline, which is used to counts the number of reads in each gene region using HTSeq (Figures 4[href=https://www.wicell.org#fig4]A and 4B).\n# We have generated a pipeline including all processes (steps 18–19) to calculate read count for each gene\n# Please conducting quality control and filtering before running this pipeline\n# Please read the usage before you run this pipeline\n# Use the above config file following the argument “--config”\n>sh /this_protocol_script_path/SVprotocol_part5.sh --samplelist SRR_Acc_List.multipleTissue --config /this_protocol_script_path/example.config --thread 3 --project multipleTissue\nEmploy differential gene expression analysis with no replicates using GFOLD (Figure 4[href=https://www.wicell.org#fig4]C).\n# After executing the above pipeline, a GFOLD input file with suffix “gfold.input” will be generated in the subdirectory “140.gfold.<project>” for each sample\n# Choose two samples you want to compare in the GFOLD analysis\n>gfold diff -s1 <sample1>.gfold.input -s2 <sample2>.gfold.input -o /outdir/<sample1>_<sample2>.gfold.diff"
  ],
  "subjectAreas": [
    "Genomics",
    "Sequence Analysis",
    "Bioinformatics",
    "Genetics",
    "Evolutionary Biology",
    "Gene Expression"
  ],
  "bigAreas": [
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}