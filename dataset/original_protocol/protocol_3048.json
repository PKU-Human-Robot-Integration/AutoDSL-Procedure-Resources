{
  "id": 3226,
  "origin_website": "Cell",
  "title": "Protocol for the automatic extraction of epidemiological information via a pre-trained language model",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\n      Here we describe the step-by-step methods for data acquisition and\n      processing, data annotation and consistency check, and model training and\n      evaluation.\n    \nData acquisition and processing\nTiming: ∼1 h (for steps 1 to 5)\n    \n      This step illustrates the progress of raw data acquisition and processing,\n      which is used to satisfy the formatting requirements in data annotation.\n    \n        Downloading the GitHub project from\n        https://github.com/AllenWangle/STARProtocols-CCIE[href=https://github.com/AllenWangle/STARProtocols-CCIE]\n        and unzipping the file to obtain the raw data (i.e.,\n        dataset_CN.xls) from the “CCIE/rawdata” folder.\n      \nNote: A brief data demonstration is shown\n      in Table 1[href=https://www.wicell.org#tbl1].\n    \ntable:files/protocols_protocol_2795_1.csv\nAlternatives: One also can request\n      raw data from the corresponding author.\n    \n        Extracting the contents of “ID” and “Original_Text_CN” in the\n        dataset_CN.xls.\n        Splitting the raw data into train.txt, dev.txt and\n        test.txt with a ratio of 8:1:1.\n      \n        Reorganizing each case report in any file into the three-column format.\n      \nNote: As shown in\n      Table 2[href=https://www.wicell.org#tbl2], each line consists of a word, an initially\n      named entity label \"O\", and a position label \"O\".\n    \ntable:files/protocols_protocol_2795_2.csv\nCritical: The Python code for the step\n      2, 3, and 4 is provided in the\n      “CCIE/rawdata/raw_data_process_for_ner_annotation.py”, and one can\n      process the raw data with using the following command.\n    \n> python raw_data_process_for_ner_annotation.py\n        Placing the processed raw data in the “CCIE/annotation/data” to\n        be imported into the entity annotation tool for annotation.\n      \nData annotation and consistency check\nTiming: ∼7 days (for steps 6 to 7)\n    \n      This step is used for data annotation to prepare supervisory training\n      data.\n    \n        Performing the named entity annotation via the entity annotation tool.\n        \nNote: As\n          Figure 3[href=https://www.wicell.org#fig3] shows, the named entity annotation\n          contains four key steps, i.e., (a) rough annotation (step a to c), (b) fine annotation (step d to g), (c) annotation result\n          export (step h) and (d) format conversion for model training\n          (step i).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2795-Fig3.jpg",
    "Figure 3. The process of named entity annotation via the entity\n              annotation tool\n            \n              (A) The illustration of rough annotation for a COVID-19 case\n              report. Only manually coded fields are marked.\n            \n              (B) The details of fine annotation that more entities are labeled\n              according to the annotation rules.\n            \n              (C) The contents of annotated file exported from the entity\n              annotation tool.\n            \n(D) The training data format obtained by the annotated file.\n            Start the annotation tool (generally it has been in the\n            “CCIE/annotation/” folder).\n          \n            Select the \"Open\" button in \"File\" option to\n            import the prepared annotation file (i.e., train.txt,\n            dev.txt and test.txt) from the\n            “CCIE/annotation/data” folder.\n          \n            Match human-coded epidemiological fields in 10,017 COVID-19 case\n            reports to the processed raw data, as shown in\n            Figure 3[href=https://www.wicell.org#fig3]A.\n          \n            Sample 10% of the case reports for manually fine annotation\n            according to the entity distribution obtained by the rough\n            annotation.\n            \nCritical: The sampling\n              objective is to minimize.\n            \n(Equation 1)\nL\no\ns\ns\n=\n1\nL\n∑\ni\n=\n1\nL\n(\nN\ng\no\nl\nd\ni\n−\nN\ns\na\nm\np\nl\ne\ni\n)\n            where\n            \n L \n            is the total number of sampled fields,\n            \nN\n g o l d \ni\n            and\n            \nN\ns a m p l\ne\ni\n            are the number of the\n            \n i  -th label in the manually coded data and the number of the\n            \n i  -th label in the sampled data, respectively.\n          \n            Formulate the rules for annotating named entity annotation.\n            \nNote: We select 11 named entities\n              (i.e., L = 11) as the identification targets. They are “age\n              (AGE)”, “gender (GED)”, “place of departure (SL)”, “place of\n              transit (TL)”, “arrival date (DT)”, “date of quarantine (IT)”,\n              “date of symptom onset (OnT)”, “date of hospitalization (TT)”,\n              “admitted hospital (TDH)”, and “date of confirmation (CT)”. For\n              each entity, we design the corresponding label symbol (i.e., the",
    "abbreviation in brackets) and the annotation span. For example,\n              the span of “Place” is constrained to “City” and the span of\n              “Date” is constrained to “day”. More details of annotation rules\n              can be referred to in\n              Appendix Table S1[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2795-Mmc1.pdf].\n            \n            Annotators require labeling the case reports again according to the\n            established entity annotation rules as shown in\n            Figure 3[href=https://www.wicell.org#fig3]B.\n          \n            Carry out the consistency check among case reports annotated by\n            different annotators.\n            \nNote: To guarantee the consistency\n              and accuracy of the manual annotation, we randomly examined and\n              modified a subset of 100 case reports after they had been\n              annotated by different graduate students. Then, three\n              public-health experts participated in the revisions. After these,\n              we discussed the annotations of the case reports to reach a\n              consensus on the modifications. Next, we continued to examine and\n              manually annotate the remaining case reports. The agreement rate\n              for our revisions reaches 90%, which suggests that the\n              inter-annotator agreement rate is acceptable. Any inconsistent\n              revisions were submitted to the experts for final revisions.\n            \n            Export the annotated results, i.e., the .xml file as shown in\n            Figure 3[href=https://www.wicell.org#fig3]C, to the “CCIE/annotation/xml”\n            folder by clicking the \"Save as\" button in the\n            \"File\" option.\n          \n            Convert data format from the “xml” style to the “BIO” style for\n            model training.\n            \nNote: According to the annotated\n              results, the \"BIO\" labels are employed to prepare the\n              training data. For the token at the start position of an entity,\n              we mark it as \"B-M\"; For the token at the middle and end\n              position of an entity, we mark it as \"I-M\". The\n              character “M” denotes the type of entities. For example, as shown\n              in Figure 3[href=https://www.wicell.org#fig3]D, the admitted hospital “Zhonghe\n              Town Health Center” is annotated as “B-TDH, I-TDH, I-TDH, I-TDH”,\n              the date of symptom onset “January 27” is annotated as “B-OnT,",
    "I-OnT”. For other tokens that beyond the scope of the eleven\n              entities, they are marked with the “O”. The script in the\n              “CCIE/annotation” folder can be used to obtain the final\n              training data.\n            \n> python data_postprocess_from_xml.py\nCritical: We have provided the\n      annotated data in the “CCIE/ner/datasets/raw/NER”. If one wants to\n      use private data to train the CCIE model, he/she needs to organize an\n      annotation team (about 10 people) to carry out the data annotation\n      according to this step.\n    \n        Performing the category label annotation via the text matching\n        technology.\n        \n            Integrate the human-coded epidemiological categories involving\n            similar specific scenes into one label.\n            \nNote: For example, “hold a party”\n              and “attend a wedding” is considered two different category labels\n              in the epidemiological coding rules, but they are regarded as a\n              “party” label in our annotation rules. In this way, we construct\n              six categories. They are respectively “Place (include three\n              labels)”, “Event (include eight labels)”, “Person (include four\n              labels)”, “Isolate (include four labels)”, “Discover (include four\n              labels)” and “Degree (include four labels)”. Each of them\n              corresponds to the integrated coding rules. More details can be\n              referred to\n              Appendix Table S2[href=https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2795-Mmc1.pdf].\n            \n            Derive a vocabulary (named key_vocab) to save all category\n            labels and all scenes in a label.\n            \nNote: This vocabulary is a data\n              structure in dictionary (i.e., key-value pairs), whose keys\n              correspond to labels and values correspond to key words of scenes.\n              Take the “Place_social” label in the “Place” category as an\n              instance, it is saved as\n              {Place: {Place_social: {Family, Social place, indoor, School,\n                Work}}}\n              in the key_vocab.\n            \n            Match each report with all the scenes in the key_vocab by\n            text matching to determine the labels of case reports.\n            \nCritical: The Python code for\n              the step 7 is provided in the\n              “CCIE/rawdata/raw_data_process_for_classification_annotation.py”,\n              and one can process the raw data with using the following command.\n            \n> python",
    "raw_data_process_for_classification_annotation.py\n            Carry out the consistency check for the matched category labels.\n            \nNote: To guarantee the accuracy of\n              the automatic annotation, we randomly examined 100 case reports\n              from all 10,017 case reports. This examination was conducted by\n              three annotators until they fully agreed with the annotation\n              results.\n            \n            Split the data into “train.txt”, “dev.txt” and “test.txt” with a\n            ratio of 8:1:1.\n            \nCritical: We have provided the\n              fully processed data in the\n              “CCIE/classification/data/disease/X” folder, where “X”\n              denotes a category name.\n            \nModel training and evaluation\nTiming: ∼3 h (for steps 8 to 11)\n    \n      In this step, we give the detailed scripts of model training and\n      evaluation.\n    \nSource code collection\nDownloading the source code through the following commands:\n> git clonehttps://github.com/AllenWangle/STARProtocols-CCIE[href=https://github.com/AllenWangle/STARProtocols-CCIE].\n      \n> unzip CCIE.zip\nCritical: If one wants to use his/her\n      own annotated data, he/she need to put the named entity recognition data\n      (i.e., train.txt, valid.txt and test.txt) to the\n      “CCIE/ner/datasets/raw/NER” folder, and put the category\n      classification data (i.e., train.txt, dev.txt and\n      test.txt) to the “CCIE/classification/data/disease/X″\n      folder, where “X” denotes a category name.\n    \n        Downloading the pre-trained language model from\n        https://drive.google.com/file/d/1buMLEjdtrXE2c4G1rpsNGWEx7IUQ0RHi/view?[href=https://drive.google.com/file/d/1buMLEjdtrXE2c4G1rpsNGWEx7IUQ0RHi/view?]\n        and unzip the model to the “CCIE/ner” folder and the\n        “CCIE/classification/” folder respectively.\n      \nNamed entity recognition model\n        Training and evaluating the named entity recognition model in the Ubuntu\n        terminal. [troubleshooting 3[href=https://www.wicell.org#troubleshooting]] [troubleshooting 4[href=https://www.wicell.org#troubleshooting]] [troubleshooting 5[href=https://www.wicell.org#troubleshooting]]\n        \n> cd CCIE/ner\n> python train_base_model_bert.py --train True # for\n              training\n        The main parameters are shown as follows:\n        \n--bert_model_path: chinese_roberta_wwm_ext_L-12_H-768_A-12.\n--mode: 0.\n--task_name: ner.\n--train: True is for training and False is for evaluation.\n--train_ratio: 1.0.\n--language: wzz1.\n--emb_drop_rate: 0.2.\n--batch_size: 64.\n--max_seq_len: 200.\n--minimal_lr: 1e-4.\n            --epochs: 40.\n            \nNote: The parameters are set as\n              default values in the train_base_model_bert.py if one does\n              not set them in the commands. The evaluation metric is F1-score,\n              which is described in Quantification and statistical analysis.\n            \nAlternatives: If one uses",
    "our docker container, they need to type the following commands to\n              train the named entity recognition model.\n            \n> docker run –-rm -it -v\n                  YOUR_LOCAL_PATH_OF_CCIE/ner:/work/CCIE-ner\n                  xuce0915/starprotocols “bash”\n> cd work/CCIE-ner\n> /root/miniconda3/bin/python train_base_model_bert.py\n                  --train True\nOptional: The function call\n              process after starting the script of model training as follows.\n              One does not need to run the following functions as they are\n              automatically called by the Python script.\n            \n            Call the process_base_bert() function to preprocess the\n            training data and generate the input features, which is located in\n            the “CCIE/ner/utils/prepro_data_Iner_BERT.py”. All parameters\n            in this function are defined in the “CCIE/ner/utils/\n            configs_bert.py”.\n            \n                Configure the BERT language model via the\n                from_json_file(config.bert_config) function, which\n                is located in the “CCIE/ner/modeling.py”. The\n                bert_config is the configuration file of BERT.\n              \n                Read and process the train/valid/test data from the dataset\n                files via read_data_and_vocab(config.train_file, config.dev_file, config.test_file) function. The train_file is the file path of the\n                training data and other two are similar meanings.\n              \n                Segmentate sentences to tokens and convert them into numeric\n                numbers via the\n                FullTokenizer(vocab_file = config.vocab_file, do_lower_case =\n                  config.word_lowercase)\n                function, which is located in the “CCIE/ner/tokenization.py”.\n                The vocab_file is the path of vocabulary file and the\n                do_lower_case is the controller for using low case.\n              \n                Generate the input features for the training data, validation\n                data and test data via the\n                file_based_convert_examples_to_features(samples, label_list, max_seq_len, tokenizer, output_file) function, which is located in the\n                “CCIE/ner/run_classifier_roberta_wwm_large.py”. The\n                samples is the input file, label_list is the list\n                of label types, max_seq_len is the maximum length of the\n                input sequence, tokenizer is the segmentation results of\n                step iii, output_file is the file path of output results.\n              \n            Call the _build_model() function in the\n            BaseModel_for_BERT() class to build the named entity\n            recognition model, which is located in the\n            “CCIE/ner/models/base.py”.\n            \n                Load the pre-trained language model via the\n                bert_embedding(input_ids, input_mask, segment_ids)\n                function. The input_ids, input_mask and",
    "segment_ids are all used to match the parameters of BERT\n                model and are obtained by the _add_placeholders()\n                function.\n              \n                Shield some neurons randomly with a given probability via the\n                Dropout(emb_drop_rate) layer in TensorFlow. The\n                emb_drop_rate is the dropout rate of neurons.\n              \n                Build the bidirectional LSTM network via the BiRNN(num_units, drop_rate, concat, activation, scope = ”bi_rnn”) function to learn long-distance dependency information among\n                different entities. The num_units is the number of neural\n                units in LSTM, drop_rate is the rate of dropout,\n                concat is the concat mode between two layers,\n                activation is the activation function. This function is\n                located in the “CCIE/ner/models/shares.py”.\n              \n                Build the CRF (Conditional Random Fields) layer to learn the\n                possibility distribution of entity label paths via\n                CRF(num_labels) function. The num_labels is\n                the number of entity labels. This function is located in the\n                “CCIE/ner/models/shares.py”.\n              \n            Call the train() function to train the named entity\n            recognition model, which is located in the\n            “CCIE/ner/models/base.py”.\n          \n            Call the evaluate_data() function to evaluate the named\n            entity recognition model, which is located in the\n            “CCIE/ner/models/base.py”.\n            \nCritical: If one wants to\n              recognize the entities in the new COVID-19 case reports after\n              training the model once, he/she can follow the steps below for\n              data preparation and entity extraction.\n            \n            Type into the “CCIE/ner” folder and put the new case reports in the\n            “Original_Text_CN” column of the “NewCases.xlxs”. Each line\n            merely contains one case report.\n          \n            Find the NewCaseExtraction.ipynb and run the cell under\n            “Pre-process the New Case Reports” to obtain the test data with the\n            same format as the training data.\n            \nNote: The original test data in\n              the “CCIE/ner/datasets/NER/test.txt” will be covered.\n              Nevertheless, one can copy the original test data to other folders\n              to prevent this problem.\n            \n            Run the cell under “Evaluation Script for Extracting New Entities”.\n            The BIO labels for entities are saved in the\n            “ckpt/wzz/base_model_wzz1_1_Roberta_wwm_wzz_1_0918/test_result.txt”.",
    "Run the cell under “Post-process the Entities in BIO to Readable\n            Entities” to obtain the entities described in the natural language.\n          \nCategory classification model\n        Typing to the classification directory of CCIE, and then using\n        the following command to train and evaluate the category classification\n        model in the Ubuntu terminal. [troubleshooting 3[href=https://www.wicell.org#troubleshooting]] [troubleshooting 4[href=https://www.wicell.org#troubleshooting]] [troubleshooting 5[href=https://www.wicell.org#troubleshooting]]\n        \n> bash run.sh\n        The main parameters are shown as follows:\n        \n--model_name_or_path: chinese_roberta_wwm_ext_pytorch.\n--task_name: disease.\n--do_train: True is for training.\n--do_eval: True is for evaluation.\n            --data_dir: data/disease/event #\n            example for the event category.\n          \n--max_seq_length: 512.\n--per_gpu_eval_batch_size: 8.\n--per_gpu_train_batch_size: 8.\n--learning_rate: 4e-5.\n--num_train_epochs: 3.\n--output_dir: output/wzz2_wwm_new.\n--model_type: bert.\n--class_type: base.\n            --gradient_accumulation_steps: 2.\n            \nNote: The parameters are set as\n              default values in run_glue.py if one does not set them in\n              the commands. The evaluation metric is also the F1-score.\n            \nAlternatives: If one uses\n              our docker container, he/she needs to type the following commands\n              to train the category classification model. [troubleshooting 6[href=https://www.wicell.org#troubleshooting]]\n            \n> docker run –-rm -it -v\n                  YOUR_LOCAL_PATH_OF_CCIE/classification:/work/CCIE-cls\n                  xuce0915/starprotocols “bash”\n> cd work/CCIE-cls\n> bash run.sh\nOptional: The function call\n              process after starting the script of model training as follows.\n              One does not need to run the following functions as they are\n              automatically called by the Python script.\n            \n            Load the pre-trained language model and tokenizer via the\n            from_pretrained() function.\n          \n            Load the training dataset from the\n            “CCIE/classification/data/disease/X” via the\n            load_and_cache_examples(args.task_name, tokenizer, evaluate = False) function. The task_name is the parameter to point out the\n            datasets, and here is set to disease. The tokenizer is\n            loaded by the step a. evaluate = False denotes there is no\n            validation during the model training.\n          \n            Call the train(train_dataset, model, tokenizer)\n            function to train the category label classification model. The\n            train_dataset is obtained by the step b, and the\n            model and tokenizer are obtained by the step a.",
    "Call the evaluate(model, tokenizer, prefix = global_step) for model evaluation.\n            \nCritical: If one wants to\n              classify the categories for the new COVID-19 case reports after\n              training the model once, they can remove the\n              --do_train term in the “CCIE/classification/run.sh”,\n              and run the bash run.sh script again."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Clinical Protocol",
    "Health Sciences"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}