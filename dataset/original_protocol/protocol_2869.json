{
  "id": 3035,
  "origin_website": "Cell",
  "title": "In vivo imaging of the human retina using a two-photon excited fluorescence ophthalmoscope",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nThis section provides detailed instructions for data acquisition (Part 1) and subsequent data processing (Part 2). The protocol results in obtaining a two-photon-excited fundus image by averaging a total of approx. 80–120 frames (we perform 4 measurement series, recording up to 30 frames during each one) and it can be extended to obtain more frames. The procedure starts with the initial alignment of the imaged subject. Next, it utilizes a femtosecond laser to acquire two-photon-excited fluorescence and reflectance images of the fundus simultaneously, which is necessary for further data processing. Before starting the process, signed informed consent must be obtained.\nPart 1a. Positioning the subject\nTiming: up to 30 min\nThis step results in the initial positioning of the subject and obtaining a reflectance image. The timing is related to the total time the subject spends with the imaging instrument during initial positioning. The subject has to fix the eye position for a much shorter time, usually less than 40 s at a time.\nCheck and adjust the laser power in the pupil plane.\nSet the scanners’ voltage to 0 V to guide the beam along the optical axis of the setup.\nRemove the NDF (OD = 0.8) from the beam path.\nSet the power to the desired level by adjusting the position of GF.\nReturn to the frame scanning mode, where the beam is not allowed to go along the optical axis.\nEnsure that the subject is seated comfortably and can place their head on a chin rest.\nAdjust the chin-rest to position the imaged eye in the center of the lens L4 in both horizontal and vertical axes.\nThe axial position of the chin-rest should be adjusted to place the pupil in the scanner’s conjugate plane.",
    "Note: The FOV should be the largest in this case (a FOV marker can be placed in the control software as a reference). The scanning lines should be relatively sharp for the subject.\nAdjust the fixation position so the eye can perceive it; it should appear on the scanning line.\nAdjust the fixation sharpness by adjusting the position of lens L7; it should appear as a sharp point.\nNote: Both the chin-rest and the fixation point can be mounted on motorized stages to facilitate subject positioning.\nStart the frame scanning mode and observe reflectance frames.\nNote: Ask the subject if they can see the scanning red line. Ask the subject whether the line is cut on the edges. If yes, correct the eye position.\nNote: The use of a pupil camera would ease the initial positioning of the subject.\nStart a line scan mode, correct the refraction error, and adjust the eye position.\nCritical: The average power for this step should be as low as possible. We use the average power within the 50–60 μW range.\nNote: This step allows for the initial correction of the refraction error.\nAsk the subject if the line is positioned vertically in the center of the FOV. Move the chin-rest up and down; the line is centrally positioned when it is the widest (Figure 4[href=https://www.wicell.org#fig4], lateral alignment).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2578-Fig4.jpg\nFigure 4. Subject’s view during eye positioning in the line scan mode\nThe sketch shows correct and incorrect views during the alignment in axial and lateral directions.\nAsk the subject if the line is positioned horizontally in the center of the FOV. Move the chin-rest left and right; the line should not be cut on the edges and should appear the widest (Figure 4[href=https://www.wicell.org#fig4], lateral alignment).",
    "Adjust the axial position of lens L4. Ask the subject to report at which position of the lens the line is the sharpest. In addition, while the beam is correctly focused, it changes its color – instead of dark red, it appears pink-white (Figure 4[href=https://www.wicell.org#fig4], axial alignment).\nNote: We did not test the system on color-blinded or color-weakness subjects so far. Nevertheless, this step is not critical. We observed that the position of the lens L4, at which the color changes, corresponds to the point at which the maximum fluorescence intensity is obtained, and this criterion is sufficient to position the subject.\nStart the frame scan mode; a reflectance image of the fundus should appear at this point. Adjust the position of fixation to display a particular fundus location.\nFor the final correction of refraction, adjust the axial position of lens L4 by looking at the integrated pixel intensity value in the reflectance channel and observing the obtained images. Find the axial position of the lens where the highest intensity and the sharpest reflectance images can be obtained.\nNote: The average laser power should be as low as possible. A maximum of 50 μW for a FOV of 17.8° should allow a sufficient quality of reflectance images to be obtained.\nNow, the fixation position could be corrected finally to display the chosen FOV at the eye fundus.\nOptional: The obtained images can be saved as a reference. Examples of such images are shown in Figure 5[href=https://www.wicell.org#fig5].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2578-Fig5.jpg\nFigure 5. Representative reflectance images\n(A) Single-frame reflectance image was obtained with 50 μW excitation power.\n(B) Single-frame reflectance image obtained with 300 μW excitation power.\nScale bars: 1 mm.\nPart 1b. Obtain two-photon excited fluorescence image series\nTiming: up to 20 min",
    "This step aims at acquiring data used for the reconstruction of fluorescence images.\nTurn off the light in the room. Cover the subject’s head and the imaging instrument with a black, nontransparent enclosure.\nNote: The measurement has to be taken in a dark room (<0.01 lux). We advise removing all unnecessary sources of light from the room (e.g., LEDs of various devices) and decreasing the brightness of the computer screen as much as possible (or additionally covering it with a red filter).\nCritical: Do not remove the subject’s head from the chin-rest at this stage, if possible (otherwise re-calibration may be necessary).\nAdjust the gain of the PMT (key resources table[href=https://www.wicell.org#key-resources-table]), based on the device manual and calibration curve, to a value with the highest ratio of signal to background noise.\nCritical: When turning on the PMT, there must be no light in the room.\nNote: An appropriate gain for a given PMT should be used; the procedure to obtain this value should be described in the PMT manual. In our case, it was 860 mV.\nRecord several frames in the fluorescence channel with a blocked laser beam and observe the level of the signal.\nNote: The average signal level should be < 0.005 photon counts per pixel. Ideally, it could be reduced to a value close to the number of dark counts of the PMT (in our case, 0.0007 photon counts per pixel). The example of image in the fluorescence channel with laser beam blocked is shown in Figure 6[href=https://www.wicell.org#fig6]A.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2578-Fig6.jpg\nFigure 6. Representative results of two-photon imaging\n(A) Background noise observed in the fluorescence channel with the laser beam blocked.\n(B) Single-frame two-photon fluorescence image obtained with 300 μW excitation power.\n(C) Single-frame reflectance image obtained with 300 μW excitation power.\nScale bars: 1 mm.",
    "Note: With these adjustments, only stray photons and dark counts are acquired. If the obtained value exceeds the above-specified number, improve light proofing of the setup or check the level of dark counts of the PMT.\nOptional: These frames can be recorded to compare the subsequently captured two-photon fluorescence signal to the background noise.\nStart the frame scanning mode with an NDF in the beam path (50 μW in the pupil plane). Observe the eye position in the reflectance channel and adjust the position if needed.\nNote: We have found that even if the eye position is acceptable, it is usually beneficial to wait several seconds to allow a proper fixation before moving to the next step.\nIf the eye position is satisfactory, start recording fluorescence frames: remove the NDF (300 μW in the pupil plane) and start saving both fluorescence and reflectance frames with consecutive numbering; record 30 frames (this step takes about 40 s with a 20-μs pixel dwell time).\nNote: The examples of single frames recorded in fluorescence and reflectance channels are shown in in Figures 6[href=https://www.wicell.org#fig6]B and 6C.\nTake a 60 s break. The galvo scanner is positioned to direct the laser beam outside the imaging setup aperture in the pupil plane.\nNote: Advise the subject to close their eyes or blink and rest.\nNote: We found out that during the 60-s break, the eye's position is usually well maintained. Even if a longer break is required and the subject removes their head from the chinrest, usually, a small alignment is sufficient to restore the previous imaging position. The realignment usually takes less than 30 s and can be done using low average power (within the 50–60 μW range).\nRepeat steps (15)–(17) three additional times.",
    "Note: This procedure will capture 120 frames in each channel; several frames will need to be discarded due to substantial eye movements or unexpected blinks. A total of 120 frames should result in 80–100 usable frame pairs for subsequent data processing. Such a number of fluorescence frames is sufficient to obtain a satisfactory image quality after registration and averaging.\nDecrease the gain of the PMT to 0 V and uncover the subject.\nNote: A dim light can be turned on at this point.\nAsk the subject to close their imaged eye and record 20–30 frames in the reflectance channel under such conditions (the exact number of frames is not critical for this stage). These data will be used for background subtraction in the final averaged reflectance image.\nNote: According to this procedure, only background noise and parasitic reflections (e.g., from the L3 lens) will be captured in the reflectance channel.\nOptional: Step 20 can be performed during the 60-s break when the subject’s eyes are closed (point 16) to shorten the overall measurement time.\nPart 2. Data processing\nTiming: 1 h\nEye movement causes a significant problem in obtaining high-contrast fluorescence images through averaging. It occurs not only between the acquisitions of each frame but also within the collection of single-frame data. The following steps are performed to correct for eye movement before frame averaging. The averaging procedure with all related steps is done by a prepared MATLAB script. The scripts use the following toolboxes: Image Processing Toolbox, Parallel Computing Toolbox, Statistics, and Machine Learning Toolbox, and were tested on MATLAB versions released in 2021 and 2022.",
    "Note: The processing time will differ depending on the computer’s specifications. For example, processing 130 frames take about 21 min on a personal computer with AMD Ryzen 9 3900×, 12-core processor, and 64GB of RAM.\nManually select the reference reflectance frame.\nInspect the recorded images.\nSelect one sufficient-quality frame from the inspected reflectance images.\nNote: A “sufficient-quality” frame contains an image with high-contrast blood vessels and no significant visible artifacts of vertical movement or eyelid presence that would cause missing data or repeated parts (due to vertical eye movement during the scan). Figure 7[href=https://www.wicell.org#fig7] shows such examples.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2578-Fig7.jpg\nFigure 7. Examples of reflectance frames with different qualities\n(A and B) Reflectance frames that are not suitable for use as reference frames.\n(C) Sufficient-quality reflectance frame.\nScale bars: 1 mm.\nNote: In future versions of the method, it might be desirable to replace the manual selection of the reference frame with an automatic approach based on image metrics or neural networks. Such an automatization would make data processing more convenient and faster.\nCalculate the background reflectance frame.\nLoad a set of reflectance background frames.\nCompute the averaged background image using the MATLAB mean() function (Figure 8[href=https://www.wicell.org#fig8]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2578-Fig8.jpg\nFigure 8. Example of averaged reflectance background frame\nScale bars: 1 mm.\nPrepare the reflectance data.\nSubtract the reflectance background image computed in 22(b) from all reflectance frames obtained during subject measurement.\nCreate a circular mask of radius R.\nNote: The mask radius and center are manually selected to cover approximately the full aperture. These values remain unchanged if there are no substantial changes in the imaging path, like changing the lenses; i.e., changing the FOV. During the processing of the presented images, the mask radius was set to R = 110 pixels.\nFor each reflectance image, normalize the intensity of the signal.",
    "Compute the mean intensity inside the mask of radius R.\nDivide each pixel value by the computed mean.\nApply Bi-Empirical Mode Decomposition (BEMD) to a normalized reference frame (selected in step 21(b)).\nNote: BEMD is the extension of the 1D Empirical Mode Decomposition into a two-dimensional signal. These methods decompose the input signal into a few Intrinsic Mode Functions (IMFs) and a residue.6[href=https://www.wicell.org#bib5] To speed up the calculations, we used the FABEMD algorithm (Fast and Adaptive BEMD).5[href=https://www.wicell.org#bib6]\nExtract the second IMF from the decomposed reference frame (Figure 9[href=https://www.wicell.org#fig9]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2578-Fig9.jpg\nFigure 9. Processing of reflectance frames\n(A) Three different examples of normalized reflectance frames from an examined subject.\n(B) Corresponding second IMFs computed using the FABEMD algorithm.\nScale bars: 1 mm.\nNote: For our reflectance images, the second IMF was used as it gave the best results. However, the other IMFs may be a better choice for different system configurations. The selection of the proper IMF allows for a decrease in the influence of elements that do not follow eye movement – aperture and the remaining part of the background. In addition, the high-frequency noise is also removed.\nFrom here, the procedure of frame alignment starts. All frames are aligned to the reference frame by comparing frame IMFs to the IMF of the reference frame. This procedure uses a three-stage alignment to obtain a set of image correction transforms that are later applied to both reflectance and fluorescence frames.\nNote: In each step, the complexity of the applied correction is increased; hence, the computation time is increased. Moreover, one can decide to initially follow the algorithm with just a first step for a faster but less accurate initial view, and then perform the complete analysis to enhance the quality.",
    "In the first alignment stage, perform the following operations for each acquired reflectance frame (excluding the reference frame):\nCompute the BEMD of the processed reflectance frame and extract the second IMF.\nCompute the 2D fast Fourier Transform (FFT) of the IMFs of both the reference and processed frames.\nNote: Do not shift the zero-frequency component to the center of the spectrum (do not apply the fftshift operation).\nCalculate x and y shifts between the reference IMF and processed reflectance frame IMF using the dftregistration() function.\nNote: The dftregistration() function is based on a report by Guizar.7[href=https://www.wicell.org#bib7],8[href=https://www.wicell.org#bib8] In the case presented here, the input for the function comprises the FFTs of the compared IMFs and an upsampling factor. We set the function’s upsampling factor to usfac=1 (no upsampling).\nApply computed shift corrections to all reflectance and corresponding fluorescence frames, using MATLAB’s built-in function imtranslate().\nIn the second alignment step for each corrected reflectance frame, perform the following steps:\nCreate a frame copy and trim the image.\nNote: All image sides are trimmed to avoid image wrapping or spaces filled with zeros. For the data presented here, the trimming was set to 10 pixels on each side. The image trimming value was set based on observation of how the investigated subject’s eye position shifted in x-y directions during the measurements. For less stable subjects, an increase in trimming value might be required.\nCompute the BEMD of the trimmed reflectance frame and extract the second IMF.\nNote: Instead of computing the BEMD for the translated reflectance frame, translation of the previously calculated IMF can be performed.\nUse MATLAB’s built-in function imregtform() to compute the image affine transform between the trimmed reference IMF and the processed reflectance frame IMF.",
    "Note: The following parameters were used when employing the imregtform() function during the processing of images presented herein:\n[optimizer, metric] = imregconfig('multimodal');\noptimizer.InitialRadius = 0.0005;\noptimizer.Epsilon = 0.5e-4;\noptimizer.GrowthFactor = 1.01;\noptimizer.MaximumIterations = 10000;\nMethod='affine';\nTransform not-trimmed copies of the reflectance images and corresponding fluorescence frames using MATLAB’s built-in function imwarp().\nNote: For each frame set (reflectance and fluorescence), use the corresponding image transform and the following parameters:\n'OutputView' is set to imref2d(size(referenceFrame))\n‘interp’ is set to ‘nearest’\nWe use the ‘nearest’ interpolation in this alignment stage and no upsampling in the previous step. We noticed that operations of subpixel alignment applied before the third stage led to some image artifacts (speckle-like patterns) in the final results.\nIn the third alignment step for each reflectance frame (other than a reference), perform the following steps:\nTrim previously shifted and transformed reflectance and fluorescence frames.\nCompute the BEMD of the trimmed reflectance frame and extract the second IMF.\nUse MATLAB’s built-in function imregdemons() to compute the image transform maps containing each pixel shift in x and y directions between the trimmed reference IMF and the processed reflectance frame IMF.\nNote: The following parameters were used during the processing of the presented images:\nNumber of iterations=5000;\nAccumulatedFieldSmoothing=3;\nPyramidLevels=5;\nFinally, transform fluorescence and reflectance images trimmed in 28(a), using MATLAB’s function imwarp().\nAfter performing these steps, a set of aligned images is obtained. In the following steps, the alignment is verified, and frame averaging is performed.\nCompute the BEMD of the reference reflectance frame and all aligned reflectance frames, extracting the second IMF.\nCompute the mean-squared error between the reference IMF and each warped IMF, using MATLAB’s function immse().\nNote: For this calculation, we use only pixels inside the circular mask created in 23(b).",
    "For a set of threshold values, Th, compute a set of average reflectance frames and a set of average fluorescence frames. The output fluorescence frames for different threshold values (and different numbers of used frames Uf) are presented in Figure 10[href=https://www.wicell.org#fig10].\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2578-Fig10.jpg\nFigure 10. Final fluorescence images were obtained for different threshold levels (Th) from Uf frames\n(A–H) The Th values range from (A) 0.5 to (H) 1.9.\nScale bars: 1 mm.\nNote: For averaging, we select frames fulfilling the following condition:\nimmse(k)< trimmean([immse(1) ... immse(N)], 20)∗Th,\nwhere immse(k) represents the mean-squared error computed for frame k, and trimmean() computes the mean value of mean-squared errors for all processed frames after excluding 20% of outliners. The 20% value was selected manually. The following Th values were used during data processing: 0.5, 0.7, 0.9, 1.1, 1.3, 1.5, 1.7, and 1.9.\nNote: The purpose of this step is to help with the manual selection of good-quality results. The threshold values can be modified freely as long as the output images are created from different sets of frames (from more similar images at low Th values to less similar ones at high Th values). In the future, a more advanced approach based on image metrics or neural networks might be used to automatically compute the best-quality output image.\nNote: The 20% value was set manually to avoid using worst-quality frames (e.g., frames containing the closed eyelid) in the computation of average mean-squared error. The “<” means that the considered frame is used for averaging if its mean-squared error is smaller than the average mean-squared error (after the removal of outliners) multiplied by the Th value.",
    "Note: The increase in intensity and contrast in Figure 10[href=https://www.wicell.org#fig10] comes from the increase in the number of used frames, which comes from the increase in threshold value. In datasets containing many low-quality frames, the contrast might decrease for high Th values as such low-quality frames might be used in the averaging process."
  ],
  "subjectAreas": [
    "Molecular/Chemical Probes",
    "Bioinformatics",
    "Health Sciences",
    "Biotechnology And Bioengineering",
    "Neuroscience",
    "Clinical Protocol",
    "Biophysics"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}