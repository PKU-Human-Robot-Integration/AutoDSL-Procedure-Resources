{
  "id": 17167,
  "origin_website": "Jove",
  "title": "Objectification of Tongue Diagnosis in Traditional Medicine, Data Analysis, and Study Application",
  "procedures": [
    "This study has been approved by the National Natural Science Foundation of China project, Constructing Dynamic Change rules of TCM Facial image Based on Association Analysis. The ethics approval number is 2021KL-027, and the ethics committee has approved the clinical study to be carried out in accordance with the approved documents which include clinical research protocol (2021.04.12, V2.0), informed consent (2021.04.12, V2.0), subject recruitment materials (2021.04.12, V2.0), study cases and/or case reports, subject diary cards and other questionnaires (2021.04.12, V2.0), a list of participants in the clinical trial, research project approval, etc. Informed consent from the patients participating in the study was obtained. The main experimental approach of this study is to use real tongue images to validate and compare the model segmentation effects. Figure 1 presents the components of tongue diagnosis objectification.\n1. Image acquisition\nUse the self-developed hand-held lingual face diagnostic instrument to collect lingual face images of patients.\nFill in the patient's name, gender, age, and disease on the computer page. Images included here are from patients who came to the clinic and agreed to be photographed after being informed of the purpose and content of the study. Confirm that the patient is sitting upright, place the whole face in the image acquisition instrument, and instruct the patient to extend their tongue out of their mouth to the maximum extent.\nHold the image acquisition device connected to a computer and verify through the images on the computer screen that the patient is in the correct position and that the tongue and face are fully exposed.\nPress the Shoot button on the computer screen three times to take three pictures.\n\tNOTE: The image acquisition instrument is currently only at the patent application stage and is not for commercial use, so it is not for sale.",
    "Manually select and filter the collected tongue and face images. Filter and exclude images that have incomplete tongue and face exposure, as well as images that are too dark due to insufficient light. Figure 2 shows the image acquisition page of the software.\nIn the experimental design, collect three images from each patient at a time as alternatives and select a relatively standard, fully exposed, well-illuminated, and clear image as the sample for subsequent algorithm training and testing.\nCollect data after the shooting, export the data for manual screening, and delete the non-standard images visible to the naked eye. Use the following filtering and exclusion criteria: incomplete tongue and face exposure, and images that are too dark as a result of insufficient light. An example of an under-lit, an incomplete, and a standard image is shown in Figure 3.\n\tâ€‹NOTE: Insufficient light is generally caused by failure of the patient to place the face entirely into the instrument. Complete exposure is usually only obtained by correctly photographing the patient.\n2. Tongue segmentation\nPerform tongue image segmentation using an online annotation tool, as described below.\n\t\nInstall Labelme, click on the Open button in the upper left corner of the label interface, select the folder where the image is located, and open the photos.\nClick on create polygon to start tracking points, track the tongue and lingual shapes, name them according to the selected areas (e.g., tongue and lingual surface), and save them.\nWhen all the marks are complete, click Save to save the image to the data folder. See Figure 4 for a detailed flow chart.\n\t\tNOTE: As the images may have pixel differences, the images cannot be directly used for algorithm training and testing.",
    "Unify the images to the same size by edge-filling the images, with the long side of the image as the target fill length and performing white edge-filling to fill the images to a square, with the long side of the image as the edge length. The image size captured by the device is 1080 x 1920 pixels, and the size of the filled image is 1920 x 1920 pixels. See Figure 5.\nApply image enhancement if needed. No enhancement was applied in this study, as the images used were taken in a fixed scene and were less affected by the environment, lighting, and other factors.\nBecause three images were collected for each patient during the shooting process to account for uncontrollable factors, such as subject blinking and lens blocking, manually screen the images from each patient to retain one image per patient.\nFor the purpose of training the model, collect data from 200 people, or 600 images. After the screening, retain about 200 usable images.\nAccording to the image number, randomly divide all the tongue images, placing 70% of them into the training set and 30% into the test set in a spreadsheet.\n3. Tongue classification\nGo to the official websites and download and install Anaconda, Python, and Labelme. Activate the environment and complete the installation and adjustment of the overall environment. See Figure 6 for a flow chart describing the installing and setting up of the software.\nBuild the deep learning algorithm model in the installed environment, tune the parameters, and complete the model training using the training set. Perform model selection and tuning as described in the following steps.",
    "Model selection: Choose the appropriate model based on the purpose of the research. After reviewing research on tongue image processing in the last 5 years, four algorithms, U-Net, Seg-Net, DeeplabV3, and PSPNet, were selected for validation in this study (see Supplementary Coding File 1, Supplementary Coding File 2, Supplementary Coding File 3, and Supplementary Coding File 4 for model codes).\nData set construction: After completing the model selection, construct the required data set in conjunction with the research content, mainly using Labelme annotation and the uniform image size methods, as described above.\nPerform model training as described below. Figure 7 shows details of the algorithm training operation.\n\t\nInput the data into the neural network for forward propagation, with each neuron first inputting a weighted accumulation of values and then inputting an activation function as the output value of that neuron to obtain the result.\nInput the result into the error function and compare it with the expected value to get the error and judge the degree of recognition by mistake. The smaller the loss function is, the better the model will be.\nReduce the error by back propagation and determine the gradient vector. Adjust the weights by the gradient vector to the trend toward results so that the error tends to zero or shrinks.\nRepeat this training process until the set is completed or the error value no longer declines, at which point the model training is complete. See Figure 8 for a flow chart of the algorithm model in training and testing.\nTest the four models using the same test data for segmentation and judge the model performance according to the segmentation effect. The four metrics of precision, recall, mean pixel accuracy (MPA), and MIoU provide a more comprehensive model performance evaluation.",
    "After the results of the four models are generated, compare their values horizontally; the higher the value is, the higher the segmentation accuracy and the better the model's performance. See Figure 9, Figure 10, and Figure 11.\nSubscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Medicine"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}