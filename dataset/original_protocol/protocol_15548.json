{
  "id": 19374,
  "origin_website": "Jove",
  "title": "Bringing the Clinic Home: An At-Home Multi-Modal Data Collection Ecosystem to Support Adaptive Deep Brain Stimulation",
  "procedures": [
    "Patients are enrolled through a larger IRB and IDE approved study into the aDBS at the University of California, San Francisco, protocol # G1800975. The patient enrolled in this study additionally provided informed consent specifically for this study.\n1. At-home system components\nCentral server and VPN\n\t\nAcquire a personal computer (PC) running a Linux-based operating system (OS) dedicated to serving a VPN. House the machine in a secure room. Disk encrypt the machine to ensure data security.\nConfigure the VPN server to be publicly accessible on at least one port.\n\t\tNOTE: In this case, this was achieved by collaborating with the IT department to give the server an externally facing static-IP address and a custom URL by the university's DNS hosting options.\nFor server installation complete the following steps once on the PC selected for serving the VPN.\n\t\t\nFirewall configuration: Run the following commands in the PC terminal to install and configure uncomplicated firewall:\nsudo apt install ufw\nsudo ufw allow ssh\nsudo ufw allow <port-number>/udp\nsudo ufw enable\nServer VPN installation: Install the open-source WireGuard VPN protocol16 on the PC and navigate to the installation directory. In the PC terminal, run umask 007 to update directory access rules.\nKey generation: In the PC terminal, run\nwg genkey | tee privatekey | wg pubkey > publickey\n\t\t\t​This generates a public/private key pair for the VPN server. This public key will be shared to any client PC that connects to the VPN.\nVPN configuration: In the PC terminal, run touch <interface_name>.conf to create a configuration file, where the file name should match the name of the interface. Paste the following server rules into this file:\n\t\t\t[Interface]\n\t\t\tPrivateKey = <contents-of-server-privatekey>\n\t\t\tAddress = ##.#.#.#/##\n\t\t\tPostUp = iptables -A FORWARD -i interface_name -j ACCEPT; iptables -t nat -A POSTROUTING -o network_interface_name -j MASQUERADE",
    "PostDown = iptables -D FORWARD -i interface_name -j ACCEPT; iptables -t nat -D POSTROUTING -o network_interface_name -j MASQUERADE\n\t\t\tListenPort = #####\n\t\t\t[Peer]\n\t\t\tPublicKey = <contents-of-client-publickey>\n\t\t\tAllowedIPs = ##.#.#.#/##\nActivating the VPN: Start the VPN by entering wg-quick up <interface_name> in the terminal. To enable the VPN protocol to automatically start whenever the PC reboots, run the following in the terminal:\n​systemctl enable wg-quick@ <interface_name>\nFor client installation complete the following steps for each new machine that needs access to the VPN.\n\t\t\nClient VPN installation: Install the VPN protocol according to the OS-specific instructions on the WireGuard16 download page.\nAdding a client to the VPN: Take the public key from the configuration file generated during installation. Paste this key into the peer section of the server's configuration file.\nActivating the VPN: Start the VPN per the OS-specific instructions on the WireGuard16 download page.\nCloud storage\n\t\nSelect a cloud storage site to enable all recorded data streams to be stored long-term in one place. Here, an Amazon web service-based cloud storage site that was compatible with the selected data transfer protocol was used.\nImplantable neuromodulation system\n\t\nFollowing IRB and IDE guidelines, select an implantable neuromodulation system (INS)11 that allows patients to manually change their stimulation settings.\nAcquire a tablet PC and install the open-source UCSF DBS application to allow for INS recordings, reporting medications and symptoms or any other patient comments14. Configure INS data that is streamed to the tablet to be uploaded to a temporary HIPPA-compliant cloud storage endpoint, for temporary storage prior to data de-identification and offloading to long-term cloud storage.\nVideo collection system\n\t\nAcquire a PC capable of collecting and storing the desired amount of video files prior to transferring them to cloud storage. Ensure that the PC motherboard includes a trusted platform module (TPM) chip.",
    "NOTE: In this case, a PC with a 500 GB SSD, a 2 TB HDD and a 6 GB GPU was selected. A 2 TB disk ensures that videos can be buffered after a lengthy recording session or in the case of losing internet connection for a couple of days, while the single PC keeps hardware minimally intrusive in the home.\nInstall the desired OS and follow prompts to enable automatic disk encryption to ensure patient privacy and to avoid data leakage. In this case a Linux-based OS with an Ubuntu distribution was chosen for its ease of use and reliability.\nSeparately encrypt any hard disks after the OS is installed. Be sure to enable automatic disk re-mounting upon system reboots.\nConfigure the PC's on-board TPM chip to maintain access to the disk-encrypted PC after a system reboot17.\n\t\tNOTE: If using a Linux OS, be sure to select a motherboard with a TPM2 chip installed to enable this step. If a Windows OS is used, automatic disk encryption and unlocking can be handled by the Bitlocker program.\nConfigure the PC as a VPN client by following the installation steps in 1.1.4. Enable the VPN protocol to automatically start whenever the PC is rebooted as in section 1.1.3.5 to ensure that researcher computers can always remotely access the PC (recommended).\nCreate a GitHub machine user account to easily automate updates to software installed on the PC. This account serves as a webhook to automate pulling from the remote git endpoint and helps identify any updates pushed from the remote machine.",
    "Select software to schedule and control video recordings and install this on the PC. To maximize patient privacy and comfort, the selected software should include a graphical user interface (GUI) to clearly indicate when recordings are ongoing, and to enable easy termination of recordings at any point in time.\n\t\tNOTE: If desired, the authors' custom video recording application with a patient-facing GUI can be installed by downloading the application and following instructions on GitHub (https://github.com/Weill-Neurohub-OPTiMaL/VideoRecordingApp).\nSelect a monitor to indicate when videos are being recorded and to enable people to easily terminate recordings. Select a monitor with touchscreen capability so that recordings can be terminated without needing to operate a keyboard or mouse.\nInstall a remote desktop application on the PC. This enables running an application with a GUI such that the GUI remains visible on both the patient side and the remote researcher side.\n\t\tNOTE: The open-source NoMachine remote desktop application worked best for a Linux OS.\nSelect USB-compatible webcams with sufficiently high-resolution for calculating poses in the given space.\n\t\tNOTE: In this case 4k-compatible webcams were chosen, which offer multiple resolution and framerate combinations including 4k resolution at 30 fps or HD resolution at 60 fps.\nSelect robust hardware for mounting webcams in the patient's home. Use gooseneck mounts with clips to secure them to the furniture to prevent the cameras from shaking.\nSelect a data transfer protocol with encryption capability and install this on the PC. Create a configuration to access the cloud storage site, then create an encryption configuration to wrap the first configuration prior to data transfer.",
    "NOTE: In this case an open-source data transfer and file syncing protocol with encryption capability was installed18. The data transfer protocol documentation explains how to configure data transfer to cloud storage. The protocol was first installed on the VPN server and an encryption configuration was created that transfers data to the offsite cloud storage site.\nWearable-sensor data components\n\t\nSelect smart watches to be worn on each wrist of the patient to track signals including movement, accelerometry and heart rate.\n\t\t​NOTE: The Apple watch series 3 was selected with a built-in movement disorder symptom monitor that generates PD symptom scores such as dyskinesia and tremor scores.\nSelect and install software on each smart watch that can start and end recordings and can transfer data to cloud storage. Select an application which uploads all data streams to its associated online portal for researchers and clinicians to analyze19.\nimgsrc://cloudfront.jove.com/files/ftp_upload/65305/65305fig02.jpg\nFigure 2: Video recording components. The hardware components to support video data collection are minimal, including a single tower PC, USB-connected webcams, and a small monitor to display the patient-facing GUI. The monitor is touchscreen-enabled to allow easy termination of any ongoing or scheduled recordings by pressing the buttons visible on the GUI. The center of the GUI shows an image of a recording light that turns to a bright red color when video cameras are actively recording. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/65305/65305fig02large.jpg]\n2. In-home configuration\nHardware installation\n\t\nDetermine an appropriate space for mounting webcams that minimizes disruptions to the home. Determine the space through discussions with the patient; here the home office area was chosen as the optimal site for balancing recording volume against privacy.",
    "Mount webcams in the identified area on the selected mounting hardware. Clipping gooseneck mounts to nearby heavy furniture prevents cameras from shaking whenever someone steps nearby.\nPlace the PC sufficiently close to the mounted webcams such that their USB cables can connect to the PC.\nPlace the tablet PC, INS components, smart watches, and smart phones near a power outlet such that all devices can stay plugged in and are ready to use at any time.\nConfirm that the VPN is ON by running route -n in the PC terminal. If not, follow instructions to activate the VPN in section 1.1.3.5.\nStart the video recording application\n\t\nVideo recording schedule: Prior to collecting any data, discuss an appropriate recording schedule with the patient. Configure this schedule on the video recording software.\n\t\tNOTE: If using the authors' custom video recording application, instructions for setting a schedule can be found on GitHub (https://github.com/Weill-Neurohub-OPTiMaL/VideoRecordingApp#installation-guide).\nUpdate recording software: Ensure that the latest version of the selected video recording software has been uploaded to the PC using the GitHub machine user account installed in 1.4.6.\nStart video recordings: Log into the PC through the installed remote desktop software and start the video recording software.\n\t\tNOTE: If using the authors' custom video recording application, instructions for starting the application can be found on GitHub (https://github.com/Weill-Neurohub-OPTiMaL/VideoRecordingApp#installation-guide).\nVideo camera calibration\n\t\nDisable autofocus: For computing intrinsic parameters such as lens and perspective distortion, follow the instructions based on the selected OS and webcams to turn off the autofocus.\n\t\tNOTE: On Linux, webcams are accessed via the video for Linux API, which by default turns on autofocus every time the computer connected to the cameras is restarted. Configuring a script to automatically disable this is necessary to preserve the focus acquired during camera calibration for processing 3D pose.",
    "Intrinsic calibration: Acquire a 6 x 8 checkerboard pattern with 100 mm squares to support 3D calibration of pose estimation software20. Record a video from each individual webcam while a researcher angles the checkerboard in-frame of all cameras. Ensure that the checkerboard has an even number of rows and an uneven number of columns (or vice versa). This will remove ambiguity regarding rotation.\nExtrinsic calibration: Record a video from all three webcams simultaneously. Be sure that videos are recorded at the same resolution as any videos to be processed for 3D pose estimates. To ensure exact time synchronization across all videos, flash an IR LED light at the beginning and end of the recording. Use video editing software to manually sync the videos by marking frames at the onset of the LED and trimming the videos to an equal length.\nCalibration matrices: Pass the videos recorded in the previous two steps through OpenPose21 to generate intrinsic and extrinsic calibration matrices.\n\t\tNOTE: OpenPose uses the OpenCV library for camera calibration, and further instructions can be found through the documentation on the OpenPose GitHub20, 22.\n3. Data collection\nPatient instructions to start recording\n\t\nCheck device battery and power: The INS device is always ON to provide constant stimulation for the subject. To start recording of neural data, ask the patient to turn on the tablet PC and ensure that the clinician telemetry modules (CTMs) for both left and right INS devices are ON and fully charged.\nCTM placement: Place the CTMs on both sides of the chest. For maximum connectivity and to reduce packet loss, position the CTMs close to the chest implants during recordings. Additional locations to place CTMs are chest pockets of a jacket or using a specialized scarf.",
    "Activate tablet connection: Once the tablet has booted up, ask the patient to open the DBS application and select Connect, which prompts a Bluetooth connection to the CTMs and subsequently the INS devices14.\nCamera activation: Ask the patient to confirm that video cameras are connected to the PC through their USB cables, and that the cameras have turned ON.\n\t\tNOTE: If using the authors' custom video recording application, ongoing recordings are clearly indicated on the patient-facing GUI by a large image of a red light that is brightly lit. This changes to a non-lit red light when recordings are OFF. The selected webcams also have a small white indicator light.\nSmart watch activation: Ask the patient to turn ON smart watches and smart phones by holding down the Power button. Next, ask them to open the smart watch application to initiate data recording and PD symptom tracking.\nGesture-based data-alignment and recording scenarios\n\t\nWrite out any desired tasks for the patient to perform during data recordings prior to starting a data collection.\nAs multi-device clock-based synchronization for aligning time stamps can be unreliable, ask the patient to perform a gesture that can be used to synchronize the time stamps from recorded data at the onset of every new recording, even when planning to record during periods of free behavior.\n\t\t​NOTE: The authors designed a simple gesture where the patient tapped both implanted INS devices while keeping their hands within view of the cameras. This tapping creates distinctive patterns in the inertial recordings from the smart watches and the INS accelerometer and is easy to observe in videos.\nPatient instructions to end recording\n\t\nSwitch the stimulation group back to the patient's preferred clinically assigned group.\nIn the patient-facing GUI of the DBS application, enter a symptom report.",
    "Close the DBS application, which will disconnect the CTMs and conclude INS streaming.\nClose the smart watch recording application and return the CTMs, smartphones and smart watch devices back to their charging ports.\nData offloading\n\t\nTransfer raw videos to cloud storage through the data transfer protocol using an encrypted configuration. Create a cron job on the video recording PC to automatically transfer recorded videos to cloud storage through the data transfer protocol18.\n\t\tNOTE: Depending on the resolution of videos and the number of hours recorded each day, internet speed must be sufficiently high to enable all videos to be transferred to cloud storage within 24 hours. If data transfer is too slow, disk space could run out, causing additional video recordings scheduled for the following day to fail.\nSave INS data to the HIPAA-secure cloud endpoint configured in step 1.3.2. Download INS data from the HIPAA-secure cloud endpoint and deidentify the data. Save the deidentified data to external cloud storage.\n\t\t​NOTE: The open source OpenMind preprocessing code23 was utilized to deidentify data and convert it from json files to a table format. The patient's tablet was configured with a HIPAA-secure cloud endpoint for temporary storage of the raw INS data; however conceivably the same cloud storage site used for long-term storage could also be used for this step provided it is HIPAA compliant, and data are encrypted prior to offloading.\nIf desired, save a copy of the smart watch data on an external cloud storage so all data streams are accessible in one location.\n4. System characterization\nRaw data visualization: In the desired coding environment, visualize all raw data streams to ensure data was recorded and transferred appropriately without loss or corruption.",
    "NOTE: The application that was selected to manage smart watch recordings has a browser app that is helpful for visualizing smart watch data24.\nVideo frame and timestamp lags: Inspect any lags between timestamps generated from different webcams. Analyze lags by recording videos with a programmable LED light placed in-frame of all webcams.\n\tNOTE: Analysis revealed that a video segmenting function25 imported by the custom video recording app was the source of increasing timestamp lags. Recording videos without the segmenting function resulted in between-webcam frame and timestamp lags that did not increase over time (See Supplementary File 1 and Supplementary Figure 1).\n5. Post-hoc data pre-processing and alignment\nPose data\n\t\nInstall software to calculate joint position estimates from recorded videos.\n\t\tNOTE: The OpenPose library was selected since it includes hand and face tracking in both 2D and 3D.\nThe OpenPose library does not automatically handle cases where multiple people are in-frame, so use a post-processing script to ensure that each person's pose estimates are continuous from one frame to the next. OpenPose provides code to easily generate animations, either in 2D or 3D, for visual checks on pose estimation quality.\nGesture-based time alignment\n\t\nFor each INS device (left and right), follow the steps described below using the authors' data-alignment GUI (https://github.com/Weill-Neurohub-OPTiMaL/ManualTimeAlignerGUI).\n\t\t\nRead in data: Access the saved INS and smart watch accelerometry data from cloud storage for the desired data session.\n\t\t\tNOTE: An additional time series can be added if desired. Figure 3 shows the pose position of the right middle fingertip in green.\nVisualize data streams in the GUI: Use the manual time align GUI to overlay the INS accelerometry, smart watch accelerometry, and pose data.",
    "Zooming in on alignment artifacts: Zoom in to the time axis and move the viewing window to the chest tapping section of the recording. Shift the aligning time series so that the peak from the chest taps on both the INS and smart watch time series signals overlap as closely as possible.\n\t\t\t​NOTE: The GUI is designed to facilitate manual alignment of arbitrary time series to a common true time. Figure 3 shows the true time series in blue, while the aligning time series are shown in orange and green. Key guides for GUI alignment are stated on the GitHub ReadMe (https://github.com/Weill-Neurohub-OPTiMaL/ManualTimeAlignerGUI#time-alignment).\nAlignment confirmation: Move the GUI window to each of the chest tapping tasks in the recording and confirm the alignment remains consistent throughout the time series. Press the Switch Aligning button and repeat alignments on remaining data streams.\nWarning flags: To indicate whether data was missing, shifted, or other general warnings regarding data quality, set warning flags in the GUI using the D, S and F keys respectively.\nZero-normalized cross correlation (ZNCC) time alignment\n\t\nIdentify the signal most likely to be closest to true time. Usually this is either the one with the highest sample frequency or the fastest internet time refresh.\nResample the two signals to have the same temporal sampling frequency, and individually z-score both signals. This ensures that the resulting ZNCC scores will be normalized to be between -1 and 1, giving an estimate of the level of similarity between the two signals, useful for catching errors.\nCalculate the cross correlation of the second signal and the first signal at every time lag.\nIf phase information of the two signals is not important, take the absolute value of the measured cross correlation curve.",
    "NOTE: If the behavior is significantly a-periodic then the phase information is not necessary, as in this case.\nAnalyze the ZNCC curve. If there is a single clear peak, with a peak ZNCC score above 0.3 then the time of this peak corresponds to the time lag between the two signals. If there are multiple peaks, no clear peak, or the ZNCC score is low across all time lags, then the two signals need to be manually aligned.\nimgsrc://cloudfront.jove.com/files/ftp_upload/65305/65305fig03.jpg\nFigure 3: Gesture-based data alignment. The top half of the figure showcases the manual alignment GUI after aligning the three streams of data. The blue line is the smartwatch accelerometry data, the orange line is the accelerometry data from the INS, and the green line is the 2D pose position of the right middle fingertip from a single webcam. The top right shows the offset between the true time from the smart watch and INS as well as various warning flags to mark any issues that arise. In this example, the INS was 20.8 s ahead of the smartwatch. The bottom left graph is zoomed in to show the five chest taps performed by the patient for data alignment. The five peaks are sufficiently clear in each data stream to ensure proper alignment. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/65305/65305fig03large.jpg]Subscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Neuroscience"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}