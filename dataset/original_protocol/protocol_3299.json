{
  "id": 3493,
  "origin_website": "Cell",
  "title": "Protocol to implement a computational pipeline for biomedical discovery based on a biomedical knowledge graph",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nThe following are detailed instructions on how to how to implement the biomedical knowledge discovery pipeline. We show examples of each step in a tutorial Jupyter Notebook project called “Knowledge_Discovery_Pipeline.ipynb”, which can be found in our GitHub repository.\niBKH data preprocessing\nTiming: 30 min\nNote: This section introduces steps for preprocessing the iBKH BKG data.\nThis protocol uses a comprehensive BKG we built, termed iBKH.1[href=https://www.wicell.org#bib1] Figure 2[href=https://www.wicell.org#fig2] illustrates the schema of iBKH. Currently, iBKH includes 11 entity types (including anatomy, disease, drug, gene, molecule, symptom, pathway, side effect, dietary supplement ingredient [DSI], dietary supplement product [DSP], and dietary’s therapeutic class [TC]) and 45 relation types within different entity pairs such as Drug- Disease (DDi), Drug-Drug (DD), Drug-Gene (DG), etc.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/3067-Fig2.jpg\nFigure 2. Schema of iBKH knowledge graph\nEach circle denotes an entity type, and each link denotes a meta relation between a pair of entities. Of note, a meta relation can represent multiple types of relations between a specific pair of entities. For example, five potential relations including ‘Associates’, ‘Downregulates’, ‘Upregulates’, ‘Inferred_Relation’, ‘Text_Semantic_Relation’ can exist between a pair of disease and gene entities.\nIn a BKG, like the iBKH, a triplet is the smallest unit for storing information. Typically, a triplet can be formulated as   (  h , r , t  )  , where   h   and   t   are the head and tail entities, and   r   is the relation linking   h   to   t  . This section describes the steps for iBKH KG data preprocessing, i.e., extracting and formatting triplets from the iBKH, which will be used to train knowledge graph embedding models.\nOpen the Anaconda-Navigator and launch the Jupyter Notebook.\nIn the Jupyter Notebook interface, run the following codes to import required packages.\n>import pandas as pd\n>import numpy as np\n>import pickle",
    ">import torch as th\n>import torch.nn.functional as fn\n>import os\n>import sys\n>sys.path.append('.')\n>import funcs.KG_processing as KG_processing\nExtract triplets from raw data of iBKH.\nSet up the input and output file paths.\n> /∗ Input iBKH-KG data path ∗/\n>kg_folder = 'Data/iBKH/'\n> /∗ Output path ∗/\n>triplet_path = 'Data/triplets/'\n>if not os.path.exists(triplet_path):\n> os.makedirs(triplet_path)\n> /∗ Output data file path ∗/\n>output_path = 'Data/dataset/'\n>if not os.path.exists(output_path):\n> os.makedirs(output_path)\nExtract triplets of different entity pair types by running following codes.\n>KG_processing.DDi_triplets(kg_folder, triplet_path)\n>KG_processing.DG_triplets(kg_folder, triplet_path)\n>KG_processing.DPwy_triplets(kg_folder, triplet_path)\n>KG_processing.DSE_triplets(kg_folder, triplet_path)\n>KG_processing.DiDi_triplets(kg_folder, triplet_path)\n>KG_processing.DiG_triplets(kg_folder, triplet_path)\n>KG_processing.DiPwy_triplets(kg_folder, triplet_path)\n>KG_processing.DiSy_triplets(kg_folder, triplet_path)\n>KG_processing.GG_triplets(kg_folder, triplet_path)\n>KG_processing.GPwy_triplets(kg_folder, triplet_path)\n>KG_processing.DD_triplets(kg_folder, triplet_path)\nNote: This will result in a set of CSV files in the “iBKH-KD-protocol/data/triplets/”, storing triplets regarding each entity pair type.\nCombine the triplets to generate a TSV file based on the DGL-KE input requirement.\n> /∗ Specify triplet types you want to use. ∗/\n>included_pair_type = ['DDi', 'DG', 'DPwy', 'DSE', 'DiDi', 'DiG',\n'DiPwy', 'DiSy', 'GG', 'GPwy', 'DD']\n> /∗ Combine triplets ∗/\n>KG_processing.generate_triplet_set(triplet_path=triplet_path)\n> /∗ Generate DGL-KE required input triplet file ∗/\n>KG_processing.generate_DGL_training_set(triplet_path=triplet_path,∖\noutput_path=output_path)\nNote: The variable “included_pair_type” specifies a list of triplet types that we plan to use for analysis. The generated data files can be found in the folder “iBKH-KD-protocol/data/dataset/”, including “training_triplets.txt”, “validation_triplets.tsv”, and “testing_triplets.tsv”, which will be used for training and evaluating the knowledge graph embedding models, as well as “whole_triplets.tsv”, which will be used for training the final models.\nKnowledge Graph Embedding Learning\nTiming: variable depending on hardware, approximately 8–24 h\nNote: This section introduces steps for learning embedding vectors for entities and relations in the iBKH.",
    "Knowledge graph embedding aims to learn machine-readable embedding vectors for entities and relations in a BKH (e.g., the iBKH) while preserving the graph structure.9[href=https://www.wicell.org#bib9],10[href=https://www.wicell.org#bib10] We engage four deep learning-based knowledge graph embedding algorithms implemented in the DGL-KE, including TransE,11[href=https://www.wicell.org#bib11] TransR,12[href=https://www.wicell.org#bib12] ComplEx,13[href=https://www.wicell.org#bib13] and DistMult.14[href=https://www.wicell.org#bib14] This section describes the steps for training the models.\nThis step trains each knowledge graph embedding model (TransE, TransR, ComplEx, and DistMult) using the iBKH.\nOpen command line (Windows OS and UNIX OS) or terminal (MAC OS) and change directory to the project as below.\n>cd [your file path]/iBKH-KD-protocol\nTrain and evaluate the knowledge graph embedding model using below command:\n> DGLBACKEND=pytorch ∖\ndglke_train --dataset iBKH --data_path ./data/dataset ∖\n--data_files training_triplets.tsv ∖\nvalidation_triplets.tsv ∖\ntesting_triplets.tsv ∖\n--format raw_udd_hrt --model_name [model name] ∖\n--batch_size [batch size] --hidden_dim [hidden dim] ∖\n--neg_sample_size [neg sample size] --gamma [gamma] ∖\n--lr [learning rate] --max_step [max step] ∖\n--log_interval [log interval] ∖\n--batch_size_eval [batch size eval] ∖\n-adv --regularization_coef [regularization coef] ∖\n--num_thread [num thread] --num_proc [num proc] ∖\n--neg_sample_size_eval [neg sample size eval] ∖\n--save_path ./data/embeddings --test\nNote: We use multiple measurements to evaluate model performances including: HITS@k, the average number of times the positive triplet is among the k highest ranked triplets; Mean Rank (MR), the average rank of the positive triplets; and Mean Reciprocal Rank (MRR), the average reciprocal rank of the positive instances. Higher values of HITS@k and MRR and a lower value of MR indicate good performance, and vice versa. Some useful arguments of the DGL-KE command are listed in Table 1[href=https://www.wicell.org#tbl1]. Detailed instructions for the DGL-KE commands can be found at: https://dglke.dgl.ai/doc/train.html[href=https://dglke.dgl.ai/doc/train.html].\ntable:files/protocols_protocol_3067_1.csv\nOnce the model can achieve a desirable performance in the testing set, we can re-train the model using the whole dataset by running:\n> DGLBACKEND=pytorch ∖\ndglke_train --dataset iBKH --data_path ./data/dataset ∖",
    "--data_files whole_triplets.tsv ∖\n--format raw_udd_hrt --model_name [model name] ∖\n--batch_size [batch size] --hidden_dim [hidden dim] ∖\n--neg_sample_size [neg sample size] --gamma [gamma] ∖\n--lr [learning rate] --max_step [max step] ∖\n--log_interval [log interval] ∖\n-adv --regularization_coef [regularization coef] ∖\n--num_thread [num thread] --num_proc [num proc] ∖\n--save_path ./data/embeddings\nNote: This will generate two output files for each model: “iBKH_[model name]_entity.npy”, containing the low dimension embeddings of entities in iBKH and “iBKH_[model name]_relation.npy”, containing the low dimension embeddings of relations in iBKH. These embeddings can be used in downstream BKD tasks.\nWe run above procedures based on TransE, TransR, ComplEx, and DistMult, respectively, to gain embedding vectors of entities and relations in the iBKH.\nNote: The user may repeat the Step 4b multiple times to find the optimal hyperparameters of each model. Here, we share the optimal hyperparameter values we found in our experiments as listed in Table 2[href=https://www.wicell.org#tbl2]. For simplicity, the user can directly use the suggested hyperparameter values to train the models. In addition, running time of the knowledge graph embedding procedure varies, depending on hardware used. For our experiments, we used a machine equipped with an Intel i7-7800X CPU, boasting 6 cores and 12 threads, with a fundamental clock speed of 3.5 GHz, coupled with 62 GB of RAM. Training the four knowledge graph embedding models within the dataset took approximately 8 hours in our experiment. The required running time could extend to 24 hours or even more if a user expects to tune the models to find the optimal hyperparameters for enhancing model performance.\ntable:files/protocols_protocol_3067_2.csv\nBiomedical knowledge discovery – biomedical hypothesis generation\nTiming: 30 min\nNote: This section introduces the implementation of BKD based on knowledge graph embeddings learned from iBKH.\nHere, we showcase a case study of drug repurposing hypothesis generation for Parkinson’s disease (PD).",
    "Turn to the Jupyter Notebook interface and run the following script to import required package packages.\n>from funcs.KG_link_pred import generate_hypothesis,∖\ngenerate_hypothesis_ensemble_model\nDefine the PD entity using\n> PD = [\"parkinson's disease\", \"late onset parkinson's disease\"]\nNote: Here we collect a list of PD terms. These PD terms can be obtained in the entity vocabularies in the “data/iBKH/entity” folder.\nThe task is to predict drug entities that don’t have “treats” and “palliates” relationships with PD in the iBKH but can potentially treat or palliate PD. Therefore, we define a relation type list:\n> r_type = [\"Treats_DDi\", \"Palliates_DDi\"]\nNote: More relation types can be found in the “data/iBKH/relation” folder.\nPredict repurposable drug candidates for PD (in this example, we use embedding vectors based on the TransE model for prediction):\n> proposed_df = generate_hypothesis(target_entity=PD,\ncandidate_entity_type='drug',\nrelation_type=r_type,\nembedding_folder='data/embeddings',\nmethod='transE_l2',\nkg_folder = 'data/iBKH',\ntriplet_folder = 'data/triplets',\ntopK=100, save_path='output',\nsave=True, without_any_rel=False)\nRunning the above code will result in an output CSV file within the “output” folder, which stores top-100 ranked repurposable drug candidates for PD based on the TransE model.\nNote: Please refer to Table 3[href=https://www.wicell.org#tbl3] for detailed information regarding arguments of the function.\ntable:files/protocols_protocol_3067_3.csv\nUsing the code in Step 8 can make predictions based on a single knowledge graph embedding model (TransE in the example). To enhance prediction performance, we also proposed an ensemble model, which combines the four embedding algorithms to make predictions. In our preliminary work, we have demonstrated that the ensemble model can improve knowledge discovery performance in iBKH.1[href=https://www.wicell.org#bib1] The following code introduces the usage of the ensemble model to predict repurposable drug candidates for PD.\n> proposed_df = generate_hypothesis_ensemble_model (target_entity=PD,\ncandidate_entity_type='drug',\nrelation_type=r_type,\nembedding_folder='data/embeddings',\nkg_folder = 'data/iBKH',\ntriplet_folder = 'data/triplets',\ntopK=100, save_path='output',\nsave=True, without_any_rel=False)",
    "Running the above code will result in an output CSV file within the “output” folder, which stores top-100 ranked repurposable drug candidates for PD based on the ensemble model.\nNote: Please refer to Table 3[href=https://www.wicell.org#tbl3] for detailed information regarding arguments of the function.\nPrediction result interpretation\nTiming: 30 min\nNote: This section introduces the procedure of interpreting the prediction results based on the iBKH.\nWe extract the shortest paths that connect the target entity (e.g., PD) with the predicted entities (e.g., the predicted repurposing drug candidates of PD) to generate a contextual subnetwork.\nTaking the PD drug repurposing task as an example, we can generate the contextual subnetwork surrounding PD and some predicted repurposing drug candidates as below.\nImport required package.\n>from funcs.knowledge_visualization as kv\nSpecify the predicted drug candidates to interpret. Here, we focus on the top four candidates predicted using the ensemble model, including Glutathione, Clioquinol, Steroids, and Taurine.\n> drug_list = ['Glutathione', 'Clioquinol', 'Steroids', 'Taurine']\nCreate a contextual subnetwork linking PD and the drug candidates.\n> kv.subgraph_visualization(target_type='Disease',\ntarget_list=PD,\npredicted_type='Drug',\npredicted_list=drug_list,\nneo4j_url = \"neo4j://54.210.251.104:7687\",\nusername = \"neo4j\", password = \"password\",\nalpha=1.5, k=0.8, figsize=(15, 10),\nsave=True)\nThis will result in a figure saved as a PDF file in the “output” folder. Please refer to Table 4[href=https://www.wicell.org#tbl4] for detailed information regarding arguments of the function.\ntable:files/protocols_protocol_3067_4.csv\nNote: The shortest path query is based on iBKH deployed using the Neo4j, an efficient graph database. Please refer to https://github.com/wcm-wanglab/iBKH#neo4j-deployment[href=https://github.com/wcm-wanglab/iBKH] for detailed information if you want to create your own iBKH Neo4j instance."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Health Sciences",
    "Bioinformatics"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Bioinformatics & Computational Biology"
  ]
}