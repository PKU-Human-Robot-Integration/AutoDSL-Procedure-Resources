{
  "id": 19905,
  "origin_website": "Wiley",
  "title": "ezTrack—A Step-by-Step Guide to Behavior Tracking",
  "procedures": [
    "Since the original publication of ezTrack, new features have been added. These include a wire-removal algorithm to improve the tracking of animals attached to wires or tethers, the ability to mask portions of the field of view that the user would like ignored, and functionality to down-sample videos to speed processing. To utilize these new features, and any features added subsequent to this publication, ensure that the latest ezTrack version and packages are installed. For the most up-to-date installation instructions, visit https://github.com/DeniseCaiLab/ezTrack[href=https://github.com/DeniseCaiLab/ezTrack].\nNecessary Resources\nComputer system (see Strategic Planning)\nMiniconda (https://docs.conda.io/en/latest/[href=https://docs.conda.io/en/latest/]) or Anaconda (https://www.anaconda.com[href=https://www.anaconda.com])\nezTrack (https://github.com/DeniseCaiLab/ezTrack[href=https://github.com/DeniseCaiLab/ezTrack])\nezTrack installation\n1. Download Miniconda/Anaconda.\nInstallation of ezTrack requires the user to download the free package management system Miniconda or its larger counterpart Anaconda (https://www.anaconda.com[href=https://www.anaconda.com]). For those with minimal programming experience, using the PKG installer is likely to be more intuitive.\n2. Create ezTrack environment.\nAfter installing Miniconda/Anaconda, the user can create the ezTrack Conda environment, which will contain all of the package dependencies necessary for ezTrack to run. This is achieved by entering the following command into one's Terminal (on OSX/Linux) or Anaconda Prompt (on Windows):\nconda create -y -n ezTrack -c conda-forge python=3.6 pandas=0.23.0 matplotlib=3.1.1 opencv=3.4.3 jupyter=1.0.0 holoviews=1.13.5 scipy=1.2.1 bokeh=2.1.1 tqdm\n3. Download ezTrack files.\nFrom the ezTrack Github page (https://github.com/DeniseCaiLab/ezTrack[href=https://github.com/DeniseCaiLab/ezTrack]), download all files necessary to run ezTrack onto your computer: On the main page for ezTrack, click the button to “Clone or download,” and download the zip folder onto your hard drive. The unzipped folder is called “ezTrack-master.” Alternatively, use git commands if you are familiar with them. The user should feel free to rename this folder and place it wherever they would like on their computer.\nIntroduction to Jupyter Notebooks",
    "ezTrack's modules are run within what are called Jupyter Notebook files (extension .ipynb). When opened, these files contain a mixture of headings and instructions, as well as chunks of iPython code, called “cells.” When the code in a cell is executed, or “run,” relevant output such as images or plots are displayed immediately below it. Most cells of code need not be modified. When minimal alterations are necessary, instructions are provided immediately above each cell, as well as in this publication. Cells of code have been numbered, and it is assumed that the user will step through the code in order. However, there are two exceptions to this. First, some cells are marked as optional and can be skipped. Second, although the user should not skip from Cell 1 to Cell 10 without running the intervening cells, it is often useful to return to a previous step. For example, the user might first run up to Cell 8, decide they would like to tweak parameters in Cell 5, and then re-run Cells 5-8.\nBelow we point the reader to several pertinent features of the Jupyter Notebook files in ezTrack, which will likely be all that is needed to get started. A screenshot is provided in Figure 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0001], and below we describe the significance of each labeled region. We refer the reader to the internet for extensive tutorials on how to use Jupyter Notebook.\n         \nA.The File menu.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/85f807ab-e3a8-4ece-8aeb-a58dfe987536/cpz1255-fig-0001-m.jpg</p>\nFigure 1",
    "Key features/utilities of ezTrack's Jupyter Notebook files. (A) The File menu can be used to save a snapshot of output by printing the document. (B) The Run button can be used to run code cells. (C) The Kernel menu can be used to restart the notebook and clear all output. (D) “Cells” of code are boxed in gray and can be run with minimal user changes. (E) The size of ezTrack output can be changed by altering output size, the first line of any cell that produces image/plot output. (F) ezTrack's output has a range of interactive tools, such as panning, zooming, refreshing, and saving.\nJupyter Notebook's File menu contains several relevant options. In particular, the user can opt to print the notebook file when they are done, which allows them to save a snapshot of all their settings and the session's output. The user can also save the notebook file with a different name, such as the name of the experiment they are scoring videos from. In this way, the user can keep a more permanent record of their workflow. Note that all selected parameters will also be saved in ezTrack's .csv output files.\n         \nB.The Run button.\nAfter clicking on a cell of code to select it, click the Run button to execute the selected cell. Alternatively, use the shortcut, Shift + Enter.\n         \nC.The Kernel menu.\nThe Jupyter Notebook Kernel holds a memory of all defined variables, loaded packages, and printed output for a session. If the user would like a clean slate, they can select Kernel, then Restart & Clear Output. All output will be removed from the notebook, and variables will be cleared from computer memory.\n         \nD.Cells of code.",
    "Each cell of code is visible as a gray box, and the user can change the code as they see fit. For text changes to take effect, the cell must be run. While a cell is actively being run, a “*” will appear in the brackets to the left of the cell. When complete, a number will appear, corresponding to the order in which that cell was run (e.g., if a user starts a session and runs Cell 1 twice, a 2 will appear when the code is done being executed). This helps the user track which cells they have run and in what order.\n         \nE.Output size.\nAll cells of code producing image output start with the line %%output size=100. This number can be increased or decreased to change the image output size. Notably, if the dimensions of the video frame are very small in one dimension (such as on a long linear track), sometimes no image will appear. Increasing the output size to 200 or more will remedy this.\n         \nF.Interactive image tools.\nAll ezTrack image output is interactive and savable. The user can use the magnifying glass with a box to select a region of an image to zoom in on. The magnifying glass with a scroll bar can be used to pan in and out (note that placing the cursor over the axis text enables the user to pan in and out selectively on that axis). The circular arrows are a refresh button. The save button allows the user to save the image as a .png file.",
    "ezTrack's Location Tracking Module assesses an animal's location across the course of a single, continuous session. The module can be used to calculate the amount of time an animal spends in user-defined ROIs and the distance that it travels. It uses the animal's center of mass to determine the animal's location in each frame of the video, and saves frame-by-frame location data in convenient and accessible .csv files. Summary files can also be created specifying the total distance traveled in user-defined time bins and the proportion of time in user-defined ROIs. The Location Tracking Module can be used either to process an individual video file or to process several video files in batch.\nThe protocol for utilizing the Location Tracking Module to analyze a single video (LocationTracking_Individual.ipynb) is outlined in detail below. This protocol provides the user with an array of visualization tools to explore parameter settings to confirm accurate tracking. We highly recommend using this protocol on a few individual videos from each experiment before proceeding to process multiple videos in batch. Given the large degree of overlap between the processing of individual files and batch processing, we will only briefly discuss batch processing (LocationTracking_BatchProcess.ipynb) and highlight the differences from individual processing.\nNecessary Resources\nPlease see Strategic Planning regarding hardware/software considerations\nOpen the relevant Location Tracking file\ni. Activate the ezTrack environment.\nFor OSX/Linux users, open a terminal. If using Windows, open Anaconda Prompt. Type the following command and then hit Enter:\nconda activate ezTrack\nii. Launch Jupyter Notebook.\nWithin Anaconda Prompt/Terminal, type the following command and then hit Enter:\njupyter notebook\niii. Select Notebook file from Jupyter Notebook.\nWithin Jupyter Notebook, navigate to the folder containing the ezTrack files downloaded from the Github site, and open either LocationTracking_Individual.ipynb or LocationTracking_BatchProcess.ipynb.\nProcess an individual file (LocationTracking_Individual.ipynb)",
    "Note that the numbering of the following instructions corresponds to the numbered blocks of code in the Jupyter Notebook file (called “cells”). Some of these cells are optional, as noted both here and in the Jupyter Notebook file itself.\n1. Load necessary packages.\nRunning Cell 1 loads all necessary packages and does not need to be modified.\n2. Set directory and file information, and specify ROI names, if any.\nIn Cell 2, the user specifies the file's location and other pertinent information about the video, as described below (see Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0002]). After entering this information, the user must run the cell.\ndpath: The directory path of the folder containing the video to be processed.\nIf you are using a Windows path with backslashes, place an “r” in front of the directory path to avoid an error (e.g., r’\\Users\\DeniseCaiLab\\Videos’).\nfile: The filename of the video, including the file extension.\nstart: The frame of the video on which to begin processing. 0 is the first frame. By knowing the video's frame rate (e.g., 30 frames/sec), the user can start processing the video at a specific timepoint. For instance, to begin processing 20 sec into the video, once the animal has been placed into the arena, one could enter 600 if the frame rate is 30 frames/sec. If you are uncertain of your video's frame rate, this information will be printed by ezTrack when Cell 3 is run.\nend: The frame of the video on which to end processing. If the user would like to process from the start frame to the end of the video, this can be set to None.",
    "region_names: If the user would like to measure the time spent in ROIs, a list containing the names of the ROIs should be provided. A Python list is defined by a set of square brackets, and each ROI name should be placed in quotations, separated by a comma. If no ROIs are to be defined, this can be set to None (i.e., ‘region_names’: None).\ndsmpl: The amount by which to down-sample each frame. If processing is going slowly, down-sampling can help. A value of 1 indicates no down-sampling, while a value of 0.25 indicates that each frame will be down-sampled to one-quarter its original size. Note that if down-sampling is performed, all pixel coordinate output will be in the dimensions of the down-sampled video.\nstretch: Allows the user to alter the aspect ratio of the presented output. This is useful when videos have irregular dimensions and are difficult to see (e.g., an aspect ratio of 1:100). The width/height will be scaled by the factor provided. Note that this only affects the appearance of visualizations and does not modify the video or the interpretation of the output.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/930dbf63-ca5d-4a46-acdd-f914b739b8e5/cpz1255-fig-0002-m.jpg</p>\nFigure 2\nExample of how to set video information in ezTrack's Location Tracking Module.\n3. Load video, and crop frame if desired.\n         \nRun Cell 3 to load the video. The video's starting frame will appear as output, and various video properties will also be printed, including the filename, the number of frames in the video, the frame rate, and the frame dimensions.",
    "The user can then define how to crop the video at this stage, if desired (see Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0003]). Select the box selection tool below the presented image (the square with a plus sign) and double-click the image to begin defining the region you would like your video cropped to. Double-click again to finalize the region. If the region is not satisfactory, rerun Cell 3 and follow the steps for cropping again.\nIf the user would like to alter the size of the image output, the number in the first line of Cell 3 (i.e., %% output size = 100) can be increased or decreased accordingly. If the dimensions of the video frame are very small in one dimension (such as on a long linear track), sometimes no image will appear. Increasing the output size to 200 or more will remedy this. If a “FileNotFoundError” emerges, the user should double-check that the directory path and filename are correctly specified.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/8de5ea0d-ff21-4349-9eca-84a4c97ccbf5/cpz1255-fig-0003-m.jpg</p>\nFigure 3\nExample of how to crop the video frame in ezTrack's Location Tracking Module.\n4. Mask regions (optional).\nCell 4 can be used to select one or more regions that the user would like ignored during processing (see Fig. 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0004]). This can be helpful if there is movement in that portion of video due to the experimenter stepping into the field of view or other external movements. Any polygonal region can be drawn, and as many regions as desired can be masked. Because the selected regions will be ignored, be sure not to select any region that the tracked animal can enter into.\n         \na.Run Cell 4. The first frame of the video will appear, with any cropping from the prior cell applied.",
    "b.To draw a region to be ignored, double-click on the image to identify its first vertex, single-click to add each additional vertex, and double-click again to finalize each region.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/fdf6213b-e58c-4db1-86a7-477ecf94f80b/cpz1255-fig-0004-m.jpg</p>\nFigure 4\nExample of how to mask portions of the field of view from analysis in ezTrack's Location Tracking Module.\n5. Define the reference frame for location tracking.\nFor location tracking to work, a view of the arena without an animal must be generated (Fig. 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0005]). There are two ways to do this. Option 1 provides a method to remove the animal from the video. This works well provided that the animal does not stay in the same location for >50% of the session. Alternatively, Option 2 allows the user to define a video file, in the same folder as the video being scored, that does not have an animal in it. Option 1 is generally recommended because it is simpler for the user to acquire and ensures the reference frame is consistent with the video. If an alternative video is used, we recommend that the user acquire a short video (e.g., 5 s) of the empty arena on each experimental day.\n         \na.Option 1: Create reference frame by removing animal from video.\nRun the cell under Option 1. An image of the environment lacking an animal should appear (Fig. 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0005]).\n         \nb.Option 2: Specify a video of the empty box (preferable only if the animal moves very little).\nRun the cell under Option 2 after specifying the name of the alternative file within the cell. An image of the environment lacking an animal should appear (Fig. 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0005]).",
    "If desired, the number of random frames can be changed with the variable “num_frames” in the relevant cell. If the animal is biased towards one spot, the median of a large number of frames may include the animal. Using a smaller number of example frames can increase the likelihood that the animal will not show up in the average.\nAlternatively, the user can manually define the frames to be used by providing a list or Numpy array of frames. In this way, the user can ensure that the animal will be in different positions on the selected frames. For example, the following code would use the specified frames to generate the reference:\nframes = [0, 100, 500, 1000, 4000]\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4da67541-a004-4b00-bafe-d6a8be9c999a/cpz1255-fig-0005-m.jpg</p>\nFigure 5\nExample output from when reference frame is generated from a video containing an animal in ezTrack's Location Tracking Module.\n6. Use interactive plot to define regions of interest (optional).\n         \nRun Cell 6. The reference frame will appear (Fig. 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0006]).\nThe user can now draw the regions of interest in the order listed above the reference frame. To draw a region, double-click on the image to identify its first vertex, single-click to add each additional vertex, and then double-click again to close this region.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/170e8509-5528-4685-9f7f-47e6712f61a5/cpz1255-fig-0006-m.jpg</p>\nFigure 6\nExample of how to select ROIs in ezTrack's Location Tracking Module.\n7. Define scale for distance calculations.\nezTrack initially tracks distance in pixel units. To convert from pixel units to a physical scale (inches, centimeters, etc.), the user can select any two points in the reference frame (Cell 7a), and then once supplied with their true physical distance (Cell 7b), ezTrack will provide distance measurements at the desired scale.\n         \nSelect two points of known distance.\n               \niRun Cell 7a; the reference frame will appear.",
    "iiClick on any two points on the reference frame. The distance between them will be presented in pixel units (Fig. 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0007]).\nDefine the real-world distance between points.\nSet the values of the variables “distance” and “scale” in Cell 7b; then run the cell. If the distance between the two selected points is 100 centimeters, the user would set the code as follows:\ndistance = 100\nscale = ‘cm’\nNote that “scale” will take any set of characters within quotations, so “centimeters” would have worked just as well as “cm.”\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/38c5fc16-ac42-43ea-9a61-50b1cb7ef8a7/cpz1255-fig-0007-m.jpg</p>\nFigure 7\nExample of how to select points in the Location Tracking Module to define the distance scale.\n8. Track location.\nCells 8a-d are used to test and choose tracking parameters, and once selected, track the location of an animal across an entire video.\n         \nSet Location Tracking parameters.\nIn Cell 8a, the user defines a number of tracking parameters (see Fig. 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0008]), each of which is described below. The defaults will often work well. However, in Cell 8b, the user can quickly visualize the accuracy of tracking with these parameters on a subset of video frames and modify parameters as they see fit. The idea is to cycle between Cell 8a and Cell 8b until one is happy with the chosen tracking parameters, and then proceed to track the entire video. Each of these parameters is described below, and the influence of changing several parameters can be seen in Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0009].",
    "loc_thresh: The parameter loc_thresh represents a percentile threshold and can take on values between 0 and 100. Each frame is compared to the reference frame. Then, to remove the influence of small fluctuations, any differences below a given percentile (relative to the maximum difference) are set to 0. The impact of changing this parameter can be seen in Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0009].\nuse_window: The parameter use_window is incredibly helpful if objects other than the animal temporarily enter the field of view during tracking (such as an experimenter's hand manually delivering a stimulus or reward). When use_window is set to True, a square window with the animal's position on the prior frame at its center is given more weight during searching for the animal's location (because an animal presumably cannot move far from one frame to the next). In this way, the influence of objects entering the field of view can be avoided. If use_window is set to True, the user should consider window_size and window_weight.\nwindow_size: This parameter only affects tracking when use_window is set to True. This defines the size of the square window surrounding the animal that will be more heavily weighted in pixel units. We typically set this to two to three times the animal's size (if an animal is 100 pixels at its longest, we will set window_size to 200). Note that to determine the size of the animal in pixels, the user can reference any image of the arena presented in ezTrack, which has the pixel coordinate scale on its axes.",
    "window_weight: This parameter affects tracking only when use_window is set to True. When window_weight is set to 1, pixels outside the window are not considered at all; at 0, they are given equal weight. Notably, setting a high value that is still not equal to 1 (e.g., 0.5-0.9) should allow ezTrack to more rapidly find the animal if, by chance, it moves out of the window.\nmethod: The parameter method determines the luminosity of the object ezTrack will search for relative to the background and accepts values of “abs,” “light,” and “dark.” The option abs does not take into consideration whether the animal is lighter or darker than the background and will therefore track the animal across a wide range of backgrounds, whereas light assumes the animal is lighter than the background, and dark assumes the animal is darker than the background. Option abs generally works well, but there are situations in which you may wish to use the others. For example, if a tether is being used that is opposite in color to the animal (e.g., a white wire and a black mouse), the abs method is much more likely to be biased by the wire, whereas dark will look for the darker mouse (see the example in Fig. 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0009]).\nrmv_wire: When rmv_wire is set to True, an algorithm is used to attempt to remove wires from the field of view. If rmv_wire is set to True, the user should consider wire_krn.",
    "wire_krn: This parameter only affects tracking when rmv_wire is set to True. This value should be set between the width of the wire and the width of the animal, in pixel units. As can be appreciated from Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0009], when this value is too small, the influence of the wire is not mitigated at all. When the value is too large, the animal is removed from the video.\n         \nb.Display examples of location tracking to confirm parameters (optional).\nAfter selecting parameters in the previous step, the user can run Cell 8b to visualize the impact of these parameters on a small subset of frames (the default is four frames).\nA random selection of frames is analyzed, and the frames are then presented, with the animal's position marked with a crosshair. Side by side with the original image, the user can also visualize the features of each frame that are being used by ezTrack to mark the location of the animal. See Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0009] for examples of when tracking is done well and when it is done poorly.\nOf note, because frames are processed individually, the impacts of use_window, window_size, and window_weight are not visible here, since the window weighting algorithm requires knowledge of the animal's position on the prior frame. The impacts of the window weighting algorithm are better observed in Cell 10, where a video of tracking can be played.\n         \nc.Track location and save results to a .csv file.",
    "By running Cell 8c, the entire video will be tracked. The resulting .csv file (the video filename with the suffix _LocationOutput) will contain the following information for each frame: the animal's x and y coordinates in pixel units, whether or not the animal is in each supplied region of interest (True/False, for each), and the distance the animal has traveled relative to its position on the previous frame in both pixel units and any other scale the user might have provided. The first five rows of this file will be presented in the notebook.\n         \nd.Display animal distance/location across a session (optional).\nRun Cell 8d to acquire several visualizations of the performed tracking (Fig. 10[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0010]). This includes a plot titled “Motion Trace,” which displays the animal's position on every frame atop the reference frame. This is useful in identifying whether any external movement has biased tracking. For instance, one would not expect the animal to ever be located outside of the arena. A plot titled “Heatmap” is also presented, which allows the user to visualize the relative amount of time the animal has spent in each area of the arena. Lastly, the plot “Distance Across Session” shows how much the animal moves on a frame by frame basis. This plot is particularly useful for detecting poor tracking, often evident in sudden leaps in distance traveled. The exact distance of such a leap will depend upon the video, but will likely be several standard deviations above the rest of the video. When such deviations are seen, it is useful to play video of the affected frames in Cell 10 to determine the source of the error.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f95b734a-5238-405e-88ac-56dede03fe6d/cpz1255-fig-0008-m.jpg</p>\nFigure 8\nExample of how to set tracking parameters in the Location Tracking Module.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/cc78d71c-fdee-459b-850f-1287fe734dcf/cpz1255-fig-0009-m.jpg</p>\nFigure 9",
    "Examples of output obtained using various tracking parameters. Above is the original grayscale image. Below, features of the image that ezTrack utilizes to define the animal's center are highlighted (from Cell 8b output). The parameters loc_thresh, method, and wire_krn were manipulated in the top, middle, and bottom rows, respectively. For each row, the center option is most ideal. Note that the ideal option will vary from one video setup to the next.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/196f4546-2307-4bd6-9dd7-ddfb691c48f0/cpz1255-fig-0010-m.jpg</p>\nFigure 10\nExample output from Cell 8d in the Location Tracking Module. After tracking, the animal's position across the session is marked atop the reference frame (top left), a heatmap of the animal's position is created (top right), and the distance the animal moves across the session is also plotted (bottom left).\n9. Create binned summary report and save (optional).",
    "In Cell 9, the user is able to obtain summary information, either for defined time bins or the entire tracking period. For each time bin requested, the total distance traveled and the proportion of time in any defined regions of interest will be saved to a .csv file (a file with the video filename with the suffix _SummaryStats). Refer to Figure 11[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0011] for an example of how to define bins. The name of each bin should be placed in quotation marks, while the numbers inside the parenthesis are the frames you want the bin to begin and end with. A comma should separate each bin. Note that defined frames are relative to the defined start frame (from Cell 2), such that 0 refers to the first tracked frame. If the user wants a summary of the entire tracking period, set bin_dict equal to None. In addition, if it's easier for the user to think in seconds, as opposed to frames, multiplication can be done within the parenthesis. For example, assuming a frame rate of 30, the following would create bins for seconds 0-100, 100-200, and 200-300:\nbin_dict = {\nˈBinName1ˈ: (0*30, 100*30),\nˈBinName2ˈ: (100*30, 200*30),\nˈBinName3ˈ: (200*30, 300*30)\n}\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/903c1d94-9dde-4aa7-a1fb-4eb578c38d7d/cpz1255-fig-0011-m.jpg</p>\nFigure 11\nExample of how to define bins for extracting summary information in ezTrack's Location Tracking Module.\n10. View video of tracking.\nAfter tracking has been performed, Cell 10 allows the user to play a portion of the video back with the animal's position marked on each frame with a crosshair. After defining the display parameters by setting the values of display_dict, running Cell 10 will result in the video being played (see Fig. 12[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0012]). Below we outline the meaning of each of these parameters.",
    "start: The frame video playback is to be started on. Note that this is relative to the start of tracking, where 0 is the first tracked frame (the defined start frame from Cell 2)\nstop: The frame video playback is to end on. Note that this is relative to the start of tracking, where 0 is the first tracked frame (the defined start frame from Cell 2)\nfps: The speed of video playback. Must be an integer. Video playback may also be slower depending upon computer speed. This is because “fps” sets the time imposed between presented frames but cannot control the duration of time it will take a user's computer to present each frame.\nresize: If the user wants the output to be larger or smaller, or they want the aspect ratio to be different, resize can be supplied as in the following example:\nˈresizeˈ: (100,200)\nHere, the first number corresponds to the adjusted width of the frame, whereas the second number corresponds to the adjusted height. Both numbers reflect pixel units and should be integers. Set resize equal to None if no resizing is to be done.\nsave_video: To save the video clip, set “save_video” to True.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a1360af5-3c3a-4ced-bdbb-0cff0784ac25/cpz1255-fig-0012-m.jpg</p>\nFigure 12\nExample of how to play a portion of the video back with the animal's position marked on each frame with a crosshair in ezTrack's Location Tracking Module.\nProcess multiple files in a batch (LocationTracking_BatchProcess.ipynb)",
    "Batch processing with ezTrack works much the same way as individual processing but is designed to be executed on all video files of a specified type within a given folder. The user will find that many of the steps are identical to individual processing, with the following exceptions. First, rather than specifying the directory and the name of an individual file, the user supplies the directory and the file extension of the videos to be processed. ezTrack will process all files of the specified file type within the given folder. Second, when batch processing, all cropping parameters, bin information, and regions of interest are assumed to be the same. The user should therefore be mindful that videos should have a consistent field of view and be of a roughly similar length. Third, the user is still able to crop the videos, supply regions of interest, mask portions of the field of view, and supply distance scales—however, these are based upon the first video file in the folder. Again, be mindful to ensure that videos are roughly the same, although minor discrepancies in the field of view are permissible. Fourth, the batch processing notebook file does not contain visualization tools to optimize tracking parameters, as it is assumed that these have been previously explored while processing a few individual files. However, at the end of processing, heatmaps and motion trace plots for each video are displayed.",
    "ezTrack's Freeze Analysis Module assesses an animal's motion and freezing across the course of a single, continuous session. The Freeze Analysis Module tracks the number of pixels that fluctuate from one frame to the next and uses this as an index of animal motion. This metric arguably provides greater sensitivity than measuring the animal's center of mass, as is done in the Location Tracking Module, while sacrificing information regarding animal position. Subsequently, the user defines a threshold for motion (both in magnitude and duration) below which freezing is defined. In this way, motion is converted into a binary variable, freezing. The Freeze Analysis Module saves frame by frame motion/freezing data in convenient and accessible .csv files. Summary files can also be created specifying the average motion and freezing in user-defined time bins. The Freeze Analysis Module can be used to process an individual video file or several video files in batch.",
    "Below, we first describe the protocol for video calibration (using FreezeAnalysis_Calibration.ipynb), which allows the user to better separate pixel fluctuations that result from animal movement from those due to stochastic fluctuations inherent in video acquisition. For this reason, if the user would like to utilize calibration, it is important to record a video of the empty arena before starting behavioral recording. A single 5- to 10-s video is typically sufficient. Next we outline the protocol for utilizing the Freeze Analysis Module to analyze a single video, FreezeAnalysis_Individual.ipynb. This protocol provides the user with an array of visualization tools to explore parameter settings in order to confirm accurate measurement of freezing. We highly recommend using this protocol on a few individual videos from each experiment before proceeding to process multiple videos in batch. Lastly, given the large degree of overlap between processing individual files and batch processing, we briefly discuss batch processing (FreezeAnalysis_BatchProcess.ipynb), with differences from individual processing highlighted.\nNecessary Resources\nPlease see Strategic Planning regarding hardware/software considerations\nOpen the relevant Freeze Analysis file\ni. Activate the ezTrack environment.\nFor OSX/Linux users, open a terminal. If using Windows, open Anaconda Prompt. Type the following command and then hit Enter:\nconda activate ezTrack\nii. Launch Jupyter Notebook.\nWithin Anaconda Prompt/Terminal, type the following command and then hit Enter:\njupyter notebook\niii. Select Notebook file from Jupyter Notebook.\nWithin Jupyter Notebook, navigate to the folder containing the ezTrack files downloaded from the Github site, and open either FreezeAnalysis_Calibration.ipynb, FreezeAnalysis_Individual.ipynb, or FreezeAnalysis_BatchProcess.ipynb.\nCalibration",
    "Even when there is no movement in the field of view, pixel brightness values fluctuate slightly from frame to frame. As such, values would be greatly biased if any pixel fluctuation was counted as animal motion. By determining the magnitude and distribution of these random fluctuations when no animal is present, one can more easily define pixel fluctuations that are actually due to the movement of an animal. Provided that the user has a short video of their environment without an animal (e.g., 5-10 sec), the user can visualize this distribution and use this to select a cutoff for subsequent steps.\nNote that the numbering of the following instructions corresponds to the numbered blocks of code in the Jupyter Notebook file (called “cells”). Some of these cells are optional, as noted both here and in the Jupyter Notebook file itself.\n1. Load necessary packages.\nRunning Cell 1 loads all necessary packages and does not need to be modified.\n2. Set directory and file information.\nIn Cell 2, the user specifies the video file location and other pertinent information about the video, described below (see Fig. 13[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0013]). After entering this information, run the cell.\ndpath: The directory path of the folder containing the video to be processed.\nIf you are using a Windows path with backslashes, place an “r” in front of the directory path to avoid an error (e.g., r’\\Users\\DeniseCaiLab\\Videos’).\nfile: The filename of the video, including the file extension.",
    "cal_frms: The number of frames in the video to calibrate based upon. If the video is 10 sec long and was shot at 30 frames/sec, the user might set this to 300. If the user is unsure of the frame rate, this information will be output by ezTrack when Cell 3 is run. Note that for longer videos, it is not necessary to calibrate based upon the total number of frames in the video.\ndsmpl: The amount to down-sample each frame. If processing is going slowly, down-sampling can help. A value of 1 indicates no down-sampling, while a value of 0.25 indicates that the frame will be down-sampled to one-quarter the original size. Note that if the user chooses to down-sample in the calibration step, this same down-sampling factor should be used for processing behavioral videos, and vice versa.\nstretch: Allows the user to alter the aspect ratio of the presented output. This is useful when videos have irregular dimensions and are difficult to see (e.g., an aspect ratio of 1:100). The width/height will be scaled by the factor provided. Note that this only affects the appearance of visualizations and does not modify the video or the interpretation of the output.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f25e4662-c257-4a24-babb-b85097016500/cpz1255-fig-0013-m.jpg</p>\nFigure 13\nExample of how to set video information in ezTrack's Freeze Analysis Module when performing calibration.\n3. Load video information. Display first frame.",
    "Run Cell 3 to load the video. The starting frame of the video will appear as output, and various video properties will also be printed, including the filename, the number of frames in the video, the frame rate, and the frame dimensions. If you would like to alter the size of the image output, the number in the first line of this cell (i.e., %% output size = 100) can be increased or decreased accordingly. If the dimensions of your video frame are very small, sometimes no image will appear. Increasing the output size to 200 or more will remedy this. If a “FileNotFoundError” emerges, the user should double-check that directory path and filename are correctly specified.\n4. Calibrate video.\nWhen Cell 4 is run, ezTrack examines how pixel grayscale intensities change on a frame-by-frame basis across the specified length of the video. A histogram displaying these grayscale intensity changes is output (Fig. 14[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0014]). By looking at the distribution of frame-by-frame change values, a threshold can then be set for determining what changes are likely to be attributable to an animal moving versus random fluctuation. This threshold is subsequently used when analyzing videos with animals in them. Currently, a suggested cutoff is printed corresponding to twice the 99.99th percentile. However, the user can adjust this value to suit their needs. Zooming tools adjacent to the plot can be used to more closely examine low-frequency change values.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/e8c6542b-92dd-43b4-92a2-61d98708b078/cpz1255-fig-0014-m.jpg</p>\nFigure 14\nExample output after performing calibration on a video of an empty box. The distribution of frame by frame pixel fluctuations is plotted, and the suggested motion cutoff (twice the 99.99th percentile) is superimposed as a red line.\nProcess an individual file (FreezeAnalysis_Individual.ipynb)",
    "Note that the numbering of the following instructions corresponds to the numbered blocks of code in the Jupyter Notebook file (called “cells”). Some of these cells are optional, as noted both here and in the Jupyter Notebook file itself.\n1. Load necessary packages.\nRunning Cell 1 loads all necessary packages and does not need to be modified.\n2. Set directory and file information.\nIn Cell 2, the user specifies the file's location and other pertinent information about the video, described below (see Fig. 15[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0015]). After entering this information, run the cell.\ndpath: The directory path of the folder containing the video to be processed. Note that if you are using a Windows path with backslashes, place an “r” in front of the directory path to avoid an error (e.g., r’\\Users\\DeniseCaiLab\\Videos’).\nfile: The filename of the video, including the file extension.\nstart: The frame of the video on which to begin processing. 0 is the first frame. By knowing the video's frame rate (e.g., 30 frames/sec), the user can start processing the video at a specific timepoint. For instance, to begin processing 20 sec into the video, once the animal has been placed into the arena, one could enter 600 if the frame rate were 30 frames/sec. If you are uncertain of your video's frame rate, this information will be printed by ezTrack when Cell 3 is run.\nend: The frame of the video to end processing on. If the user would like to process from the start frame to the end of the video, this can be set to None.",
    "dsmpl: The amount to down-sample each frame. If processing is going slow, down-sampling can help. A value of 1 indicates no down-sampling, while a value of 0.25 indicates that the frame will be down-sampled to one-quarter the original size. Note that if down-sampling is performed, all pixel coordinate output will be in the dimensions of the down-sampled video.\nstretch: Allows the user to alter the aspect ratio of the presented output. This is useful when videos have irregular dimensions and are difficult to see (e.g., an aspect ratio of 1:100). The width/height will be scaled by the factor provided. Note that this only affects the appearance of visualizations and does not modify the video or the interpretation of the output.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/9956c3b4-8573-4689-8033-d62b2e0a0ca2/cpz1255-fig-0015-m.jpg</p>\nFigure 15\nExample of how to set video information in ezTrack's Freeze Analysis Module when processing an individual video.\n3. Load video, and crop frame if desired.\n         \nRun Cell 3 to load the video. The video's starting frame will appear as output, and various video properties will also be printed, including the filename, the number of frames in the video, the frame rate, and the frame dimensions.\nThe user can then define how to crop the video at this stage, if desired (see Fig. 16[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0016]). Select the box selection tool below the presented image (the square with a plus sign) and double-click the image to begin defining the region you would like your video cropped to. Double-click again to finalize the region. If the region is not satisfactory, rerun Cell 3 and follow the steps for cropping again.",
    "If you would like to alter the size of the image output, the number in the first line of Cell 3 (i.e., %% output size = 100) can be increased or decreased accordingly. If the dimensions of the video frame are very small in one dimension (such as on a long linear track), sometimes no image will appear. Increasing the output size to 200 or more will remedy this. If a “FileNotFoundError” emerges, the user should double-check that the directory path and filename are correctly specified.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f7e5ac00-730e-4213-89c7-c5e4a9f51ada/cpz1255-fig-0016-m.jpg</p>\nFigure 16\nExample of how to crop the video frame in ezTrack's Freeze Analysis Module.\n4. Analyze motion across session.\nIn Cells 4a and 4b, the user defines a threshold for detecting frame-by-frame pixel fluctuations that are indicative of motion, and ezTrack then calculates motion across the session.\n         \nSet motion threshold. To measure motion, ezTrack's Freeze Analysis Module detects the number of pixels that change in intensity from one frame to the next. However, a certain degree of variability is expected by chance and not due to the motion of the animal. To account for this, the user must set a threshold, “mt_cutoff,” defining how large a pixel intensity change is enough to count as motion. Define this threshold in Cell 4a and run the cell.\nUsing too small a cutoff will result in motion being detected even when the animal is motionless; too high a cutoff will result in no motion being detected when the animal is moving. If the user has run the Calibration protocol, mt_cutoff can be set to the suggested value. The impact of this parameter can be visualized in Cell 6. Again, see steps 4-7 for a protocol to estimate a good cutoff.",
    "Detect motion and plot. Run Cell 4b. Here, ezTrack loops through all frames and detects the number of pixels whose grayscale change exceeds mt_cutoff per frame. The results are then plotted in an interactive nature. In addition to changing the overall output size in the first line of this cell, the aspect ratio can also be changed. In line 3 of this cell, the height (h) and width (w) of the plot are chosen as follows:\nh, w = 300, 1000\nThe first number reflects the height of the plot, and the second number reflects the width.\n5. Analyze session freezing.\nIn Cells 5a and 5b, the user first defines the maximal amount of motion the animal can exhibit and be considered to be freezing (FreezeThresh), as well as the duration the animal must remain below this cutoff (MinDuration) to be considered freezing.\n         \nSelect freezing parameters.\nHere, the user sets FreezeThresh, corresponding to the maximal number of pixels per frame that can change (i.e., motion) where the animal would still considered to be freezing. This can first be set by examining motion values in the output to Cell 4b (Fig. 17[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0017]). A value should be chosen that captures the low troughs in motion, likely attributable to freezing, and should not be so low that very small blips (attributable to breathing or slight head bobbing) in motion are counted. In Figure 17[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0017], for example, a value between 100 and 200 would likely be appropriate. However, this can be subsequently titrated to user preference by watching video playback in Cell 6. MinDuration reflects the duration, in frames, that the animal's motion must drop below FreezeThresh before freezing accrues. This is often set in the 0.5- to 1-sec range.\n         \nb.Measure freezing and save.",
    "Here, ezTrack determines freezing on a frame-by-frame basis and saves the data (Figure 18[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0018]). The resulting .csv file (the video filename with the suffix _FreezingOutput, in the folder containing the video file) will contain the following information for each frame: the amount of motion (in pixel change units) and whether or not the animal is freezing (0 = not freezing, 100 = freezing). The results are then plotted in an interactive nature. The dimensions of this plot can be changed in the same way described for Cell 4b.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/74de6535-6236-4c82-a734-f332c94479a6/cpz1255-fig-0017-m.jpg</p>\nFigure 17\nExample output from measuring motion across the course of a session. The user should note markedly low spots, which may correspond to periods of freezing.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/62e56a24-186b-40b3-800f-1587e0c97dca/cpz1255-fig-0018-m.jpg</p>\nFigure 18\nExample output after freezing has been calculated. The animal's motion is replotted, this time highlighting periods of freezing in gray. Interactive zooming tools can be used to explore questionable periods, and the user can then playback these video portions in Cell 6 to optimize their parameters.\n6. Play video with scoring.\nAfter scoring has been performed, Cell 6 allows the user to play back a portion of the video (Fig. 19[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0019]). Classification of the animal as active/freezing is displayed (Fig. 19[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0019]). Additionally, the detected motion in the video is made visible. After the display parameters have been defined by setting the values of display_dict, running Cell 6 will result in the video being played. Below we outline the meaning of each of these parameters (Fig. 20[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0020]).\nstart: The frame video playback is to be started on. Note that this is relative to the start of tracking, where 0 is the first tracked frame (the defined start frame from Cell 2).",
    "stop: The frame video playback is to end on. Note that this is relative to the start of tracking, where 0 is the first tracked frame (the defined start frame from Cell 2).\nfps: The speed of video playback. Must be an integer. Video playback may also be slower depending upon computer speed. This is because fps sets the time imposed between presented frames but cannot control the duration of time it will take a user's computer to present each frame.\nresize: If the user wants the output to be larger or smaller, or the aspect ratio to be different, resize can be supplied as in the following example:\n‘resize’: (100,200)\nHere, the first number corresponds to the adjusted width of the frame, whereas the second number corresponds to the adjusted height. Both numbers reflect pixel units and should be integers. Set resize equal to None if no resizing is to be done.\nsave_video: If the user would like to save the video clip, save_video can be set to True.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/b59bbb64-f819-4844-be72-cad1008eca90/cpz1255-fig-0019-m.jpg</p>\nFigure 19\nExample of how to play a portion of the video back in the Freeze Analysis Module. The scoring of the animal (either “Active” or “Freezing”) will be displayed, as well as the detected motion.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/c3bd41a6-8a5b-48da-9599-a81e6ba52493/cpz1255-fig-0020-m.jpg</p>\nFigure 20",
    "Video playback in the Freeze Analysis Module. (A) Still frame from playback of a moving animal, with the original image on top, detected motion on bottom, and the state of the animal marked (“Active” or “Freezing”). If the animal is freezing but said to be active, try increasing FreezeThresh. If the animal is moving and said to be freezing, try decreasing FreezeThresh. (B) When mt_cutoff is too low, pixel fluctuations not associated with the animal's movement will be visible. Alternatively, when mt_cutoff is too high, animal motion will be invisible or very low.\n7. Create binned summary report and save (optional).\nIn Cell 7, the user is able to obtain summary information, either for defined time bins or the entire tracking period. For each time bin requested, the animal's average motion and freezing will be saved to a .csv file (a file with the video filename with the suffix “_SummaryStats” in the folder containing the video file). Refer to Figure 21[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.255#cpz1255-fig-0021] for an example of how to define bins. Each bin's name should be placed in quotation marks, while the numbers inside the parenthesis indicate the bin's desired start and end frames. Each bin should be separated by a comma. Note that defined frames are relative to the defined start frame (from Cell 2), such that 0 refers to the first tracked frame. If the user wants a summary of the entire tracking period, set bin_dict equal to None. In addition, if it is easier for the user to think in seconds, as opposed to frames, multiplication can be done within the parenthesis. For example, assuming a frame rate of 30, the following would create bins for seconds 0-100, 100-200, and 200-300:\nbin_dict = {\nˈBinName1ˈ: (0*30, 100*30),\nˈBinName2ˈ: (100*30, 200*30),\nˈBinName3ˈ: (200*30, 300*30)\n}\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/9df88162-0353-4e2d-b9da-9e65d3b57ad3/cpz1255-fig-0021-m.jpg</p>\nFigure 21",
    "Example of how to define bins for extracting summary information in ezTrack's Freeze Analysis Module.\nProcess multiple files in a batch (FreezeAnalysis_BatchProcess.ipynb)\nBatch processing with ezTrack works much the same way as individual processing, but is designed to be executed on all video files of a specified type within a given folder. The user will find that many of the steps are identical to individual processing, with the following exceptions. First, rather than specifying the directory and the name of an individual file, the user supplies the directory and the file extension of the videos to be processed. ezTrack will process all files of the specified file type within the given folder. Second, when batch processing, all cropping parameters and bin information are assumed to be the same. The user should therefore be mindful that videos have a consistent field of view and be of a roughly similar length. Third, the user is still able to crop the video, as with individual processing; however, cropping is based upon the first video file in the folder. Again, be mindful that videos need be roughly the same, although minor discrepancies in the field of view are permissible. Fourth, the batch processing notebook file does not contain visualization tools to optimize tracking parameters. It is assumed that these have been previously explored while processing a few individual files."
  ],
  "subjectAreas": [
    "Neuroscience"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}