{
  "id": 3005,
  "origin_website": "Cell",
  "title": "Deep texture representation analysis for histopathological images",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nCreate annotation\nTiming: 5 min for each WSI\nDepending on the desired analysis, select the regions that represent the histological characteristics of the variables. Here we select the regions of interest (ROIs) that represent the tumor’s histology (Figure 1[href=https://www.wicell.org#fig1]A). Square image patches will be extracted strictly inside the annotated ROIs. Therefore, users should ensure that the ROI size is sufficiently large for the intended patch size in the pixel, at least to accommodate the size of a single patch. Please also avoid areas with artifacts such as folded or torn tissue (Figures 1[href=https://www.wicell.org#fig1]B and 1C), areas with non-uniform staining or with excessive noise, etc, areas of normal tissues, and largely white areas without any tissues because these conditions will wrongly represent the tumor types the users are working on. Based on our experiences, selecting a histologically uniform area (Figure 1[href=https://www.wicell.org#fig1]F) works well for the applications described here. This step is skippable when users already have patch images, such as pan-cancer histological image patch in our dataset (Zenodo: https://doi.org/10.5281/zenodo.5889558[href=https://doi.org/10.5281/zenodo.5889558]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2546-Fig1.jpg\nFigure 1. Example of WSI annotation using QuPath 0.3.2\nThis WSI is used only to demonstrate the annotation process. We do not use this WSI in our next analysis.\n(A) Annotation with yellow polygons represents the tumor area. The scale bar size is 2 mm relative to the picture.\n(B–E) Export object data as Pretty JSON when using QuPath 0.3.2, (B) and (C) Pointed by the yellow arrows are examples of artifacts (torn or folded tissue) to be avoided, some insignificant artifacts, such as (D) small tear and (E) slightly different tissue thickness, can be included in the annotations when impossible to avoid.\n(F) Higher magnification example of a bad and good area for annotation.",
    "Create a polygon or a line to mark the ROI using QuPath (compatible for a wide range of formats, including ∗.svs and ∗.ndpi) or NDP View2 (for ∗.ndpi only) (Figure 1[href=https://www.wicell.org#fig1]).\nStore both WSI file (∗.ndpi or ∗.svs); and its annotation file (∗.ndpi.ndpa or ∗.svs.ndpa or ∗.geojson) in the same directory.\nExtract patches from the annotated regions\nTiming: 2–4 min for each WSI\nSmaller image patches will be extracted from annotated ROIs. Users can specify the magnification through the source size in pixels or micrometers (with --micrometer) of the patch and the output patch size in pixels. The bigger source size denotes lower magnification. The patch angle is randomly selected to reduce the effects of the patch angle in the patient-level analysis. We assume that users have >100 WSIs. Therefore, executing the commands automatically one after another will help reduce the time needed to extract patches. Here, we demonstrate one method of creating a shell script. However, piping the commands from the terminal is also possible. This step is skippable when users already have patch images, such as pan-cancer histology image patch in our dataset (Zenodo: https://doi.org/10.5281/zenodo.5889558[href=https://doi.org/10.5281/zenodo.5889558]).\nOpen JupyterLab from the terminal by typing.\n>jupyter lab\nNote: For Windows, select “Anaconda (64bit)” -> “Jupyter Notebook (deeptexturewin64)” from the Start Menu.\nCreate shellscript in JupyterLab.\n>import os\n>import glob\n>WSI_dir = '/directory_of_WSI/'\n>Patch_dir = '/directory_of_extracted_patch/'\n>ndpis = glob.glob(os.path.join(WSI_dir, \"∗/∗/∗.ndpi\")) # get all WSI file in the chosen directory\n>src_size = 1024 # increase the parameter to get lower magnification image patches and decrease to get higher magnification\n>patch_size = 256\n>ann_type = “qupath”\n>with open(\"histopatch.sh\", \"w\") as fout:\n  >print(“#!/bin/sh”, file=fout)\n  >for ndpi in ndpis:\n    >command = f”histopatch {ndpi} {Patch_dir} ∖\n    --annotation_type (ann_type) --nparent 2 ∖\n    --src_size (src_size) --patch_size (patch_size)\" ∖",
    "# create a line of command for each WSI file and ∖\n    store them in histopatch.sh\n    >print(command, file=fout)\nCritical: Modification of symbols used in the codes may be necessary for Windows OS users such as “∖” in place of “/” and straight quotation mark instead of curly double quotation mark.\nAlternatively, open the text editor in JupyterLab and edit histopatch.sh as follows:\n>#!/bin/sh\n>patchdir=/directory_of_extracted_patch # write the intended directory\n>src_size = 1024\n>patch_size = 256\n>ann_type = “qupath”\n>for wsi in /directory_of_WSI/∗/∗/∗.ndpi:\n  > histopatch $wsi $patchdir --annotation_type ∖\n  $ann_type --nparent 2 --src_size $src_size ∖\n  --patch_size $patch_size # directly excecute a ∖\n  command to extract patch for each WSI file\nIn the terminal, activate the histopatch environment:\n>conda activate histopatch\nNote: Windows user should run the codes in Anaconda prompt.\nThen run:\n>chmod +x /directory_of_the_script/histopatch.sh\n>/directory_of_the_script/histopatch.sh\nTroubleshooting 2[href=https://www.wicell.org#sec5.3].\nNote: Twenty patches will be extracted from annotated regions of one WSI as the output by default. To change the number of outputs, please specify --n along with src_size, patch_size, and ann_type for example:\n>#!/bin/sh\n>patchdir=/directory_of_extracted_patch\n>src_size = 1024\n>patch_size = 256\n>ann_type = “qupath”\n>n = 100\n>for wsi in /directory_of_WSI/∗/∗/∗.ndpi:\n  > histopatch $wsi $patchdir --annotation_type ∖\n  $ann_type --nparent 2 --src_size $src_size -- ∖\n  patch_size $patch_size --n $n\nNote: The output will be saved as .png.\nCalculate the DTRs of image patches\nTiming: 20–30 min\nThe calculation can be performed for each image or multiple images at once. Here, we present the codes to calculate the representation of multiple images at once.\nActivate the deeptexture environment by typing.\n>conda activate deeptexture\nCreate a DataFrame of the extracted patch (Figure 2[href=https://www.wicell.org#fig2]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2546-Fig2.jpg\nFigure 2. Example of DataFrame created with patient ID, tumor type, and image file information\n>import os\n>import pandas as pd\n>import glob\n>import os.path.basename as bn\n>import os.path.dirname as dn",
    ">imgfiles = sorted(glob.glob(“./directory_of_extracted_patch/∗.png”))\n>patients = []\n>tum_type = []\n>for img in imgfiles:\n        >patients.append((bn(dn(img).split('-')[0]))) # extract patient ID from the filename\n        >tum_type.append(bn(dn(dn(img)))) # extract tumor type from the filename\n>df = pd.DataFrame({'Patient':patients, 'Tumor_Type':tum_type, 'Image_Files':imgfiles})\nNote: Any other variables can be added to the DataFrame such as malignancy grade or treatment response. More variables can be accommodated by creating a csv file and reading it in the notebook by running.\n>import pandas as pd\n>import glob\n>imgfiles = sorted(glob.glob(“./directory_of_extracted_patch/∗.png”))\n>df = pd.read_csv ('file_name.csv')\n>df\nCritical: The order of the files in imgfiles and other associated variables such as for DataFrames and DTRs should be the same throughout the analysis.\nCalculate the DTRs of the images.\n>import deeptexture as dt\n>dtr_obj = dt.DTR()\n>dtrs = dtr_obj.get_dtr_multifiles(imgfiles, angle=[0,90])\nCritical: Creating dtr_obj consumes a large portion of memory in the GPU.\nNote: Adding the function “angle” will calculate the DTR of an image and its copy rotated by 90°. This will allow rotational invariance of the DTR.\nTroubleshooting 3[href=https://www.wicell.org#sec5.5].\nCalculate the mean DTR of each patient.\n>dtrs_mean, cases_mean, df_mean = dtr_obj.get_mean_dtrs(dtrs, df.Patient, df)\nVisualize the data in plot\nTiming: 10 min\nDTR is a high-dimensional data (e.g., 1,024 dimensions as default). Linear or nonlinear dimensionality reduction methods such as PCA,3[href=https://www.wicell.org#bib3] UMAP,4[href=https://www.wicell.org#bib4] or t-SNE5[href=https://www.wicell.org#bib5] can be used to create an interpretable visualization. Here, we demonstrate the code to create a two-dimensional UMAP plot. Cosine distance is used as a distance metric in this package for UMAP.\nImage plotting on UMAP coordinate.\n>from deeptexture import plt_dtr\n>X = dtrs_mean\n>mean_X_emb = plt_dtr.plt_dtr_image(X, df_mean.Image_Files, method=\"umap\", axis=True)\nNote: This code plots the mean DTR of each case with one image representation from each case shown in the plot.\nAttribute plotting on UMAP coordinate:\n>from deeptexture import plt_dtr\n>import plotly.express as px",
    ">mean_X_attr, fig = plt_dtr.plt_dtr_attr(dtrs_mean, df_mean.Tumor_Type, df_mean.Patient, method=\"umap\", use_plotly=True, axis=True)\n>fig.show()\nNote: This code plots the mean DTR of each case with the tumor type as differentiating attribute.\nContent-based image retrieval\nTiming: 15 min\nIdentification of histologically similar images within the database is performed by calculating the cosine similarity of DTRs. Database images are ranked and retrieved from the most similar to the least.\nCreate the database.\n>import pandas as pd\n>from deeptexture import cbir\n>df_attr = df\n>cbir_obj = cbir.CBIR(dtr_obj, project='DB', working_dir='CBIR')\n>cbir_obj.create_db(df_attr, dtrs=dtrs, img_attr='Image_Files', save=True)\nCritical: The order of the dtrs should be the same as the df_attr\nTroubleshooting 4[href=https://www.wicell.org#sec5.7].\nRetrieve similar images.\n>qimgfile = “/directory_of_query_image/image.jpg”\n>plt.rcParams[‘figure.figsize’] = 30,30\n>cbir_obj.search(qimgfile, img_attr='Image_Files', case_attr='Patient', type_attr=’Tumor_Type’, n=11)\nNote: Specify the number of retrieved images with the variable n.\nTroubleshooting 5[href=https://www.wicell.org#sec5.9].\nSupervised learning model with logistic regression to the DTR\nTiming: 10 min\nCompared to conventional training using a convolutional neural network (CNN), supervised learning using DTR is easier and faster. Additionally, it is expected to be less prone to overfitting in users with limited training data. The performance of supervised learning with image representation is comparable to conventional training at least in such a small data scenario as discussed in Komura et al.1[href=https://www.wicell.org#bib1] The ml module of the deep texture package automatically splits the whole dataset into train and test data with a 3:1 ratio. This module is useful for histologic subtypes, grade prediction, and histologic differentiation. For binary and multi-class classification, logistic regression and multi-class linear Support Vector Machines (SVM) are used, respectively.\nRun the following code for supervised learning:\n>from deeptexture import ml\n>ml_obj = ml.ML(dtrs, imgfiles)\n>y = df.Tumor_Type # specifies the target variable of the classification\n>cases = df.Patient\n>result = ml_obj.fit_eval(y, cases)",
    "Note: The train and test data ratio can be modified by specifying test_size between 0.0 and 1.0 as an additional parameter in fit_eval(). Please refer to the library documentation.\nTroubleshooting 6[href=https://www.wicell.org#sec5.11]."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Microscopy",
    "Cancer"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Bioengineering & Technology"
  ]
}