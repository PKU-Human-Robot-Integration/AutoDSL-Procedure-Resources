{
  "id": 10041,
  "origin_website": "Jove",
  "title": "Quantification of Oculomotor Responses and Accommodation Through Instrumentation and Analysis Toolboxes",
  "procedures": [
    "The study, for which this instrumentation and data analysis suite was created and successfully implemented was approved by the New Jersey Institute of Technology Institution Review Board HHS FWA 00003246 Approval F182-13 and approved as a randomized clinical trial posted on ClinicalTrials.gov Identifier: NCT03593031 funded via NIH EY023261. All the participants read and signed an informed consent form approved by the university's Institutional Review Board.\n1. Instrumentation setup\nMonitoring the connections and hardware\n\t\nThe VE2020 system assigns the monitors spatially in clockwise order. Check that the primary control monitor is indexed as 0 and that all the successive monitors are indexed from 1 onwards. Ensure that all the monitors are managed by a single computer (see Table of Materials).\nEnsure proper spatial configuration of the stimulus monitors. From the controller desktop home screen, right-click on the controller monitor, select the Display settings, and navigate to screen resolution. Select Identify; this will provide a visualization of the assigned monitor indices for each stimulus display connected to the control computer (Figure 1).\nPhysical equipment configuration\n\t\nEnsure that the eye-tracking system is on the optical midline with a minimum camera distance of 38 cm. Check that the autorefractor system is on the optical midline and 1 m ± 0.05 m from the eyes.\nValidate the configuration of the hardware and equipment by referencing the dimensions within Figure 1.\nEye tracking system\n\t\nEnsure the desktop and corresponding eye-tracking hardware are configured and calibrated according to the manufacturer's instructions (see Table of Materials).\nEstablish BNC cable wiring from the desktop's analog outputs into the data acquisition (DAQ) board via an analog breakout terminal box (NI 2090A). See Table 1 for the default BNC port configurations for VE2020.",
    "NOTE: Deviations from the default wiring require modification of the assigned ports described in the Acquire.vi and/or TriggerListen.vi files or editing of the default header order in the standard.txt file.\nConfigure the analog terminal breakout box reference switches by identifying the single-ended/differential (SE/DIFF) switch (see Figure 2), and set the switch to SE. Then, identify the ground selection (RSE/NRSE) switch (see Figure 2), and set the ground reference to referenced single-ended (RSE).\nAccommodative response acquisition\n\t\nPerform the orientation of the autorefractor (see Table of Materials) as per the manufacturer's recommendations. Configure the autorefractor in direct alignment, and perform manual operator-based triggering of the autorefractor to store the autorefractor recording data.\nEnsure that an external removable storage device is utilized to save the autorefractor data. Remove the external drive prior to starting the autorefractor software, and reinsert the drive once the software is running. Create a folder directory within the corresponding storage device for identifying the participant profiles, session timings, and stimuli. Follow this practice for each experimental recording session.\nFollowing the autorefractor software activation and the insertion of an external storage device, begin the calibration of the autorefractor.\nMonocularly occlude the left eye of the participant with an infrared transmission filter (IR Tx Filter)10. Place a convex-sphere trial lens in front of the IR Tx filter (see Table of Materials).\nBinocularly present a high acuity 4° stimulus from the physically near stimulus monitors.\n\t\tNOTE: Once the participant reports the stimulus as visually single and clear (acute), the participant must utilize the handheld trigger to progress with the calibration.\nBinocularly present a high acuity 16° stimulus from the physically near stimulus monitors.\n\t\tNOTE: Once the participant reports the stimulus as visually single and clear (acute), the participant must utilize the handheld trigger to progress.",
    "Repeat these calibration procedures (steps 1.4.4-1.4.6) for each convex-sphere lens as follows (in diopters): −4, −3, −2, −1, +1, +2, +3, and +4.\nimgsrc://cloudfront.jove.com/files/ftp_upload/64808/64808fig01.jpg\nFigure 1: Haploscope control and recording equipment configuration. Example of the VE2020's display indexing for clockwise monitor ordering and dimensioning. Here, 1 is the control monitor, 2 is the near-left display monitor, 3 is the far-left display monitor, 6 is the calibration board (CalBoard), 4 is the far-right display monitor, and 5 is the near-right display monitor. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64808/64808fig01large.jpg]\nTable 1: BNC port map. The convention for BNC connections. Please click here to download this Table.[href=https://www.jove.com/files/ftp_upload/64808/Table_1.xlsx]\nimgsrc://cloudfront.jove.com/files/ftp_upload/64808/64808fig02.jpg\nFigure 2: Breakout box switch references. Demonstration of the proper NI 2090A switch positions. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64808/64808fig02large.jpg]\n2. Visual stimulation utilizing the VE2020 visual displays and VE2020 LED targets\nBegin the calibration of the VisualEyes2020 stimulus display(s).\n\t\nOpen the virtual instrument (VI) file named Pix2Deg2020.vi. Select the monitor to be calibrated by utilizing the stretch mode ID input field and the monitor's corresponding display index (Figure 3).\nSelect a stimulus image (e.g., RedLine.bmp) by typing the stimulus filename into the Line input field.\n\t\tNOTE: It is important to note that Pix2Deg2020.vi utilizes .bmp files, not .dds files.\nRun Pix2Deg2020.vi, and adjust the stimulus position until it superimposes on a measured physical target.\nOnce the virtual image aligns with the physically measured target, record the on-screen pixel value for the given degree value. Record a minimum of three calibration points with varying stimulated degree demands and their corresponding pixel values.",
    "Ensure that after recording each calibration point, VE2020 produces an output file named Cals.xls. Utilizing the calibration points in Cal.xls, apply a best-fit linear regression to map the experimentally required eye movement stimulus demands, in degrees of rotation, into pixels. An example five-point degree to recorded pixel calibration is shown in Figure 4.\nRepeat this procedure for different stimulus images (i.e., the background or second visual stimulus, as necessary) and each stimulus monitor that is expected to be utilized.\nimgsrc://cloudfront.jove.com/files/ftp_upload/64808/64808fig03.jpg\nFigure 3: Stimulated degrees to monitor pixels. Depiction of the operator view for calibrating the VE2020. From left to right, a table of values for the recorded pixels corresponding to a known degree value is provided for a given stimulus monitor selection (stretch mode ID) with a fixed aspect ratio, given file name, background stimulus (BG), and foreground stimulus (Line). Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64808/64808fig03large.jpg]\nimgsrc://cloudfront.jove.com/files/ftp_upload/64808/64808fig04.jpg\nFigure 4: Pixel to degree calibration slopes. Monocular calibration curve for known degree values and measured pixel values. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64808/64808fig04large.jpg]\n3. LED calibration\nDetermine the experimental degrees of rotation by utilizing trigonometric identities in the vertical or horizontal planes (Figure 5). Plot the degrees of rotation as a function of the LED number.\nLinearly regress the LED number as a function of the degrees of rotation. Use the obtained relationship to calculate the initial and final LED numbers, which will be used as visual stimuli during the experiment.\nimgsrc://cloudfront.jove.com/files/ftp_upload/64808/64808fig05.jpg\nFigure 5: Calculated degrees of rotation. Method of calculating the angular displacement for both saccadic eye movements and vergence movements with a known distance to the target (X) and inter-pupillary distance (IPD). Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64808/64808fig05large.jpg]\n4. Software programming",
    "Define the VisualEyes display input file, and save it to the stimulus library as follows.\n\t\nTo define each stimulus, open a new text (.txt) file prior to the experiment. On the first row of this text file, confirm the presence of four required tab-delimited parameters: stimulus timing (s); X-position (pixels); Y-position (pixels); and rotation (degrees). Additionally, confirm the presence of two optional successive parameters: scaling X (horizontal scaling); and scaling Y (vertical scaling).\nCalculate the pixel value for each desired stimulus degree by utilizing the linear regression equation derived from the calibration (see step 2.1.5).\nConfirm within the next row of the text file that the length (s) for which the stimulus is presented at its initial position and subsequent final position are present and tab-delimited.\nSave the stimulus file in the directory as a VisualEyes input (VEI) file with an informative file name (e.g., stimulus_name_movement_size.vei).\n\t\tNOTE: Each stimulus file is positioned monocularly, so a separate file must be generated for the complementary eye to evoke a binocular movement.\nRepeat these procedures for each desired experimental stimulus, respective movement type, movement magnitude, and eye as appropriate.\n5. DC files\nCreate a stimulus library for each stimulus monitor. Name these libraries as dc_1.txt through dc_7.txt. For the settings contained within the dc_1.txt and dc_2.txt files, reference Table 2.\n\t\nValidate the numerical ID for each stimulus monitor by clicking on Display > Screen Resolution > Identify. Ensure that the device ID is the primary GPU (starting index 0) and that the window mode is 1.\nVerify that left defines the left boundary of the screen (in pixels), top defines the top boundary of the screen (in pixels), width is the longitudinal width of the screen (in pixels), and height is the vertical height of the screen (in pixels).",
    "Establish the stimulus number (Stim#), which associates the stimulus file name and location (.dds) and, provided the nostimulus.vei file is stimulus number zero, associates them to a stimulus index number. For the subsequent stimulus_name.vei, list the various stimulus files that are able to be used within the experimental session.\n\t\tNOTE: The nostimulus.vei file is beneficial when using ExpTrial as nostimulus.vei does not present a stimulus (blank screen).\nTable 2: DC file configuration. The table provides an overview of the DC text file format. Please click here to download this Table.[href=https://www.jove.com/files/ftp_upload/64808/Table_2.xlsx]\n6. LED input file definition and stimulus library storing\nOpen a new text (.txt) file, and within the file, utilize tab delimitation. End each line within the text file with two tab-delimited zeros.\nIn the first row, define the initial time (s) and LED (position) values. In the second row, define the final time (s) and final LED position values. Save the stimulus_name.vei file in the directory, and repeat these steps for all stimuli.\nOnce completed, save all the stimulus file(s) into the stimulus library, array_config.txt.\nEnsure that the first row in the array_config.txt file is the communication (COM) port that VisualEyes uses to communicate with the flexible visual stimulator with the default input value COM1; the second row is the baud rate with the default input value as 9,600; the third row is the data bit capacity with the default input value as 8 bits; and the fourth row is the data parity index with the default input value as 0. The succeeding rows in the file contain the stimulus file of the flexible visual stimulator (Figure 6).\nCheck the profile number, as seen in Figure 6; this refers to the corresponding row index of any given stimulus file name, which starts at index zero.\nimgsrc://cloudfront.jove.com/files/ftp_upload/64808/64808fig06.jpg",
    "Figure 6: Stimulus library. Utilizing text-editing software, the format shown for identifying the port communications, baud rate, data size, and parity, as well as the library of stimulus files (.vei), provides the VE2020 with the necessary configurations and stimulus file names to run successfully. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64808/64808fig06large.jpg]\n7. Script creation for experimental protocols\nOpen a new text (.txt) file to script the experimental protocol commands for VE2020 to read and execute. Check the proper syntax for the experimental protocol commands and documentation. Table 3 provides an overview of the VE2020 syntax conventions.\n\tNOTE: VE2020 will read these commands sequentially.\nSave the text file in the directory as a VisualEyes Script (VES), such as script_name.ves. From the previous VisualEyes version manual11, check for a table of software functions that have input and output capabilities. Table 3 demonstrates three newly implemented updated functions.\nTable 3: VE2020 function syntax. VE2020 has specific syntax, as demonstrated in the table for calling embedded functions and commenting. Please click here to download this Table.[href=https://www.jove.com/files/ftp_upload/64808/Table_3.xlsx]\n8. Participant preparation and experiment initiation\nAcquiring consent and eligibility\n\t\nUse the following general participant eligibility criteria: aged 18-35 years, 20/25 (or greater) corrected monocular visual acuity, stereo acuity of 500 s (or better) of arc, and 2 weeks (or greater) of utilizing proper refractive correction.\nUse the following convergence insufficiency (CI) participant eligibility criteria following established practices12: Convergence Insufficiency Symptom Survey (CISS)13 score of 21 or greater, failure of Sheard's criterion14, 6 cm (or greater, at break) near point of convergence (NPC), and 4Δ (or greater) exodeviation (near compared to far).",
    "Use the following control participant eligibility criteria: CISS score less than 21, less than 6Δ difference between near and far phoria, less than 6 cm (at break) NPC, passing of Sheard's criterion, and sufficient minimum amplitude of accommodation as defined by Hofstetter's formula15.\nUse the following general participant ineligibility criteria: constant strabismus, prior strabismus, or refractive surgery, dormant or manifested nystagmus, encephalopathy, diseases that impair accommodative, vergence, or ocular motility, 2Δ (or greater) vertical heterophoria, and inability to perform or comprehend study-related tests. The CI ineligibility criteria further include participants with less than 5 diopters accommodative responses via Donder's push-up method16.\nOnce informed consent is acquired, direct the participant to be seated in the haploscope.\nPosition the participant's forehead and chin against a fixed headrest to minimize head movement, and adjust the participant's chair height so that the participant's neck is in a comfortable position for the entire duration of the experiment.\nAdjust the eye movement recording camera(s) to ensure the participant's eyes are captured within the camera's field of view.\nAfter being properly seated in the haploscope and eye tracker/autorefractor, ask the participant to visually fixate on a visually presented target. During this setup, ensure the participant's eyes are centered so that visual targets are presented on the midsagittal plane.\n\t\nAchieve eye centering by having high acuity targets presented binocularly at the visual midline. The participant is aligned at the visual midline when physiological diplopia (double vision) occurs centered around the target of fixation.\nThen, adjust the eye-tracking gating and eye-tracking signal gains to capture anatomical features such as the limbus (the boundary between the iris and sclera), pupil, and corneal reflection.\nValidate the capture of the eye movement data by asking the participant to perform repeated vergence and/or saccadic movements.",
    "Following the preliminary validation and physical monitor calibrations, open ReadScript.vi. Once ReadScript.vi has opened, select the experimental protocol script by typing in the file name in the top-left corner. Run the protocol via ReadScript.vi by pressing the white arrow in the top-left corner to execute Acquire.vi.\nProvide the participant with a handheld trigger button, and explain that when the trigger is pressed, the data collection will commence. A file will automatically appear on the control monitor screen, Acquire.vi, which plots a preview of the recorded eye movement data. When the experimental protocol is complete, ReadScript.vi automatically stops, and data output files are automatically generated and stored.\n9. VNEL eye movement analysis program (VEMAP)\nData preprocessing\n\t\nBegin the analysis by selecting the Preprocess Data button. A file explorer window will appear. Select one or many recorded data files from VE2020 for preprocessing.\nFilter the data with a 20-order Butterworth filter: 40 Hz for vergence eye movements and 120 Hz or 250Hz for saccadic eye movements. The completed preprocessed data files will be stored within the VEMAP Preprocessed folder as .mat files.\n\t\tNOTE: The filtering frequency for VEMAP can be adjusted to the user's preferred cut-off frequency, dependent on the application.\nCalibration\n\t\nUtilizing the three stimulated monocular calibration movements respectively for the left and right eye positions evoked from the VE2020 script, create a linear regression of the eye movement stimuli in degrees as a function of the recorded voltage values. As shown in the lower plots of Figure 7, use the respective Pearson correlation coefficients and regression formulas for the quantitative assessment of the fitment.\nUtilize the slope of each regression as the respective monocular calibration gain to convert the recorded (raw) voltages to degrees (calibrated).",
    "Identify from the experimental calibrations an appropriate gain value for the left and right eye movement responses. Consistently apply the calibration gain to each recorded eye movement stimulus section. Following the calibration of all the movement subsections, a confirmation window will appear.\n\t\tNOTE: Monocular eye movement calibrations are chosen due to the potential inability of patients with convergence insufficiency, the primary ocular motor dysfunction investigated by our laboratory, to perceive a binocular calibration as a single percept. If the recorded calibration signals are saturated or not linearly correlated (due to not attending to the stimulus, blinking, saccadic movements, eye tearing, or closure of the eyes), then apply standardized calibration gains for the left and right eye movement responses. This should be done sparingly, and these calibration gain values should be derived from large group-level averages of previous participants for the left and right eye movement response gains, respectively.\nClassification\n\t\nFollowing the calibration, inspect each eye movement response manually, and categorize using a variety of classification labels, such as blink at transient, symmetrical, asymmetrical, loss of fusion, no movement (no response), and saturated eye movement.\nCheck Figure 8 for reference. The upper (positional data) plot is the response from a 4° symmetrical vergence step stimulus. The combined convergence movement is shown in green, the right eye movement is shown in red. and the left eye movement is shown in blue. The version trace is shown in black. The lower plot shows the first-derivative velocity of the eye movement position response, with the same color pattern as described above.\nData analysis",
    "Perform the final step in the VEMAP processing dataflow of data analysis, which is accessible within the VEMAP user interface (UI) as a button and is previewed in Figure 9. Plot the eye movements within a particular stimulus type and classification label together as an ensemble plot, as shown on the right side of Figure 9.\nSelectively analyze the subsets of eye movements via their classification labels or holistically without any applied classification filters via the Choose Classes button.\nCheck that the primary eye movement metrics correspond to each recorded eye movement, such as the latency, peak velocity, response amplitude, and final amplitude.\nInspect each eye movement response to ensure that each recorded metric is valid. If a metric does not appear appropriate, remeasure the recorded metrics accordingly until appropriate values accurately reflect each movement. In addition, omit eye movements or reclassify their provided classification labels via the Reclassify button if the recorded metrics cannot adequately describe the recorded eye movement.\nimgsrc://cloudfront.jove.com/files/ftp_upload/64808/64808fig07.jpg\nFigure 7: Monocular calibration and correlation slopes. An example of the calibration of eye movement data from voltage values to degrees of rotation. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64808/64808fig07large.jpg]\nimgsrc://cloudfront.jove.com/files/ftp_upload/64808/64808fig08.jpg\nFigure 8: Eye movement software classification. Classification of the stimulated eye movement responses. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64808/64808fig08large.jpg]\nimgsrc://cloudfront.jove.com/files/ftp_upload/64808/64808fig09.jpg\nFigure 9: Eye movement response software analysis. An example of plotted convergence responses stimulated by a 4° symmetrical step change (right), with individual eye movement response metrics presented tabularly (left) and group-level statistics displayed tabularly below the response metrics. Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64808/64808fig09large.jpg]\n10. Accommodative Movement Analysis Program (AMAP)\nData configuration",
    "Utilizing the external storage device that contains the autorefractor data, export the data to a device with the AMAP installed. The AMAP is available as a standalone executable as well as a local application via the MATLAB application installation.\nStart the AMAP application. From the AMAP, select either File Preprocessor or Batch Preprocessor. The file preprocessor processes an individual data folder, while the batch preprocessor processes a selected data folder directory.\nCheck the AMAP's progress bar and notifications, as the system provides these when the selected data have been preprocessed. Folder directories are generated from the AMAP's preprocessing for data processing transparency and accessibility via the computer's local drive under AMAP_Output.\nIf an AMAP feature is selected without prior data processing, check for a file explorer window that appears for the user to select a data directory.\nPerform the AMAP data analysis as described below.\n\t\nFollowing preprocessing, select a data file to analyze via the Load Data button. This will load any available files into the current file directory defaulted to a generated AMAP_Output folder. The selected data filename will be shown in the current file field.\nUnder the eye selector, check the default selection, which presents binocularly averaged data for the recorded accommodative refraction.\nSwitch the data type between accommodative refraction and oculomotor vergence (gaze) via the Type Selector. Check further graphical customizations available to present the data metrics and first-order and second-order characterizations. Check Figure 10 for the combinations of graphical options that can be selected for the operator to visualize.",
    "Check the default metrics for the AMAP, which are as follows: peak velocity (degrees/s); response amplitude (degrees); final amplitude (degrees); response starting index (s); peak velocity index (s); response ending index (s); gaze (vergence) velocity (degrees/s); gaze response amplitude (degrees); gaze final amplitude (degrees); gaze response starting index (s); gaze velocity index (s); gaze response ending index (s); and classification (binary 0 - bad, 1 - good).\nPerform modifications to the response starting index, response ending index, and peak velocity index through the metric modification spinners (Figure 10).\nFollowing the analysis of all the recorded movements displayed, save the analyzed metrics for each data file in the movement ID field or via the leftward and rightward navigation arrows.\nSelect the Save button to export the analyzed data to an accessible spreadsheet. Unanalyzed movements have a default classification of not-a-number (NaN) and are not saved or exported.\nPerform manual classification (good/bad) for each movement to ensure complete analysis by any operator.\nimgsrc://cloudfront.jove.com/files/ftp_upload/64808/64808fig10.jpg\nFigure 10: AMAP software frontend. The figure displays the main user interface for the AMAP with highlighted sections for the graphical presentation (graphical options) of data and data analysis (metric modifications). Please click here to view a larger version of this figure.[href=https://www.jove.com/files/ftp_upload/64808/64808fig10large.jpg]Subscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Bioengineering"
  ],
  "bigAreas": [
    "Bioengineering & Technology"
  ]
}