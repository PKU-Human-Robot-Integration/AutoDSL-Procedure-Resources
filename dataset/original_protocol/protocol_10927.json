{
  "id": 11405,
  "origin_website": "Jove",
  "title": "Using Eye Movements Recorded in the Visual World Paradigm to Explore the Online Processing of Spoken Language",
  "procedures": [
    "All subjects must give informed written consent before the administration of the experimental protocols. All procedures, consent forms, and the experimental protocol were approved by the Research Ethics Committee of the Beijing Language and Culture University.\nNOTE: A comprehension study using the visual world paradigm normally consists of the following steps: Introduce the theoretical problems to be explored; Form an experimental design; Prepare the visual and auditory stimuli; Frame the theoretical problem with regard to the experimental design; Select an eye-tracker to track participants' eye movements; Select a software and build a script with the software to present the stimuli; Code and analyze the recorded eye-movements data. A specific experiment can differ from each other in any of the described steps. As an example, a protocol is introduced to conduct the experiment and discuss some points that researchers need to keep in mind when they build and conduct their own experiment using the visual world paradigm.\n1. Prepare Test Stimuli\nVisual stimuli\n\t\nDownload 60 clip arts of animals that are free of copyright from the internet. Open each image one by one with an image editor (e.g., Pixelmator), click Tools | Quick selection tool to select and delete the background. Click Image | Image Size to resize them to 120 x 120 pixels.\nInvite a student majoring in painting to draw four light green boxes, as illustrated in Figure 1. Use the image editor to rescale the big open box to 320 x 240 pixels, the small closed box with the size of 160 x 160 pixels, and the two small open boxes to 160 x 240 pixels, respectively.",
    "Click Pixelmator | File | New to build a template of the test image with the size of 1024 768 pixels. Drag the animals and the boxes to correction locations being illustrated in Figure 1.\n\t\tNOTE: The layout of the test image varies between studies, but the optimal way is to use four objects and to put them at the four quadrants of the test image. In this way, it is easier to counterbalance the spatial position of the objects.\nCreate 60 test images like Figure 1, with each animal image being used twice. Counterbalance the spatial locations of the four boxes among the images.\n\t\tNOTE: The number of the images does not have to be exact 60, as long as their effect is dissociable from that of the experimental manipulations.\nSpoken language stimuli\n\t\nDesign four test sentences corresponding to each test image and 240 test sentences in total to be recorded. Ensure that three of the four sentences are in the form of Figure 2; and the filler sentence is in the form of Xiaoming's box doesn't contain a rooster but a cow.\n\t\tNOTE: The test sentences should be presented in the native language that participants speak. The participants in this experiment are Chinese from Beijing, Mainland China, so the test language is Mandarin Chinese.\nRecruit a female native speaker (a native speaker of Mandarin Chinese in this experiment) to record four example statements like Figure 2, as well as audio of all the animals being used in the experiment. When recording the isolated animal names, ask the speaker to imagine that the names of the animals are intact components of a simple sentence, such as Xiaoming's box contains a/an ___, but she only needs to pronounce the name of the animal overtly.",
    "Replace the audio segments of the two animals in the example statements with the audio of the two animals used in each trial to create the full list of the test audios. First, open Praat (Any other audio editing software is an eligible alternative) and click Open | Read from file | Navigate to the file | Open and edit, navigate to an element to be replaced, and click View and Edit | Edit | Copy selection to sound clipboard. Second, use the same steps to open an example statement, click paste after selection. Third, click Save | save as wav file to save the edited statement. Repeat the process for all the elements to be changed and all the test sentences.\nRecruit about 10 native speakers of the test language (Mandarin Chinese here) to determine whether or not the constructed test audio is intelligible and natural.",
    "NOTE: The test audio is traditionally recorded as a whole, rather than as separate words. This traditional recording method is reasonable if the test audio are themselves separate words. If the spoken language stimuli are sentences rather than separate words, however, this traditional method has several shortcomings: First, a ubiquitous property of a continuous speech is that two or more speech sounds tend to temporally and spatially overlap, which makes it hard to determine the onset of the critical word. Second, the variance between the length of different trials also makes it difficult to combine all the trials together for statistical analyses. Third, the traditional recording method is often time consuming especially when the numbers of the test audio are relatively large, such as the experiments we reported in the protocol. To overcome the shortcomings of the traditional recording method, a different method is proposed to construct the spoken test audios. First, a list of sample sentences containing the words that are common among all the test audio was recorded. Second, all words that change between trials were also recorded in isolation. Finally, sample sentences were replaced with the recorded words to construct the full list of the test audios. Compared to the traditional method, the new method has several advantages. First, all the test audio is exactly the same except for the critical words, and all potential confounding effects in the test audio are henceforth controlled. Second, being the same in length also makes the segmentation of the test audios easier than when the test audios are recorded as a whole. One potential disadvantage of this method is that the constructed audio might be not natural. Henceforth, the naturalness of the test audio has to be evaluated before they are eligible for the actual testing..",
    "Divide the 240 test sentences into four groups, with each group containing 15 conjunctive statements, 15 disjunctive statements, 15 but statements, and 15 filler sentences. Ensure that each participant encounters only one group of 240 trials: he/she sees all the test images but hears only one group of the test audios.\n\tNOTE: This is to address the concern that if the same stimulus is repeated, participants might be getting accustomed to these stimuli and possibly even becoming strategic about how they have responded to the stimuli.\nSave all important information regarding the test stimuli into a tab-delimited txt file, with each row corresponding to each of the 240 trials. Ensure that the file includes at least the following columns: experiment_group, sentential_connective, trial_number, test_image, test_audio, test_audio_length, ia_top_left, ia_top_right, ia_bottom_left, ia_bottom_right, animal_1_image, animal_1_audio, animal_1_audio_length, animal_2_image, animal_2_audio, animal_2_audio_length.",
    "NOTE: experiment_group is used to split the 240 trials into 4 groups. sentential_connective corresponds to different experimental conditions. animal_1_image corresponds to the image of the animal that will be firstly presented to familiarize the participants with the animals used in the test image. test_image, test_audio, and test_audio_length refer to the test image and the test audio as well its length used in the current trial. ia_top_left, ia_top_right, ia_bottom_left, ia_bottom_right refer to the name of the four interest areas in the current trial, i.e., whether it is a \"Big open\" box, \"small closed\" box, the small open box containing the \"first mentioned\" animal in the test audio, or the small open box containing the \"second mentioned\" animal in the test audio. animal_1_audio and animal_1_audio_length refer to the audio and length of the audio corresponding to the animal_1_image. animal_2_image, animal_2_audio, and animal_2_audio_length correspond the second animal that will be presented. One thing to stress is that the sequence to present the two animals is counterbalanced with respect to whether the animal is mentioned in the first or in the second half of the test audios.\n2. Frame the Theoretical Prediction with regard to the Experimental Design.\nEnsure participants' behavioral responses and eye-movements in the experimental design can be used to differentiate comprehensions of the test sentences and can be used to adjudicate between different accounts to be tested.",
    "NOTE: Given the experimental design, the correct response to a conjunctive statement is the big open box, such as Box A in Figure 1. The correct response to a but-statement is the small open box containing the animal being mentioned in the first half of the test audios, such as Box D in Figure 1. Participants' responses to the disjunctive statement, however, depend on whether and/or how the two discussed inferences are processed. If participants compute neither the scalar implicature nor the ignorance inference, then all the four boxes are eligible options. If participants compute the scalar implicature but not the ignorance inference, then the big open, such as box A in Figure 1, will be ruled out, and the remaining three boxes B, C, and D are all eligible options. If participants compute the ignorance inference but not the scalar implicature, then the small open boxes will be ruled out, i.e., boxes C and D will be ruled out. To summarize, the small closed box, such as box B in Figure 1, will not be chosen as the final option of a disjunctive statement until the scalar implicature and the ignorance inferences are both computed.\n3. Build the Experimental Script\nOpen the Experiment Builder, click File | New to create an experiment project. Input the project name such as vwp_disjunction. Select the project location. Check EyeLink Experiment and choose Eyelink 1000plus from the drop list. These operations will create a subdirectory containing all files related to the experiment It will create a subdirectory named vwp_disjunction with a file named \"graph.ebd\" in the folder.",
    "NOTE: Experiment Builder is used to build the experimental script to present the test stimuli and to record participants' eye movements as well as their behavioral responses. The Experiment Builder is a What-You-See-Is-What-You-Get tool to build experimental script. It is easy to use, but any other stimuli presentation software is an eligible alternative.\nVisualize the hierarchical structure of a typical eye-tracking experiment using the visual world paradigm as seen in Figure 3. Each pink rectangle in the figure is implemented as a SEQUENCE object by Experiment Builder; and each object with gray background is implemented as a node object.\n\tNOTE: A SEQUENCE in the Experiment Builder is an experimental loop controller used to chain together different objects as a complex node. A sequence always begins with a START node. And a data source can be attached to a sequence node to supply different parameters for each trial.\nBuild the Experiment sequence\n\t\nClick File | Open, browse to the directory of experiment and double click the graph.ebd file in the project directory to open the saved experiment project.\nClick Edit | Library Manager | Image | Add to load the images into the experiment Project. Similarly, click Edit | Library Manager | Sound | Add to load the audio into the experiment project.\nDrag a DISPLAY_SCREEN object into the work space and change its label value on the properties panel to rename it as Instruction. Double click to open the Instruction node, and click the Insert Multiline Text Resource button to input the experimental instruction. Ensure the instruction contains the following information:",
    "In each trial, first you will see images of two animals, one animal each printed on the screen in turn, along with the audio of the animals played on the two speakers situated at both sides of the screen. A black dot will then be presented at the center of the screen. You should press the SPACE key while fixating on the dot. Next, you will see a test image consisting of four boxes printed on the screen and hear a test sentence being played via the two speakers. Your task is to locate Xiaoming's box according to the test sentence you heard and press the corresponding button as soon as possible:\n\t\tTop left box --- Left arrow\n\t\tTop Right Box --- Up arrow\n\t\tBottom left box --- Left arrow\n\t\tBottom right box --- Right arrow\n\t\tIn each test image, you will see four boxes situated at the four quadrants and two animals containing in the boxes. The four boxes can vary in two dimensions: its closeness and its size. Whether a box is closed or not influences our epistemic knowledge on that box, but not the animal(s) it contains. If a box is open, then the animal(s) contained in that box is known. If a box is closed, then the animal(s) contained in that box is unknown. The size of a box affects the number of animals contained in the box, but not our epistemic knowledge on that box. No matter the box is closed or not, a small box only and always contains one animal, and a big box always contains two different animals.",
    "If you are comfortable with the experimental aim and the procedure, please let the experimenter know and we will help you to perform the standard eye tracking calibration and validation routines. If you have any questions, please don't hesitate to ask.\n\t\t​Note: This is an instruction that will be printed on the screen prior to the experiment (The instructions should be written in the native language the participants speak, such as Mandarin Chinese here).\nDrag a KEYBOARD object into the work space.\n\t\tNOTE: This step is used to end the Instruction screen\nDrag a SEQUENCE object into the work space and rename it as Block.\nSelect the Block sequence, click the value field of the Data Source property to bring up the Data Source Editor. Click the Import Data Button on the data source editor screen, brow to the .txt file created in step 1.4 to import the data source.\nClick the Randomization Setting button in the data source editor, check Enable Trial Randomization, select trial_number from the value field of the Column field, and select experimental_group from the drop-list of the Splitting Column field.\nDrag the second DISPLAY_SCREEN object to the work space and rename it as Goodbye. Double click the Goodbye node and insert the following information:  in participants' native language (Mandarin Chinese in this protocol):The experiment is finished and Thank you for very much your participation.\nLeft-click on the START node, drag the arrow to the Instruction node, and release the mouse button to connect the START node to the Instruction node. Repeat the same mouse moves to connect Instruction node to the KEYBOARD node, KEYBOARD node to Block node, then Block node to the Goodbye node. Click View | Arrange Layout to arrange the nodes in the workspace.\nBuild the block sequence",
    "Double click to open the Block sequence. Drag an El_CAMERA_SETUP node into the Block sequence to bring up a camera setup screen on the EyeLink Host PC for the experimenter to perform camera setup, calibration, and validation. Click the Calibration Type field in the Properties panel and choose HV5 from the dropdown list.\n\t\tNOTE: The number of locations in the mapping process varies between different experimental designs. The more locations sampled and the more space covered, the greater the accuracy can be expected. But more samples mean more time to finish the processes. So practically, the number of locations in a specific study cannot be very big, especially when participants are preliterate children or clinical patients. In the visual world paradigm, the number of the interest areas is relatively small, and the areas of interest are normally relatively big. The mapping process can reach a satisfying level with relatively small number of locations. In the protocol I described, I used a five points' calibration and validation.\nDrag a SEQUENCE node into the Block sequence and rename it as Trial. Connect the START node to the CAMERA_SETUP node, then to the SEQUENCE node.\nBuild the Trial sequence\n\t\nDouble click to open the Trial sequence, drag a DISPLAY_SCREEN node into the Trial sequence and rename it as animal_1_image. Double click to open the Screen Builder node and click the Insert Image Resource button on the Screen Builder toolbar to insert an animal image from the uploaded image sources. Click the value field of the Source File Name property, navigate to the DataSource attached to the Block Sequence; and double click the Animal_1_Image column to connect the DISPLAY_SCREEN with the correct column of the data source.",
    "Drag a PLAY_SOUND node into the Trial sequence and rename it as animal_1_audio. Click the Sound File property of the animal_1_audio node and connect it with the correct column of the data source (as being described in step 3.5.1).\nDrag a TIMER node into the Trial sequence and rename it as animal_1_audio_length. Click the Duration Property of the TIMER node and navigate to the correct column of the data source created in 3.4.1.\nDrag another DISPLAY_SCREEN node, another PLAY_SOUND node, and another TIMER node into the Trial sequence, rename them as animal_2_image, animal_2_audio, and animal_2_audio_duration, repeat the steps being described in steps 3.5.1 - 3.5.3.\n\t\tNOTE: These steps are included to control for the potential confounding that the same image might be named differently by different participants. Counterbalance the sequence of presenting the two animals with respect to whether it is mentioned in the first or second half of the test audios.\nDrag a Prepare Sequence object into the Trial Sequence and change the property Draw To Eyelink Host to IMAGE.\n\t\tNOTE: This node is used to preload the image and audio files to memory for real-time image drawing and sound playing. And it is also used to draw feedback graphics on the Host PC so that the participants' gaze accuracy can be monitored.\nDrag a DRIFT_CORRECT node into the Trial sequence to introduce the drift correction.\nDrag a new SEQUENCE node and rename it as Recording. Connect the START to these nodes one after one.\nBuild the Recording sequence\n\t\nCheck the Record field in the property panel of the Recording sequence, and double click to open the Recording sequence.\n\t\tNOTE: A sequence with Record property checked means that participants' eye movements during this period will be recorded.",
    "Drag a new DISPLAY_SCREEN into the Record sequence, rename it as test_image. Add the message test_image_onset into the Message property of the test_image node.\n\t\tNOTE:  In data analyses stage, the message in the test_image node and the message in the test_audio node (section 3.6.6) are important to locate the onset of the test images and the onset of the test audios in each trial.\nDouble click to open the Screen Builder node and click the Insert Image Resource button on the Screen Builder toolbar to insert any animal image from the uploaded image sources. Click the value field of the Source File Name property, navigate to the DataSource attached to the Block Sequence; and double click the test_image column to connect the DISPLAY_SCREEN with correct column of the data source.\nDouble click the DISPLAY_SCREEN node to open the Screen Builder, click the button of Insert Rectangle Interest Area Region, and draw four rectangular areas of interest as illustrated by the blue boxes in Figure 1. Change the labels of the four areas of interest to Top_Left, Top_Right, Bottom_Left, and Bottom_Right, and connect the DataViewer Name filed with the correct columns of the data source.\n\t\tNOTE: These areas are invisible to the participants. To make the areas of interest more meaningful, label the name of top left area in the example as \"Box A (big open)\", area top right area as \"Box B (small closed)\", bottom left area as \"Box C (second mentioned)\", and area bottom right area as \"Box D (First Mentioned)\", because the two small open boxes contain the two animals being mentioned in the first and second half of the test audios, respectively.\nDrag a TIMER node into the workspace, rename it as Pause, and change the Duration property to 500 ms.",
    "NOTE: This TIMER node adds some time lag between the onset of the test image and the onset of the test audio. The time lag gives participants a chance to familiarize with the test images. Participants' eye movements during this preview period also provide a baseline for determining the effects of the spoken language input, especially when the critical words are situated at the beginning of the test audios.\nDrag a PLAY_SOUND node in to the work space and rename it as test_audio. Click the Sound File property and connect it with the correct column of the data source (as being described in step 3.5.1) and add the message test_audio_onset into the Message property.\nDrag a TIMER node into the work space, rename it as test_audio_length. Change the Duration Property to 10500 ms.\nDrag a Null Action -node into the work space.\nAdd a new TIMER node, rename it as record_extension, and change the Duration property to 4000 ms.\nAdd a new KEYBOARD node into the work space, rename it as behavioral responses, and change the acceptable Keys property to \"[Up, Down, Right, Left]\".\n\t\tNOTE: Participants' behavioral choices can be used to double check the validity of the conclusion deduced from participants' eye movements.\nConnect the START node to Pause, test_audio, test_audio_length, NULL Action, then to Record_extension node. Add another connection from test_audio_length to behavioral_responses node.\n\t\tNOTE: By adding these connections, current trial will end and a new trial will start after participants made a key press to choose Xiaoming's Box, or 4000 ms after the offset of the test audio.\nDrag a VARIABLE node into the work space, rename it as key_pressed, and connect its value property to behavioral_Responses Keyboard | Triggered Data | Key.",
    "Drag a RESULT_FILE node into the work space, drag an ADD_TO_RESULT_FILE node into the work space, and connect both the record_extension node and the behavioral_responses node to the ADD_TO_RESULT_FILE node.\nClick Experiment | Build to build the experimental script, click Experiment | Test run to test run the experiment. After everything is done, click Experiment | Deploy to create an executable version of the experimental project.\n\tNOTE: For more information on how to use the Experiment Builder, please consult the software manual27.\n4. Recruit Participants\nEnsure the participants to have normal or corrected normal vision. Recommend that the short-sighted participants to wear contact lenses, but glasses are also acceptable as long as the lenses are clean. Ensure that all participants are native speakers of the testing language, such as Mandarin Chinese here.\n\tNOTE: As a general guideline, a participant is regarded as eligible as long as the participant can see the test images at a distance of about 60 centimeters. In terms of the number of participants, according to some rules of thumb, the number of participants for regression analysis should be no less than 50. Here, thirty-seven postgraduate students from the Beijing Language and Culture University participated in the experiment, which is a little smaller than the recommended amount.\n5. Conduct the Experiment\nNOTE: When participants are normal developed adults, one experimenter is enough to conduct the conduct the experiment. But if participants are special populations, such as children, two or more experimenters are required.\nSelect an eye tracker to record participants' eye movements.",
    "NOTE: The eye tracker used in this experiment is Eyelink 1000plus running under the free-to-move head mode. This is a video-based, desktop mounted eye tracking system, using the principle of pupil with corneal reflection (CR) to track eye's rotation. When running under the free-to-move head mode, the eye tracker has the monocular sampling rate of 500 Hz, with a spatial resolution of 0.01° and an average error of less than 0.5°. For more detailed information of the system, please consult its technical specification28,29. Alternative trackers can be used, but the ones with remote tracking mode are better, especially when participants are preliterate children.\nBoot the system on the Host PC to start the Host application of the camera.\nTo configure the system to desktop remote mode, click the Set Option button, set the Configuration option to Desktop -- Target Sticker -- Monocular -- 16/25mm length -- RTARBLER.\nClick the executable version of the experimental project on the Display PC, input participant's name, and choose a group from the prompt window to select the condition value to run.\n\tNOTE: Each test session will create a folder with the inputted name under the subdirectory Results of the experiment project. The EDF file under the folder contained relevant eye movements data.\nAsk the participants to sit approximately 60 cm from a 21 inch, 4:3 color monitor with 1024px x 769px resolution, where 27 pixels equals to 1 degree of angle.\nAdjust the height of the Display PC monitor, to ensure that when the participant is seated and looking straight ahead, they are looking vertically at the middle to top 75% of the monitor.",
    "NOTE: The chair, desk, and/or the PC monitor are preferred if they are adjustable in height. The chair and the desk with casters should be avoided, as they tend to cause unintentional move and roll.\nPlace a small target sticker on the participant's forehead, to track the head position even when the pupil image is lost, such as during blinks or sudden movements.\n\tNOTE: Different eye trackers might use different methods to track participants' head. To maximize the lateral movement range of the subject, the tracked eye should be on the same side as the illuminator.\nRotate the focusing arm on the desk mount to bring the eye image into focus.\nClick the Calibrate button on the host PC to conduct the calibration process by asking participants to fixate a grid of five fixation targets in random succession with no overt behavioral responses, to map participants' eye movements to the gaze of regard in the visual world.\nClick the Validate button on the host PC to validate the calibrated results by asking participants to fixate the same grid of fixation targets. Repeat the calibration and validation routines, when the error is bigger than 1°.\nConduct the two routines at the beginning of the experiment and whenever the measurement accuracy is poor (e.g., after strong head movements or a change in the participants' posture).\nClick the Record button on the host PC to start the experiment.\nPerform a drift check on each trial by asking participants to press the SPACE key on the keyboard while fixating at the black dot presented in the center of the screen.",
    "NOTE: When the participants are preliterate children or clinical patients, explicitly instructing them to press the keyboard while fixating the black dot is normally impractical. But their attention and eye fixations tend to be automatically attracted by the displayed black dot. In this case, the experimenter should be the person to press the keyboard while the participant is fixating on the black dot.\nPresent the visual stimuli via the Display PC monitor and play the auditory stimuli via a pair of external speakers situated to the left and right of the monitor (earphones are also acceptable).\n\tNOTE: The recordings are played from the hard disk as 24 kHz mono sound clips. If there is no special reason, mono sound clips are preferred to stereo sound clips. In a stereo sound clip, the difference between the two sound tracks, as well as the difference between the two speakers might affect participants' eye movements. For more information on how to use the eye tracker, please consult the user manual30.\n6. Data Coding and Analyses\nOpen Data Viewer, click File | Import File | Import Multiple Eyelink Data Files to import all the recorded eye tracker files (with the extension of EDF), and save them into a single .EVS file.\nOpen the saved EVS file and click Analysis | Reports | Sample Report to export the raw sample data with no aggregation.",
    "NOTE: If the eye tracker has a sampling rate of 500 Hz, the exported data will have 500 data points, henceforth 500 rows, per second per trial. If participants' left eye is tracked, ensure the following columns as well as the variables created in the data source are exported: RECORDING_SESSION_LABEL, LEFT_GAZE_X, LEFT_GAZE_Y, LEFT_INTEREST_AREA_LABEL, LEFT_IN_BLINK, LEFT_IN_SACCADE, LEFT_PUPIL_SIZE, SAMPLE_INDEX, SAMPLE_MESSAGE. For more information on how to use the Data Viewer, please consult the software manual31.\nRestrict the statistical analyses to the temporal window from the onset of the test image to the offset of the test audios, i.e., the temporal window with the duration of 11 s.\nDelete the samples where participants' eye movements are not recorded, such as participants blink their eyes, which roughly affects 10% of the recorded data.\n\tNOTE: This is an optional step, as the results are normally the same no matter whether these samples deleted.\nCode the data. To construct the data for a specific area of interest in a certain sampling point, code the data as 1 if participants' eye fixation is situated in the area of interest to be analyzed at that sampling point. Code the data as 0 if the eye fixation is not situated in the areas of interest at that sampling point.\nDraw a proportion-of-fixation to visualize the obtained data. To calculate the proportion-of-fixations over certain area of interest, average the coded data for all the trials and for all the participants in each sample point under each condition. Plot the calculated proportion-of-fixations on the y-axis against the sampling point on the x-axis, with different panels denoting different areas of interest and with the plotting colors denoting different experimental conditions.",
    "NOTE: In the experiment, the four panels depicted participants' fixation patterns on the four areas of interest. The red, green, and blue lines illustrated participants' fixation patterns when the test statements were conjunctions (S1 and S2), but-statements (S1 but not S2), and disjunctions (S1 or S2), respectively. The software used to draw the descriptive plot is the ggplot2 package from R environment. Other software is also available. Figure 5 is an example of such plot.\nFit a binomial generalized linear mixed model (GLMM) on each area of interest at each sampling point, as the data was coded as either 1 or 0, depending on whether the participant's fixation is situated in or out of the area of interest at that sampling point.\n\tNOTE: As the data is not binned, and the coded data can only be 1 or 0, so the distribution of the coded data is binary rather than normal. Henceforth, a GLMM model with the family of binomial distribution is used. The GLMM model includes a fixed term, the experimental conditions, and two random terms, participants and items. The formula evaluated to the two random terms includes both the intercepts and the slope of the experimental conditions. The software used to do the model fitting is the lme4 package from R environment. Other software is also available. One thing should be mentioned is that the baseline of the fixed items differed when the analyzed interest area, i.e., the analyzed boxes, are different. To be specific, the conjunction (S1 and S2) was chosen as the baseline when analyzing the big-open box (Box A), the disjunction (A and B) was chosen as the baseline when analyzing the small-closed box (Box B), and the but-statement was chosen as the baseline when analyzing the first-mentioned box (Box D).",
    "Bonferroni adjust the p values obtained with Wald z test, to reduce the familywise error induced by multiple comparisons.\n\tNOTE: Bonferroni adjustment is the traditional way to tackle the familywise error induced by multiple comparisons. Other methods are also available, as we described in the introduction section.\nSubscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Behavior"
  ],
  "bigAreas": [
    "Ecology & Environmental Biology"
  ]
}