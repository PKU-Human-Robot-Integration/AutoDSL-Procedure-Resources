{
  "id": 8868,
  "origin_website": "Jove",
  "title": "Tracking Rats in Operant Conditioning Chambers Using a Versatile Homemade Video Camera and DeepLabCut",
  "procedures": [
    "All procedures that include animal handling have been approved by the Malmö-Lund Ethical committee for animal research.\n1. Building the video camera\nNOTE: A list of the components needed for building the camera is provided in the Table of Materials. Also refer to Figure 1, Figure 2, Figure 3, Figure 4, Figure 5.\nAttach the magnetic metal ring (that accompanies the fisheye lens package) around the opening of the camera stand (Figure 2A). This will allow the fisheye lens to be placed in front of the camera.\nAttach the camera module to the camera stand (Figure 2B). This will give some stability to the camera module and offer some protection to the electronic circuits.\nOpen the camera ports on the camera module and microcomputer (Figure 1) by gently pulling on the edges of their plastic clips (Figure 2C).\nPlace the ribbon cable in the camera ports, so that the silver connectors face the circuit boards (Figure2C). Lock the cable in place by pushing in the plastic clips of the camera ports.\nPlace the microcomputer in the plastic case and insert the listed micro SD card (Figure 2D).\n\tNOTE: The micro SD card will function as the microcomputer’s hard drive and contains a full operating system. The listed micro SD card comes with an installation manager preinstalled on it (New Out Of Box Software (NOOBS). As an alternative, one can write an image of the latest version of the microcomputer’s operating system (Raspbian or Rasberry Pi OS) to a generic micro SD card. For aid with this, please refer to official web resources18. It is preferable to use a class 10 micro SD card with 32 Gb of storage space. Larger SD cards might not be fully compatible with the listed microcomputer.",
    "Connect a monitor, keyboard and a mouse to the microcomputer, and then connect its power supply.\nFollow the steps as prompted by the installation guide to perform a full installation of the microcomputer’s operating system (Raspbian or Rasberry Pi OS). When the microcomputer has booted, ensure that it is connected to internet either through an ethernet cable or Wi-Fi.\nFollow the steps outlined below to update the microcomputer’s preinstalled software packages.\n\t\nOpen a terminal window (Figure 3A).\nType “sudo apt-get update” (excluding quotation marks) and press the Enter key (Figure 3B). Wait for the process to finish.\nType “sudo apt full-upgrade” (excluding quotation marks) and press enter. Make button responses when prompted and wait for the process to finish.\nUnder the Start menu, select Preferences and Raspberry Pi configurations (Figure 3C). In the opened window, go to the Interfaces tab and click to Enable the Camera and I2C. This is required for having the microcomputer work with the camera and IR LED modules.\nRename Supplementary File 1 to “Pi_video_camera_Clemensson_2019.py”. Copy it onto a USB memory stick, and subsequently into the microcomputer’s /home/pi folder (Figure 3D). This file is a Python script, which enables video recordings to be made with the button switches that are attached in step 1.13.\nFollow the steps outlined below to edit the microcomputer’s rc.local file. This makes the computer start the script copied in step 1.10 and start the IR LEDs attached in step 1.13 when it boots.\n\tCAUTION: This auto-start feature does not reliably work with microcomputer boards other than the listed model.\n\t\nOpen a terminal window, type “sudo nano /etc/rc.local” (excluding quotation marks) and press enter. This opens a text file (Figure 4A).\nUse the keyboard’s arrow keys to move the cursor down to the space between “fi” and “exit 0” (Figure 4A).",
    "Add the following text as shown in Figure 4B, writing each string of text on a new line:\n\t\tsudo i2cset -y 1 0x70 0x00 0xa5 &\n\t\tsudo i2cset -y 1 0x70 0x09 0x0f &\n\t\tsudo i2cset -y 1 0x70 0x01 0x32 &\n\t\tsudo i2cset -y 1 0x70 0x03 0x32 &\n\t\tsudo i2cset -y 1 0x70 0x06 0x32 &\n\t\tsudo i2cset -y 1 0x70 0x08 0x32 &\n\t\tsudo python /home/pi/Pi_video_camera_Clemensson_2019.py &\nSave the changes by pressing Ctrl + x followed by y and Enter.\nSolder together the necessary components as indicated in Figure 5A, and as described below.\n\t\nFor the two colored LEDs, attach a resistor and a female jumper cable to one leg, and a female jumper cable to the other (Figure 5A). Try to keep the cables as short as possible. Take note of which of the LED’s electrodes is the negative one (typically the short one), as this needs to be connected to ground on the microcomputer’s general-purpose input/output (GPIO) pins.\nFor the two button switches, attach a female jumper cable to each leg (Figure 5A). Make the cables long for one of the switches, and short for the other.\nTo assemble the IR LED module, follow instructions available on its official web resources19.\nCover the soldered joints with shrink tubing to limit the risk of short-circuiting the components.\nSwitch off the microcomputer and connect the switches and LEDs to its GPIO pins as indicated in Figure 5B, and described below.\n\tCAUTION: Wiring the components to the wrong GPIO pins could damage them and/or the microcomputer when the camera is switched on.",
    "Connect one LED so that its negative end connects to pin #14 and its positive end connects to pin #12. This LED will shine when the microcomputer has booted and the camera is ready to be used.\nConnect the button switch with long cables so that one cable connects to pin #9 and the other one to pin #11. This button is used to start and stop the video recordings.\n\t\tNOTE: The script that controls the camera has been written so that this button is unresponsive for a few seconds just after starting or stopping a video recording.\nConnect one LED so that its negative end connects to pin #20 and its positive end connects to pin #13. This LED will shine when the camera is recording a video.\nConnect the button switch with the short cables so that one cable connects to pin #37 and the other one to pin #39. This switch is used to switch off the camera.\nConnect the IR LED module as described in its official web resources19.\n2. Designing the operant conditioning protocol of interest\nNOTE: To use DeepLabCut for tracking the protocol progression in videos recorded from operant chambers, the behavioral protocols need to be structured in specific ways, as explained below.\nSet the protocol to use the chamber’s house light, or another strong light signal, as an indicator of a specific step in the protocol (such as the start of individual trials, or the test session) (Figure 6A). This signal will be referred to as the “protocol step indicator” in the remainder of this protocol. The presence of this signal will allow tracking protocol progression in the recorded videos.\nSet the protocol to record all responses of interest with individual timestamps in relation to when the protocol step indicator becomes active.",
    "3. Recording videos of animals performing the behavioral test of interest\nPlace the camera on top of the operant chambers, so that it records a top view of the area inside (Figure 7).\n\tNOTE: This is particularly suitable for capturing an animals’ general position and posture inside the chamber. Avoid placing the camera’s indicator lights and the IR LED module close to the camera lens.\nStart the camera by connecting it to an electrical outlet via the power supply cable.\n\tNOTE: Prior to first use, it is beneficial to set the focus of the camera, using the small tool that accompanies the camera module.\nUse the button connected in step 1.13.2 to start and stop video recordings.\nSwitch off the camera by following these steps.\n\t\nPush and hold the button connected in step 1.13.4 until the LED connected in step 1.13.1 switches off. This initiates the camera’s shut down process.\nWait until the green LED visible on top of the microcomputer (Figure 1) has stopped blinking.\nRemove the camera’s power supply.\n\t\tCAUTION: Unplugging the power supply while the microcomputer is still running can cause corruption of the data on the micro SD card.\nConnect the camera to a monitor, keyboard, mouse and USB storage device and retrieve the video files from its desktop.\n\tNOTE: The files are named according to the date and time when video recording was started. However, the microcomputer does not have an internal clock and only updates its time setting when connected to the internet.\nConvert the recorded videos from .h264 to .MP4, as the latter works well with DeepLabCut and most media players.\n\tNOTE: There are multiple ways to achieve this. One is described in Supplementary File 2.\n4. Analyzing videos with DeepLabCut",
    "NOTE: DeepLabCut is a software package that allows users to define any object of interest in a set of video frames, and subsequently use these to train a neural network in tracking the objects’ positions in full-length videos15,16. This section gives a rough outline for how to use DeepLabCut to track the status of the protocol step indicator and the position of a rat’s head. Installation and use of DeepLabCut is well-described in other published protocols15,16. Each step can be done through specific Python commands or DeepLabCut’s graphic user interface, as described elsewhere15,16.\nCreate and configure a new DeepLabCut project by following the steps outlined in16.\nUse DeepLabCut’s frame grabbing function to extract 700‒900 video frames from one or more of the videos recorded in section 3.\n\tNOTE: If the animals differ considerably in fur pigmentation or other visual characteristics, it is advisable that the 700‒900 extracted video frames are split across videos of different animals. Through this, one trained network can be used to track different individuals.\n\t\nMake sure to include video frames that display both the active (Figure 8A) and inactive (Figure 8B) state of the protocol step indicator.\nMake sure to include video frames that cover the range of different positions, postures and head movements that the rat may show during the test. This should include video frames where the rat is standing still in different areas of the chamber, with its head pointing in different directions, as well as video frames where the rat is actively moving, entering nose poke openings and entering the pellet trough.",
    "Use DeepLabCut’s Labeling Toolbox to manually mark the position of the rat’s head in each video frame extracted in step 4.2. Use the mouse cursor to place a “head” label in a central position between the rat’s ears (Figure 8A,B). In addition, mark the position of the chamber’s house light (or other protocol step indicator) in each video frame where it is actively shining (Figure 8A). Leave the house light unlabeled in frames where it is inactive (Figure 8B).\nUse DeepLabCut’s “create training data set” and “train network” functions to create a training data set from the video frames labeled in step 4.3 and start the training of a neural network. Make sure to select “resnet_101” for the chosen network type.\nStop the training of the network when the training loss has plateaued below 0.01. This may take up to 500,000 training iterations.\n\tNOTE: When using a GPU machine with approximately 8 GB of memory and a training set of about 900 video frames (resolution: 1640 x 1232 pixels), the training process has been found to take approximately 72 h.\nUse DeepLabCut’s video analysis function to analyze videos gathered in step 3, using the neural network trained in step 4.4. This will provide a .csv file listing the tracked positions of the rat’s head and the protocol step indicator in each video frame of the analyzed videos. In addition, it will create marked-up video files where the tracked positions are displayed visually (Videos 1-8).\nEvaluate the accuracy of the tracking by following the steps outlined below.",
    "Use DeepLabCut’s built-in evaluate function to obtain an automated evaluation of the network’s tracking accuracy. This is based on the video frames that were labeled in step 4.3 and describes how far away on average the position tracked by the network is from the manually placed label.\nSelect one or more brief video sequences (of about 100‒200 video frames each) in the marked-up videos obtained in step 4.6. Go through the video sequences, frame by frame, and note in how many frames the labels correctly indicate the positions of the rat’s head, tail, etc., and in how many frames the labels are placed in erroneous positions or not shown.\n\t\t\nIf the label of a body part or object is frequently lost or placed in an erroneous position, identify the situations where tracking fails. Extract and add labeled frames of these occasions by repeating steps 4.2. and 4.3. Then retrain the network and reanalyze the videos by repeating steps 4.4-4.7. Ultimately, tracking accuracy of >90% accuracy should be achieved.\n5. Obtaining coordinates for points of interest in the operant chambers\nUse DeepLabCut as described in step 4.3 to manually mark points of interest in the operant chambers (such as nose poke openings, levers, etc.) in a single video frame (Figure 8C). These are manually chosen depending on study-specific interests, although the position of the protocol step indicator should always be included.\nRetrieve the coordinates of the marked points of interest from the .csv file that DeepLabCut automatically stores under “labelled data” in the project folder.\n6. Identifying video segments where the protocol step indicator is active\nLoad the .csv files obtained from the DeepLabCut video analysis in step 4.6 into a data management software of choice.",
    "NOTE: Due to the amount and complexity of data obtained from DeepLabCut and operant conditioning systems, the data management is best done through automated analysis scripts. To get started with this, please refer to entry-level guides available elsewhere20,21,22.\nNote in which video segments the protocol step indicator is tracked within 60 pixels of the position obtained in section 5. These will be periods where the protocol step indicator is active (Figure 6B).\n\tNOTE: During video segments where the protocol step indicator is not shining, the marked-up video might seem to indicate that DeepLabCut is not tracking it to any position. However, this is rarely the case, and it is instead typically tracked to multiple scattered locations.\nExtract the exact starting point for each period where the protocol step indicator is active (Figure 6C: 1).\n7. Identifying video segments of interest\nConsider the points where the protocol step indicator becomes active (Figure 6C: 1) and the timestamps of responses recorded by the operant chambers (section 2, Figure 6C: 2).\nUse this information to determine which video segments cover specific interesting events, such as inter-trial intervals, responses, reward retrievals etc. (Figure 6C: 3, Figure 6D).\n\tNOTE: For this, keep in mind that the camera described herein records videos at 30 fps.\nNote the specific video frames that cover these events of interest.\n(Optional) Edit video files of full test sessions to include only the specific segments of interest.\n\tNOTE: There are multiple ways to achieve this. One is described in Supplementary File 2 and 3. This greatly helps when storing large numbers of videos and can also make reviewing and presenting results more convenient.\n8. Analyzing the position and movements of an animal during specific video segments",
    "Subset the full tracking data of head position obtained from DeepLabCut in step 4.6 to only include video segments noted under section 7.\nCalculate the position of the animal’s head in relation to one or more of the reference points selected under section 5 (Figure 8C). This enables comparisons of tracking and position across different videos.\nPerform relevant in-depth analysis of the animal’s position and movements.\n\tNOTE: The specific analysis performed will be strongly study-specific. Some examples of parameters that can be analyzed are given below.\n\t\nVisualize path traces by plotting all coordinates detected during a selected period within one graph.\nAnalyze proximity to a given point of interest by using the following formula:\nimgsrc://cloudfront.jove.com/files/ftp_upload/61409/61409equ01.jpg\nAnalyze changes in speed during a movement by calculating the distance between tracked coordinates in consecutive frames and divide by 1/fps of the camera.\nSubscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Behavior"
  ],
  "bigAreas": [
    "Ecology & Environmental Biology"
  ]
}