{
  "id": 2905,
  "origin_website": "Cell",
  "title": "A practical guide to deep-learning light-field microscopy for 3D imaging of biological dynamics",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nImplementation of light-field microscopy\nTiming: ∼1–4 days\n    \n      This section describes the construction procedures of a light-field\n      microscope (Figure 1[href=https://www.wicell.org#fig1]) by augmenting a commercial\n      epifluorescence microscope (Olympus, BX51) (see\n      key resources table[href=https://www.wicell.org#key-resources-table]). The basic\n      principle of LFM is that light with different angles can be re-allocated\n      on different pixels of the camera sensor by using a microlens array (MLA)\n      (see key resources table[href=https://www.wicell.org#key-resources-table]), thereby\n      permitting the recording of both position and angular information of 3D\n      signals through a single 2D light field snapshot. So, the critical steps\n      of building a light-field microscope include 1. the insertion of\n      MLA at the native image plane (NIP) of the microscope to modulate the\n      light with different angles and 2. correct positioning of the\n      camera sensor at the back focal plane of MLA to collect the encoded light.\n      Considering the focal length of MLA is usually very short (e.g., 3,500 μm)\n      to allow the native focal plane of MLA being directly placed on the camera\n      sensor, a relay of 1:1 lens (RL3, RL4) is required to relay the back focal\n      plane of the MLA onto the camera sensor.\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2412-Fig1.jpg\n          Figure 1. Epi-illumination light-field microscope based on a simple\n          retrofit of an upright fluorescence microscope\n        \n          (A) The schematic drawing of epi-illumination mode. Objectives: OBJ;\n          dichromatic mirror: DM1, DM2; Tube lens: TL; mirror: M1, M2; relay\n          lens: RL1-RL4; bandpass filter: BF1-BF2; microlens array: MLA; camera:\n          CAM1-2.\n        \n          (B) The off-the-shelf optical and mechanical elements of the LFM are\n          labeled with black and blue annotations, respectively (camera should\n          be labeled in black).\n        \n          (C) The overview and top view of the epi-illumination light-field\n          imaging prototype.\n        \n        Mount a water immersion objective (OBJ1) on the microscope to collect\n        the epifluorescence signals from samples.\n      \n        Place a mirror (M1) to reflect the upright detected light from the",
    "microscope and then position a pair of 1:1 relay lens (RL1, RL2) to\n        relay the reflected light.\n      \n        Build a pair of 1:1 relay lens (RL3, RL4) and camera before the\n        insertion of the MLA to relay the image of NIP, as shown in\n        Figure 2[href=https://www.wicell.org#fig2]A.\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2412-Fig2.jpg\n              Figure 2. The schematic drawing of steps 3–9\n            \n              (A) Relay the NIP (indicated with dash line) onto camera sensor\n              plane in steps 3–5.\n            \n(B) Insert the MLA at the NIP in steps 6 and 7.\n              (C) Move the RL3 to relay the back focal plane of the MLA on\n              camera senor plane in steps 8 and 9.\n            \n              (D and E) The reference images showing how to visually judge\n              whether the MLA has been correctly inserted at the relayed NIP in\n              step 5.\n            \n        Focus the image via twisting the focus ring of the microscope until the\n        image in the ocular is as sharp as possible.\n      \nNote: Now you know the sample is in the\n      focus of the objective, so you can find the conjugate focal plane with the\n      camera.\n    \n        Adjust the camera along the optical axis until the image in the camera\n        is as clear as in the ocular.\n      \n        Insert the MLA at the relayed NIP after the first pair of relay lens RL1\n        and RL2 (Figure 2[href=https://www.wicell.org#fig2]B) to double check the precise\n        positioning of MLA.\n      \nNote: Figures 2[href=https://www.wicell.org#fig2]D and\n      2E shows the correct/incorrect images when the MLA has/hasn’t been placed\n      at NIP precisely. When the MLA is placed at NIP, each microlens is\n      adjacently arranged without any overlap or gap.\n    \nCritical: Rotate the camera or MLA to\n      ensure that the microlens grid is as aligned with the pixel of the camera\n      as possible. Troubleshooting 3[href=https://www.wicell.org#troubleshooting].\n    \n        Check the size of each microlens in the image, make sure the pitch of",
    "microlens is the same as the theoretical size (e.g., 150 μm in our\n        setup). Troubleshooting 4[href=https://www.wicell.org#troubleshooting].\n      \n        Move RL3 backward to relay the back focal plane of the MLA (e.g.,\n        3.5 mm) instead of the native image plane (Figure 2[href=https://www.wicell.org#fig2]C).\n      \n        Check the light-field PSF by imaging the sub-diffraction fluorescent\n        beads (distributed in a piece of 0.8% low-melting agarose hydrogel) (see\n        key resources table[href=https://www.wicell.org#key-resources-table]). Make sure that\n        the RL3 has been placed at the precise position to relay the back focal\n        plane.\n      \n        As shown in Figure 3[href=https://www.wicell.org#fig3]A, finely adjust the position of\n        RL3 until the experimental PSF looks as sharp as possible and the\n        distribution at different depth is highly similar with the theoretical\n        PSFs generated by our forward projection code in VCD-Net package (see\n        key resources table[href=https://www.wicell.org#key-resources-table]).\n        Troubleshooting 5[href=https://www.wicell.org#troubleshooting].\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2412-Fig3.jpg\n              Figure 3. Characterization of VCD-LFM system\n            \n              (A) Comparison between theoretical and experimental light-field\n              PSFs. Scale bar, 5 μm.\n            \n              (B)The reconstruction results of fluorescent beads. Scale\n              bar, 10 μm.\n            \nOptional: Add another light\n          field-channel for dual-color imaging.\n        \n            Insert a dichroic mirror (DM2) after the RL4 to divide the detection\n            light.\n          \n            Add an extra camera sensor (CAM2) for simultaneously capturing\n            dual-color light-field signals.\n          \n            Adjust the camera’s position until the signals of the dual\n            light-field channel are registered.\n          \n            Insert different bandpass filter (BF1, BF2) before the cameras to\n            filter the dual-channel signals, respectively.\n          \nPre-processing of training datasets\nTiming: 2–3 h\n        Calculate light-field PSF.\n        \n            Download light field 3D reconstruction software package.3[href=https://www.wicell.org#bib3]\n            Open MATLAB, and click ‘PSF_compute_GUI.m’ in which contains all\n            simulation parameters. Table 1[href=https://www.wicell.org#tbl1] gives users\n            detailed information about these parameters as follow.\n            table:files/protocols_protocol_2412_3.csv\nNote: After PSF computation, users will\n      get light-field PSF data in the term of ‘.mat’ files stored in ‘PSFMatrix’\n      folder. Users can use this file to convert 3D stack image (like TIFF\n      images) to 2D LF image.",
    "Generate training dataset for VCD network.\n        \nNote: We provide a MATLAB package with\n          a GUI as Figure 4[href=https://www.wicell.org#fig4] shows to generate training\n          dataset including 3D stacks and corresponding 2D LF images for\n          VCD-net. Detailed descriptions of parameters involved in datasets\n          generation are listed in Table 2[href=https://www.wicell.org#tbl2].\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2412-Fig4.jpg\n              Figure 4. Screenshot of light field projection module\n            \ntable:files/protocols_protocol_2412_4.csv\nOpen ‘Main.m’ in the folder ‘VCD-Net/datapre/Code’.\n            Set the parameters in the ‘’Rectify and Augment HR data’ panel.\n          \n            Rectify the raw images and augment the rectified images by cropping,\n            rotating and flipping for training.\n            \nNote: This program will create a\n              folder (‘VCDNetPre/Data/Substacks’) automatically to save the\n              cropped ‘Substacks’ from selected original stacks.\n              Troubleshooting 6[href=https://www.wicell.org#troubleshooting].\n            \n            Generate light-field projection.\n            \n                Set the projection parameters and click projection. A file\n                selecting window will pop up.\n              \n                Choose the light field PSF calculated in step 12. The\n                light-field projections will be saved in the folder\n                (‘./Data/LFforward’).\n                \nCritical: There must only\n                  exist cropped stacks in the folder (‘./Data/Substacks’)\n                  mentioned above.\n                  Troubleshooting 7[href=https://www.wicell.org#troubleshooting].\n                \n            Crop ‘Substacks’ and light-field projection into training pairs.\n            \n                Click ‘Crop Test’. The sum value and variance of each cropped\n                block from several sub-stacks will be displayed in command\n                window of MATLAB. And these blocks will be save at\n                ‘./Data/Training Pair/WF’.\n              \n                Check these saved images. Users can get a threshold to decide\n                one patch whether to be saved or not. Then, set the parameters\n                in Section C and click ‘Crop’. Those cropped patches will be\n                saved in a folder (‘./Data/TrainingPair’).\n                \nCritical: Make sure depth\n                  of the patches match the ‘StackDepth’ in step a or you choose\n                  to discard some slices when it’s less than ‘StackDepth’.\n                  (Error when bigger than ‘StackDepth’).\n                \n            Check the training dataset of VCD network.\n            \nNote: After cropping\n              high-resolution stacks and their corresponding light field images,\n              the users need a quick check on the datasets, for example, drag",
    "all the LF images into ImageJ to find whether exist images with no\n              signal. Also, make sure the lateral size and numbers of training\n              pairs are identical.\n            \nTraining a VCD model\nTiming: 2–5 h\n        Set VCD model parameters.\n        \n            Open ‘config.py’ included in ‘./vcdnet’ as shown in\n            Figure 5[href=https://www.wicell.org#fig5]A.\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2412-Fig5.jpg\n                  Figure 5. Initiation of VCD model training\n                \n                  (A) VCD network package is available in official code source\n                  (see key resources table[href=https://www.wicell.org#key-resources-table]).\n                \n                  (B) Setting various parameters about VCD training (basic\n                  information of training patches and model settings).\n                \n(C) Starting training via Anaconda Prompt.\n            Set the basic information of training datasets and network\n            parameters as shown Figure 5[href=https://www.wicell.org#fig5]B.\n          \n        Train VCD model.\n        \n            Run ‘train.py’ to start model training as illustrated in\n            Figure 5[href=https://www.wicell.org#fig5]C. For example, type the following\n            commands in the command prompt of Anaconda.\n            Troubleshooting 8[href=https://www.wicell.org#troubleshooting].\n            \n> activate vcd-net\n> python train.py\n            Check the performance of the training model.\n            \n                Check the loss printed in the console panel to determine whether\n                the model is properly optimized.\n                Troubleshooting 9[href=https://www.wicell.org#troubleshooting].\n              \n                Open the folder ‘./sample/{label}’ which contains the temporal\n                outputs of network and compare the quality of reconstruction\n                stacks and corresponding ground truths.\n                Troubleshooting 10[href=https://www.wicell.org#troubleshooting].\n                \nOptional: If the quality\n                  of network inference is not satisfactory, users can stop the\n                  training procedure early. Then adjust the data processing\n                  function (e.g., normalization), hyper-parameters (e.g.,\n                  learning rate, batch size, etc.) and model structure (e.g.,\n                  non-linear activation function, the number of layers, base\n                  blocks, features, etc.) in ‘config.py’ to enhance the fitting\n                  ability of model, which permits the training loss to be\n                  further converged.\n                \nNote: This step could be\n                  time-consuming, depending on the performance of GPU and the\n                  size of training datasets (typically 4–8 h for 1,500–3,000\n                  training pairs with data size of 176 × 176 (∼1.4 GB–2.8 GB),\n                  varying according to the size of the data and the performance",
    "of the computer’s processors (CPU or GPU) and memory.\n                \nLight-field imaging of dynamic biological samples\nTiming: 1–5 min\n        Image freely moving C.elegans using the light-field microscope.\n        \n            Load the awake L4-stage worms into a microfluidic chamber.\n            \nNote: To allow the worms free\n              movement within the field of view (FOV) of a 40× objective\n              (LUMPlanFLN 40×/0.8 NA, Olympus), the size of this chamber is\n              about the same size as the FOV, namely, about\n              300 × 300 × 50 μm3.\n            \n            Mount the microfluidic chamber on the stage installed on the\n            microscope.\n          \n            Move the sample stage of the microscope to make the microfluidic\n            chamber positioned within the center of the FOV.\n            \nCritical: The focus ring of\n              the microscope needs to be adjusted before imaging in order to\n              focus the imaging plane at the desirable plane which is the center\n              plane of the whole sample in most cases.\n            \nNote: When the imaging depth of\n              the light field system is much larger than the thickness of the\n              sample (e.g., the thickness of C.elegans is about 30 μm and\n              the imaging depth of our system using 40×/0.8 NA is more than\n              60 μm), the imaging plane can be focused on the top of the sample\n              to acquire more angular information.\n            \n            In order to remove motion noise, record the calcium signals\n            (GCaMP6(f)) and red fluorescent protein (RFP) signals simultaneously\n            at a volume rate of 100Hz by the dual-channel light-field path.\n            \nNote: The live\n              C.elegans labeled with GFP and RFP are excited under the\n              illumination of mercury lamp at 10%–30% of the maximum power and\n              imaged by two cameras equipped with corresponding emission filters\n              (510 nm–550 nm for GFP, 575 nm–625 nm for RFP). The exposure time\n              is set as 2 ms to avoid motion blur. With the 40× magnification",
    "objective, the pixel size of captured 2048 × 2048 LF images is\n              0.1625 μm, yielding a FOV of 332.8 μm × 332.8 μm.\n            \n        Image fast dynamics in beating zebrafish heart using the light-field\n        microscope.\n        \n            Slightly anesthetize the zebrafish larvae with ∼100 μL tricaine\n            (3-aminobenzoic acid ethyl ester, 0.1 mg/mL, Sigma Aldrich, MO)\n            while maintaining the heart beating.\n          \n            Mount the fish on a cover glass or a culture dish (e.g., 500 μL)\n            using 1% low-melting agarose.\n            \nNote: Dilute the agarose by\n              0.1 mg/mL tricaine to prevent the revival of the zebrafish larvae.\n            \n            Move the sample by the sample stage of microscope to ensure the\n            signals of interest to be positioned within the FOV of the detection\n            objective (Fluor 20X/0.5 NA water, Olympus).\n            \nCritical: Before cardiac\n              imaging, the focal plane should be focus at the center of the\n              heart by twisting the focus ring of the microscope.\n            \n            Record light-field videos of the RBCs, myocytes nuclei or myocardium\n            of the beating zebrafish heart.\n            \nNote: The exposure time is set to\n              be 5 ms enabling 200 Hz high-speed volumetric imaging. The frame\n              size is 768 × 768 that corresponds to a lateral FOV of around\n              250 × 250 μm2. About 450 frames should be imaged which\n              covers 4–5 cardiac cycles.\n            \nModel forward inference\nTiming: Typically 0.05 s\n      per light field image patch (varies according to the size of the data,\n        as well as the performance of the computer’s processors (CPU or GPU) and\n        memory)\n        Preprocess experimental data (Figure 6[href=https://www.wicell.org#fig6]A).\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2412-Fig6.jpg\n              Figure 6. Data preprocessing of experimental LF and VCD model\n              inference\n            \n              (A) Rectification and background subtraction of raw LF\n              measurement.\n            \n              (B) Parameters of VCD model inference and model inference via\n              Anaconda Prompt.\n            \n            Get rectification parameters for subsequent light-field image\n            calibration.\n            \nNote: In order to extract\n              different views accurately from one light-field image, we use",
    "LFDisplay (see\n              key resources table[href=https://www.wicell.org#key-resources-table]) to\n              manually identify the center position of each lenslet. Then the\n              rectification settings would be exported as a text file. More\n              detailed guides can be found in the manual of LFDisplay.\n            \n            Open the ‘Image rectification’ section in light field 3D\n            reconstruction software package (see\n            key resources table[href=https://www.wicell.org#key-resources-table]) and input\n            the text file from step a.\n          \n            Click ‘Run Rectification’ and the results will be saved at\n            ‘./Data/02_Rectified’.\n          \n            Subtract average background value in the rectified light-field\n            image.\n            \nNote: The pixel value of even\n              background can be estimated by selecting an area of background\n              (usually located at the corner of an image) in ImageJ.\n            \n        Load trained model and implement network inference (Figure 6[href=https://www.wicell.org#fig6]B).\n        \n            Change the following inference settings in ‘config.py’:\n            LF2D_path: The location of the light-field images.\n            Saving_path: The location of the reconstructed stacks (If this\n            folder doesn’t exist, it will be created automatically).\n          \nRun the following commands in the command prompt of Anaconda:\n> python eval.py\n      The reconstruction results will be saved in the ‘Saving_path’ or the\n      default folder ‘./results’.\n    \nNote: For convenience, an ImageJ plugin\n      that implements the inference stage of VCD-Net is contained in the VCD-net\n      package (located at ‘VCD-Net/vcdnet/ImageJ’). We provide a guidance how to\n      install this plugin and load model at\n      https://github.com/feilab-hust/VCD-Net/tree/main/vcdnet/ImageJ[href=https://github.com/feilab-hust/VCD-Net/tree/main/vcdnet/ImageJ]. When the inference is completed, reconstruction results will be opened\n      in ImageJ, which is more convenient for subsequent analysis.\n    \nCritical: Make sure ‘label’ in\n      ‘config.py’ is the same as the trained model’s name when in inference\n      stage. Troubleshooting 11[href=https://www.wicell.org#troubleshooting].\n    \nData analysis\nTiming: 1–3 h\n        Quantitatively analyze neuronal activity and behavior of moving\n        C.elegans.\n            Map calcium dynamics based on the reconstructed results of VCD-Net.\n            \n                Perform semi-automated tracking of the intensity fluctuation of\n                each individual neuron using TrackMate Fiji Plugin.6[href=https://www.wicell.org#bib6]\nTroubleshooting 12[href=https://www.wicell.org#troubleshooting].",
    "Average all the pixels within the ROI of the neurons detected to\n                generate the signal intensity of the neurons.\n                \nNote: The calculated intensity\n                  of GCaMP and RFP is represented as Fg and Fr.\n                \n                Calculate the real calcium dynamics by measuring (F-F0)/F0.\nNote: Where F=Fg/Fr is the ratio of GCaMP\n                  fluorescence Fg to RFP fluorescence\n                  Fr and F0 is\n                  the neuron-specific baseline being the average of the lowest\n                  100 value of F.\n            Analyze the behavior of moving C.elegans based on the\n            reconstructed results of VCD-Net.\n            \n                Highlight weak worm outline by implementing histogram\n                equalization towards the MIPs of the reconstructed volumes.\n              \n                Use a standard U-Net7[href=https://www.wicell.org#bib7] to automatically\n                segment the shape of the worm body.\n                \nCritical: Manually\n                  annotate the worm body to build a training dataset at first.\n                \nNote: The number of basic\n                  filter and depth of the U-Net are 32 and 4.\n                \n                Compute the edge of the worm body using Canny filter based on\n                the segmented images.\n              \n                Calculate the Euclidean distance transform inside the worm body\n                on the edge image and the ridge of distance transformation\n                indicates the center line of the worm.\n              \n                Calculate the changing curvatures and motion velocities of the\n                worm based on the segmented center lines at different time.\n              \n        Quantitatively analyze the Velocity map of RBCs and volume-based\n        ejection fraction in the beating zebrafish heart.\n        \n            Track and calculate the velocity map of RBCs flowing in the beating\n            zebrafish heart.\n            \n                Perform the tracking of the flowing RBCs using Imaris (v 9.0.1,\n                Bitplane) or TrackMate Fiji Plugin.6[href=https://www.wicell.org#bib6]\n                Calculate the velocity of the identified RBCs and visualize by\n                Mayavi.8[href=https://www.wicell.org#bib8]\n            Analyze the volume-based ejection fraction of myocardium during the\n            heartbeat.\n            \n                Segment the volume of the reconstructed myocardium using Amira\n                (v 6.0.1).\n                \nNote: Use a blow tool (in\n                  segmentation section) to semi-automatically segment the inner",
    "wall of the ventricle at each slice. Calculate the volume of\n                  the ventricle based on the thickness and segmented area at all\n                  slices.\n                \n                Calculate the volume change ratio of the ventricle by (V −\n                ESV)/EDV.\n                \nNote: Where V is the volume of\n                  the ventricle and ESV and EDV represent the volumes at the end\n                  of systole and diastole during heartbeat."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Microscopy",
    "Biotechnology And Bioengineering",
    "Neuroscience",
    "Model Organisms",
    "High Throughput Screening"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Bioengineering & Technology",
    "Molecular Biology & Genetics"
  ]
}