{
  "id": 8566,
  "origin_website": "Jove",
  "title": "A Method for 3D Reconstruction and Virtual Reality Analysis of Glial and Neuronal Cells",
  "procedures": [
    "1. Image Processing Using Fiji\nOpen the image stack by dragging and dropping the native file from the microscope containing the stack, or by dragging and dropping the folder containing the whole image stack into the software window.\nNOTE: Fiji is able to automatically recognize all the standard image formats, such as .jpg, .tif, and .png, as well as proprietary file formats from microscope providers. While the following protocol has been optimized for image stacks from 3DEM, these steps might be used for light microscopy datasets as well.\nOnce the stack is opened, go to Image > Properties to make sure the voxel size has been read from the metadata. If not, it can be manually entered.\nMake sure to transform the image into 8-bit. Click on Image > Type and select 8-bit.\nIf the original stack was in different sequential files/folders, use Concatenate to merge them in a single stack by selecting Image > Stacks > Tools > Concatenate.\nSave the stack by selecting File > Save as. It will be saved as a single .tif file and can be used as a backup and for further processing.\nIf the original stack is acquired as different tiles, apply stitching within TrakEM2.\nCreate a new TrakEM2 project using New > TrakEM2 (blank).\nWithin the viewport graphical user interface (GUI), import the stacks by clicking the right mouse button > Import > Import stack, and import all the tiles opened in the Fiji main GUI. Make sure to resize the canvas size to fit the entire stack by right-clicking > Display > Resize Canvas/Layer Set, and choose the y/x pixel size depending on the final size of the montage.",
    "NOTE: For instance, if the montage should be finalized using four tiles of 4,096 x 4,096 pixels, the canvas size should be at least 8,192 x 8,192 pixels. Consider make it slightly bigger, and crop it later.\nSuperimpose the common portions of individual tiles by dragging and dropping them on the layer set. Use the top left slider to modify the transparency to help with the superimposition.\nMontage and realign the stacks by clicking the right mouse button > Align, and then select one of the options (see note below).\nNOTE: SBEM or FIB-SEM are usually already well aligned, but minor misalignments might occur. Align layers is the automated pipeline for z alignment; Montage multiple layers is the automated pipeline for aligning tiles on each z stack, for the entire volume; Align multilayer mosaic combines the two previous options. This operation is time-consuming and subject to the random-access memory (RAM) of the machine. Consider using a high-end computer and let it run for hours/days, depending on the size of the stack.\nFinally, export the image stacks and save them by using mouse right click > Make flat image, and then make sure to select the first to the last image in the drop-down menus Start and End.\nUsing TrakEM2 embedded functions, segment structures of interest if needed.\n\t\t\nIn the TrakEM2's GUI, right-click on Anything under the Template window and select Add new child > Area_list.\nDrag and drop Anything on top of the folder under Project Objects, and one Anything will appear there.\nDrag and drop the Area list from the template to the Anything located under Project Objects.",
    "On the image stack viewport, select the Z space with the cursor. The area list will appear, with a unique ID number. Select the brush tool on top (bottom right) and use the mouse to segment a structure by filling its cytosol, over the whole z stack.\nExport the segmented mask to be used in ilastik as seed for carving by right-clicking either on the Area list object on the Z space list, or on the mask in the viewport, and select Export > Area lists as labels (tif).\nDepending on the resolution needed for further reconstructions, if possible, reduce the pixel size of the image stack by downsampling, considering the memory requirements of the software that will be used for segmentation and reconstruction. Use Image > Adjust > Size.\nNOTE: ilastik handles stacks of up to 500 pixels on xy well. Also, downsampling will reduce the resolution of the stack; therefore, take into account the minimum size at which the object still appears recognizable and, thus, can be segmented.\nTo enhance contrast and help the segmentation, the Unsharp mask filter can be applied to make membranes crisper. Use Process > Filter > Unsharp mask.\nExport the image stack as single images for further processing in segmentation software, (e.g., ilastik), using File > Save as... > Image sequence and choose .tif format.\n2. Segmentation (Semi-automated) and Reconstruction Using ilastik 1.3.2\nIn the main ilastik GUI, select the Carving module.\nLoad the image stack using Add New > Add a single 3D/4D volume from sequence.\nSelect Whole directory and choose the folder containing the image stack saved as single files. At the bottom of the new window, where options for loading the images are present, make sure to keep Z selected.",
    "For the following steps, all operations and buttons can be found on the left side of the main software GUI. Under the Preprocessing tab, use the standard options already checked. Use bright lines (ridge filters) and keep the filter scale at 1.600. This parameter can be modified afterward.\nOnce the preprocessing is finished, select the next page in the drop-down menu of the Labeling module. One Object and one Background are present by default.\nSelect the object seed by clicking on it, and draw a line on top of the structure of interest; then, select the background seed and draw one or multiple lines outside the object to be reconstructed. Then, click on Segment, and wait.\n\tNOTE: Depending on the power of the computer and the size of the stack, segmentation could take from a few seconds to hours. Once it's done, a semitransparent mask highlighting the segmentation should appear on top of the segmented structure.\n\t\nScroll through the stack to check on the segmentation. The segmentation might not be accurate and not follow the structure of interest accurately or spill out from it. Correct any spillover by placing a background seed on the spilled segmentation, and add an object seed over the nonreconstructed segment of the object of interest.\nIf the segmentation is still not correct, try to modify with the Bias parameter, which will increase or decrease the amount of uncertain classified pixels as accepted. Its value is 0.95 by default; decrease it to limit any spillover (usually to not less than 0.9) or increase if the segmentation is too conservative (up to 1).",
    "Another possibility is to go back to step 2.4 (by clicking on Preprocessing) and to modify the size of the filter; increasing the value (e.g., to 2) will minimize the salt-and-pepper noise-like effects but will also make membranes more blurred and smaller details harder to detect. This might limit spillover.\nReiterate as much as needed, as long as all desired objects have been segmented. Once an object is finished, click on Save current object, below Segment. Two new seeds will appear, to start the segmentation of a new object.\nExtract surface meshes right away as .obj files by clicking on Export all meshes.\nIf further proofreading is needed, segmentation can be extracted as binary masks. Select the mask to export it by clicking Browse objects; then, right-click on Segmentation > Export and, then, under Output file info, select tif sequence as format.\n3. Proofreading/Segmentation (Manual) in TrakEM2 (Fiji)\nLoad the image stack and create a new TrekEM2 project as per step 1.6.1.\nImport the masks that need proofreading using the option Import labels as arealists, available in the same import menu used to import the image stack.\nSelect the arealist imported in the Z space and use the Brush tool to review the segmentation.\nVisualize the model in 3D by right-clicking on Area list > Show in 3D. In the following window asking for a resample number, a higher value will generate a lower resolution mesh.\nNOTE: Depending on the size of the object, consider using a value between 4 and 6, which usually gives the best compromise between resolution and morphological detail.\nExport the 3D mesh as wavefront .obj by choosing from the menu File > Export surfaces > Wavefront.\n4. 3D Analysis",
    "Open Blender. For the following steps, make sure to install the NeuroMorph toolkit (available atÂ available at https://neuromorph.epfl.ch/index.html) and the glycogen analysis toolkit (available at https://github.com/daniJb/glyco-analysis).\nImport the objects using the NeuroMorph batch import by clicking Import Objects under the Scene menu, to import multiple objects at once. Make sure to activate Use remesh and Smooth shading.\nNOTE: It is not recommended to click on Finalize remesh if there is uncertainty about the initial size of the object. Import tree depth at value 7 (default) is usually good to maintain an acceptable resolution and morphology of the object.\nSelect the object of interest from the outliner and modify the octree depth of the remesh function under the Modifiers menu iteratively to minimize the number of vertices and to avoid losing details in resolution and correct morphology. When changing the octree depth, the mesh on the main GUI will change accordingly. Once finished, click on Apply to finalize the process.\nNOTE: Values around 4 are usually good for small objects (such as postsynaptic densities), values around 7 for larger ones (such as long axons or dendrites), and values around 8 or 9 for complete cellular morphologies where details of different resolutions should be maintained.\nUse the image superimposition tool Image stack interactions on the left panel, under the NeuroMorph menu, to load the image stack. Make sure to enter the physical size of the image stack for x, y, and x (in microns or nanometers, depending on the units of the mesh) and select the path of the stack by clicking on Source_Z. X and Y are orthogonal planes; they are optional and will be loaded only if the user inserts a valid path.",
    "Then, select a mesh on the viewport by right-clicking on it, enter the edit mode by pressing Tab, select one (or more) vertices using mouse right click, and finally, click on Show image at vertex. One (or more) cut plane(s) with the micrograph will appear superimposed on top of the mesh.\nSelect the cut plane by right-clicking on it, press Ctrl + Y, and scroll over the 3D model using the mouse scroll. This can also be used as proofreading method.\nUse Measurement tools for quantifications of surface areas, volumes, and lengths. These operations are documented in more details by Jorstad et al.19 and on the NeuroMorph website24.\nUse the Glycogen analysis tool for quantifying glycogen proximity toward spines and boutons. Operations are documented in more details in a previous publication10 and in the glycogen analysis repository25.\nRun GLAM23. The code, executable, and some test files are available in the GLAM repository26.\nPrepare two geometry files in .ply format: one source file containing glycogen granules and one target file containing the morphology surfaces.\nPrepare three .ascii colormap files (each line containing t_norm(0..1) R(0..255) G(0..255) B(0..255)) for representing local absorption values, peak values, and average absorption values.\nExecute the C++ script GLAM, with the geometry files (step 4.2.1) and colormap files (step 4.2.2), by setting the influence radius (in microns), the maximum expected absorption value, and a normalized threshold for clustering the absorption peaks. For information about other parameters, execute the script with the -help option.\nExport the results as .ply or .obj files: target objects color-mapped with GLAM values, absorption peak markers represented as color-coded spheres, and target objects color-coded with respect to the mean absorption value.\nOpen VR Data Interact. VR Data Interact-executable code is available in a public repository27.",
    "Import meshes to be visualized, following the instructions in the VR menu. Make sure to import the .obj files computed in step 4.2.1, in case a GLAM analysis is needed."
  ],
  "subjectAreas": [
    "Neuroscience"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}