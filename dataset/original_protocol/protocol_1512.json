{
  "id": 1617,
  "origin_website": "Cell",
  "title": "Quantitative neuronal morphometry by supervised and unsupervised learning",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nWe begin by illustrating how to extract quantitative morphological attributes from digital reconstructions of neurons represented in the standard SWC file format (Nanda et al., 2018[href=https://www.wicell.org#bib12]). Then, we demonstrate the procedure to run unsupervised, supervised, and statistical analysis on these attributes.\nData quantification\nTiming: 2 h\nThese steps describe how to process and quantify neural morphologies stored in the SWC file format (Figure 1[href=https://www.wicell.org#fig1]). These are most typically representing neurons and glia (Abdellah et al., 2018[href=https://www.wicell.org#bib1]; Ascoli et al., 2017[href=https://www.wicell.org#bib3]) but have also been used for vascular reconstructions (Wright et al., 2013[href=https://www.wicell.org#bib19]). The output is a set of quantitative morphological attributes for every neuron, which could be used for supervised, unsupervised, or any other quantitative analysis.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1053-Fig1.jpg\nFigure 1. Sample visualized neuron from the zebrafish data along with the corresponding rows of its SWC file\nRun L-Measure in your operating system.\nIn Linux you need to run java -jar Lm.jar in the command line from the folder where the L-Measure executable scripts are located.\nCheck the troubleshooting[href=https://www.wicell.org#troubleshooting] section of the L-Measure website if you run into errors.\nFigure 2[href=https://www.wicell.org#fig2] shows the main view of L-Measure software being executed and related sample output.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1053-Fig2.jpg\nFigure 2. Neuronal quantification\n(A) Graphical User Interface of the L-Measure software (open on the default ‘specificity’ tab).\n(B) Sample output file produced by L-Measure.\nIn the ‘function’ tab select the morphological attributes that you wish to extract from the SWC files. The selected morphological attributes utilized here (referred to henceforth as “core metric functions”) are: 'N_bifs', 'N_branch', 'Width', 'Height', 'Depth', 'Length', 'EucDistance', 'PathDistance', 'Branch_Order', 'Contraction', 'Fragmentation', 'Partition_asymmetry', 'Bif_ampl_local', 'Bif_ampl_remote', 'Fractal_Dim' (see also Note below).",
    "Note: The ‘included’ and ‘excluded’ points in step 6 are not user-defined, but rather automatically determined by L-Measure and displayed for information only. Specifically, these values refer to the number of rows in the SWC file used to calculate the desired morphometric. For instance, the functions measuring the bifurcation angles (Bif_ampl_local and Bif_ampl_remote) only operate on the arbor bifurcation points, and not on the stems, continuations, and terminations. Thus, the included points would be in this example the counts of bifurcations and the excluded points would be the rest of the arbor tracing points. For exact definitions and more information about core metric functions and the measurements extracted in Step 2, visit the L-Measure website (http://cng.gmu.edu:8080/Lm/help/index.htm[href=http://cng.gmu.edu:8080/Lm/help/index.htm]).\nIn the ‘input’ tab select the SWC files corresponding to one or more neurons from the local drive.\nIn the ‘output’ tab specify a file name and the location where you wish to store the extracted values.\nFinally, in the ‘go’ tab press ‘go’ for L-Measure to run the analysis and calculate the numerical attributes for each SWC file.\nIn the output file, L-Measure produces neuron name, name of the core metric function, total sum of the function over all tracing points (Total_Sum), number of points included in the analysis (Count_considered_compartments), number of points excluded (Count_discarded_compartments), minimum value (Min), average (Ave), maximum (Max), and standard deviation (S.D.) of each metric function (Scorcioni et al., 2008[href=https://www.wicell.org#bib16]).",
    "Edit these quantifications by removing irrelevant or redundant statistics in each row and save the resultant file in comma separated value (CSV) format (see Note below for more details). Every line in this CSV file should represent a single neuron. File ‘neuron_features.csv’ in the scripts/data folder was obtained in that way from the output of L-Measure for this experimental dataset. Alternatively, you can run ‘python convert.py -i /location/of/input-file -o /location/to/save/output-file.csv’; this script reads the results yielded fromL-Measure from the user’s specified location and creates a CSV file (output-file.csv) that can be used for both the supervised and unsupervised subsequent steps.\nNote: Step 7 is required because not all elements of the L-Measure default statistical summary are appropriate for every feature. Therefore, selecting the most suitable statistic for the analysis is important. For more information on the used parameters and their relevance see Table 1[href=https://www.wicell.org#tbl1] below. It is important to mention that L-Measure does not calculate median values and mean-based statistics may not be appropriate if the data is not normally distributed. A detailed description of the values L-Measure can extract from the SWC files and their applications is provided by the reference paper (Scorcioni et al., 2008[href=https://www.wicell.org#bib16]).\ntable:files/protocols_protocol_1053_1.csv\nUnsupervised clustering\nTiming: 2 h\nThe following steps describe the unsupervised analysis, a process that groups neurons based on their morphological features independent of any a priori knowledge about the cells. We start with the fundamental K-means algorithm for data clustering and then use graphical mixture models to find innate distributions within the dataset.\nFrom your Python environment or command line, run ‘python unsupervised-elbow-curve.py -i ./Data/neuron_features.csv’.\nThis script reads the neuronal data from the data folder and normalizes them.\nThen it plots the elbow curve, which shows the trade-off between residual variance and the number of clusters (Figure 3[href=https://www.wicell.org#fig3]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1053-Fig3.jpg",
    "Figure 3. K-means clustering results\n(A) Elbow curve to determine the optimal number of clusters.\n(B) Scatter plot of the neurons based on their first two principal components (PC1 and PC2) and color-coded clusters (each color represents a cluster).\n(C) Distribution of different cluster assignments found by K-means algorithm.\nElbow curve is used to determine the optimal number of clusters. This heuristic method calculates the sum of squared distances of all points from the center of their respective cluster as a function of the number of clusters. The point(s) of maximum inflection (visible bends) are appropriate choices for the number of clusters. In this case, the curve has two high-inflection points (3 and 5) and we selected 3 as the number of clusters for our analysis.\nFrom your Python environment or command line, run ‘python unsupervised-kmeans.py -i ./Data/neuron_features -k 3’.\nThis script reads the neuronal data from the data folder and groups them into 3 clusters utilizing the K-means algorithm.\nThe script prints the cluster label for each neuron as well as the centers of the three clusters.\nThe program also performs a principal component analysis to optimize the spatial display of the feature distribution along the main directions of their variance.\nThe program then plots (i) a scattered visualization of the data based on the first two principal components and (ii) a bar plot depicting the count of neurons in each cluster (Figures 3[href=https://www.wicell.org#fig3]B and 3C).\nYou can alter the number of clusters by changing the input parameter “k” before running the script.\nFrom your Python environment or command line, run ‘python unsupervised-BIC-curve.py -i ./Data/neuron_features.csv’.\nThis script reads the neuronal data from the data folder and normalizes them.",
    "Then it plots the Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) curves, which quantify the grouping distinctiveness as a function of the number of clusters (Figure 4[href=https://www.wicell.org#fig4]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1053-Fig4.jpg\nFigure 4. Gaussian Mixture Model (GMM) clustering results\n(A) Bayesian Information Criterion (BIC) and Akaike Information Criterion (AIC) scores to determine the optimal number of clusters.\n(B) Scatter plot of the neurons based on their first two principal components (PC1 and PC2) and color-coded clusters (each color represents a cluster).\n(C) Distribution of different cluster assignments found by GMM.\nBayesian Information Criterion (BIC) is a likelihood-based method for model selection among a finite set of possibilities. This is a heuristic score to determine the optimal number of clusters.\nAkaike Information Criterion (AIC) measures the relative amount of information lost by a given model for a certain number of clusters.\nGenerally, lower values of AIC and BIC are more desirable. For our analysis, we selected 3, where both curves have relatively low value compared to neighboring points.\nFrom your Python environment or command line, run ‘python unsupervised-GMM.py -i ./Data/neuron_features.csv -k 3’.\nThis script reads the neuronal data from the data folder and groups them into 3 clusters using Gaussian mixture distributions.\nThe script prints the group label for each neuron.\nThe program also performs a principal component analysis to optimize the spatial display of the feature distribution along the main directions of their variance.\nThe program then plots (i) a scattered visualization of the data based on the first two principal components and (ii) a bar plot depicting the label distribution (Figures 4[href=https://www.wicell.org#fig4]B and 4C).\nYou can alter the number of clusters by changing the input parameter “k” before running the script.",
    "Optional: Scikit-learn includes various algorithms to perform grouping and clustering datasets. Based on the nature and inherent characteristics of the data, different clustering models might achieve more interpretable outcomes (Bijari et al., 2018[href=https://www.wicell.org#bib6]). To explore different clustering techniques please visit https://scikit-learn.org/stable/modules/clustering.html[href=https://scikit-learn.org/stable/modules/clustering.html] for an overview of suitable alternatives and code snippets to execute them.\nSupervised clustering\nTiming: 2 h\nMany datasets of interest, including the examples employed here, have specific labels assigned to each neuron, representing knowledge that the researcher has about these cells that is in principle unrelated to the arbor morphology. Examples of such labels (often referred to as “true classes”) include electrophysiological characteristics, anatomical location of the soma, expression of particular genes, functional specificity, and many more (Ascoli and Wheeler, 2016[href=https://www.wicell.org#bib4]). Having the data labels in hand, the next series of steps describes the process of supervised analysis. This process assesses the morphological features that best separate among the chosen labels. We begin with visualizing the dataset to gain a spatial perception of the different data points based on their principal components. Then we add the labels to the results of the previous unsupervised methods to check whether the intrinsic morphological characteristics of the cells, as discovered by unsupervised learning, correspond to any independent functional property (distinct ground truth classes) and ascertain how adequately the supervised algorithms perform relative to the known information. At the end, we train and test a battery of supervised classification methods on the labeled data to quantify how accurately the morphological features can discern among the labels.",
    "For this step, neural data should have explicit class labels. There are typically no limits to the number of these labels (ground truth information) and they are typically determined by the experimentalist while preparing the dataset based on biologically relevant knowledge. To prepare your labels, please create or format your data labels as described below.\nUse ‘labels (template).xlsx’ in the ‘Scripts’ folder and label each neuron with the proper label based on your experiment.\nIf you have more than one class, use additional columns.\nSave the template file for your records. Also, export a CSV file to provide input for the next steps in the analysis.\nFrom your Python environment or command line, run ‘python supervised-visualize.py -i ./Data/neuron_features.csv -l ./Data/neuron_labels.csv’.\nThis script reads the neuronal data and their corresponding labels from the data folder.\nThe program next performs a principal component analysis to optimize the spatial display of the feature distribution along the main directions of their variance.\nThe program then outputs a series of scatter plots, one for each set of labels, with each neuron data point associated with its label symbol (Figure 5[href=https://www.wicell.org#fig5]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1053-Fig5.jpg\nFigure 5. Distribution of the neurons based on their principal components (PC1 and PC2) and their ground truth labels\n(A) Scatter plot based on different neuromast labels (A: anterior, L: lateral, T: trunk, D: dorsal; numbers associated with the labels indicate closeness of the neuron to the head of the animal, with 1 being the closest and 6 being furthest).\n(B) Scatter plot based on different tuning labels (u: unknown, r: rostral, c: caudal).\n(C) Scatter plot based on different region labels (trunk, tail, posterior lateral line, dorsal lateral line, and anterior lateral line).\n(D) Scatter plot based on different hemisphere labels (right and left).",
    "From your Python environment or command line, run ‘python labeled-kmeans.py -i ./Data/neuron_features.csv -l ./Data/neuron_labels.csv -k 3’.\nThis script reads the neuronal data and groups them into 3 clusters.\nThe program then outputs the homogeneity score and a series of scatter plots, each displaying the color-coded K-means clusters with symbols (shapes) corresponding to the true class labels in each label category (Figure 6[href=https://www.wicell.org#fig6]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1053-Fig6.jpg\nFigure 6. Visualization of the neurons based on K-means results (color-coded clusters) and their ground truth labels (shapes)\nFor label meanings, see Figure 5[href=https://www.wicell.org#fig5] legend.\nHomogeneity is a clustering metric based on the ground truth labels. It checks if clusters contain only samples belonging to a single class. A clustering result satisfies homogeneity if all of its clusters contain only data points that are members of their ground truth class. This metric is bounded between 0 and 1, with lower values indicating low homogeneity (Rosenberg and Hirschberg, 2007[href=https://www.wicell.org#bib15]).\nYou can alter the input parameters before running the python script.\nFrom your Python environment or command line, run ‘python labeled-GMM.py -i ./Data/neuron_features.csv -l ./Data/neuron_labels.csv -k 3’.\nThis script reads the neuronal data and groups them into 3 clusters, just as in steps 9a-d above.\nThe program then outputs the homogeneity score (as described in 14.c) and a series of scatter plots, each displaying the color-coded distributions with symbols (shapes) corresponding to the true class labels in each label category (Figure 7[href=https://www.wicell.org#fig7]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1053-Fig7.jpg\nFigure 7. Visualization of the neurons based on GMM results (color-coded groups) and their ground truth labels (shapes)\nFor label meanings, see Figure 5[href=https://www.wicell.org#fig5] legend.\nYou can alter the input parameters before running the python script.\nFrom your Python environment or command line, run ‘python supervised-classification.py -i ./Data/neuron_features.csv -l ./Data/neuron_labels.csv’ and follow the command line instructions.",
    "This script reads and pre-processes the neuronal data and their corresponding labels from the data folder.\nThe script asks which class label you want to select for the task of classification. This could vary based on your choice of dataset.\nThe program then calculates the feature importance using the ‘ExtraTreesClassifier’ algorithm (see Note below) and plots those values in a bar graph.\nNote: The ExtraTreesClassifier algorithm used in step 16 is an approach to feature selection based on the Random Forest method. Random Forest creates several tree-based models using the features and attempts to classify the data via those decision trees. The importance of the features depends on the number of times that feature is used to split a node and on the number of samples it splits. Specifically, this algorithm weighs the total decrease in “node impurity” by the proportion of samples reaching that node. A Mean Decrease in Impurity (MDI) is then calculated as the average of this measure over all trees of the ensemble. In other words, the fewer splits a feature needs to classify the data, and the larger the proportion it classifies, the “purer”, hence more important, it is (Geurts et al., 2006[href=https://www.wicell.org#bib8]).\nNext, the program trains four distinct machine learning algorithms on the labeled data: ‘logistic regression’, ‘decision tree’, ‘k-nearest-neighbors’, and ‘multilayer perceptron’.\nThe program then calculates and plots the accuracy of each of those four algorithms in two different scenarios: if considering all morphological attributes of the neurons; and if only considering the most important attribute.\nHere we show a representative classification example for the class label ‘region’ (Figure 8[href=https://www.wicell.org#fig8]). To obtain the results for other dimensions, follow command line instructions and change ‘class label’ when the command prompt asks for the class label.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1053-Fig8.jpg\nFigure 8. Supervised analysis results",
    "(A) Feature importance of the data.\n(B) Classification accuracy of logistic regression, decision tree, K-nearest-neighbor (K-NN), and multilayer perceptron (MLP) using all features and just the top feature. For more information on the morphological features see step 7 and for details on feature importance see step 16.\nFrom your Python environment or command line, run ‘python supervised-density.py -i ./Data/neuron_features.csv -l ./Data/neuron_labels.csv’ and follow the command line instructions.fakn\nThis script reads and pre-processes the neuronal data and their corresponding labels from the data folder.\nThe script then asks which feature and which class label you want to select for the density analysis. This varies based on your choice of dataset.\nIn this example, we have selected the morphological features ‘Contraction’ and ‘EucDistance’ to run density analysis. Nevertheless, the analysis could be repeated on any other dimension of the data. To do so, follow the command line instructions.\nAfterward, from the command line instructions we select ‘lateral line (LL)’ class labels to perform density analysis within this class for different class labels, namely, ‘ALL’ and ‘PLL’ for anterior and posterior LL, respectively..\nThe program plots the density curve for the ‘Contraction’ and ‘EucDistance’ data considering different ‘lateral line’ class labels (Figure 9[href=https://www.wicell.org#fig9]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1053-Fig9.jpg\nFigure 9. Density analysis\nDensity plot for features ‘Contraction’ (branch tortuosity) (A) and ‘EucDistance’ (maximum straight distance from soma to tips) (B) in the entire dataset as well as in selected sub-class labels (ALL and PLL).\nFrom your Python environment or command line, run ‘python supervised-pdv-comparison.py -p ./Data/PDVs -l ./Data/neuron_labels.csv’ and follow the command line instructions.\nThis script reads the persistent diagram vectors and the data labels from the data folder (see Note below).",
    "Note: The persistence diagram vectors (PDVs) utilized in step 18 are based on the concept of topological persistence in the field of computational topology (Kanari et al., 2018[href=https://www.wicell.org#bib9]). PDVs are derived from a simplified representation of the neural arbor that only considers the stems, bifurcations, and terminations while ignoring all continuation points along the branches. In other words, this process creates a straight segment between any two topological nodes of the tree representing the Euclidean length of each branch. The descriptor function is the total length of the unique path (i.e., the sum of all branch lengths) from the tree stem to any point in the tree. The stem is the tree root, typically corresponding to its point of emergence from the soma (Li et al., 2017[href=https://www.wicell.org#bib10]). Starting from this simplified representation, persistence analysis sweeps the neuron tree in decreasing function values, i.e., beginning from the farthest terminal tip, while tracking the appearance (end point) and disappearance (start point) of each neurite branch. The persistence diagram summarizes all resulting appearances and disappearances into a set of 2D points whose (x, y) coordinates represent the distance from the soma of the end and start points of each branch. Mathematically, the set of points in the persistence diagram captures a nested branch decomposition of the neuron tree with respect to the simplified Euclidean branch length description. Finally, the persistence diagram summary is converted into a 100-dimensional vector representing the function values at 100 positions sampled uniformly between the minimum and maximum values, corresponding respectively to the beginning of the tree and the farthest terminal tip. Intuitively, persistence diagram vectors capture the morphological information similar to that represented in Sholl diagrams (Garcia-Segura and Perez-Marquez, 2014[href=https://www.wicell.org#bib7]; Sholl, 1953[href=https://www.wicell.org#bib17]).",
    "The advantage of this approach, however, is that it produces a proper metric space which allows quantitative applications such as those illustrated in this protocol. PDVs for the data used in this analysis are available on NeuroMorpho.Org[href=http://NeuroMorpho.Org]. For instructions to generate PDVs from SWC files or additional information on their interpretation, refer respectively to the corresponding GitHub page https://github.com/Nevermore520/NeuronTools[href=https://github.com/Nevermore520/NeuronTools] or to the Frequently Asked Question entry of NeuroMorpho.Org[href=http://NeuroMorpho.Org] http://neuromorpho.org/myfaq.jsp?id=qr11[href=http://neuromorpho.org/myfaq.jsp?id=qr11]",
    "The script then asks which feature, class label, and values you want to select for the density analysis. This varies based on your choice of dataset.\nIn this example, we illustrated the statistical analysis on lateral line (LL) to compare class labels ‘ALL’ and ‘PLL’. Nevertheless, the comparison could be repeated for any other class labels. To do so, follow the command line instructions and provide your desired input values.\nThe program calculates pairwise arccosine distances of different vectors based on their labels and groups them in ‘within’ or ‘across’ populations for same labels (ALL/ALL or PLL/PLL) and different labels (ALL/PLL), respectively.\nAfterward, the software runs student’s t-test and displays a statement indicating whether the two groups (‘within’ vs ‘across’) are statistically different based on the resulting p-value.\nThe program also outputs a bar plot of the average value of ‘within’ and ‘across’ populations with error bars indicating S.D. (Figure 10[href=https://www.wicell.org#fig10]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1053-Fig10.jpg\nFigure 10. Statistical analysis of persistence diagram vectors (PDVs) relative to lateral lines (LL)\nThe script first calculates the pairwise arccosine distances between PDVs of ‘within’ and ‘across’ populations with respect to class labels (ALL and PLL), and then performs their statistical comparison. Bar plot shows the average of ‘within’ and ‘across’ distances with error bars indicating standard deviations.\nNote: Model-specific parameters (e.g., number of clusters) will be inserted by the user through the command prompt and users will be guided with appropriate help messages from the command prompt in the clustering and classification algorithms. Furthermore, users should be advised that we have accepted the default python package parameters (e.g., number of iterations, penalties, and learning rates) offered by the Scikit-learn toolkit. Please refer to the Scikit-learn package manuals to check if the default settings are suitable for your data.",
    "Optional: To disregard some of the data points from the visualization or analysis based on their class or relevance, simply remove the corresponding entries from both the ‘neuron_features.csv’ and ‘neuron_labels.csv’ files. Save a copy of the original files prior to any modifications."
  ],
  "subjectAreas": [
    "Bioinformatics",
    "Computer Sciences",
    "Microscopy",
    "Cell Biology",
    "Neuroscience"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Bioengineering & Technology",
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}