{
  "id": 19902,
  "origin_website": "Wiley",
  "title": "Glia Cell Morphology Analysis Using the Fiji GliaMorph Toolkit",
  "procedures": [
    "GliaMorph was implemented as macros in Fiji, allowing it to run across platforms and be easily adaptable by end-users (Rueden & Eliceiri, 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0033]; Schindelin et al., 2015[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0037]). Again, we will not cover the installation of Fiji itself, as there is extensive information online (https://imagej.net/Fiji/Downloads[href=https://imagej.net/Fiji/Downloads]).\nThis protocol aims to cover the setup of GliaMorph, including (a) which update sites are needed and how to do this, (b) the download of all required code, and (c) the download of the example data.\nNecessary Resources\nHardware\nAs described in the section Strategic Planning\nSoftware\nAll GliaMorph macros can be found at https://github.com/ElisabethKugler/GliaMorph[href=https://github.com/ElisabethKugler/GliaMorph]\nFiles\nAll example data can be downloaded at https://zenodo.org/record/5747597[href=https://zenodo.org/record/5747597] [doi: 10.5281/zenodo.5747597]\nFiji update sites and additional features\n1a. To update the Fiji update sites, first open “Fiji > Help > Update > Manage update sites” (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0002]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/888bdb34-80e5-411f-8fb3-fa6dbdda147f/cpz1654-fig-0002-m.jpg</p>\nFigure 2\nSteps to update Fiji sites.\n2a. Select “3D ImageJ Suite,” “Neuroanatomy,” and “IJBP” plugins.\n3a. Click “Close.”\n4a. Click “Apply Changes.”\nThese updates are required because the 3D ImageJ Suite is needed for 3D segmentation and processing (Ollion, Cochennec, Loll, Escudé, & Boudier, 2013[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0025]) while the Neuroanatomy plugin is needed to summarize skeleton features.\n5a. Download two extensions to Fiji manually.\nBoth are required for the “Alternate Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0004]” point spread function (PSF) deconvolution.\na.Extension 1: Diffraction PSF 3D to generate a theoretical PSF: details at https://www.optinav.info/Iterative-Deconvolve-3D.htm[href=https://www.optinav.info/Iterative-Deconvolve-3D.htm].\n•Download Diffraction_PSF_3D.class from https://github.com/ElisabethKugler/GliaMorph[href=https://github.com/ElisabethKugler/GliaMorph] (found under “other”). Copy and paste this it into Fiji > Plugin folder > restart Fiji.\nAuthor: Bob Dougherty; Permission: 13.12.2021—via email between Bob Dougherty and Elisabeth Kugler; link: https://www.optinav.info/Diffraction-PSF-3D.htm[href=https://www.optinav.info/Diffraction-PSF-3D.htm]; Licence: Copyright 2005, OptiNav, Inc. All rights reserved.\n•Check if \"Plugins > Diffraction PSF 3D\" is there.\nb.Extension 2: DeconvolutionLab2 for PSF deconvolution (Sage et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0034]): follow the installation guide at http://bigwww.epfl.ch/deconvolution/deconvolutionlab2/[href=http://bigwww.epfl.ch/deconvolution/deconvolutionlab2/].\nCode download",
    "1b. Go to https://github.com/ElisabethKugler/GliaMorph[href=https://github.com/ElisabethKugler/GliaMorph] and download the macros as a .zip file (Fig. 3A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0003]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/f63fa343-5b92-4c20-ae6b-f1506ae9257c/cpz1654-fig-0003-m.jpg</p>\nFigure 3\nSteps to code. (A) Code download from https://github.com/ElisabethKugler/GliaMorph[href=https://github.com/ElisabethKugler/GliaMorph]. (B,C) Downloaded code is in the folder “GliaMorph”; the macros are in the folder “Macros.” (D,D’) Steps to opening code either via Plugins > Macros > Run, or “Drag and Drop” from folder. (E) The macros are designed to run on all tiff files in folders; thus each macro prompts for the selection of an “input folder.”\n2b. Extract the .zip file in the folder where you downloaded it.\nYou can move the folder containing the macros “GliaMorph” to another location, e.g., internal or external drive.\nIn the folder, you will find subfolders. The macros (.ijm files) used in this protocol (Fig. 3B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0003],C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0003]) are in the Macros subfolder.\n3b. Open macros:\n         \nOption 1: Fiji > Plugins > Macros > Run (Fig. 3D[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0003])\nFor more information visit https://imagej.net/scripting/macro[href=https://imagej.net/scripting/macro] and https://imagej.nih.gov/ij/developer/macro/macros.html[href=https://imagej.nih.gov/ij/developer/macro/macros.html].\n         \nOption 2: Open Fiji, then drag and drop the macro (.ijm) from the folder GliaMorph>Macros into Fiji (Fig. 3D[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0003]).\nAll macros are designed to iterate over all .tiff files of the selected input folder.\nThey are called stepX_name.ijm, where X denotes a number, name is a descriptive name, and .ijm is the macro file ending.\nWhere appropriate, a graphical user interface (GUI) will appear with options (for example number of channels or image output size).\nAll macros will automatically create output folders in the input folder with output data that are automatically named (i.e., files are overwritten if the same macro is run iteratively on the same folder).\nExample data download\nWe provide example data for each of the following protocols, including input data, output data, results files, and descriptions as appropriate.\nDownload data from https://zenodo.org/record/5747597[href=https://zenodo.org/record/5747597] [DOI: 10.5281/zenodo.5747597].",
    "Data are named ExampleDataProtocolX where X denotes the number of the protocol, respectively.",
    "As your data are 3D, it is important to understand and examine them in 3D. This protocol is suggested to be performed for each new dataset and/or when imaging parameters are changed. Understanding your data quality will allow you to improve, for example, sample preparation (e.g., tissue penetration of antibodies) or image acquisition (e.g., laser power settings). The outcomes of this protocol are however not needed for GliaMorph per se, and are here solely to guide the user on data understanding and data quality; i.e., if your segmentation is unsatisfactory, this is most likely due to data quality, etc.\nGliaMorph performs data analysis in 3D; therefore end users must consider all three image dimensions equally. This is particularly important because images are usually examined as stacks with a slider when only one slice is visible at a time, but data are 3D; thus slices should be examined in the context of their neighbors. Similarly, in confocal imaging, the penetration decreases over the z-axis, leading to uneven data quality in the third dimension, meaning that typically the “top” of the stack will have a higher quality than the “bottom” (Tröger et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0042]). We previously examined cell properties and data acquisition considerations in 3D (Kugler et al., 2023[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0017a]). Here, we give you pointers on how to check data integrity visually by examining voxel properties and using the Fiji 3D Viewer (Schmid, Schindelin, Cardona, Longair, & Heisenberg, 2010[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0038]), as well as how to assess image quality quantitatively using signal-to-noise (SNR) or contrast-to-noise ratio (CNR). To some readers, this protocol might sound intuitive, but we have encountered various instances where quantification outcomes were wrong due to, for example, altered voxel properties (typically happening during image export, import, or saving).\nExamining voxel properties",
    "Voxels are 3D cubes (Fig. 4A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0004]), which needs to be considered when performing 3D processing and analysis steps, as many calculations depend on this. Additionally, due to resolution limits, voxels are typically longer in (z) than they are wide in (x,y), meaning voxels are shaped like a rectangular cuboid, making them “anisotropic.”\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/291f9059-9213-4d88-8e82-63c31d4c922e/cpz1654-fig-0004-m.jpg</p>\nFigure 4\n(A) Pixel and voxel properties. (B) MIP of MG at 72hpf with an isotropic resolution of 0.0496225 × 0.0496225 × 0.19 μm (x,y,z, respectively). The dotted line indicates homogenous signal distribution across the x-axis. (C) Image (B) but horizontally resliced, showing signal decay across z-axis (white dotted line, yellow dotted line would be the ideal) and displaying image angle (white arrow, with yellow, are being the ideal). (D) Image (B) but with voxel properties being artificially altered to 1 × 1 × 1 μm. (E) ROI for CNR and SNR quantification.\n1a. Open Fiji and open TP1venusPest_72hpf_originalResolution.tiff from the folder ExampleDataProtocol2 in Fiji.\nTypically, data are 3D stacks of 2D slices in a .tiff format or a specific microscopy format, such as Zeiss .czi. If you are unsure on how to open these, the user is referred to the following pages from Fiji (https://imagej.net[href=https://imagej.net] › mbf › importing_image_files) or the Bioformats library (Linkert et al., 2010[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0020]).\n2a. Click Fiji Image > Properties.\nThis will provide information on channels, slices, frames, and voxel x,y,z dimensions, as well as unit (e.g., micrometer or px). You do not need to note this information, as it is only for your understanding.\nCharacteristic issues that we have seen at this step are (i) slices are interchanged with frames (e.g., 3D becomes 2D+time) or (ii) that correct voxel dimensions are lost [e.g., voxels become falsely isotropic (Fig. 4B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0004]-D[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0004]].",
    "At the end of this step, you should understand the shape of your voxels. Example (a), if your z-stack interval is 5 µm while your x,y size is 0.03 µm—do you think it is meaningful to apply a filter that things your voxels are a cube? Example (b), if your structure of interest is 1 µm—what is the ideal sampling frequency to capture this?\nWhile this step might seem intuitive, in our experience, suboptimal voxel properties and data acquisition are the main cause for meaningless data quantification, i.e., rubbish in = rubbish out.\nVisualizing data using the Fiji 3D viewer\n1b. Keep the above image open.\n2b. Open it with the Fiji 3D Viewer by selecting Plugins > 3D Viewer.\n3b. Do not change the default settings.\n4b. Select “OK.”\nThe web page https://imagej.net/3D_Viewer[href=https://imagej.net/3D_Viewer] contains very useful demos. If you have never used the 3D viewer before, we suggest you examine this link first (Schmid et al., 2010[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0038]).\n5b. Examine the data in 3D by rotating and zooming (also see https://www.youtube.com/watch?v=Hb3tDVJ4KXU[href=https://www.youtube.com/watch?v=Hb3tDVJ4KXU] from approximately 46 min).\nThe 3D rendering might take a few minutes to finish. You can leave this running in the background and assess the outcomes later.\nAt this stage, data quality can be visually assessed, e.g., pay attention to the following. (i) Do the data looked squished or stretched (see “voxel properties” above); (ii) are the data at a significant angle, rendering them challenging to quantify (Fig. 4C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0004]); (iii) do the data suffer from significant z-axis signal loss, requiring them to be specifically pre-processed (Fig. 4C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0004]); or (iv), what does the point-spread-function (PSF) look like.\nAgain, this step itself is not needed for GliaMorph, but is crucial to inform back on image acquisition improvements and the requirements that must be met for further analysis.\nCNR and SNR",
    "CNR and SNR are assessments of image quality that can be used to study how individual processing steps change image quality, how different visualization techniques perform against each other, or differences within cells. Here we show an example, examining MG cell bodies, protrusions, and endfeet. However, for some studies, it might be sufficient to examine only one of these.\n8-bit conversion before the following steps can be useful for image comparability ((Image>type>8 bit).\nRegion of interest (ROI) selection example\n1c. Open one 3D tiff stack.\nThis can be one color or multiple, but we would suggest to first start with individual channels as to not confuse ourselves. For example: Fiji > Image > Color > Split Channels.\n2c. Once the stack is opened > move slider to a position in the stack where a full MG can be seen.\n3c. Select circular ROI in the cells of interest [e.g., for MG: cell body (5 × 5 μm), protrusion (2 × 5 μm), and/or endfoot (5 × 3 μm); Figure 4E[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0004] ROI 1-3; example ROIset is in the folder ExampleDataProtocol2]. Select the oval sign on the toolbar and then draw the circle on the region of interest.\n4c. Measure signal by selecting Analyze > Histogram.\nThis will bring up a window asking “Include all XXX images?”, where XXX denotes the number of open images. Select “no,” which means you measure the intensity only in the selected slice.\n5c. Write down mean signal “Mean.”\nSee a similar approach but for blood vessels at https://www.youtube.com/watch?v=Hb3tDVJ4KXU[href=https://www.youtube.com/watch?v=Hb3tDVJ4KXU] from approximately 56 min. This will be referred to as “mean inside,” as it is measured within cells.\n6c. In the histogram window, select the button “live,” meaning the values will automatically change when the ROI is moved.",
    "7c. Select the image and move to background within the eye (Fig. 4E[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0004], ROI 4).\n8c. Measure mean signal “Mean.”\nThis will be referred to as “mean outside,” as it is outside the cell, but within the tissue.\n9c. Move to background outside the eye (Fig. 4E[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0004] ROI 5).\n10c. Measure the standard deviation “StdDev.”\nSaving individual ROIs\nAfter creating the ROIs and measuring the respective intensity values, the ROIs need to be saved, so you can later use the same ROIs to measure the impact of image processing.\n1d. After drawing the ROI and measuring the values (see above), add each ROI-to-ROI manager:\n2d. “Edit > Selection > Add to Manager”\n3d. Select the image instead of histogram.\n4d. Click “Add (t)” in ROI manager.\n5d. In the ROI manager: More > Save (save ROI with a meaningful name, e.g. ROI_cellBody_sample1).\nCalculate CNR or SNR\nSee example calculations in the file ExampleCNR in folder ExampleDataProtocol2 (Fig. 4F[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0004]; see example Excel file ExampleCNR.xlsx).\nFormulas:\n               \nformula:\n$$\\begin{equation} CNR = {\\rm{\\ }}\\frac{{{{\\rm{\\umu }}}_s{\\rm{\\ }} - {\\rm{\\ }}{{\\rm{\\umu }}}_{ns}}}{{{\\sigma }_{bg}}} \\end{equation}$$\nformula:\n$$\\begin{equation} SNR = {\\rm{\\ }}{{\\rm{\\umu }}}_s{\\rm{\\ }} - {\\rm{\\ }}{{\\rm{\\umu }}}_{ns} \\end{equation}$$\nwhere µs is the mean signal, µns is the mean non-signal, and σbg is the standard deviation of the background.\nFor example, in ExampleCNR.xlsx µs are ROI_cellBody, ROI2_protrusion, ROI3_endfoot; µns is ROI2_nonSignal; and σbg is ROI5_outsideRetina.\nAnalyzing the impact of processing (optional)\nThis would typically be done after pre-processing steps, such as PSF deconvolution.",
    "1e. Process the above images (e.g., to try it you could use a median filter, which is a filter that allows you to smooth the data and is often used to remove image speckles; you can find it under “Process > Filters > Median 3D.” You can just test a scale of 5 × 5 × 5 for this exercise).\n2e. Perform measurements again using the original ROIs.\n3e. Recalculate CNR and SNR.\n4e. Assess whether filtering increased or decreased CNR and/or SNR.\nAt the end of this protocol, you will understand how your data are composed by voxels, how to assess 3D image properties using a 3D viewer, and how to quantitatively assess image quality using SNR/CNR measurements. While these steps seem simple on their own, performing them after processing steps will give you an idea about how data are changed using selecting processing steps.",
    "In this protocol, we cover the image preprocessing of data acquired with AiryScan microscopy. As AiryScan microscopy and processing are specific to the microscope with which the data were acquired, the reader is referred to perform AiryScan processing [non-iterative linear Wiener deconvolution algorithm (Huff, 2015[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0013]; Zeiss, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0048]) as indicated by their microscope manual].\nBelow, the following 4 operations will be presented:\n         \nFile format conversion: e.g., .czi to .tiff using Bioformats (Linkert et al., 2010[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0020]).\nIncreasing image comparability by image cropping and orienting.\nSplitting of multi-channel images.\nData analysis using a 1D-vector.\n(a) File format conversion using cziToTiffTool\nThis tool is to convert .czi to .tiff to allow subsequent data processing in .tiff format. It works on single and multi-color images.\n1a. Drag and drop step1_cziToTiffTool.ijm from the folder GliaMorph > Macros into Fiji.\n2a. Click “Run.”\n3a. Select folder with folder with input files (i.e., folder with .czi files).\n4a. Click “OK.”\n5a. Check the generated output folder “tiff” in the input folder with stacks and MIPs.\nExample Data: In the folder ExampleDataProtocol3 you will find three .czi files and the output folder “tiff.”\nBefore running the macro on the example data (do not do this for real analysis workflows), we would recommend you renaming the “tiff” folder to something like “tiffExample”; otherwise, the folder and everything in it will be overwritten (this holds true for all the subsequent steps).\nShould you have accidentally overwritten the “tiff” folder, you can always re-download the example data folders and start afresh.\n(b) Establishing data comparability using the subregionTool",
    "This step aims to orient all stacks in the same direction via x-y rotation and making stacks the same size (x,y) and depth (z) by cropping (Fig. 5A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0005],B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0005]). The default parameters are based on manual measurements (Kugler et al., 2023[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0017a]) and can be adjusted according to data needs (for non-square data, see 90DegreeRotationTool below).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/65419902-6ce4-4c1d-9833-45e386f066cb/cpz1654-fig-0005-m.jpg</p>\nFigure 5\n(A) GUI of the rotationTool Macro, including parameters for output image width, image depth, and sigma. (B) Overview of rotationTool processing. (C) Images can be square or rectangular in x-y. (D) GUI for the 90DegreeRotationTool. (E) Images are rotated clockwise or anti-clockwise with the 90DegreeRotationTool.\nOptions of the GUI (Fig. 5A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0005])\nWidth of the output stack in μm. Default output image width = 60 μm, as we found this to include ∼5-6 MG laterally (code variable “xySize”). If your stacks tend to be wider, you can make the output wider. While this increases the information, this ultimately increases computation time.\nDepth of the output stack in μm. Default output image depth = 10 μm, as we found this to include ∼1-2 MG in depth (code variable “zDepth”). If your stack is larger, you can increase this. We suggest 10 μm as a minimum, due to MG thickness/properties. When going over 10 μm, we suggest to go back to Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0002] to assess whether this is meaningful, or whether z-axis signal decay is too severe.",
    "Addition of length to the ROI to account for retinal curvature, and or to include blood vessels that underlie the endfeet. Default sigma size = 10 μm. When setting this to 0 μm, lateral endfeet tend to be cut-off due to retinal curvature. Depending on your samples you can increase/decrease this value. Generally, this adds only minimal computation time when added, so we suggest to keep this. Input data are in the folder with .tiff files (e.g., after step1) and a RoiSetLine.zip, which is in the same folder as the .tiffs. See below how to create this.\n1b. Open MIPs (.tiff format, but MIPs can be multi-channel) of each image individually to place ROI.\nIMPORTANT NOTE: Open images in the order they appear in the folder (can be any bit-format, e.g., 8-bit or 16-bit).\n2b. Place a line ROI starting from the inner part of the retina and extending outwards on each image.\n3b. IMPORTANT NOTE: Place ROI at the position with the best signal and data; this will be the center of the created output. Click “Straight” on tool bar.\n4b. Add each ROI to the ROI manager [Edit > Selection > Add to Manager > Add (t)].\nIf you have just one image, then add the ROI line twice to the manager.\n5b. Save all ROIs as RoiSetLine.zip. Make sure to rename the default RoiSet.zip to RoiSetLine.zip (this was implemented to avoid overwriting of ROI folders).\nIMPORTANT NOTE: Save this ROI folder into the “tiff” folder (not in the MIP folder) and run on the 3D “tiff.”\n6b. Close everything.\n7b. Drag and drop step4_subregionTool.ijm from the folder “GliaMorph>Macros” into Fiji.\n8b. Click “Run.”\n9b. Select folder with folder with input files (i.e., folder “tiff” from Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0002] or a folder containing .tiff files).",
    "10b. Change the width, depth, or sigma if desired.\nEven though the ROIs were drawn on the MIPs, the SubregionTool will run on the 3D stacks.\n11b. Click “OK.”\n12b. Check the generated output folder “zDir,” containing images rotated along the x-y axis and reduced in the z-axis direction.\nExample Data: In the folder ExampleDataProtocol3\\tiff\\90DegreeRotated you will find input data and the RoiSetLine.zip. The output data are in the folder “zDir.”\n90DegreeRotationTool: Typically, acquired images are in a square format, meaning x and y have the same dimension (Fig. 5C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0005]-E[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0005]). In some cases, images might be acquired in a more rectangular fashion. If this is the case, “90DegreeRotationTool” has to be applied before the “subregionTool.”\nThis step is optionally applied before the rotationTool, to be applied to images that are not quadratic (e.g., 1920 × 1920 in x and y, respectively) but rectangular (e.g., 512 × 1920 in x and y, respectively).\nOptions\nNo—application will exit;\nright (clockwise) – images will be rotated 90 degrees in clockwise orientation;\nleft (anti-clockwise)—images will be rotated 90 degrees in anti-clockwise orientation.\nDrag and drop step3_90DegreeRotationTool.ijm from the folder “GliaMorph>Macros” into Fiji.\nClick “Run.”\nSelect folder with folder with input files (i.e., folder “tiff” from Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0002] or a folder containing .tiff files).\nClick “OK.”\nCheck the output folder “90DegreeRotated” in the input folder with stacks and MIPs.\nExample Data: In the folder ExampleDataProtocol3/tiff you will find the input data, and the output data in ExampleDataProtocol3/tiff/90DegreeRotated that were rotated clockwise.\nIn some cases, images might be in very large stacks where the cells of interest are not located at the start of the stack, but more centrally (for example to visualize clones). We here suggest drawing 3D line ROIs within the stack.\nOpen stack (caveat: before we used the MIPs .tiff).",
    "Select plane of interest.\nDraw line ROI as above along cells of interest.\nSave ROISetLine.zip as above.\nDrag and drop step4_subregionToolWithinStack.ijm from the folder “GliaMorph>Macros” into Fiji.\nClick “Run.”\nSelect folder with folder with input files (i.e., folder “tiff” from Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0002] or a folder containing .tiff files).\nClick “OK.”\nThis will rotate and perform x,y-reduction as above, but additionally extract the sub-stack within the stack (the drawn ROI will be at the center, given there are enough slices above and below—if not, then the stack will be started in a position where it fits).\n(c) Separating multi-channel images with the splitChannelsTool\nThis step automatically splits channels of all images in a folder using the Fiji split channels option and saves them in separate folders.\nOptions: Select number of channels (channels 1-4; Fig. 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0006]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/218d771d-7100-401e-8809-160210e700f2/cpz1654-fig-0006-m.jpg</p>\nFigure 6\nGUI of the splitChannelsTool.\n1c. Drag and drop step5_splitChannelsTool.ijm from the folder “GliaMorph>Macros” into Fiji.\n2c. Click “Run.”\n3c. Select number of channels from the drop-down menu.\n4c. Select folder with folder with input files (i.e., folder “zDir” from Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0003]).\n5c. Click “OK.”\n6c. Check output folder “XCDir” (“X” indicating the channel number) in the input folder with stacks named XC-name (“X” indicating the channel and “name” being the original filename).\nExample Data: In the folder ExampleDataProtocol3\\tiff\\90DegreeRotated\\zDir you will find the input data, and the output folders 1CDir, 2CDir, and 3CDir.\n(d) Examining data integrity and texture with the ZonationTool\nThe zonationTool produces 1D vector from 3D stack to derive apical-to-basal intensity plots. These plots indicate the relative position of MG sub-domains along the apicobasal axis. Plotting the derived intensities allows insights into data integrity, similarity, and texture, e.g., “Are the SubregionTool outputs satisfactory?” or “Are the examined data acceptably age-matched?” (Fig. 7A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0007],B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0007]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/832ec481-372a-4ef6-a0d2-c87da4006bc7/cpz1654-fig-0007-m.jpg</p>\nFigure 7",
    "ZonationTool overview. (A) GUI of the zonationTool. (B) Example of plots from three different images, where gray and white arrowhead indicate overlap, and the black arrowhead indicates that cell bodies in this sample are not aligned with the others. (C,D) Same data plotted without and with scale normalization, respectively.\nOptions (Fig. 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0007])\nPerform intensity plotting: If “no” is selected nothing happens.\nPerform scale normalization:\n               \n○“yes”—output data will be scaled to the same size (selected in the next box);\n○“no”—output data are the size of the input (Fig. 7C[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0007],D[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0007]).\nOutput image width: Image scale for rescaling to make images comparable.\nHeight of sigma: To measure retina height (sigma selected in rotationTool—i.e., if you selected 10 µm before, add 10 µm here).\nFor the input folder, select the folder with tiff files after the subregionTool (it is important that these are the same size and comparable—i.e., it will not work on original input tiffs); for multi-channel images apply splitChannelsTool first and apply to images from different channels individually.\n1e. Drag and drop step6_zonationTool.ijm from the folder “GliaMorph>Macros” into Fiji.\n2e. Click “Run.”\n3e. Select steps and parameters.\n4e. Select folder with folder with input files [for single channel images—zDir; for multichannel images—the respective folder (1C/2C/3C/4C) inside the zDir folder).\n5e. Click “OK.”\n6e. Check the output folder “ZonationTool” in the input folder containing 1D vectors with LUT fire, a .csv with apical-to-basal intensity profiles (“ZonationToolProfiles”) and a .csv file with measurements (“ZonationResults”).\nExample Data: The input data are in the folder ExampleDataProtocol3\\tiff\\90DegreeRotated\\zDir\\2CDir, while the example output data can be found in ExampleDataProtocol3\\tiff\\90DegreeRotated\\zDir\\2CDir\\ZonationTool.\nAfter this, continue with Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0005].",
    "This protocol sequence is like the above; however, the input data are acquired with conventional confocal microscopy rather than AiryScan microscopy, meaning that a separate deconvolution step is required. This is due to the fact that when imaging, convolution of light in 3D produces a so called point spread function (PSF; see Gennaro & Geoff, 2010[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0010]; or https://bitesizebio.com/22166/a-beginners-guide-to-the-point-spread-function-2/[href=https://bitesizebio.com/22166/a-beginners-guide-to-the-point-spread-function-2/]), which means that, for example, originally spherical objects appear elliptical—using computational deconvolution based on image acquisition knowledge the object can be deconvolved to the original spherical shape.\nTherefore, PSF deconvolution is applied to improve data quality (Shaw & Rawlins, 1991[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0039]). The DeconvolutionTool (Fig. 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0008]) supports PSF deconvolution at different wavelengths using the DeconvolutionLab2 Plugin (Sage et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0034]; http://bigwww.epfl.ch/deconvolution/deconvolutionlab2/[href=http://bigwww.epfl.ch/deconvolution/deconvolutionlab2/] which was downloaded in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0001], “b” steps for code download) and supports the use of existing or non-existing PSF files (Fig. 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0008]). In the case of non-existing PSF files, it is modeled using analytical derivation based on Fraunhofer diffraction using the “Diffraction PSF 3D” Plugin (https://imagej.net/Deconvolution[href=https://imagej.net/Deconvolution]; https://www.optinav.info/Diffraction-PSF-3D.htm[href=https://www.optinav.info/Diffraction-PSF-3D.htm]’ again, these were downloaded in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0001], “b” steps for code download).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/a359a048-a455-41c6-a493-851509c5c26a/cpz1654-fig-0008-m.jpg</p>\nFigure 8\nGUI of the DeconvolutionTool.\nIf you have access to another deconvolution tool, you can proceed to the next step, i.e., image segmentation, and use your.tiff files that were deconvolved elsewhere as input.\nHere we use PSF deconvolution using Richardson and Lucy with 1 iteration (DeconvolutionLab2) to be applied to confocal images (redundant for AiryScan processed data); the tool works with and without existing PSF.\nOptions:\n         \nMultiple channels: Specify the number of channels in the input images. The default will be “no,” assuming that there is only one channel as input. Select “yes” if there are multiple channels as input.",
    "C1-C4: Select which fluorophores were imaged: e.g., if your first channel is GFP, then you need to write 510 into the first box; if your second channels is dsRed, then you need to write 586 into the second box.\nSelect objective NA: Only required if PSF does not exist, meaning it will produce a theoretical PSF; specify based on the objective numerical aperture.\nDoes PSF exist?: If PSF does not exist, it will generate a theoretical PSF using Diffraction PSF 3D. If PSF exists (experimental or theoretical), the macro will prompt the user to select a PSF file for each channel.\nFor the input, select the folder with .tiff files acquired in confocal mode. We recommend that you apply the SubregionTool first, as this will make the data smaller and therefore the deconvolution step quicker. Caveat: deconvolution is computationally intensive, so it might not be suitable for low-capacity machines.\n1. Drag and drop step2_DeconvolutionTool.ijm from the folder “GliaMorph>Macros” into Fiji.\n2. Click “Run.”\n3. Select options.\n4. Select folder with folder with input files (for single channel images—zDir; for multichannel images—the respective folder (1C/2C/3C/4C) inside the zDir folder).\n5. Click “OK.”\nAs this step is computationally very intensive, we advise it to be run on data that are reduced in size by step4_subregionTool.ijm. Once the tool has finished, all windows can be closed.\n6. Check the Output:\n         \nChannDir—directory with images of individual channels.\nPSFDir—directory with theoretically produced PSF files.\nDeconvDir—directory with the deconvolved images—these are the data that should be used for subsequent data analysis steps.\nExample Data: The folder ExampleDataAlternateProtocol3 contains input files and output folders created with PSF deconvolution, using theoretically created PSF files.",
    "For confocal data, we suggest applying the SubregionTool before the deconvolutionTool, as PSF deconvolution is computationally intense and reduced data size reduces the time needed for computation.",
    "Subsequent to image understanding and pre-processing, image segmentation is applied to extract cells from images by binarization (i.e., foreground/cells = 1; background = 0). Computationally, this is very challenging, as glia have complex morphologies (MacDonald et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0021]; Wang et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0046]) and suffer visualization heterogeneity (Halford et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0012]; Escartin et al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0008]), making them more difficult to segment than, for example, cells with a round shape and homogenous signal distribution.\nAs image segmentation depends on a myriad of factors, we can here only provide a brief overview and the reader is referred to publications on segmentation factors and assessment (Bolón-Canedo & Remeseiro, 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0006]; Jeevitha, Iyswariya, RamKumar, Basha, & Kumar, 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0014]; Udupa et al., 2006[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0044]).\nIndependent of segmentation approach and desired analysis, for segmentation to be meaningful and automatable, images need to be comparable in quality and properties. Often this can be achieved by visual grading [e.g., are images looking the same throughout the dataset, are they roughly the same size, are there any artifacts (Koho, Fazeli, Eriksson, & Hänninen, 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0017])] and measurements such as CNR (see Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0001]).\nIn our experience, researchers often use the first segmentation approach that looks suitable for their data. However, we find that this often means sub-optimal segmentation results unless there is further assessment, optimization, or validation. Therefore, even if data segmentation and quantification are presented in manuscripts, they must be assessed with caution.\nHere we examine how to examine segmentation workflows, with the caveat that project-specific segmentation is likely to require optimization.\nExamining segmentation for suitability",
    "1a. Manual testing: Before the automation of any segmentation workflows, it is advised to examine different segmentation methods, parameters, and settings manually to gauge ranges. See basic introduction on preprocessing, segmentation, and analysis at https://imagej.net/imaging/segmentation[href=https://imagej.net/imaging/segmentation]. Importantly, making notes or using the recording tool (Plugins > Macros > Record) helps to achieve a structured approach in examining segmentation approaches.\n2a. SegmentationTest.ijm: Macro that allows testing of six commonly used segmentation approaches, namely 3D simple segmentation using the 3D Segmentation plugin (Ollion et al., 2013[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0025]), Hysteresis (histogram-derived) using the 3D Segmentation plugin (Ollion et al., 2013[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0025]), Otsu thresholding (Otsu, 1979[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0026]), Moments (Tsai, 1995[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0043]), Percentile (Doyle, 1962[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0007]), and Maximum Entropy (Kapur, Sahoo, & Wong, 1985[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0015]).\n3a. Do SegmentationTest.ijm with pre-processing.\nTests the above six thresholding methods, but with image smoothing and background removal (uncomment line 40 in the macro).\n4a. SegmentationTest.ijm with PSF deconvolution:\nAs in step 3a but with prior PSF deconvolution (see Alternate Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0004]).\n5a. SegmentationTest.ijm with pre-processing with PSF deconvolution:\nCombination of step 3a and step 4a.\nFor the input folder, select the folder with .tiff files (if you applied the SubregionTool to AiryScan data, then use the folder “zDir” as input; if you performed deconvolution, then either use “zDir” or “DeconvDir” depending on which you performed last).\n1b. Drag and drop SegmentationTest.ijm from the folder “GliaMorph>AlternativeMacros” into Fiji.\n2b. Click “Run.”\n3b. Select options.\n4b. Select folder with folder with input files [depending on your previous steps you may select for single-channel images—zDir; for multichannel images—the respective folder (1C/2C/3C/4C) inside the zDir folder. If you performed deconvolution, select the respective DeconvDir).\n5b. Click “OK.”\nOutput: Segmented tiff stacks and folder containing MIPs.",
    "Following the testing of different processing and segmentation workflows, comparing the results can be used to assess their suitability. Directly comparing segmentation workflows enables visual assessment, showing things such as under- (Fig. 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0009], cyan box) or over-segmentation (Fig. 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0009], magenta box).\nComparing segmentation outcomes can also be used to fine-tune the parameters of the respective segmentation approaches (Fig. 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0009], green box).\nSegmentation is a multi-step process, often containing preprocessing and segmentation steps (see macro code SegmentationTest.ijm line 40 and 41). Testing various parameters, such as filtering (line 54) or background removal (line 57), will give you an idea on how your data respond to individual steps.\nHowever, in-depth explanations on how to optimize and validate segmentation goes beyond this protocol, and the interested reader is referred to Kugler, Rampun, Chico, & Armitage (2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0018]).\nAfter visual assessment, validation of segmentation workflows including aspects such as accuracy, robustness, or speed is required (Padfield & Ross, 2009[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0027]; Kugler et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0018]). This is of particular importance, as only accurate segmentation will deliver biologically relevant information. In addition, the more sensitive a, segmentation approach, the fewer samples are typically needed to extract biological differences between examined groups. However, while extensive segmentation validation and testing are applied in medical image analysis, this is still often lacking in biomedical image analysis, and in our experience, segmentation approaches are often not optimized or validated and are therefore to be treated with caution.\nExample Data: The folder “ExampleDataProtocol4” contains input and output files.\nGliaMorph contains two main segmentation approaches: (a) CytosolSegmentation.ijm that was optimized for Tg(TP1bglob:VenusPest)s940 (Ninov, Borius, & Stainier, 2012[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0024]) and Tg(CSL:mCherry)jh11 (also known as Tg(Tp1bglob:hmgb1-mCherry)jh11 (Parsons et al., 2009[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0028]), and (b) MembraneSegmentation.ijm that was optimized for Tg(TP1:CAAX-eGFP)u911.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/594668d8-87f7-4cad-895a-7e8bba3e1c2a/cpz1654-fig-0009-m.jpg</p>\nFigure 9",
    "Preliminary tests for segmentation. (A) Original. (B) Segmentation methods tested on images without PSF deconvolution and with (upper panel) or without pre-processing (lower panel). (C) Segmentation methods tested on images with PSF Richardson-Lucy (RL) (1 iteration) deconvolution and with (upper panel) or without pre-processing (lower panel; MF – Median filter, RB – Rolling Ball algorithm). Examples: Cyan box: Under-segmentation, magenta box: over-segmentation, green box: Suitable segmentation for further fine-tuning.\nFor the input folder, select the folder with .tiff files (if you applied the SubregionTool to AiryScan data, then use the folder “zDir” as input; if you performed deconvolution, then either use “zDir” or “DeconvDir” depending on which you performed last).\n1c. Drag and drop MembraneSegmentation.ijm or CytosolSegmentation.ijm from the folder “GliaMorph > AlternativeMacros” into Fiji.\n2c. Click “Run.”\n3c. Select folder with folder with input files.\n4c. Click “OK.”\nIf you work with other data, you would need to optimize the existing segmentation workflow or produce a new one.\n5c. Check output: Segmented tiff stacks and folder containing MIPs.",
    "Following image binarization using image segmentation, object features such as volume, surface, or thickness can be quantified. Building on this, extraction of the 3D skeleton, such as by 3D thinning (Lee, Kashyap, & Chu, 1994[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0019]), allows for the quantification of skeleton length, branching points, or endpoints. Depending on the biological question, different features might be more relevant than others, and higher-order analysis of feature selection or clustering might be insightful (Bolón-Canedo & Remeseiro, 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0006]).\nThis step is to extract shape features from segmented images.\nFor the input folder, select with segmented .tiff files —data need to be pre-processed (i.e., subRegionTool).\n1. Drag and drop step8_QuantificationTool.ijm from the folder “GliaMorph>Macros” into Fiji.\n2. Click “Run.”\n3. Select folder with folder with input files (depending on your above steps—select the “TH” folder you selected for Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-prot-0005]).\n4. Click “OK.”\nOnce the tool has finished, all windows can be closed.\n5. Check Output:\n         \n○Folders and files:\n               \n▪outZone folder: Apicobasal profiles of segmented images.\n▪QuantEDM: .tiff stacks and MIPs of 3D Euclidean Distance Maps (EDM) showing local MG thickness (brighter = thicker; darker = thinner).\n▪QuantSkel: Apicobasal profiles of skeletons; skeleton .tiffs images; MIP folder (contains edges, MIP skeleton, MIP thickness).\n▪Files:\n                     \nQuantificationResults: Volume [um3], PercCov [%], SurfaceVol [um3], Thickness [um].\nUse these to plot and analyze in another program.\nSkeleton Stats: Contains max branch length, mean branch length, # of trees, # of branches, # of junctions, # of endpoints, # of triple points, # of quadruple points, sum of voxels.\nUse these to plot and analyze in another program.\n○3D data: Data from “QuantificationResults” and “Skeleton Stats”—these can be plotted and analyzed with other programs such as Excel or GraphPad Prism (see Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-tbl-0001] for details).",
    "○Apicobasal texture: Plotting apicobasal (top-to-bottom) data distribution of original, segmented, and skeletonized data provides insights into the texture of data. Data interpretation by experts provides further insights, such as that branching is highest in the so-called IPL or that cell bodies are located in the ONL (for example results, see Fig. 10[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-fig-0010]).\nExample Data: The folder ExampleDataProtocol5 contains input and output files.\nThe quantificationTool extracts 3D skeletons using the Fiji \"Skeletonize 2D/3D\" Plugin (by Ignacio Arganda-Carreras), based on 3D thinning (Lee et al., 1994[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0019]) using a layer-by-layer removal. As this method is sensitive to small surface heterogeneities that could result in spurious branches (Attali, Boissonnat, & Edelsbrunner, 2009[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0005]), users are advised to examine pre-processing using surface smoothing or post-processing using spurious branch pruning (Sanderson, Cohen, Henderson, & Parker, 1994[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.654#cpz1654-bib-0035]).\nTable 1.\n                Overview of Analyzed Features With Their Variables, Units, and Description (N Indicates Embryos)\ntable:\n﻿Feature,Variable,Unit,Description\nImage height,IN,[µm],Total height of the image\nMG height,MGN,[µm],Radial extension of MG\nNumber/count,NN,,Number of objects in ROI\nVolume,VN,[µm3],\"Volume of object voxel, derived after segmentation\"\nVolume coverage,VCN,[%],Percentage of image volume covered with MG (lowest = 0; highest = 100)\nSurface,SN,[µm3],\"Number of object surface voxels, derived after segmentation (given in [µm3] for comparability between experiments)\"\nSurface: Volume ratio,S:VN,,Ratio of surface to volume (lowest = 0; highest = 1)\nThickness,TN,,Distance from local centerline to the corresponding surface\nSkeleton length,LN,[µm],Skeleton voxels (given in [µm] for comparability between experiments)\n# of Junction,JN,,Number of points where 2/more sub-branches branch off\n# of Endpoints,EPN,,Number of blind-ended object points\nAverage branch length,BL,[µm],Average length of individual skeleton branches\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/83ba3714-d5a3-42e0-8320-9b9427d85dc8/cpz1654-fig-0010-m.jpg</p>\nFigure 10\nExample results. (A) Micrographs of representative SubregionTool output image at 3dpf and 5dpf (input for segmentation and quantification). (B-D) Heatmaps showing the apicobasal texture of the original, segmented, and skeletonized image. (E-N) Whole-image quantification outputs."
  ],
  "subjectAreas": [
    "Neuroscience"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}