{
  "id": 17948,
  "origin_website": "Jove",
  "title": "Cloud-Based Phrase Mining and Analysis of User-Defined Phrase-Category Association in Biomedical Publications",
  "procedures": [
    "NOTE: We have developed this protocol based on the Python programming language. To run this program, have Anaconda Python and Git pre-installed on the device. The commands provided in this protocol are based on Unix environment. This protocol provides the detail of downloading data from PubMed (MEDLINE) database, parsing the data, and setting up a cloud computing platform for the phrase mining and quantification of user-defined entity-category association.\n1.Getting code and python environment setup\nDownload or clone the code repository from Github (https://github.com/CaseOLAP/caseolap[href=https://github.com/CaseOLAP/caseolap.git]) or by typing ‘git clone https://github.com/CaseOLAP/caseolap.git[href=https://github.com/CaseOLAP/caseolap.git]’ in the terminal window.\nNavigate to the ‘caseolap’ directory. This is the root directory of the project. Within this directory, the ‘data’ directory will be populated with multiple data sets as you progress through these steps in the protocol. The ‘input’ directory is for user-provided data. The ‘log’ directory has log files for troubleshooting purposes. The ‘result’ directory is where the final results will be stored.\nUsing the terminal window, go to the directory where you cloned our GitHub repository. Create the CaseOLAP environment using the ‘environment.yml’ file by typing ‘conda env create -f environment.yaml’ in the terminal. Then activate the environment by typing ‘source activate caseolap’ in the terminal.\n2. Downloading documents\nMake sure that the FTP address in ‘ftp_configuration.json’ in the config directory is the same as the Annual Baseline or Daily Update Files link address, found in the link (https://www.nlm.nih.gov/databases/download/pubmed_medline.html[href=https://www.nlm.nih.gov/databases/download/pubmed_medline.html]).\nTo download baseline only or update files only, set ‘true’ in the ‘download_config.json’ file in the ‘config’ directory. By default, it downloads and extracts both baseline and update files. A sample of extracted XML data can be viewed at (https://github.com/CaseOLAP/caseolap-pipelines/blob/master/data/extracted-data-sample.xml[href=https://github.com/CaseOLAP/caseolap-pipelines/blob/master/data/extracted-data-sample.xml])",
    "Type ‘python run_download.py’ in the terminal window to download abstracts from the Pubmed database. This will create a directory called ‘ftp.ncbi.nlm.nih.gov’ in the current directory. This process checks the integrity of the downloaded data and extracts it to the target directory.\nGo to the ‘log’ directory to read the log messages in ‘download_log.txt’ in case the download process fails. If the process is completed successfully, the debugging messages of the download process will be printed out in this log file.\nWhen the download is complete, navigate through ‘ftp.ncbi.nlm.nih.gov’ to make sure that there is ‘updatefiles’ or ‘basefiles’ or both directories based on download configuration in ‘download_config.json’. The file statistics become available at ‘filestat.txt’ in the ‘data’ directory.\n3. Parsing documents\nMake sure that downloaded and extracted data is available at ‘ftp.ncbi.nlm.nih.gov’ directory from step 2. This directory is the input data directory in this step.\nTo modify the data-parsing schema, select parameters in ‘parsing_config.json’ file in the ‘config’ directory by setting their value to ‘true’. By default, it parses the PMID, authors, abstract, MeSH, location, journal, publication date.\nType ‘python run_parsing.py’ in the terminal to parse the documents from downloaded (or extracted) files. This step parses all downloaded XML files and creates a python dictionary for each document with keys (e.g., PMID, authors, abstract, MeSH of the file based on parsing schema setup at step 3.2).\nOnce data parsing is completed, make sure that parsed data is saved in the file called ‘pubmed.json’ in the data directory. A sample of parsed data is available at Figure 3.\nGo to the ‘log’ directory to read the log messages in ‘parsing_log.txt’ in case the parsing process fails. If the process is completed successfully, the debugging messages will be printed out in the log file.\n4. Mesh to PMID mapping",
    "Make sure that parsed data (‘pubmed.json’) is available at the ‘data’ directory.\nType ‘python run_mesh2pmid.py’ in the terminal to perform MeSH to PMID mapping. This creates a mapping table where each of the MeSH collects associated PMIDs. A Single PMID may fall under the multiple MeSH terms.\nOnce the mapping is completed, make sure that there is ‘mesh2pmid.json’ in the data directory. A sample of the top 20 mapping statistics is available in Table-2, Figures 4 and 5.\nGo to the ‘log’ directory to read the log messages in ‘mesh2pmid_mapping_log.txt’ in case this process fails. If the process is completed successfully, the debugging messages of the mapping will be printed out in this log file.\n5. Document indexing\nDownload the Elasticsearch application from https://www.elastic.co[href=https://www.elastic.co/downloads]. Currently, the download is available at (https://www.elastic.co/downloads/elasticsearch[href=https://www.elastic.co/downloads/elasticsearch]). To download the software in the remote cloud, type ‘wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-x.x.x.tar.gz[href=https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-x.x.x.tar.gz]’ in the terminal. Make sure that ‘x.x.x’ in the above command is replaced by proper version number.\nMake sure that downloaded ‘elasticsearch-x.x.x.tar.gz’ file appears in the root directory then extract the files by typing ‘tar xvzf elasticsearch-x.x.x.tar.gz’ in the terminal window.\nOpen a new terminal and go to the ElasticSearch bin directory by typing ‘cd Elasticsearch/bin’ in the terminal from the root directory.\nStart the Elasticsearch server by typing ‘./Elasticsearch’ in the terminal window. Make sure that the server is started without error messages. In case of error on starting Elasticsearch server, follow the instructions at (https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html[href=https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html]).\nModify the contents in the ‘index_init_config.json’ in ‘config’ directory to set index initiation. By default, it will select all items present.",
    "Type ‘python run_index_init.py’ in the terminal to initiate an index-database in the Elasticsearch server. This initializes the index with a set of criteria known as index information (e.g., index name, type name, number of shards, number of replicas). You will see the message mentioning index is successfully created.\nSelect the items in the ‘index_populate_config.json’ in the ‘config’ directory by setting their value to ‘true’. By default, it will select all items present.\nMake sure that parsed data (‘pubmed.json’) is present in the ‘data’ directory.\nType ‘python run_index_populate.py’ in the terminal to populate the index by creating bulk data with two components.A first component is a dictionary with metadata information on the index name, type name, and bulk id (e.g., ‘PMID’). A second component is a data dictionary containing all the information on the tags (e.g., ‘title’,’abstract’,’MeSH’). \nGo to the ‘log’ directory to read the log messages in ‘indexing_log.txt’ in case this process fails. If the process is completed successfully, the debugging messages of the indexing will be printed out in the log file.\n6. Text-cube creation\nDownload the latest MeSH Tree available at (https://www.nlm.nih.gov/mesh/filelist.html[href=https://www.nlm.nih.gov/mesh/filelist.html]). The current version of the code is using MeSH Tree 2018 as ‘meshtree2018.bin’ in the input directory.\nDefine the categories of interest (e.g., Disease names, Age groups, Gender). A category may include one or more MeSH descriptors (https://meshb-prev.nlm.nih.gov/treeView[href=https://meshb-prev.nlm.nih.gov/treeView]). Collect MeSH IDs for a category. Save the names of the categories in the file ‘textcube_config.json’ in the config directory (see a sample of the category in ‘Age Group’ in the downloaded version of ‘textcube_config.json’ file).",
    "Put the collected categories of MeSH IDs in a line separated by a space. Save the category file as ‘categories.txt’ in the ‘input’ directory (see a sample of ‘Age Group’ MeSH IDs in the downloaded version of ‘categories.txt’ file). This algorithm automatically selects all descendent MeSH descriptors. An example of root nodes and descendants are presented in Figure 4.\nMake sure that ‘mesh2pmid.json’ is in the ‘data’ directory. If the MeSH Tree has been updated with a different name (e.g., ‘meashtree2019.bin’) in ‘input’ directory, make sure that this is properly represented in the input data path in the ‘run_textube.py’ file.\nType ‘python run_textcube.py’ in the terminal to create a document data structure called Text-Cube. This creates a collection of documents (PMIDs) for each category. A single document (PMID) may fall under multiple categories, (see Table 3A, Table 3B, Figure 6A and Figure 7A).\nOnce Text-Cube creation step is completed, make sure that following data files are saved in the ‘data’ directory: (1) a cell to PMID table as “textcube_cell2pmid.json”, (2) a PMID to cell mapping table as “textcube_pmid2cell.json”, (3) a collection of all descendant MeSH terms for a cell as “meshterms_per_cat.json” (4) Text-Cube data statistics as “textcube_stat.txt”.\nGo to the ‘log’ directory to read the log messages in ‘textcube_log.txt’ in case this process fails. If the process is completed successfully, the debugging messages of the Text-Cube creation will be printed out in the log file.\n7. Entity count\nCreate user-defined entities (e.g., protein names, genes, chemicals). Put one entity and its abbreviations in a single line separated by “|”. Save the entity file as ‘entities.txt’ in the ‘input’ directory. A sample of entities can be found in Table 4.",
    "Make sure that Elasticsearch server is running. Otherwise, go to step 5.2 and 5.3 to restart the Elasticsearch server. It is expected to have an indexed database called ‘pubmed’ in your Elasticsearch server which was established in step 5.\nMake sure that ‘textcube_pmid2cell.json’ is in the ‘data’ directory.\nType ‘python run_entitycount.py’ in the terminal to perform Entity Count operation. This searches the documents from the indexed database and counts the entity in each document as well as collects the PMIDs in which entities were found.\nOnce the Entity Count is completed, make sure that the final results are saved as ‘entitycount.txt’ and ‘entityfound_pmid2cell.json’ in the ‘data’ directory.\nGo to the ‘log’ directory to read the log messages in ‘entitycount_log.txt’ in case this process fails. If the process is completed successfully, the debugging messages of the Entity Count will be printed out in the log file.\n8. Metadata update\nMake sure that all input data (‘entitycount.txt’, ‘textcube_pmid2cell.json’, ‘entityfound_pmid2cell.txt’) are in the ‘data’ directory. These are the input data for Metadata Update.\nType ‘python run_metadata_update.py’ in the terminal to update the metadata. This prepares a collection of metadata (e.g., cell name, associated MeSH, PMIDs) representing each text document in the cell. A sample of Text-Cube metadata is presented in Table 3A and Table 3B.\nOnce the Metadata Update is completed, make sure that ‘metadata_pmid2pcount.json’ and ‘metadata_cell2pmid.json’ files are saved in ‘data’ directory.\nGo to the ‘log’ directory to read the log messages in ‘metadata_update_log.txt’ in case this process fails. If the process is completed successfully, the debugging messages of the metadata update will be printed out in the log file.\n9. CaseOLAP score calculation\nMake sure that ‘metadata_pmid2pcount.json’ and ‘metadata_cell2pmid.json’ files are present in the ‘data’ directory. These are the input data for score calculation.",
    "Type ‘python run_caseolap_score.py’ in the terminal to perform CaseOLAP score calculation. This calculates the CaseOLAP score of the entities based on user-defined categories. The CaseOLAP score is the product of Integrity, Popularity, and Distinctiveness.\nOnce the score computation is completed, make sure that this saves the results in multiple files (e.g., popularity as ‘pop.csv’, distinctiveness as ‘dist.csv’, CaseOLAP score as ’caseolap.csv’), in the ‘result’ directory. The summary of the CaseOLAP score calculation is also presented in Table 5.\nGo to the ‘log’ directory to read the log messages in ‘caseolap_score_log.txt’ in case this process fails. If the process is completed successfully, the debugging messages of the CaseOLAP score calculation will be printed out in the log file."
  ],
  "subjectAreas": [
    "Medicine"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}