{
  "id": 3114,
  "origin_website": "Cell",
  "title": "A universal AutoScore framework to develop interpretable scoring systems for predicting common types of clinical outcomes",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\n      As detailed in this section, the AutoScore framework is implemented in\n      several general steps. We use Roman Numbers (i.e., (i), (ii), etc.) to\n      denote general AutoScore steps, which often consist of several protocol\n      steps (indicated by digits 1, 2, etc.).\n      Table 1[href=https://www.wicell.org#tbl1] provides an overview of AutoScore steps and\n      corresponding functions in the R package, and in the following\n      subsections, we will describe the installation instruction and usage.\n    \ntable:files/protocols_protocol_2687_1.csv\nInstall the package and the prerequisites\nTiming:  < 5 min\n    \n      This step describes how to install the AutoScore package, which\n      automatically installs all dependencies in the\n      key resources table[href=https://www.wicell.org#key-resources-table].\n    \nInstall the stable version of AutoScore from CRAN:\n> install.packages(\"AutoScore\")\nor the latest (development) version from GitHub:\n> install.packages(\"devtools\") # If not already\n          installed\n> library(devtools)\n> install_github(repo = \"nliulab/AutoScore\",\n          build_vignettes = TRUE)\nCritical: The commands above\n      automatically install all dependencies of AutoScore (see the\n      key resources table[href=https://www.wicell.org#key-resources-table]).\n      Troubleshooting 1[href=https://www.wicell.org#sec5.1] suggests a solution to possible\n      installation errors.\n    \nData processing and checking\nTiming:  < 15 min\n    \n      This step checks and processes data to meet all requirements. AutoScore\n      has specific requirements on the outcome, predictors and missing values.\n    \n        Load data.\n        \nRead data from CSV or Excel files.\n            For this demo, use the integrated sample datasets in the package.\n          \n> library(AutoScore)\n> data(\"sample_data\") # Load data with binary\n          outcome\n> data(\"sample_data_survival\") # Load data with survival\n          outcome\n> data(\"sample_data_ordinal\") # Load data with ordinal\n          outcome\nCritical: These sample datasets are\n      simulated to demonstrate the workflow. Any results and scoring systems\n      described in this protocol are created solely for the demonstration of\n      AutoScore usage and may not be clinically meaningful. Variable names are\n      intentionally masked to avoid misinterpretation and misuse of data and\n      models.\n    \nNote: These sample datasets used <500MB",
    "memory when loaded in R and generally consumed <1GB memory in the\n      processing steps to be described below.\n      Troubleshooting 2[href=https://www.wicell.org#sec5.3] discusses how to monitor memory\n      usage and handle possible issues in subsequent steps when working with\n      larger clinical datasets.\n    \n        Check outcomes.\n        \n            For binary and ordinal outcomes, change the name of the outcome to\n            “label” and make sure that no other variables use this name. The\n            code below changes the name of the binary outcome in “sample_data”\n            from “Mortality_inpatient” to “label”:\n            \n> names(sample_data)[names(sample_data) ==\n                  \"Mortality_inpatient\"] <- \"label\"\n            For survival outcomes, change outcome names for the time variable\n            and censoring status to “label_time” and “label_status”,\n            respectively, and make sure that no other variables use these names.\n            \nNote: Binary outcomes and\n              censoring status of survival outcomes should be coded as “factor”\n              data type with two categories, and ordinal outcomes should be\n              “factor” with three or more categories. The following functions\n              check data requirements for different types of outcomes:\n            \n> check_data(sample_data) # For binary outcomes\n> check_data_ordinal(sample_data_ordinal) # For ordinal\n                  outcomes\n> check_data_survival(sample_data_survival) # For survival\n                  outcomes\n        Check variables. The functions “check_data()”, “check_data_survival()”\n        and “check_data_ordinal()” demonstrated above also check whether\n        predictors in the data fulfill the following requirements:\n        \n            No special characters are available in variable names, e.g., “[“,\n            “]”, “(“, “)”, “,”. (Suggest using “_” to replace them if needed).\n          \n            The name of variables should be unique and not entirely included in\n            other variable names.\n          \n            Independent variables should be numeric (class: “numeric“ or\n            ”integer”) or categorical (class: “factor” or “logical”).\n            \nCritical: All data problems\n              reported by “check_data()”, “check_data_survival()” or\n              “check_data_ordinal()” must be fully resolved before proceeding to\n              the modeling phase. Troubleshooting 3[href=https://www.wicell.org#sec5.5] and\n              4[href=https://www.wicell.org#sec5.7] elaborate on common data problems and\n              suggested solutions.\n            \n        Check missing values. The functions “check_data()”,\n        “check_data_survival()” and “check_data_ordinal()” will report missing",
    "rates for any variable with missing entries (coded as “NA” in R):\n        \n            AutoScore expects the input dataset to be complete with no missing\n            values. Users can proceed with modeling if the data is complete and\n            fulfill other requirements described in steps 3 and 4.\n          \n            If there are missing values in the dataset and users believe the\n            missingness is informative and prevalent enough to be preserved as\n            “NA” rather than excluded or imputed, users can proceed with\n            modeling because AutoScore can automatically handle missing values\n            by treating them as a new category named “Unknown”.\n          \n            Otherwise, users should handle missing values using appropriate\n            methods (e.g., imputation or complete data analysis) before\n            proceeding with modeling.\n            \nCritical: If feasible, users\n              are highly recommended to carefully handle missing values in the\n              input dataset during data pre-processing and provide a complete\n              dataset without missing values to AutoScore.\n            \nNote: When imputing missing values\n              or treating them as a new category, high missing rates (e.g.,\n              >80%) may reduce model stability and should be handled with\n              caution. For simplicity, in this protocol, we only demonstrate\n              sample data with complete information, and interested users can\n              refer to Demo 3 in Chapters 4 to 6 in our online guidebook (https://nliulab.github.io/AutoScore/[href=https://nliulab.github.io/AutoScore/]) for more details on data with missing values.\n            \n        Optional operations.\n        \nCheck variable distribution.\nHandle outliers.\nNote: The raw electronic health records\n      data may contain outliers caused by system errors or clerical mistakes.\n      Users are recommended to handle them appropriately before using AutoScore\n      to ensure optimal modeling performance.\n    \nSplitting data\nTiming:  < 10 min\n    \n      This step aims to randomly split the dataset into three separate datasets\n      (training, validation, and test datasets) for model training, validation\n      and testing.\n    \nSplit the dataset into training, validation, and test datasets.\n> set.seed(4)\n> out_split <- split_data(data = sample_data, ratio = c(0.7,\n          0.1, 0.2))",
    "> train_set <- out_split$train_set\n> validation_set <- out_split$validation_set\n> test_set <- out_split$test_set\nNote: The split-sample approach\n      demonstrated above is suitable when there is a sufficient sample size,\n      e.g., 20,000 observations in “sample_data”. AutoScore provides a\n      cross-validation option for small sample sizes (see\n      https://nliulab.github.io/AutoScore/[href=https://nliulab.github.io/AutoScore/]). Users can skip this step if the three datasets have been prepared and\n      have passed the check operations in the previous subsection.\n    \nAutoScore step (i): Generate a variable ranking list\nTiming:  < 10 min (depending on your\n      data and computer)\n    \n      This is the first step of the AutoScore workflow, which uses machine\n      learning algorithms to identify the top-ranking predictors for subsequent\n      score generation.\n    \nNote: From this step onwards, we describe\n      R commands and outputs for the example with a binary outcome and provide\n      additional information regarding survival and ordinal outcomes in Note.\n    \n        To rank all current candidate variables, run the following command:\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2687-Fig1.jpg\n              Figure 1. Main AutoScore output for variable ranking and selection\n            \n              (A) Variable importance from step 8 and (B) parsimony plot from\n              step 9.\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2687-Fig2.jpg\n              Figure 2. Main AutoScore functions for survival outcomes and\n              corresponding output for score development and evaluation\n            \n              These sample datasets are simulated to demonstrate the workflow\n              and any results and scoring systems described here are created\n              solely for the demonstration.\n            \n              (A) Variable importance from step 8, (B) parsimony plot step 9,\n              (C) initial scoring table and performance measures from step 11,\n              (D) fine-tuned scoring table and performance measures from step\n              13, (E) performance measure of the final scoring model from step\n              14, and (F) conversion table and visualization of predicted\n              probabilities from steps 15 and 16.\n            \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2687-Fig3.jpg\n              Figure 3. Main AutoScore functions for ordinal outcomes and\n              corresponding output for score development and evaluation\n            \n              These sample datasets are simulated to demonstrate the workflow,",
    "and any results and scoring systems described here are created\n              solely for the demonstration.\n            \n              (A) Variable importance from step 8, (B) parsimony plot step 9,\n              (C) initial scoring table and performance measures from step 11,\n              (D) fine-tuned scoring table and performance measures from step\n              13, (E) performance measure of the final scoring model from step\n              14, and (F) conversion table and visualization of predicted\n              probabilities from steps 15 and 16.\n            \n> ranking <- AutoScore_rank(train_set = train_set, method =\n          \"rf\")\nNote: Refer to\n      Table 1[href=https://www.wicell.org#tbl1] for a detailed description of all arguments\n      available to each AutoScore function. The resulting variable ranking is\n      shown in Figure 1[href=https://www.wicell.org#fig1]A.\n      Troubleshooting 5[href=https://www.wicell.org#sec5.9] elaborates on suggested solutions\n      for debugging when facing some unexpected errors.\n    \nNote: For survival data, please use\n      “AutoScore_rank_Survival()” instead (see Figure 2[href=https://www.wicell.org#fig2]A),\n      which ranks variables using the random survival forest.\n    \nNote: For ordinal data, please use\n      “AutoScore_rank_Ordinal()” instead (see Figure 3[href=https://www.wicell.org#fig3]A),\n      which ranks variables using the random forest for multiclass\n      classification.\n    \n      AutoScore step (ii): Select the best model with a parsimony plot\n    \nTiming:  < 10 min\n    \n      The second step of the AutoScore workflow helps users select a\n      parsimonious list of variables for the final scoring model using a\n      parsimony plot. Variable selection is flexible and can incorporate\n      clinical knowledge and user preference in addition to model performance.\n    \n        To generate the parsimony plot based on the variable ranking (“ranking”)\n        from step 8, simply run the following:\n        \n> AUC <- AutoScore_parsimony(\n  train_set = train_set, validation_set =\n              validation_set,\n  rank = ranking, max_score = 100, n_min = 1, n_max =\n              20,\n  categorize = \"quantile\", quantiles = c(0,\n              0.05, 0.2, 0.8, 0.95, 1),\n  auc_lim_min = 0.5, auc_lim_max =\n              \"adaptive\"\n)\n            Key input arguments are the training and validation datasets\n            (“train_set” and “validation_set”) and variable ranking (“ranking”).\n            Other arguments can be adjusted to users’ needs.",
    "Refer to Table 1[href=https://www.wicell.org#tbl1] for a detailed description of\n            all input arguments. Performance with an increasing number of\n            variables will be printed out on the screen, and the parsimony plot\n            (i.e., model performance against complexity) will be available (see\n            Figure 1[href=https://www.wicell.org#fig1]B).\n            Troubleshooting 5[href=https://www.wicell.org#sec5.9] elaborates on suggested\n            solutions for debugging when facing some unexpected errors.\n            \nOptional: Users could use the\n              AUC for further analysis or export it as the CSV to other software\n              for plotting.\n            \n> write.csv(data.frame(AUC), file =\n                  \"AUC.csv\")\nNote: For survival data, please\n              use “AutoScore_parsimony_Survival()” instead (see\n              Figure 2[href=https://www.wicell.org#fig2]B). To obtain a single overall\n              performance metric in the parsimony plot, we use the integrated\n              AUC (iAUC), a weighted average of AUC(t) over the follow-up period\n              (the range of “label_time”).\n            \nNote: For ordinal data, please use\n              “AutoScore_parsimony_Ordinal()” instead (see\n              Figure 3[href=https://www.wicell.org#fig3]B, where performance is measured using\n              mean AUC (mAUC) across dichotomized comparisons. Users have the\n              additional option to choose the link function in the ordinal\n              regression using the parameter “link”, which affects predictive\n              performance. The default is link=“logit” corresponding to the\n              commonly used proportional odds model, and users may consider\n              “cloglog” or “probit”. The same “link” parameter must be used\n              throughout all AutoScore functions.\n            \n        Determine the optimal number of variables (“num_var”) based on the\n        parsimony plot obtained in step 9. The final list of variables can be\n        the first “num_var” (e.g., the first 6) variables:\n      \n> num_var <- 6\n> final_variables <- names(ranking[1:num_var])\nOptional: Users can adjust the finally\n      included variables “final_variables” based on their clinical preferences\n      and knowledge, e.g., select the top 6 variables and the 9th and 10th\n      variables:\n    \n> num_var <- 6\n> final_variables <- names(ranking[c(1:num_var, 9, 10)])\n      AutoScore step (iii): Generate initial scores with the final list of\n      variables\n    \nTiming:  < 10 min\n    \n      This is the third step of the AutoScore workflow, which generates initial",
    "scores with the final list of variables selected in step 10.\n    \n        Generate initial cutoff values (“cut_vec”) for all continuous variables\n        in the list of variables from step 10 (“final_variables”), which can be\n        fine-tuned in step 12:\n      \n> cut_vec <- AutoScore_weighting(\n  train_set = train_set, validation_set =\n          validation_set,\n  final_variables = final_variables, max_score = 100,\n  categorize = \"quantile\",\n  quantiles = c(0, 0.05, 0.2, 0.8, 0.95, 1)\n)\n      The initial scoring table corresponding to the cutoff values above and the\n      resulting intermediate performance evaluation (based on ROC evaluation for\n      binary outcomes) will be displayed (see Figure 4[href=https://www.wicell.org#fig4]A).\n      Users can proceed to the next steps if the intermediate evaluation results\n      are satisfactory. Otherwise, they may repeat steps 10–11 to adjust the\n      final variable list and assess performance measures with the updated\n      scoring table until satisfactory performance is reached.\n      Troubleshooting 5[href=https://www.wicell.org#sec5.9] elaborates on suggested solutions\n      for debugging when facing some unexpected errors.\n    \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2687-Fig4.jpg\n          Figure 4. AutoScore output for intermediate scoring table evaluation\n          and fine-tuning\n        \n          (A) Initial scoring table and performance measures from step 11 and\n          (B) fine-tuned scoring table and performance measures from step 13.\n        \nNote: For survival data, please use\n      “AutoScore_weighting_Survival()” instead (see\n      Figure 2[href=https://www.wicell.org#fig2]C). This function requires an additional\n      argument, “time_point”, to specify the time points at which time-dependent\n      AUC (t) is to be evaluated.\n    \nNote: For ordinal data, please use\n      “AutoScore_weighting_Ordinal()” instead (see\n      Figure 3[href=https://www.wicell.org#fig3]C). Users have the additional option to choose\n      the link function for the ordinal regression (see Note of step 10 for\n      detail). Performance is measured using mAUC.\n    \nAutoScore step (iv): Fine-tune the initial score\nTiming:  < 10 min\n    \n      This step gives users an opportunity to revise the data-driven cutoff\n      values for each continuous variable from step 11, by combining categories,\n      rounding cutoff values up to meaningful values, or changing cutoffs\n      according to clinical knowledge, user preference or implementation\n      requirement.",
    "After checking the initial scores and their cutoff values, users may\n        revise the cutoff values for each continuous variable using the codes as\n        follow.\n      \n> cut_vec$Age <- c(50, 75, 90)\n> cut_vec$Lab_H <- c(0.2, 1, 3, 4)\n> cut_vec$Lab_K <- c(10, 40)\n> cut_vec$Lab_B <- c(10, 17)\n> cut_vec$Vital_A <- c(70, 98)\nNote: This step is optional.\n    \n        Run the following command to regenerate the scoring table with the\n        updated “cut_vec” from step 12 (or the original data-driven “cut_vec”\n        from step 11 if step 12 is skipped).\n      \n> scoring_table <- AutoScore_fine_tuning(\n  train_set = train_set, validation_set =\n          validation_set,\n  final_variables = final_variables, cut_vec =\n          cut_vec,\n  max_score = 100\n)\n      The updated scoring systems and performance based on the validation set\n      are reported (see Figure 4[href=https://www.wicell.org#fig4]B). For example, the cutoff\n      values for age are updated from default quantile-based values to 50, 75\n      and 90, as specified in step 12 (indicated by blue rectangles in\n      Figure 4[href=https://www.wicell.org#fig4]), and the points for age categories are\n      updated by retraining the model.\n    \n      If the intermediate evaluation results for the current scoring system\n      (i.e., cutoff values, score values, variables, etc.) are satisfactory,\n      users may proceed with testing in the next step. Otherwise, users may\n      repeat steps 12–13 to revise fine-tuning or steps 10–13 to refine not only\n      cutoff values but also the variable list, until satisfactory performance\n      is achieved.\n    \nNote: For survival data, please use\n      “AutoScore_fine_tuning_Survival()” instead (see\n      Figure 2[href=https://www.wicell.org#fig2]D), with an additional “time_point” argument\n      for time points to evaluate the time-dependent AUC(t) at.\n    \nNote: For ordinal, please use\n      “AutoScore_fine_tuning_Ordinal()” instead (see\n      Figure 3[href=https://www.wicell.org#fig3]D), with an additional “link” argument to\n      specify the link function for ordinal regression. Performance is evaluated\n      using mAUC with 95% bootstrap CI (computed from “n_boot=100” bootstrap\n      samples by default).\n    \n      AutoScore step (v): Evaluate final risk scores on the test dataset\n    \nTiming:  < 10 min",
    "This step is to evaluate the final scoring system based on the unseen\n      testing dataset.\n    \n        Using the scoring table (“scoring_table”) generated from step 13, run\n        the following command to generate predicted scores (“pred_score”) for\n        each subject in the testing set (“test_set”) and print out the\n        performance indicators (and/or performance curves, including ROC curve).\n        The testing performance is shown in Figure 5[href=https://www.wicell.org#fig5].\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2687-Fig5.jpg\n              Figure 5. Performance measure of the final scoring model on the\n              test set from step 14\n            \n> pred_score <- AutoScore_testing(\n  test_set = test_set, final_variables =\n          final_variables,\n  cut_vec = cut_vec, scoring_table = scoring_table,\n  threshold = \"best\", with_label = TRUE\n)\nOptional: Use\n      “print_roc_performance()” to generate the performance under different\n      score thresholds (e.g., 90).\n    \n> print_roc_performance(pred_score$Label, pred_score$pred_score,\n          threshold = 90)\nNote: For survival data, please use\n      “AutoScore_testing_Survival()” instead (see\n      Figure 2[href=https://www.wicell.org#fig2]E), with an additional “time_point” argument\n      for time points to evaluate the time-dependent AUC(t) at.\n    \nNote: For ordinal, please use\n      “AutoScore_testing_Ordinal()” instead (see Figure 3[href=https://www.wicell.org#fig3]E),\n      with an additional “link” argument to specify the link function for\n      ordinal regression. In addition to mAUC, a generalized c-index is reported\n      for the test set with 95% CI computed from “n_boot=100” bootstrap samples\n      by default. Users can also apply “print_performance_ordinal()” to\n      predictions to print mAUC with or without the generalized c-index (see\n      Figure 3[href=https://www.wicell.org#fig3]E).\n    \nMap score to risk\nTiming:  < 10 min\n    \n      This step describes how to map risk scores to predicted probabilities and\n      visualize the probabilities.\n    \n        Map risk scores to predicted probabilities using the following\n        conversion table.\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2687-Fig6.jpg\n              Figure 6. Conversion tables for binary outcomes\n            \n              Conversion tables generated by cut-offs in (A) predicted risks or\n              (B) score values based on the test data.\n            \nNote: For binary outcomes, users can\n      generate conversion tables (with predictive performance measures) for\n      specific levels of risk (e.g., 0.01, 0.05, 0.1, 0.2, 0.5) or score",
    "thresholds (e.g., 20, 40, 60, 75) using the commands below. Corresponding\n      outputs are shown in Figures 6[href=https://www.wicell.org#fig6]A and 6B, respectively.\n      The tables are printed as text output, and users can copy and paste the\n      tables as Excel tables when using appropriate column delimiters.\n    \n> conversion_table(pred_score, by =\"risk\",\n          values =\n          c(0.01,0.05,0.1,0.2,0.5))\n> conversion_table(pred_score, by = \"score\", values =\n          c(20,40,60,75))\nNote: For survival data, please use\n      “conversion_table_survival()” instead, which reports predicted survival\n      probabilities and selected time points (“time_point”) using specified\n      score thresholds (“score_cut”) (see Figure 2[href=https://www.wicell.org#fig2]F).\n    \nNote: For ordinal data, please use\n      “conversion_table_ordinal()” instead, which reports predicted\n      probabilities of being in each ordinal category using specified score\n      thresholds (“score_breaks”) (see Figure 3[href=https://www.wicell.org#fig3]F).\n    \n        The predicted risk corresponding to risk scores can be visualized using\n        an interactive figure (see Figure 7[href=https://www.wicell.org#fig7] for screenshot).\n        Users can use the built-in toolbar to zoom in for closer inspection or\n        download it as a PNG file.\n        \nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2687-Fig7.jpg\n              Figure 7. Predicted risk corresponding to risk scores for a binary\n              outcome\n            \n> plot_predicted_risk(pred_score = pred_score, max_score =\n          100,\n            final_variables =\n          final_variables,\n            scoring_table =\n          scoring_table)\nNote: For survival data, the Kaplan-Meier\n      curve can be plotted using the “plot_survival_km()” function with selected\n      score thresholds (“score_cut”). See Figure 2[href=https://www.wicell.org#fig2]F.\n    \nNote: For ordinal data, the same function\n      (“plot_predicted_risk()”) can be used to visualize predicted risk for each\n      category in an ordinal outcome. See Figure 3[href=https://www.wicell.org#fig3]F."
  ],
  "subjectAreas": [
    "Computer Sciences",
    "Health Sciences"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research"
  ]
}