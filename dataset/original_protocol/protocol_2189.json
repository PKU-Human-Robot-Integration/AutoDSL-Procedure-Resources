{
  "id": 2305,
  "origin_website": "Cell",
  "title": "A beginner's guide to assembling a draft genome and analyzing structural variants with long-read sequencing technologies",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nVisualizing read-length distribution\nTiming: 1 h\nThe continuous long-read (CLR) mode of Pacific Biosciences (PacBio) or Oxford Nanopore Technologies (ONT) will generate reads of varying sizes, thus necessitating the use of statistics to determine whether or not the sequencing was successful. It is important to visualize read-length distributions and ensure that your reads were properly generated. N50, a read- or contig-length distribution statistic, can be used for assessing the read-length quality. N50 is the shortest read or contig length obtained when the cumulative length of the longest read or contig length equals 50% of the total read or assembly length. We present scripts to visualize the read-length distributions of the three long-read datasets.\nNote: The following scripts contain seven symbols, such as `, ', \", ‘, ’, “, and ”. These seven symbols appear similar to each other; however, they serve distinct functions in a script. To accurately use the scripts, please do not copy and paste them in MS Word; otherwise, Word may automatically transform one symbol into another, and the script may not function at all.\nRun the following scripts in your terminal to create a read-length table:\n#!/usr/bin/env bash\n# Create a new file and generate a header line\necho \"platform,length\" > length.csv\n# Add each read length into the length.csv file.\nbioawk -c fastx '{print \"PacBio_CLR,\" length($seq)}' SRR11906525_WGS_of_drosophila_melanogaster_female_adult_subreads.fastq.gz >> length.csv\nbioawk -c fastx '{print \"PacBio_HiFi,\" length($seq)}' SRR12473480_Drosophila_PacBio_HiFi_UltraLow_subreads.fastq.gz >> length.csv\nbioawk -c fastx '{print \"ONT,\" length($seq)}' SRR13070625_1.fastq.gz >> length.csv\nVisualize the read-length distribution data using R ggplot2. Save this script as a new file and run it, or type the following script directly into R or Rstudio. The output will be similar to that presented in Figure 2[href=https://www.wicell.org#fig2]A:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1799-Fig2.jpg\nFigure 2. De novo genome assembly read-length distribution and quality assessment",
    "(A) Read-length distributions of the three publicly available datasets used in this study. Each vertical dotted line represents the mean value of each dataset.\n(B) Cumulative coverage plot for the contig/scaffold length.\n(C) BUSCO analysis used to determine the number of single-copy orthologs known in a lineage.\n#!/usr/bin/env Rscript\n# Please specify your working directory using setwd\nsetwd(\"/path/to/Input_CSV_file\")\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(cowplot)\n# Import the read-length distribution table\nread_length_df <- read.csv(\"length.csv\")\n# Organize the imported read-length table\n# You can replace the level arguments for your platform, species, or strains\nread_length_df$platform <- as.factor(read_length_df$platform)\nread_length_df$platform <- factor(read_length_df$platform,level = c(\"PacBio_CLR\",\"PacBio_HiFi\",\"ONT\"))\n# Calculate the average read-lengths for each platform\nsummary_df <- ddply(read_length_df, \"platform\", summarise, grp.mean=mean(length))\n# Draw a read-length distribution plot for all reads\ntotal.length.plot <- ggplot(read_length_df, aes(x=length, fill=platform, color=platform)) +\n  geom_histogram(binwidth=100, alpha=0.5, position=\"dodge\") +\n  geom_vline(data=summary_df, aes(xintercept=grp.mean, color=platform), linetype=\"dashed\", size =0.2) +\n  scale_x_continuous(labels = comma) +\n  scale_y_continuous(labels = comma) +\n  labs(x = \"Read length (bp)\", y = \"Count\") +\n  theme_bw()\n# Draw a read-length distribution plot for reads ≤ 20 kb in length\n20 kb.length.plot <- ggplot(read_length_df, aes(x=length, fill=platform, color=platform)) +\n  geom_histogram(binwidth=50, alpha=0.5, position=\"dodge\") +\n  geom_vline(data=summary_df, aes(xintercept=grp.mean, color=platform), linetype=\"dashed\", size=0.2) +\n  scale_x_continuous(labels = comma, limit = c(0,20000)) +\n  scale_y_continuous(labels = comma) +\n  labs(x = \"Read length (bp)\", y = \"Count\") +\n  theme_bw()\n# Merge both the read-length distribution plots\nplot <- plot_grid(total.length.plot, 20 kb.length.plot, ncol = 1)\n# Save the figure using the file name, “read.length.pdf”\npdf(\"read.length.pdf\",width=6,height=8,paper='special')\nprint(plot)\ndev.off()\nCalculate N50 statistics using assembly-stats. You can save or type this script in your terminal to run it:\n#!/usr/bin/env bash\n# Unzipped FASTA/Q files are required for assembly-stats\n# You can unzip your fastq.gz files using the command “gzip -d file_name.fastq.gz”\n# For general usage, specify the read or contig file names after “assembly-stats”",
    "# Calculate summary stats and save the output as an “N50_stat” file\nassembly-stats SRR11906525_WGS_of_drosophila_melanogaster_female_adult_subreads.fastq >> N50_stat\nassembly-stats SRR12473480_Drosophila_PacBio_HiFi_UltraLow_subreads.fastq >> N50_stat\nassembly-stats SRR13070625_1.fastq >> N50_stat\n# You can see the output of assembly-stats by typing “cat N50_stat” in your terminal\n> cat N50_stat\n# The following is the output of “cat N50_stat” command (result of assembly-stats)\nstats for SRR11906525_WGS_of_drosophila_melanogaster_female_adult_subreads.fastq\nsum = 12016661679, n = 1437524, ave = 8359.28, largest = 99345\nN50 = 13094, n = 321336\nN60 = 11342, n = 419876\nN70 = 9489, n = 535376\nN80 = 7388, n = 678061\nN90 = 4902, n = 874814\nN100 = 50, n = 1437524\nN_count = 0\nGaps = 0\nstats for SRR12473480_Drosophila_PacBio_HiFi_UltraLow_subreads.fastq\nsum = 25600110705, n = 2301518, ave = 11123.14, largest = 26462\nN50 = 11151, n = 976954\nN60 = 10586, n = 1212627\nN70 = 10055, n = 1460775\nN80 = 9530, n = 1722273\nN90 = 8996, n = 1998694\nN100 = 369, n = 2301518\nN_count = 0\nGaps = 0\nstats for SRR13070625_1.fastq\nsum = 7133020037, n = 640215, ave = 11141.60, largest = 417450\nN50 = 21491, n = 83878\nN60 = 16642, n = 121685\nN70 = 12824, n = 170598\nN80 = 9526, n = 235039\nN90 = 6112, n = 327186\nN100 = 1, n = 640215\nN_count = 0\nGaps = 0\nNote: For the PacBio CLR mode and ONT, high-quality DNA would have >10-kb N50 read lengths, and a high-quality genome assembly would have >1-Mb N50 contig lengths (Kim et al., 2019a, 2020, 2021[href=https://www.wicell.org#bib30]; ; ).\nApproximate genome-size estimation\nTiming: 5 h",
    "This part of the protocol is required when generating data for a novel species. After estimating the genome size, you can determine the required sequencing throughput for your species. A high-quality genome assembly necessitates more than 20× sequencing coverage. We propose three methodologies that can be employed depending on the situation. You can skip this step if you are analyzing public datasets.\nThe estimated genome size of your species can be found in public databases:\nAnimal: Animal Genome Size Database (http://www.genomesize.com/index.php[href=http://www.genomesize.com/index.php])\nPlant: Plant DNA C-values Database (https://cvalues.science.kew.org/[href=https://cvalues.science.kew.org/]\nIf you have short-read DNA sequencing data, the k-mer-based genome size estimation can be applied:\n#!/usr/bin/env bash\n# KAT is a toolkit for addressing assembly completeness through k-mer counts (Mapleson et al., 2017[href=https://www.wicell.org#bib18])\n# More information about KAT in: https://github.com/TGAC/KAT[href=https://github.com/TGAC/KAT]\n# You can use the short-read DNA sequencing data provided in the Key Resource Table (Accession number: SRX8624462) to run the following script\n# You need to provide the file path to the sequencing data or run this script in the same folder where the sequencing data is saved\nkat hist -o prefix -t 10 SRR12099722∗ 1> kat.output.txt\necho dme_size >> genome_size.txt\ngrep -i \"Estimated\" kat.output.txt >> genome_size.txt\n# hist: a kat module for drawing histograms and estimating genome size\n# -o: output prefix; you can specify “prefix” for your species or strain names\n# -t: the number of threads that will be used to run the kat program\n# You can replace SRR12099722∗ with your short-read DNA sequencing data# You can replace dme_size with the name of your species\n# You can check the kat output by typing “cat genome_size.txt” in your terminal\n> cat genome_size.txt\n# Genome size can be estimated using the short-read DNA sequencing data\ndme_size\nEstimated genome size: 166.18 Mbp\nEstimated heterozygous rate: 0.41%",
    "Calculate the transcript-based coverage using short-read DNA/RNA sequencing data.\nCritical: For an accurate estimation, high-quality transcriptome assembly is required.\nConduct de novo transcriptome assembly using Trinity (Grabherr et al., 2011[href=https://www.wicell.org#bib6]):\n#!/usr/bin/env bash\n# Trinitiy is a package for conducting de novo transcriptome assembly from RNA-seq data\n# For more information: https://github.com/trinityrnaseq/trinityrnaseq/wiki[href=https://github.com/trinityrnaseq/trinityrnaseq/wiki]\n# You can use the short-read RNA sequencing data provided in the Key Resource Table (Accession number: GSM5452671, GSM5452672) to run the following script\n# You need to provide the file path to the sequencing data or run this script in the same folder where the sequencing data are saved\nTrinity --seqType fq --max_memory 120G --left /home/assembly/analysis/00_STARprotocol/SRR15130841_GSM5452671_Control_CM1_Drosophila_melanogaster_RNA-Seq_1.fastq.gz,/home/assembly/analysis/00_STARprotocol/SRR15130842_GSM5452672_Control_CM2_Drosophila_melanogaster_RNA-Seq_1.fastq.gz --right /home/assembly/analysis/00_STARprotocol/SRR15130841_GSM5452671_Control_CM1_Drosophila_melanogaster_RNA-Seq_2.fastq.gz,/home/assembly/analysis/00_STARprotocol/SRR15130842_GSM5452672_Control_CM2_Drosophila_melanogaster_RNA-Seq_2.fastq.gz --CPU 8 --output Dmel.trinity\n# --seqType: sequence type; as short-read sequencing data are typically present in the FASTQ format, you can specify this as “fq”\n# # --max_memory: maximum memory required to run the Trinity. “120G” indicates 120 GB\n# --left and --right: input files required for trinity analysis. Currently, short-read sequencing is mainly performed in a “paired-end” mode. Each DNA molecule is sequenced at both the ends, producing two paired files. You should specify one as “--left” and the other as “—right”\n# --CPU: the number of threads required for Trinity analysis\n# --output: output prefix\n# Trinity output should be in the “Dmel.trinity” (or “your_species_trinity”) folder\n# Assembled transcript FASTA file will be “Dmel.trinity.Trinity.fasta” (or “your_species_trinity.Trinity.fasta”)\n# You can assess the assembled quality of transcriptomes using assembly-stat\n> assembly-stats Dmel.trinity.Trinity.fasta\n# The following is the output of the “assembly-stats Dmel.trinity.Trinity.fasta” command (result of assembly-stats)\nstats for Dmel.trinity.Trinity.fasta\nsum = 72662995, n = 67038, ave = 1083.91, largest = 27780\nN50 = 2454, n = 8357\nN60 = 1816, n = 11781\nN70 = 1180, n = 16695\nN80 = 653, n = 25022",
    "N90 = 364, n = 40185\nN100 = 201, n = 67038\nN_count = 0\nGaps = 0\nMap the short DNA reads to the transcriptome using HISAT2:\n#!/usr/bin/env bash\n# HISAT2 was used to map DNA sequencing reads to the assembled transcripts, and SAMtools was used to process the alignment data\n# For more information about HISAT2: http://daehwankimlab.github.io/hisat2/[href=http://daehwankimlab.github.io/hisat2/]\n# HISAT2 ref: https://www.nature.com/articles/s41587-019-0201-4[href=https://www.nature.com/articles/s41587-019-0201-4]\n# For more information about SAMtools: http://www.htslib.org/[href=http://www.htslib.org/]\n# SAMtools ref: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2723002/[href=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2723002/]\n# Index your assembled transcript FASTA file using the prefix “Dmel”\nhisat2-build Dmel.trinity.fa Dmel\n# Map your short-read DNA sequences to the assembled transcript using the index\n# You can use the short-read DNA sequencing data provided in the Key Resource Table (Accession number: SRX8624462) to run the following script\n# You need to provide the file path to the sequencing data or run this script in the same folder where the sequencing data are saved\nhisat2 -x Dmel -p 10 -1 SRR12099722∗_1∗ -2 SRR12099722∗_2∗ --very-sensitive | samtools sort -@ 10 -o Dmel.very_sensitive.bam\n# For HISAT2, the parameters are as follows:\n# -x: index prefix\n# -p: the number of threads required by HISAT2\n# -1 and -2: paired-end files; you can change the name of your sequencing data\n# --very-sensitive: sensitivity option\n# For SAMtools, the parameters are as follows:\n# sort: SAMtools module to sort the mapped read information\n# -@: the number of threads required by SAMtools\n# -o: output file name\n# Index your read mapping file\nsamtools index Dmel.very_sensitive.bam\nEstimate the genome size:\n#!/usr/bin/env bash\n# Calculate average coverage of each transcript\nsamtools coverage Dmel.very_sensitive.bam | awk '{print $7}' | tail -n +2 | grep -vw \"0\" | awk '{sum+=$1}END{print sum/NR}' > average.coverage.txt\n# coverage: SAMtools module to calculate the coverage of each transcript (or contig, scaffold, etc.)",
    "# awk '{print $7}': select the coverage column in the output of SAMtools coverage\n# tail -n +2: remove the header line\n# grep -vw \"0\": remove “0” coverage rows\n# awk '{sum+=$1}END{print sum/NR}': calculate the average coverage with non-zero values\n# Calculate the total read length of the DNA sequencing file\nbioawk -c fastx '{sum+=length($seq)}END{print sum}' SRR12099722_WGS_Drosophila_melanogaster_adult_ISCs_1.fastq.gz > total.read.length.txt\n# Estimate the genome size\npaste average.coverage.txt total.read.length.txt | awk '{print \"Estimated genome size = \" $2∗2/$1/1000000 \" Mb\"}'\n# After running the preceding script, the following result will be displayed in your terminal\nEstimated genome size = 185.04 Mb\nLong-read sequencing-based genome assembly\nTiming: 1 day for step 7\nTiming: 1.5 day for step 8\nTiming: 30 min for step 9\nLong-read sequencing data are now typically produced using PacBio or the ONT sequencing technology. Here, we summarize the assembly method when using PacBio’s two data types, i.e., CLR and high-fidelity (HiFi) modes, as well as ONT’s Long data type. You can select one of the 7–9 scripts according to your data type:\nPacBio CLR data type:\n# Typically, canu assembler (Koren et al., 2017[href=https://www.wicell.org#bib12]) will use as much as CPU and memory resources in your computer\n# You can use the PacBio CLR data provided in the Key Resource Table (Accession number: SRX8453114) to run the following command\n> canu -p Dmel -d Dmel genomeSize=170 m -pacbio SRR11906525_WGS_of_drosophila_melanogaster_female_adult_subreads.fastq.gz\n# -p: output prefix\n# -d: directory where Canu will run\n# genomeSize=: estimated genome size of your species\n# -pacbio: name of your platform\n# For more information about Canu: https://github.com/marbl/canu[href=https://github.com/marbl/canu]\n# You can check the assembly statistics of the canu assembler using assembly-stats\n> assembly-stats Dmel.contigs.fasta\n# After running the preceding command, the assembly statistics of the Canu assembler will be displayed on your terminal\nstats for Dmel.contigs.fasta",
    "sum = 141740149, n = 452, ave = 313584.40, largest = 23607911\nN50 = 9177974, n = 5\nN60 = 5147831, n = 7\nN70 = 4576628, n = 9\nN80 = 2051575, n = 15\nN90 = 187381, n = 39\nN100 = 1381, n = 452\nN_count = 0\nGaps = 0\nPacBio HiFi data type:\nConstruct a genome using the hifiasm assembler (Cheng et al., 2021[href=https://www.wicell.org#bib3]), which is dedicated to the HiFi data type:\n# You can use the PacBio HiFi data provided in Key Resource Table (Accession number: SRX8967562) to run the following command\n> hifiasm -o Dmel -t 20 ../SRR12473480_Drosophila_PacBio_HiFi_UltraLow_subreads.fastq.gz\n# -o: output prefix\n# -t: the number of threads\n# For more information about hifiasm: https://github.com/chhylp123/hifiasm[href=https://github.com/chhylp123/hifiasm]\nConvert the GFA file to a typical FASTA file:\n# You can check assembly statistics of the hifiasm assembler using assembly-stats\n> assembly-stats Dmel.bp.hap1.p_ctg.fa\n# After running the preceding command, the assembly statistics of the hifiasm assembler will be displayed on your terminal\nstats for Dmel.bp.hap1.p_ctg.fa\nsum = 168692738, n = 654, ave = 257939.97, largest = 24502687\nN50 = 4127200, n = 10\nN60 = 2167675, n = 15\nN70 = 1125434, n = 26\nN80 = 496545, n = 50\nN90 = 79880, n = 130\nN100 = 9867, n = 654\nN_count = 0\nGaps = 0",
    "Note: The HiFi sequencing data used in this guide were generated using ultra-low input DNA; thus, these data significantly differ from typical HiFi data with sufficient input DNA. PacBio HiFi data are typically generated through a strict size selection, with an average quality > Q30. The HiFi read-length distribution will be 15–20 kb, and HiFi data for diploid genome assembly are typically superior to CLR data in terms of phasing, contiguity, and computation time. Because of the high accuracy of the process, diploid variants can be resolved and phased more easily, and the correction step required for CLR data can be omitted for HiFi data.\nONT Long data type:\n# Shasta (Shafin et al., 2020[href=https://www.wicell.org#bib20]) is a long-read sequencing assembler which works efficiently on ONT data. Raw-read FASTQ files should be unzipped for Shasta assembler\n# You can use the ONT Long data provided in Key Resource Table (Accession number: SRX9518233) to run the following command\n# Unzip your ONT raw-read FASTQ file\n> gzip -d SRR13070625_1.fastq.gz\n# Run shasta to assemble the reads into contigs\n> shasta --config Nanopore-Oct2021 --threads 8 --input SRR13070625_1.fastq\n# --config: configuration options\n# --threads: the number of threads required by Shasta\n# --input: input file name; the file should be unzipped\n# For more information about Shasta: https://github.com/chanzuckerberg/shasta[href=https://github.com/chanzuckerberg/shasta]\n# You can check assembly statistics of the shasta assembler using assembly-stats\n> assembly-stats Assembly.fasta\n# After running the preceding command, assembly statistics of the shasta assembler will be displayed on your terminalstats for Assembly.fasta\nsum = 133002022, n = 208, ave = 639432.80, largest = 27938801\nN50 = 18567724, n = 3\nN60 = 15335596, n = 4\nN70 = 6235146, n = 6\nN80 = 5092624, n = 8\nN90 = 917306, n = 13\nN100 = 21, n = 208\nN_count = 0",
    "Gaps = 0\nQuality assessment\nTiming: 10 min for step 10\nTiming: 20 min/sample for step 11\nTiming: 10 min/sample for step 12\nThe quality of a de novo assembled genome can be determined according to the contiguity of its contigs, which can be determined by the length of contigs and identification of universal single-copy ortholog genes. Furthermore, if a high-quality reference genome exists for the species you have assembled, the quality can be evaluated using a comparison to your own genome.\nProduce a coverage plot. This cumulative coverage plot depicts contig-length distributions. Contig lengths are sorted in descending order, and the proportion of each contig length to its total genome assembly length is calculated. Their cumulative sum is shown on the x-axis, and the length of the corresponding contig is presented on the y-axis. Based on the definition of N50, each horizontal line that crosses the vertical line in each assembly can be interpreted as N50, which allows different assemblies to be visually compared.\nConduct preprocessing:\n#!/usr/bin/env bash\n# This script will create the coverage table required to obtain the cumulative graph\nSTRAIN1=Hifi_Dmel # Specify your species or strain name\nREF1=/path/to/Hifi_Dmel.bp.hap1.p_ctg.fa # should be changed for your genome file path\nTYPE1=contig # Specify your genome assembly type, such as contig, scaffold, chromosome, etc.\nLEN1=`bioawk -c fastx '{sum+=length($seq)}END{print sum}' $REF1` # Size of assembled genome\n# Create the output file having a header line\necho \"line,length,type,coverage\" > length.csv\n# Calculate cumulative sum and write result to the output file (HiFi data)\ncat $REF1 | bioawk -c fastx -v line=\"$STRAIN1\" '{print line\",\"length($seq)\",\"length($seq)}' | sort -k3rV -t \",\" | awk -F \",\" -v len=\"$LEN1\" -v type=\"$TYPE1\" 'OFS=\",\"{ print $1,$2,type,(sum+0)/len; sum+=$3 }' >> length.csv\n# Calculate cumulative sum and write result to the output file (CLR data)\nSTRAIN2=CLR_Dmel",
    "REF2=/path/to/CLR_Dmel.contigs.fasta # should be changed your genome name\nTYPE2=contig\nLEN2=`bioawk -c fastx '{sum+=length($seq)}END{print sum}' $REF2`\ncat $REF2 | bioawk -c fastx -v line=\"$STRAIN2\" '{print line\",\"length($seq)\",\"length($seq)}' | sort -k3rV -t \",\" | awk -F \",\" -v len=\"$LEN2\" -v type=\"$TYPE2\" 'OFS=\",\"{ print $1,$2,type,(sum+0)/len; sum+=$3 }' >> length.csv\n# Calculate cumulative sum and write result to the output file (ONT data)\nSTRAIN3=ONT_Dmel\nREF3=/path/to/ONT_Assembly.fasta # should be changed your genome name\nTYPE3=contig\nLEN3=`bioawk -c fastx '{sum+=length($seq)}END{print sum}' $REF3`\ncat $REF3 | bioawk -c fastx -v line=\"$STRAIN3\" '{print line\",\"length($seq)\",\"length($seq)}' | sort -k3rV -t \",\" | awk -F \",\" -v len=\"$LEN3\" -v type=\"$TYPE3\" 'OFS=\",\"{ print $1,$2,type,(sum+0)/len; sum+=$3 }' >> length.csv\nMake a cumulative graph. Save this script as a new file and run it, or type the following script directly into R or RStudio. The output will be similar to that presented in Figure 2[href=https://www.wicell.org#fig2]B:\n#!/usr/bin/env Rscript\nsetwd(\"/path/to/Input_CSV_file\")\nlibrary(ggplot2)\n# Import the cumulative sum table\ncontig_cumulative_sum_df <- read.csv(\"length.csv\", header = TRUE)\n# Organize the table\ncontig_cumulative_sum_df$type <- factor(contig_cumulative_sum_df$type, levels=c(\"scaffold\", \"contig\")) # or any other assembly types\n# Create a plot for cumulative sum\nplot <- ggplot(data=contig_cumulative_sum_df, aes(x=coverage, y=length/1000000, color=line)) +\n  geom_vline(xintercept = 0.5, linetype=\"dotted\", size=0.5) +\n  xlim(0, 1) +\n  geom_step(aes(linetype=type)) +\n  labs(x = \"Cumulative coverage\", y = \"Length (Mb)\")\n# Save the plot as a “coverage.pdf” file\npdf(\"coverage.pdf\",width=4,height=3,paper='special')\nprint(plot)\ndev.off()\nPerform Benchmarking Universal Single-Copy Orthologs (BUSCO) analysis (Manni et al., 2021[href=https://www.wicell.org#bib17]). BUSCO analysis determines whether well-known single-copy orthologs in specific lineages are correctly assembled or fragmented in contigs of a genome assembly. In a more contiguous genome assembly, complete BUSCO values would be higher.\nSelect the specific lineage of your species among the following datasets:\n> busco --list-datasets\n# For more information about BUSCO: https://busco.ezlab.org/[href=https://busco.ezlab.org/]\nRun the BUSCO analysis:\n#!/usr/bin/env bash\nfor assembly in `ls ../∗fasta∗`;do\nname=$(basename -s .fasta $assembly)",
    "busco -i $assembly -c 10 -o $name -m genome -l diptera_odb10\ndone\n# -c 10: number of threads to run BUSCO\n# -m genome: mode of BUSCO\n# -l diptera_odb10: lineage-specific dataset name selected in the list generated by the “busco --list-datasets” command\n# For general usage, use this script:\n# > busco -i assembly.fasta -o species_name -m genome -l your_lineage\nParse the BUSCO output results:\n#!/usr/bin/env bash\n# BUSCO will measure the quality of single copy orthologs in four different categories: “complete and single-copy,” “complete and duplicated,” “fragmented,” and “missing.” This script will parse the number of data points in each of the categories to create a boxplot\n# Create the BUSCO output file having a header line\necho \"Strain,Complete_single_copy,Complete_duplicated,Fragmented,Missing\" > busco.csv\n# Extract the count for each BUSCO category (CLR data)\nPREFIX1=CLR_Dmel.contigs\n# (S) represents “complete and single-copy”\ncat $PREFIX1/short∗.txt | grep \"(S)\" | awk -v strain=\"$PREFIX1\" '{print strain\",\"$1}' > complete_single.txt\n# (D) represents complete and duplicated\ncat $PREFIX1/short∗.txt | grep \"(D)\" | awk '{print $1}' > complete_duplicated.txt\n# (F) represents “fragmented”\ncat $PREFIX1/short∗.txt | grep \"(F)\" | awk '{print $1}' > fragmented.txt\n# (M) represents “missing”\ncat $PREFIX1/short∗.txt | grep \"(M)\" | awk '{print $1}' > missing.txt\npaste -d \",\" complete_single.txt complete_duplicated.txt fragmented.txt missing.txt >> busco.csv\n# Extract the count for each BUSCO category (HiFi data)\nPREFIX2=Hifi_Dmel.bp.hap1.p_ctg\ncat $PREFIX2/short∗.txt | grep \"(S)\" | awk -v strain=\"$PREFIX2\" '{print strain\",\"$1}' > complete_single.txt\ncat $PREFIX2/short∗.txt | grep \"(D)\" | awk '{print $1}' > complete_duplicated.txt\ncat $PREFIX2/short∗.txt | grep \"(F)\" | awk '{print $1}' > fragmented.txt\ncat $PREFIX2/short∗.txt | grep \"(M)\" | awk '{print $1}' > missing.txt\npaste -d \",\" complete_single.txt complete_duplicated.txt fragmented.txt missing.txt >> busco.csv\n# Extract the count for each BUSCO category (ONT data)\nPREFIX3=ONT_Assembly\ncat $PREFIX3/short∗.txt | grep \"(S)\" | awk -v strain=\"$PREFIX3\" '{print strain\",\"$1}' > complete_single.txt",
    "cat $PREFIX3/short∗.txt | grep \"(D)\" | awk '{print $1}' > complete_duplicated.txt\ncat $PREFIX3/short∗.txt | grep \"(F)\" | awk '{print $1}' > fragmented.txt\ncat $PREFIX3/short∗.txt | grep \"(M)\" | awk '{print $1}' > missing.txt\npaste -d \",\" complete_single.txt complete_duplicated.txt fragmented.txt missing.txt >> busco.csv\n# Delete temporary files\nrm complete_single.txt complete_duplicated.txt fragmented.txt missing.txt\n# You can check the table summarizing BUSCO output in your terminal\n> cat busco.csv\n# After running the preceding command, BUSCO result will be displayed in your terminal\nStrain,Complete_single_copy,Complete_duplicated,Fragmented,Missing # Header\nCLR_Dmel.contigs,3228,13,18,26\nHifi_Dmel.bp.hap1.p_ctg,3140,81,16,48\nONT_Assembly,2989,7,153,136\nVisualize the results using the following R script:\n#!/usr/bin/env Rscript\nsetwd(\"/path/to/Input_CSV_file\")\nlibrary(ggplot2)\nlibrary(reshape2)\nlibrary(tidyverse)\n# Import the BUSCO table\nbusco_df <- read.csv(\"busco.csv\", header = TRUE)\n# Organize and rearrange the imported table\nbusco_df$Strain <- as.factor(busco_df$Strain)\nbusco_df.melted <- melt(busco_df, id.vars = \"Strain\")\nbusco_df.melted$variable <-relevel(busco_df.melted$variable, \"Missing\")\n# Create a stacked bar plot for the BUSCO outputs\nbusco_plot <- ggplot(busco_df.melted, aes(x=Strain, fill=fct_rev(variable), y=value)) +\n  geom_bar(position= \"stack\", width = 0.7, stat=\"identity\") +\n  labs(x = \"Strain\", y = \"BUSCO\", fill = \"Type\") +\n  scale_y_continuous(labels=comma) +\n  theme_bw() +\n  theme(axis.text.x = element_text(angle=45, hjust=1, size = 12), axis.text.y = element_text(size = 12), axis.title=element_text(size=12))\n# Save the plot as “busco.pdf”\npdf(\"busco.pdf\",width=8,height=5,paper='special')\nprint(busco_plot)\ndev.off()\nCompare your genome with the reference genome. This step is highly recommended if a chromosome-level reference genome is available. If a chromosome-level genome assembly is already available, you can connect your contigs into larger chunks using homology between your contigs and the chromosomes. Such larger chunks with unidentified gaps are referred to as “scaffolds.”\nMake the scaffolds using RagTag (Alonge et al., 2021[href=https://www.wicell.org#bib1]):\n#!/usr/bin/env bash\nfor assembly in `ls ../∗fasta∗`;do\nref=/path/to/reference/dmel-all-chromosome-r6.44.fasta\nname=$(basename -s .fasta $assembly)\nragtag.py scaffold -t 10 -u -o $name $ref $assembly\ndone\n# -t: the number of threads required by RagTag\n# -u: add a suffix to all unscaffolded contigs\n# -o: output folder name",
    "# Final output scaffolds should be saved in “$name/ragtag.scaffold.fasta”\n# For general usage, you can use this script:\n# > ragtag.py scaffold -u -o output_folder_name reference.fasta your_assembly.fasta\n# For more information about RagTag: https://github.com/malonge/RagTag[href=https://github.com/malonge/RagTag]\nPrepare genomic FASTA files, which have common chromosomes, to compare synteny between a chromosome-level reference genome (reference genome) and your scaffolds (RagTag output):\n#!/usr/bin/env bash\n#1. Remove RagTag identifier from the header of scaffold\nfor scaffold in `ls ../ragtag.∗`;do\nname=$(basename -s .scaffold.fasta $scaffold)\nsed 's/_RagTag//' $scaffold > ${name}_rename.scaffold.fasta\ndone\n#2. Only chromosomes with the same name should be left in both genomic FASTA files\n# chromosome.name.list.txt: The names of the chromosomes to be compared are contained in this file\nfor i in `cat chromosome.name.list.txt`; do\n  cat chromosome-level_genome_assembly.fa | bioawk -c fastx -v chr=\"$i\" '$name==chr{print \">chr\"$name; print $seq}' >> reference_chromosome.fa\n  cat ragtag.scaffold.fasta | bioawk -c fastx -v chr=\"$i\" '$name==chr{print \">chr\"$name; print $seq}' >> your_scaffold.fa\ndone\n# To run the preceding script, the chromosome.name.list.txt file should be provided\n# Example of chromosome name list file contain main chromosomes of Drosophila melanogaster\n> cat chromosome.name.list.txt”\n# Standard output of “cat chromosome.name.list.txt”\n# By copying and pasting the result below, you can create chromosome.name.list.txt file\n2L\n2R\n3L\n3R\n4\nX\nY\nPerform whole-genome alignment using minimap2 (Li, 2021[href=https://www.wicell.org#bib15]):\n> minimap2 -a -x asm5 --eqx reference_chromosome.fa your_scaffold.fa > syri.sam\n# -a: output will be saved as the SAM format\n# -x asm5: preset for aligning two assemblies with ∼0.1% sequence divergence\n# --eqx: contain =/X CIGAR strings\n# For more information about minimap2: https://github.com/lh3/minimap2[href=https://github.com/lh3/minimap2]\nMake a conda environment for synteny analysis using SyRi (Goel et al., 2019[href=https://www.wicell.org#bib5]):\n# At the time of writing, SyRi only functions in Python 3.5, so you should specify the Python version that conda will employ\n> conda create -n syri python=3.5",
    "# Activate the environment for SyRi\n> conda activate syri\n# Install dependencies for SyRi\n> conda install cython numpy scipy pandas=0.23.4 biopython psutil matplotlib=3.0.0\n> conda install -c conda-forge python-igraph\n> conda install -c bioconda pysam\n# Then download SyRi version 1.4 and unzip the downloaded file\n> wget https://github.com/schneebergerlab/syri/archive/refs/tags/v1.4.tar.gz[href=https://github.com/schneebergerlab/syri/archive/refs/tags/v1.4.tar.gz]\n> tar -xzf v1.4.tar.gz\n> cd syri-1.4\n# Install SyRi\n> python setup.py install\n# Let the SyRi command executable\n> chmod +x syri/bin/syri\n# For more information about SyRi: https://schneebergerlab.github.io/syri/[href=https://schneebergerlab.github.io/syri/]\nNote: Currently, SyRi only works with Python 3.5 version.\nRun SyRi to visualize the synteny information:\n#!/usr/bin/env bash\n#1. Run SyRi\npython /path/to/syri-1.4/syri/bin/syri -c syri.sam -r chromosome-level_genome_assembly.fa -q your_scaffold.fa -k -F S\n# -k: keep intermediate files; you can turn off this option\n# -F S: input file is in the SAM (S) format\n#2. Visualizing genomic alignments predicted by SyRi\npython /path/to/syri-1.4/syri/bin/plotsr syri.out chromosome-level_genome_assembly.fa your_scaffold.fa -H 8\n# -H: Specify the height of the plot\nDiscovery of structural variation\nTiming: 10 min/sample\nStructural variations (SVs) are genetic variants that differ in size by ≥50 bp from the reference genome. Long-read sequencing technologies out-perform short-read sequencing ones in terms of SV accuracy and specificity owing to their larger read size.\nTwo methods are available for calling SVs: read-based SV calling and assembly-based SV calling. For read-based SV calling, you should map your long reads to a reference genome before calling SVs using the mapping information. For assembly-based SV calling, you should align your genome assembly to a reference genome before calling SVs. Assembly-based SV calling is typically more accurate than read-based SV calling because most of the read errors are corrected during genome assembly; however, it requires significantly greater sequencing read depth because de novo genome assembly requires ∼20× coverage.",
    "SVIM (Heller and Vingron, 2019[href=https://www.wicell.org#bib7]) and SVIM-asm (Heller and Vingron, 2020[href=https://www.wicell.org#bib8]) are sister SV callers developed for read- and assembly-based SV calling, respectively. Both SV callers are simple to install and easy to run. If you have low-depth read data, use SVIM; if you have high-depth read data and the corresponding genome assembly, use SVIM-asm. Smaller variants can be determined by both SVIM and SVIM-asm using the “--min sv size” option; for example, “--min sv size 5” to call ≥5-bp variants.\nModify the SVIM and SVIM-asm figure output options:\n# First, you should find your path to SVIM_plot.py for SVIM\n> whereis svim | sed 's∖/bin/svim∖/lib/python3.∗/site-packages/svim/SVIM_plot.py∖; s∖svim: ∖∖'\n# Example result of the above code: /home/assembly/miniconda3/envs/assembly/lib/python3.∗/site-packages/svim/SVIM_plot.py\n# for SVIM-asm\n> whereis svim-asm | sed 's∖/bin/svim-asm∖/lib/python3.∗/site-packages/svim_asm/SVIM_plot.py∖; s∖svim-asm: ∖∖'\n# Example result of the above code: /home/assembly/miniconda3/envs/assembly/lib/python3.∗/site-packages/svim_asm/SVIM_plot.py\n# check the printed path and replace \"png\" to \"pdf\"\n> sed -i 's/png/pdf/' /path/to/envs/env_name/lib/python3.∗/site-packages/svim/SVIM_plot.py # for SVIM\n> sed -i 's/png/pdf/' /path/to/envs/env_name/lib/python3.∗/site-packages/svim_asm/SVIM_plot.py # for SVIM-asm\n# Then run your SVIM or SVIM-asm\nConduct read-based SV calling using SVIM:\n> svim reads --cores 10 --aligner minimap2 output_folder_name your_read.fq.gz your_genome_assembly.fa\n# reads: SVIM module for detecting SVs using raw reads rather than SAM/BAM alignment files\n# --cores 10: number of threads\n# --aligner: You can use other long-read aligners by changing “minimap2” to your desired aligner\n# your_read.fq.gz: should be long-read sequencing data\n# For more information about SVIM: https://github.com/eldariont/svim[href=https://github.com/eldariont/svim]\nConduct assembly-based SV calling.\nAlign two genomes using minimap2:\n# Align your genome assembly to the reference genome and sort the alignment information\n> minimap2 -a -x asm5 --cs -r2k -t 10 genome1.fa genome2.fa | samtools sort -m4G -@ 10 -O BAM -o genome2_to_genome1.bam # genome1=reference, genome2=query\n# For minimap2, the parameters are as follows:\n# -a: output will be printed as the SAM format",
    "# -x asm5: preset for aligning two assemblies with ∼0.1% sequence divergence\n# --cs: the output file will contain cs tags\n# -r: chaining bandwidth\n# -t: number of threads\n# For SAMtools, the parameters are as follows:\n# sort: SAMtools module to sort read mapping information\n# -m: maximum memory for each thread\n# -@: number of threads\n# -O BAM: output as a BAM format\n# Index your assembly-assembly alignment file\n> samtools index genome2_to_genome1.bam\nPerform SV calling using SVIM-asm:\n# Call SVs between the reference genome and yours\n> svim-asm haploid output_folder_name genome2_to_genome1.bam genome1.fa\n# haploid: SVIM-asm module for calling SVs between two haploid genomes\n# For more information about SVIM-asm: https://github.com/eldariont/svim-asm[href=https://github.com/eldariont/svim-asm]\nGene annotation\nTiming: 8 h/sample\nTo annotate genes for your genome, you should (1) mask your genome assembly, (2) map your RNA-seq reads to the masked genome assembly, and (3) predict gene structures based on this RNA-seq evidence. The BRAKER gene annotation pipeline, which will be used by us, prefers repeat-masked genome assemblies to unmasked ones to accurately determine the gene structure (Hoff et al., 2016[href=https://www.wicell.org#bib9]). The repeat-masking process can be performed using RepeatMasker (Smit et al., 2013–2015[href=https://www.wicell.org#bib21]) and RepeatModeler (Smit and Hubley, 2008–2015[href=https://www.wicell.org#bib22]). Additionally, as coding and non-coding genes are transcribed to produce RNA molecules, RNA-seq data provide important evidence for gene structure.\nMake a conda environment for repeat masking:\n# Type the following script directly in your terminal\n# Create the conda environment for RepeatModeler and RepeatMasker\n# The RepeatModeler package contains the RepeatMasker package\n> conda create -c bioconda -n repeatmodeler repeatmodeler\n# Activate the environment for RepeatModeler\n> conda activate repeatmodeler\n# Install the dependencies for RepeatModeler\n> conda update -c conda-forge perl-file-which\n# Download the NINJA package for large-scale neighbor-joining phylogeny inference and clustering\n> mkdir bin\n> cd bin",
    "> wget https://github.com/TravisWheelerLab/NINJA/archive/refs/tags/0.95-cluster_only.tar.gz[href=https://github.com/TravisWheelerLab/NINJA/archive/refs/tags/0.95-cluster_only.tar.gz]\n> tar -zxvf 0.95-cluster_only.tar.gz\n> cd NINJA-0.95-cluster_only/NINJA/\n> make # Create the “Ninja” executable file\n> pwd\n# The pwd Linux command prints the current working directory path\n# The standard output of the \"pwd\" command will be used as a parameter of RepeatModeler\nRepeat masking using known metazoan repeats with RepeatMasker:\n#!/usr/bin/env bash\n# You should use scaffold files as the input in RepeatMasker\nfor sample in `ls ∗fa`;do\nRepeatMasker -species metazoa -s -parallel 10 -xsmall -alignments $sample\ndone\n# -s: sensitive\n# -parallel 10: number of threads\n# -xsmall: softmasking, that is, change the repeat regions into lowercase, rather than N\n# Output of RepeatMasker\nyour_genome_assembly.fa.masked # masked FASTA file\nyour_genome_assembly.fa.tbl # repeat summary\nIdentify previously unknown repeats in your genome assembly using RepeatModeler:\n#!/usr/bin/env bash\n#1. Create a Database for RepeatModeler\nBuildDatabase -name CLR CLR_scaffold.fa\nBuildDatabase -name ONT ONT_scaffold.fa\nBuildDatabase -name Hifi Hifi_scaffold.fa\n# -name: The name of the database to create\n#2. Run RepeatModeler\nRepeatModeler -database CLR -pa 10 -LTRStruct -ninja_dir /home/assembly/bin/NINJA-0.95-cluster_only/NINJA\nRepeatModeler -database ONT -pa 10 -LTRStruct -ninja_dir /home/assembly/bin/NINJA-0.95-cluster_only/NINJA\nRepeatModeler -database Hifi -pa 10 -LTRStruct -ninja_dir /home/assembly/bin/NINJA-0.95-cluster_only/NINJA\n# -database: prefix name of the database that is used in the BuildDatabase function\n# -pa: number of threads\n# -LTRStruct: runs the LTR structural discovery pipeline for discovering LTR retrotransposons\n# -ninja_dir: specify the NINJA folder\n# Output of RepeatModeler\nPREFIX-families.fa\nRepeat masking with RepeatMasker using species-specific repeats that were found by RepeatModeler:\n#!/usr/bin/env bash\nRepeatMasker -lib CLR-families.fa -s -parallel 10 -xsmall -alignments CLR_scaffold.fa.masked\nRepeatMasker -lib ONT-families.fa -s -parallel 10 -xsmall -alignments ONT_scaffold.fa.masked\nRepeatMasker -lib Hifi-families.fa -s -parallel 10 -xsmall -alignments Hifi_scaffold.fa.masked\n# -lib: specify your species-specific repeat FASTA file produced by RepeatModeler\n# -s: sensitive\n# -xsmall: softmasking, that is, change the repeat regions into lowercase, rather than N\n# Output of RepeatMasker",
    "your_genome_assembly.fa.masked.masked # masked FASTA file\nyour_genome_assembly.fa.masked.tbl # repeat summary\nConduct gene annotation.\nMap RNA sequencing reads to the masked genome:\n#!/usr/bin/env bash\n#1. Create the masked genome index\n#Usage: hisat2-build repeat_masked_genome_assembly.fa PREFIX\nhisat2-build CLR_scaffold.fa.masked.masked CLR\nhisat2-build ONT_scaffold.fa.masked.masked ONT\nhisat2-build Hifi_scaffold.fa.masked.masked Hifi\n#2. Mapping RNA sequencing reads to the masked genome\nhisat2 -x CLR -p 10 -1 /home/assembly/analysis/00_STARprotocol/SRR15130841_GSM5452671_Control_CM1_Drosophila_melanogaster_RNA-Seq_1.fastq.gz -2 /home/assembly/analysis/00_STARprotocol/SRR15130841_GSM5452671_Control_CM1_Drosophila_melanogaster_RNA-Seq_2.fastq.gz | samtools sort -@ 10 -O BAM -o CLR.bam\nhisat2 -x ONT -p 10 -1 /home/assembly/analysis/00_STARprotocol/SRR15130841_GSM5452671_Control_CM1_Drosophila_melanogaster_RNA-Seq_1.fastq.gz -2 /home/assembly/analysis/00_STARprotocol/SRR15130841_GSM5452671_Control_CM1_Drosophila_melanogaster_RNA-Seq_2.fastq.gz | samtools sort -@ 10 -O BAM -o ONT.bam\nhisat2 -x Hifi -p 10 -1 /home/assembly/analysis/00_STARprotocol/SRR15130841_GSM5452671_Control_CM1_Drosophila_melanogaster_RNA-Seq_1.fastq.gz -2 /home/assembly/analysis/00_STARprotocol/SRR15130841_GSM5452671_Control_CM1_Drosophila_melanogaster_RNA-Seq_2.fastq.gz | samtools sort -@ 10 -O BAM -o Hifi.bam\n# For HISAT2, the parameters are as follows:\n# -x: index prefix\n# -p: the number of threads HISAT2 will use\n# -1 and -2: paired-end files. You can change the name of your sequencing data\n# For SAMtools, the parameters are as follows:\n# sort: SAMtools module to sort the mapped read information\n# -@: the number of threads SAMtools will use\n# -o: output file name\n# -O BAM: output as a BAM format\nMake a conda environment for gene annotation:\n# Type the following script directly in your terminal\n# Create the conda environment for braker2\n> conda create -n braker -c bioconda braker2\n# Activate the environment for braker2\n> conda activate braker\n# Download GeneMark-EX program(gmes_linux_64.tar) and GeneMark key(gm_key_64) from http://exon.gatech.edu/GeneMark/license_download.cgi[href=http://exon.gatech.edu/GeneMark/license_download.cgi] (the GeneMark-ES/ET/EP) option\n# Due to license and distribution restrictions, GeneMark and ProtHint should be separately installed for BRAKER2 to become fully functional\n#1. GeneMark-EX program\n> tar -xvf gmes_linux_64.tar\n> cd gmes_linux_64\n> perl change_path_in_perl_scripts.pl \"/usr/bin/env perl\"\n# This is required for BRAKER to accurately find the \".gm_key\". See the \"2. GeneMark key\" section\n> pwd\n# The pwd Linux command prints the current working directory path",
    "# Standard output of “pwd” command will be used parameter of braker\n#2. GeneMark key\n# GeneMark-EX will only run if a valid key file resides in your home directory\n# The key file will expire after 200 days, which means that you have to download a new GeneMark-EX release and a new key file after 200 days.\n> cd # change to your home directory\n> mv gm_key_64 .gm_key\nPredict gene models using BRAKER:\n#!/usr/bin/env bash\n# Making working directory before the execution of braker program\nmkdir CLR\nbraker.pl --genome=CLR_scaffold.fa.masked.masked --bam=CLR.bam --softmasking --cores 10 --workingdir=./CLR --GENEMARK_PATH=/home/assembly/bin/gmes_linux_64\nmkdir ONT\nbraker.pl --genome=ONT_scaffold.fa.masked.masked --bam=ONT.bam --softmasking --cores 10 --workingdir=./ONT --GENEMARK_PATH=/home/assembly/bin/gmes_linux_64\nmkdir Hifi\nbraker.pl --genome=Hifi_scaffold.fa.masked.masked --bam=Hifi.bam --softmasking --cores 10 --workingdir=./Hifi --GENEMARK_PATH=/home/assembly/bin/gmes_linux_64\n# --cores 10: number of threads\n# --bam: input BAM file which created by Hisat2\n# --softmasking: repetitive sequences of the input genome is soft-masked\n# --GENEMARK_PATH: specify the Genemark-EX program folder\n# For more information about BRAKER: https://github.com/Gaius-Augustus/BRAKER[href=https://github.com/Gaius-Augustus/BRAKER]\n# Outputs of BRAKER\naugustus.hints.aa # Amino acid FASTA sequences for your coding genes\naugustus.hints.codingseq # Nucleotide FASTA sequences for your coding genes\naugustus.hints.gtf # GTF file for your coding genes, which include their positions, orientation, and ID, etc."
  ],
  "subjectAreas": [
    "Genomics",
    "Sequencing",
    "Bioinformatics"
  ],
  "bigAreas": [
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}