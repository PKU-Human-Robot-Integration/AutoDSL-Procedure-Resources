{
  "id": 2209,
  "origin_website": "Cell",
  "title": "A protocol for applying a population-specific reference genome assembly to population genetics and medical studies",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nPart 1: Variants detection from the short-read sequences using linear alignment\nTiming: 1 h\nA population-specific reference genome is of importance in detecting variants. We first apply a linear approach for the short read mapping and variant calling, using NH1 as the reference genome. To evaluate the variant call rate and genotyping accuracy, we use the simulated short reads from the HX1 sequences in this step. The path of the scripts and files should be properly indicated when running the code provided in this protocol.\nSimulate the short-read sequences (SRS) using EAGLE.\nFilter out the HX1 contigs < 1 Kb in length and create a sequence dictionary.\n>python Filter_HX1_fasta_contig_length.py HX1.chr22.fa HX1.chr22.fa.fai\nHX1.chr22.filtered.fa\n>gatk CreateSequenceDictionary -R HX1.chr22.filtered.fa -O HX1.chr22.filtered.dict\nCritical: Although not required by EAGLE, contig filtration is necessary for this step, as we find EAGLE is always interrupted when dealing with those short contigs (troubleshooting 1[href=https://www.wicell.org#sec6.1]).\nInput the filtered HX1 sequences to EAGLE to simulate the raw reads of short-read sequencing, with a read length of 101 bp and sequencing depth of 30×.\n>path/to/configureEAGLE.pl\n  --run-info=path/to/RunInfo_PairedReads1x1Tiles.xml\n  --reference-genome=HX1.chr22.filtered.fa\n  --coverage-depth=30\n  --motif-quality-drop-\ntable=path/to/MotifQualityDropTables/DefaultMotifQualityDropTable.tsv\n  --quality-table=path/to/QualityTables/DefaultQualityTable.read1.length101.qval\n  --quality-table=path/to/QualityTables/DefaultQualityTable.read2.length101.qval\n>cd EAGLE\n>make fastq -j 15\n>cd 111206_EAS987_0567_FC1234TST\n>bgzip EAGLE_S1_L001_R1_001.fastq\n>bgzip EAGLE_S1_L001_R2_001.fastq\nMap the simulated reads to the reference genome, and detect variants.\nMap the simulated HX1 short reads to the NH1 genome using the BWA package.\n>bwa index NH1.chr22.fa\n>bwa mem -M -t 10 -R \"@RG\\tID:HX\\tSM:HX1\\tLB:HX1\\tPU:HX1\\tPL:ILLUMINA\" NH1.chr22.fa\n  EAGLE_S1_L001_R1_001.fastq.gz EAGLE_S1_L001_R2_001.fastq.gz | samtools view -bS - > HX1.pe.bam\n>samtools sort -@ 5 -m 4G HX1.pe.bam -T HX1 -o HX1.bam\n>samtools index HX1.bam\nRemove duplicated reads.\n>gatk --java-options \"-Xmx4g -Djava.io.tmpdir=HX1/\" MarkDuplicates -I HX1.bam -O\n  HX1.dedup.bam --VALIDATION_STRINGENCY SILENT --REMOVE_DUPLICATES true -M metrics_HX1.txt\n  -AS true --CREATE_INDEX true\nDetect variants based on the short-read mapping using GATK.",
    "Note: Despite that GATK consumes a large amount of RAM, we do not suggest any alternative software as it is the most widely used toolkit for sequencing reads processing and variants calling with good performance. The users can refer to the Best Practices Workflows (https://gatk.broadinstitute.org/hc/en-us/sections/360007226651-Best-Practices-Workflows[href=https://gatk.broadinstitute.org/hc/en-us/sections/360007226651-Best-Practices-Workflows]) for more instructions to apply GATK.\n#Variants calling for each chromosome\n>ref_chr_list=`cat NH1.chr22.fa.fai | awk '{print $1}'`\n>gatk CreateSequenceDictionary -R NH1.chr22.fa -O NH1.chr22.dict\n>gatk --java-options \"-Xmx3G -XX:ParallelGCThreads=2 -Dsamjdk.compression_level=5\"\n  HaplotypeCaller -R NH1.chr22.fa -ploidy 1 -L GWHAAAS00000500 -I HX1.dedup.bam -O\n  HX1.GWHAAAS00000500.g.vcf.gz -ERC GVCF -G\n  StandardAnnotation -G AS_StandardAnnotation -G StandardHCAnnotation --seconds-between-progress-updates 30\n>sh Combine_list.sh\n>sh 170.JointCalling.sh\n>cd 170.GenotypeGVCFs.joint.calling\n>for chr in $ref_chr_list\n  do\n    sh 170.${chr}.sh\n  done\n#Combine the VCFs of all chromosomes\n>sh 170.combine.sh\n>tabix -p vcf HX1.genomewide.hc.vcf.gz\n#An optional step to rename the VCF files as we only analyze chromosome 22 here\n>mv HX1.genomewide.hc.vcf.gz HX1.chr22.vcf.gz\n>mv HX1.genomewide.hc.vcf.gz.tbi HX1.chr22.vcf.gz.tbi\nPerform GATK hard-filtering to filter out probable artifacts from the call set (De Summa et al., 2017[href=https://www.wicell.org#bib8]).\nNote: Here we do not recommend using the GATK VQSR module to do variant filtering, as it relies on known and highly validated variant resources. This step does not guarantee that all the variants in the filtered calls are reliable, and some variants of particular interest need careful check.\n>gatk SelectVariants -select-type SNP -V HX1.chr22.vcf.gz -O HX1.chr22.snp.vcf.gz\n>gatk VariantFiltration -V HX1.chr22.snp.vcf.gz\n  --filter-expression \"QD < 2.0 || MQ < 40.0 || FS > 60.0 || SOR > 3.0\"\n  --filter-name \"Filter\" -O HX1.chr22.snp.filter.vcf.gz\n>gatk SelectVariants -select-type INDEL -V HX1.chr22.vcf.gz -O HX1.chr22.indel.vcf.gz\n>gatk VariantFiltration -V HX1.chr22.indel.vcf.gz\n  --filter-expression \"QD < 2.0 || FS > 200.0 || SOR > 10.0\"\n  --filter-name \"Filter\" -O HX1.chr22.indel.filter.vcf.gz\n>gatk MergeVcfs -I HX1.chr22.snp.filter.vcf.gz -I HX1.chr22.indel.filter.vcf.gz -O\n  HX1.chr22.filter.vcf.gz\n>bcftools view -f PASS HX1.chr22.filter.vcf.gz | bgzip -c > HX1.chr22.filtered.vcf.gz\n>tabix -p vcf HX1.chr22.filtered.vcf.gz",
    "Evaluate the accuracy of variants detection by comparing the results of simulated data to that obtained in real data.\nAlign the NH1 and HX1 genomes using Minimap2, which has been integrated into the script below.\nNote: BWA does not support the alignment of two genomes, and thus should not be used in this step as an alternative software package to Minimap2.\n>paftools_wgs_call.sh NH1.chr22.fa HX1.chr22.fa\n>bgzip NH1.chr22.HX1.chr22.vcf\n>tabix -p vcf NH1.chr22.HX1.chr22.vcf.gz\nRun RTGtools to compare the two VCF files – one callset of real data generated in this step, and another obtained from simulated data in the above step, and output summary metrics on the screen (Figure 2[href=https://www.wicell.org#fig2]).\n>rtg format -o NH1.sdf NH1.chr22.fa\n>rtg vcfeval -b NH1.chr22.HX1.chr22.vcf.gz -c HX1.chr22.filtered.vcf.gz -o output -t\n  NH1.sdf\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1684-Fig2.jpg\nFigure 2. Screenshot of the output summary metrics of RTGtools\n“Threshold” represents the genotyping quality (GQ) threshold, and you can refer to the row where the threshold is “None”. “baseline” represents the true data, and “call” represents the GATK callset. Therefore, “True-pos-baseline” means baseline variants that match between the baseline and calls; “True-pos-call” means called variants that match between the baseline and calls; “False-pos” means called variants not matched in the baseline; “False-neg” means baseline variants not matched in the call set. “Precision” means the precision rate of called variants; “Sensitivity” means the recall rate of baseline variants; “F-measure” means the weighted harmonic mean of its precision and sensitivity.\nPart 2: Variants detection from the short-read sequences using the graph-based approach\nTiming: 1.5 h",
    "The graph-based method is an alternative to the linear method for variants detection in the SRS data, and it has a much better performance, especially on the genotyping of insertions. The performance of the linear method only relies on the input sequencing data, while that of the graph-based method is additionally related to the quality of graph construction. Here we apply the vg toolkit and use NH1, which provides a comprehensive and reliable variant list representing Han Chinese, to construct a genome graph.\nNote: The vg toolkit is updated frequently, and the results vary largely across different updates. Make sure to use an identical version of vg (≥1.23 is recommended) to complete the entire process, and that your commands conform to the syntax requirements of that version.\nConstruct a graph genome using vg with a reference genome in a FASTA file (e.g., the current human reference genome assembly, GRCh38) and a set of variants in a VCF file (e.g., the NH1 variant call set named “NH1.GRCh38.chr22.vcf.gz” in the test dataset) (troubleshooting 2[href=https://www.wicell.org#sec6.3] and 3[href=https://www.wicell.org#sec6.5]).\nNote: The information lines of the VCF file will affect the output as the merged information generated by different software may conflict in vg processing, so we recommend keeping only necessary information, such as “##fileformat” and “##contig”. The option “-S” should be specified in “vg construct” if structural variants are needed in constructing the genome graph.\n>workdir=`pwd`\n>vg construct -C -S -a -R chr22 -r GRCh38.chr22.fa -v NH1.GRCh38.chr22.vcf.gz -t 1 -m 32\n  --flat-alts 1>chr22.vg 2>chr22.vg.log\nCritical: To keep vg running smoothly, symbolic structural variants in the VCF file should be converted into explicit representations. Relevant format specifications can be found at https://samtools.github.io/hts-specs/VCFv4.2.pdf[href=https://samtools.github.io/hts-specs/VCFv4.2.pdf], and we provide the “explicitVCF.converter.pl” script on GitHub and Zenodo (see key resources table[href=https://www.wicell.org#key-resources-table]) to convert the format.",
    "Build an xg index and a gcsa index for the output graph in the .vg file.\nNote: The xg index is a compressed version of the graph that allows fast node, edge and path lookups; the gcsa index is a pruned substring index used only for read mapping.\n#Build xg index\n>vg ids -j chr22.vg\n>vg index -b ${workdir} -x wg.xg chr22.vg\n#Prune the graph and build gcsa index\n>vg prune -r chr22.vg > chr22.pruned.vg\n>vg index -b ${workdir} -g wg.gcsa chr22.pruned.vg\nMap the paired SRS to the graph for variants detection and genotyping. The augmentation step is for detecting rare variants.\nNote: The earlier versions of vg only read input sequencing data in FASTQ files, while recent versions also support BAM files, which will potentially save the overall analysis time.\n#Map the short-read sequences\n>vg map -x wg.xg -g wg.gcsa -f EAGLE_S1_L001_R1_001.fastq.gz -f\n  EAGLE_S1_L001_R2_001.fastq.gz > 001.mapped.gam\n>vg convert wg.xg -p > wg.xg.pg\n#Augmentation\n>vg augment wg.xg.pg 001.mapped.gam -m 4 -q 5 -Q 5 -A wg-001.aug.gam > wg-001.aug.pg\n>vg snarls wg-001.aug.pg > wg-001.aug.snarls\n>vg pack -x wg-001.aug.pg -g wg-001.aug.gam -o wg-001.aug.pack\n>vg call wg-001.aug.pg -r wg-001.aug.snarls -k wg-001.aug.pack -s vg-001 > wg-001.aug.vcf\nPart 3: Haplotype phasing using the population-specific genome as a reference\nTiming: 8 h\nHaplotype phasing benefits from population-specific contexts like appropriate reference panels. Here we phase the HGDP Han Chinese genomes using the variant callset of the Han Chinese genome assembly as reference. We then provide a script to estimate the switch error rate, which can be further compared across the haplotypes inferred by using different reference panels.\nPerform variant calling of the NH1 and HX1 genomes based on the human reference genome GRCh38.\n>run-dipcall HX1NH1_refb38 GRCh38.chr22.fa NH1.chr22.fa HX1.chr22.fa > HX1NH1_refb38.mak\n>make -j2 -f HX1NH1_refb38.mak\n>python prepare_HX1NH1_bcf.py HX1NH1_refb38.dip.vcf.gz",
    "Run SHAPEIT4 to phase the genotypes of 4 randomly selected Han Chinese samples from the HGDP dataset (Table 1[href=https://www.wicell.org#tbl1]). A genetic map can be optionally specified with “--map”, and SHAPEIT4 provides the HapMap genetic map in GRCh38 coordinates (https://github.com/odelaneau/shapeit4/raw/master/maps/genetic_maps.b38.tar.gz[href=https://github.com/odelaneau/shapeit4/raw/master/maps/genetic_maps.b38.tar.gz]) (The International HapMap Consortium, 2007[href=https://www.wicell.org#bib27]).\n>shapeit4 --input 4Han.HGDP.snp.chr22.b38.vcf.gz --region chr22\n  --reference HX1NH1_refb38.dip.filtered.bcf --thread 5\n  --output 4Han.HGDP.snp.chr22.b38.phased.vcf.gz --sequencing\nEstimate the switch error rate for each sample (provided in a “.switch_error.txt” file, Figure 3[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1684-Fig3.jpg\nFigure 3. Screenshot of the output file of the switch error estimation for phased genotypes\nNote: This estimation is based on the allelic configuration for the adjacent heterozygotes (Lou et al., 2022[href=https://www.wicell.org#bib19]), and requires the input of .bam files. It takes more than 7 h.\n>python run_switch_script.py\nPart 4: Resolve fine-scale population structure\nTiming: 4 h\nThe population-specific reference genome may also facilitate population genetic analyses, and in particular, resolve the population differentiation and genetic structure at a fine scale. This part illustrates the analyses of population genetic relationship and demographic history using whole-genome variants called from SRS with the population-specific assembly as reference. Here we use 9 Tujia and 43 Han Chinese genome sequences from HGDP as the test data (see key resources table[href=https://www.wicell.org#key-resources-table]).\nPrincipal component analysis (PCA).\nSelect biallelic SNPs with missing rate < 0.01 and minor allele frequency > 0.05 for further analyses.\n>bcftools view -i 'F_MISSING<0.01 && MAF>0.05' 9Tujia_43Han.HGDP.snp.chr22.b38.vcf.gz |\n  bgzip > 9Tujia_43Han.HGDP.snp.chr22.b38.miss001.maf005.vcf.gz\nCarry out SNP downsampling according to the physical distance of 50 Kb to roughly exclude possible linkage between loci.\n>plink -vcf 9Tujia_43Han.HGDP.snp.chr22.b38.miss001.maf005.vcf.gz --make-bed --double\n  --bp-space 50000 --thin 0.99 --out 9Tujia_43Han.HGDP.snp.chr22.b38.miss001.maf005.thin50",
    "Run FlashPCA2, and plot the first two PCs using a script provided (Figure 4[href=https://www.wicell.org#fig4]). The script requires an additional file assigning the color of each individual on the plot – two columns denoting the name of individuals and the colors, respectively. An R package named “hash” needs to be installed in R before applying this script. This analysis only takes a few minutes.\n>flashpca --bfile 9Tujia_43Han.HGDP.snp.chr22.b38.miss001.maf005.thin50k\n  -f .9Tujia_43Han.pca\n>Rscript pc_plot.r pcs.9Tujia_43Han.pca 9Tujia_43Han.color pcs.9Tujia_43Han.pca.pdf\nInfer population demographic history. We apply a multiple sequentially Markovian coalescent approach (MSMC2) to estimate the effective population size (Ne) of the Tujia and Han Chinese populations over time. We select 4 samples (8 haplotypes) from each population (Table 1[href=https://www.wicell.org#tbl1]).\nNote: Both MSMC and MSMC2 work for this analysis, while the memory usage and time consumption are less for the latter. In addition, MSMC loses power in ancient times with increasing numbers of input genomes (Schiffels and Wang, 2020[href=https://www.wicell.org#bib24]).\nGenotype phasing of the Tujia and Han Chinese samples using population-specific reference genomes, following part 3: haplotype phasing using the population-specific genome as a reference[href=https://www.wicell.org#sec3.3]. Here we use the Han Chinese genomes (HX1 and NH1) as a reference to infer the Tujia haplotypes, while we suggest using the Tujia reference genome (Lou et al., 2022[href=https://www.wicell.org#bib19]) instead to achieve better performance.\nPerform GenMap to generate a list of low-mappability genomic regions for custom reference genomes (NH1).\nNote: In principle, read mapping tools like BWA can be used to compute the genome mappability. However, they rely on the read alignment to the reference genome and thus are not applicable to analyzing the NH1 genome assembly. The GEM mappability program is also widely used (Derrien et al., 2012[href=https://www.wicell.org#bib10]). Here we recommend GenMap as it is a magnitude faster than GEM.\n>genmap index -F NH1.chr22.fa -I index",
    ">genmap map -K 35 -E 1 -I index/ -O ./ -r -t -w -bg 2> err.log\n>awk -F '\\t' '$4==1' NH1.chr22.genmap.bedgraph |cut -f1-3 > NH1.chr22.mappability.bed\nGenerate a VCF file for each individual, and run BamCaller.py implemented in MSMC2 for quality control.\n>for i in {1..4}\n  do\n    bcftools view -s sample${i} -r chr22 <4Han/4Tujia>.HGDP.snp.chr22.b38.phased.vcf.gz |\nbgzip > sample${i}.chr22.vcf.gz\n    samtools mpileup -q 20 -Q 20 -C 50 -u -r <chr> -f <ref.fa> <bam> | bcftools call -c -V\nindels | python bamCaller.py <mean_cov> <out_mask.bed.gz> | gzip -c > <out.vcf.gz>\n  done\nMask the low mappability regions in the individual files and make an input file for MSMC2.\n>generate_multihetsep.py --mask=NH1.chr22.mappability.bed\n  --mask=<sample1_mask.bed.gz> --mask=<sample2_mask.bed.gz>\n  --mask=<sample3_mask.bed.gz> --mask=<sample4_mask.bed.gz>\nsample1.chr22.vcf.gz sample2.chr22.vcf.gz sample3.chr22.vcf.gz sample4.chr22.vcf.gz > msmc_input\nRun MSMC2 to estimate Ne and plot the Ne dynamics. The Tujia and Han Chinese populations should be analyzed independently.\nNote: The final output (.final.txt) contains the scaled begin and end time of the interval and scaled inverse Ne of the interval. The ne_plot.r script allows for converting scaled times and Ne to real numbers (saved in .converted.txt file, Figure 5[href=https://www.wicell.org#fig5]) with given parameters including a mutation rate of the human genome (1.25 × 10−8 per bp per generation by default) and a generation time (25 years per generation by default), and to plot the dynamic changes of Ne (saved in a .final.converted.pdf file, Figure 6[href=https://www.wicell.org#fig6]). This script limits the plot of Ne dynamics during the period of 1,000–1,000,000 years ago.\n>msmc2 --fixedRecombination -o msmc_output msmc_input\n>Rscript ne_plot.r msmc_output <mutation_rate> <generation_time> <color>\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1684-Fig4.jpg\nFigure 4. PCA plot of the Tujia and Han Chinese populations, represented by orange and green dots, respectively\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1684-Fig5.jpg\nFigure 5. Screenshot of the converted results of Ne estimation by MSMC2\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1684-Fig6.jpg\nFigure 6. Plot of the Ne dynamics in Han Chinese (red) and Tujia (blue) populations",
    "Part 5: Discover variants in medically relevant genes\nTiming: 4 h\nIn this part, we focus on the recall of genome variants in the previously compiled medically relevant genes (see compile a list of medically relevant genes[href=https://www.wicell.org#sec1.3] in before you begin[href=https://www.wicell.org#before-you-begin]). We examine the HX1 variants detected by mapping the simulated SRS to the NH1 genome, and generate a list of medically relevant variants genotyped. These variants may provide special insights into the genetic basis of some phenotypes or diseases in the corresponding population, and deserve further investigations.\nAnalysis of the medically relevant genes requires a liftover of the gene coordinates to match the NH1 genome coordinates. We apply Liftoff to convert the GFF formatted gene annotation (e.g., GENCODE human release 34, see key resources table[href=https://www.wicell.org#key-resources-table]) file to NH1, using GRCh38 as the reference genome and the NH1 assembly as the target genome.\n>liftoff -g gencode.v34.annotation.gff3 -a 0.9 -s 0.9 -exclude_partial -p 10 -o\n  NH1.gencode.v34.gff -u NH1_unmapped.txt NH1.chr22.fa GRCh38.chr22.fa\n>python get_NH1_medically_genes.py chr22 GRCh38_mrg_full_gene.bed NH1.gencode.v34.gff\n>mv NH1.chr22.fa /path/to/snpeff_test/sequences.fa\nExtract the variants located in the medically relevant genes in the HX1 callset generated in part 1: variants detection from the short-read sequences using linear alignment[href=https://www.wicell.org#sec3.1], and then perform variants annotation using SnpEff.\n>bcftools view -R NH1.medically_gene.bed HX1.chr22.filtered.vcf.gz | bgzip >\n  HX1.chr22.medically_gene.filtered.vcf.gz\n>python snpeff_config.py /path/to/snpEff/ /path/to/snpeff_test/\n>java -Xmx4g -jar snpEff.jar -v NH1 HX1.chr22.medically_gene.filtered.vcf.gz | bgzip 1>\n  HX1.chr22.medically_gene.filtered.snpeff_ann.vcf.gz\nCritical: We recommend using SnpEff version 4.3t, which requires Java 11.0.1. If the structure of some genes in the converted GFF files is incomplete, SnpEff will report an error (troubleshooting 4[href=https://www.wicell.org#sec6.7]). In this case, we should manually remove these genes in the GFF file. The GFF file should not be compressed, otherwise, we would not obtain accurate annotations for some loci (troubleshooting 5[href=https://www.wicell.org#sec6.9])."
  ],
  "subjectAreas": [
    "Genomics",
    "Bioinformatics",
    "Systems Biology",
    "Health Sciences",
    "Genetics"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}