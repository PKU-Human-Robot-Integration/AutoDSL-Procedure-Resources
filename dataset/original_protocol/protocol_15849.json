{
  "id": 19675,
  "origin_website": "Wiley",
  "title": "Improving Species Level-taxonomic Assignment from 16S rRNA Sequencing Technologies",
  "procedures": [
    "This protocol presents the results obtained from DADA2 pipeline to use the ASV sequences, which is the input data for the other protocols described here. The DADA2 pipeline is extensively detailed in the DADA2 developer's webpage (https://benjjneb.github.io/dada2/tutorial.html[href=https://benjjneb.github.io/dada2/tutorial.html]); therefore, we only state the steps followed. Briefly, low-quality reads were filtered and trimmed out based on the observed quality profiles by using the filterAndTrim function, truncating forward and reverse reads below 290 and 230, respectively, and considering a value of 2 as the maximum expected error. In detail, the function arguments for filterAndTrim are:\nmaxN=0, maxEE=c(2,2), truncQ=2, trimLeft=10, truncLen=c(290.230), rm.phix=T, compress=T, multithread = T (the last argument is indicating the use of multiple cores)\nFurthermore, the first 10 nucleotides of each read were removed. We combined identical sequencing reads into unique sequences, made a sample inference from the matrix of estimated learning errors, and merged paired reads. For the sample inference step (see https://benjjneb.github.io/dada2/tutorial.html[href=https://benjjneb.github.io/dada2/tutorial.html]) the argument of the pool was defined as True. Chimeras and contaminants are often rare but spread across samples, making them much more effectively-identified when the samples are pooled (pool =T). Chimeric sequences were removed by using the removeBimeraDenovo function and taxonomy was assigned utilizing the SILVA 16S rRNA database (v.138.1).\nNecessary Resources\nHardware\nLinux, MacOS, or Windows (with Subsystem for Linux) operating system, with sufficient available random-access memory (RAM) and disk space (see Strategic Planning)\nSoftware\nThe following software must be installed and available in the PATH environment variable to be executable as a binary system:\n               \nR (v4.1.2): (https://cran.r-project.org/bin/windows/base/old/4.1.2/[href=https://cran.r-project.org/bin/windows/base/old/4.1.2/])\nRstudio (v1.4.1106): (https://posit.co/download/rstudio-desktop/[href=https://posit.co/download/rstudio-desktop/])\nWe recommend having a fundamental understanding of R and RStudio, along with familiarity with their basic commands, which will be utilized in the current protocol.\nR packages:\n               \nDADA2 (v1.22): (https://benjjneb.github.io/dada2/dada-installation.html[href=https://benjjneb.github.io/dada2/dada-installation.html])\nDECIPHER (v2.22.0): (https://bioconductor.org/packages/release/bioc/html/DECIPHER.html[href=https://bioconductor.org/packages/release/bioc/html/DECIPHER.html])\nggplot2 (v3.4.1): (https://cran.r-project.org/web/packages/ggplot2/index.html[href=https://cran.r-project.org/web/packages/ggplot2/index.html])\nphangorn (v2.6.2): (https://cran.r-project.org/web/packages/phangorn/index.html[href=https://cran.r-project.org/web/packages/phangorn/index.html])\ntidyverse (v1.3.1): (https://cran.r-project.org/web/packages/tidyverse/index.html[href=https://cran.r-project.org/web/packages/tidyverse/index.html])",
    "R script files can be run interactively in R/Rstudio or in command line with Rscript. We will use Rscript in this protocol.\nFiles\nseqtab.nochim_pooling.rds, ASV table file obtained after chimeras removal (see https://benjjneb.github.io/dada2/tutorial.html[href=https://benjjneb.github.io/dada2/tutorial.html]).\nSample files\nAll the required files of the present protocol are included in the Figshare link (see Data availability statement) to show the protocol process.\n1. Download the sample and reference files.\nThe present protocol uses the last available update of SILVA reference database: v138.1. Files silva_nr99_v138.1_train_set.fa.gz and silva_species_assignment_v138.1.fa.gz which can be downloaded from the DADA2 tutorial download webpage (Zenodo repository: https://zenodo.org/record/4587955#.ZEaxi2j7SUk[href=https://zenodo.org/record/4587955#.ZEaxi2j7SUk]):\nwget https://zenodo.org/record/4587955/files/silva_nr99_v138.1_train_set.fa.gz\nwget https://zenodo.org/record/4587955/files/silva_species_assignment_v138.1.fa.gz\n2. Assignment of the taxonomy as follows:\n         \nOnce the ASV table without chimeras (file seqtab.nochim_pooling.rds, Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-tbl-0001]) is obtained, the DNA sequence assigned for each ASV is mapped to a specific Bacteria or Archaea lineage.\nOpen, customize the file 08.assign_taxonomy.R to your directory pathway and run:\nRscript 08.assign_taxonomy.R. As shown in the R script file, you need the object seqtab.nochim_pooling.rds. Submit this object first to assignTaxonomy and second to addSpecies functions, pay attention to the R script of the reference databases that use each of both functions.\nOutput generated to use downstream: taxa_silva138.1_pooling.rds\nFor this task, DADA2 implements a naive Bayesian classifier method. Different reference taxonomic databases for 16S rRNA exist, but the most up-to-date and maintained DADA2 reference database is SILVA, which is an ELIXIR Core Data Resource from the DSMZ-German Collection of Microorganisms and Cell Cultures (GmbH, available at https://www.arb-silva.de[href=https://www.arb-silva.de]).\nTable 1.\n                Data Structure of the rds Object seqtab.nochim_pooling.rds\ntable:\nï»¿Unnamed: 0,ASV DNA sequence 1,ASV DNA sequence n\nSample 1,\"Reads 1,1\",\"Reads 1,n\"\nSample m,\"Reads m,1\",\"Reads m,n\"\n3. Phylogenetic tree (optional step). To create a phylogenetic tree, if you consider opportune to use in your 16S rRNA downstream statistical analysis, customize to your directory pathway and run:\nRscript 09.phylogenetic_tree.R",
    "To perform this step, apart from the dada2 R package, you also need the R packages of phangorn and DECIPHER.\nThis script will generate output, which we will use downstream: tree_dada2_16S.rds\nAs input, the 09.phylogenetic_tree.R script imports the previous rds object of seqtab.nochim_pooling.rds. Then it extracts the sequences through the getSequences function. It performs the first crucial step to create a phylogenetic tree: multiple sequence alignment (MSE) using the DECIPHER R package. Then it calculates the distance matrix through dist.ml function, and once obtained, you can construct from the distance matrix a neighbor-joining tree. Then pml function computes the likelihood of a phylogenetic tree based on the given sequence alignment and the model, and optim.pml function optimizes the different model parameters.\n4. Obtention of the final rds objects: Run Rscript 10.object_rds.R\nIn this script, you will obtain the *.rds file dada2lineage_ASVDNA.rds that you will use in Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0004].\ndada2lineage.rds is a data frame that contains the ASV sequence and the corresponding lineages that derive from taxa_silva138.1_pooling.rds but we correct the species name to its correct format: binomial name of a genus name and specific epithet.",
    "This protocol includes two steps. The first one describes the basic bash and R script commands to follow for creating a custom BLASTN database that will be used to classify the ASV sequences obtained in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0001]. The second step describes the procedure from the BLASTN output to obtain a specific lineage based on E-value and percentage of identical matches (pident) parameters from the BLAST tool and other user-defined parameters detailed on an R script. All this process has been automatized to be robust and reproducible over time. Nevertheless, the output obtained in this protocol needs manual checks (but only for cases where only one specific lineage could be defined for a particular ASV sequence).\nNecessary Resources\nHardware\nLinux, MacOS or Windows (with Subsystem for Linux) operating system, with sufficient available random-access memory (RAM) and disk space (see Strategic Planning)\nSoftware\nThe following software must be installed and available in the PATH environment variable to be executable as a binary system:\nBLAST (v2.7.1): (https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/2.7.1/[href=https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/2.7.1/])\nPython (v3.9.12): (https://www.python.org/downloads/[href=https://www.python.org/downloads/])\nR (v4.1.2): (https://cran.r-project.org/bin/windows/base/old/4.1.2/[href=https://cran.r-project.org/bin/windows/base/old/4.1.2/])\nRstudio (v1.4.1106): (https://posit.co/download/rstudio-desktop/[href=https://posit.co/download/rstudio-desktop/])\nSeqKit (v2.3.0): (https://github.com/shenwei356/seqkit[href=https://github.com/shenwei356/seqkit])\nLinux system commands: wget, gzip, tr, awk, grep, nl, paste\nWe recommend having a fundamental understanding of bash commands, and a minimum reading of the BLASTN and SeqKit basic user manual is recommended.\nFiles\nAll the input files are detailed in the present protocol and available on the Figshare link.\nSample files\nSee Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0002] on the Figshare link.\n1. Download databases to create custom BLASTN database.",
    "In a specific directory of your choice, download the following reference SILVA databases (some used in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0001]). Nevertheless, we use another species multifasta file not used in the DADA2 standard pipeline in the addSpecies R function (see Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0001]). Furthermore, we also used the original SILVA 138 reference database instead of the DADA2-trained reference customized by DADA2 developers.\n         \nwget \nhttps://www.arb-silva.de/fileadmin/silva_databases/release_138.1/Exports/SILVA_138.1_SSURef_NR99_tax_silva.fasta.gz\nwget \nhttps://zenodo.org/record/4587955#.ZEaxi2j7SUk/silva_species_assignment_v138.1.fa.gz\nwget \nhttps://zenodo.org/record/4587955#.ZEaxi2j7SUk/silva_nr99_v138.1_wSpecies_train_set.fa.gz\nIn detail, from your working directory to perform this protocol, execute the above commands in the terminal to download the three reference databases to use to create the custom BLASTN database:\n2. Decompress all three databases.\n         \ngzip -d SILVA_138.1_SSURef_NR99_tax_silva.fasta.gz\ngzip -d silva_nr99_v138.1_wSpecies_train_set.fa.gz\ngzip -d silva_species_assignment_v138.1.fa.gz\n3. Change .fasta extension to .fa:\n         \nmv SILVA_138.1_SSURef_NR99_tax_silva.fasta SILVA_138.1_SSURef_NR99_tax_silva.fa\n4. Concatenate all three fasta files:\n         \ncat *.fa >> silva_dada2_arb.fa\nCheck if repeated identifiers exist (use check_no_duplicates.py provided in the Figshare link or download from the GitHub source: https://github.com/peterjc/galaxy_blast/blob/master/tools/ncbi_blast_plus/check_no_duplicates.py[href=https://github.com/peterjc/galaxy_blast/blob/master/tools/ncbi_blast_plus/check_no_duplicates.py]):\npython3 check_no_duplicates.py silva_dada2_arb.fa\n4.1 Error obtained:\nBLAST Database creation error: Error: Duplicate seq_ids are found:\nLCL|BACTERIA;PROTEOBACTERIA;GAMMAPROTEOBACTERIA;PSEUDOMONADALES;PSEUDOMONADACEAE;PSEUDOMONAS;\nThis occurs because some headers of species fasta files are identical. We expect this scenario.\n5. Create a new identifier for all the sequences in the multifasta file of silva_dada2_arb.fa (solve 4.1):\n         \ncat silva_dada2_arb.fa | seqkit replace -p.+ -r â{nr}â --nr-width 7 > 3basesdades.fa\nThe previous function with seqkit replaces all identifiers of fasta sequence to a correlative number, starting from 1 up to 1322260. We have 1322260 sequences.\n6. Create a new directory where you will create the custom BLASTN database.\nLet's name the new directory as the database:\nmkdir\tdatabase\nMove the 3basesdades.fa (that contains no repeated identifiers) to the database directory:\nmv 3basesdades.fa./database\n7. Run BLASTN in your machine.\nThis step expects that BLAST (2.7.1) is installed and available in the path.",
    "makeblastdb -in 3basesdades.fa -parse_seqids -title silva138_1_dada2 -dbtype nucl -max_file_sz â2GBâ -out customblastdatabase\nOnce run, check in the database directory that you have the newly created files:\ncustomblastdatabase.nin,ââââââââââââââââââââââcustomblastdatabase.nhr, customblastdatabase.nsq,ââââââââââââââââââââcustomblastdatabase.nsi, customblastdatabase.nsi,ââââââââââ customblastdatabase.nsdââââââââ and customblastdatabase.nog\nThe custom BLASTN database is created satisfactorily.\n8. Create another txt file from the BLASTN database.\n         \nLocate the folder that harbors the silva_dada2_arb.fa file and perform the next bash commands:\ngrep -e â>â silva_dada2_arb.fa > headers.txt\nnl -nrz headers.txt > headers_2.txt\ncat headers_2.txt | cut -f1,2 | sed âs/^0*//â > lineage.txt\ncut -f2- lineage.txt | awk â{if(substr($0, 3, 1) â¼ /[A-Z0-9]/) {$1=ââ; sub(/^[[:space:]]+/, ââ)} print}â > ranknames.txt\npaste lineage.txt ranknames.txt >conjunt_ranknames.txt\n9. All the output *.txt files are available in Figshare folder for your check. Customize the file seqtab.nochim_pooling.rds.\nThe R script script_1_CP.R reads the rds object seqtab.nochim_pooling.rds (obtained in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0001]). This is the abundance table (see Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-tbl-0001]) where you can find the DNA sequence for each ASV retrieved. Then the ASV sequences are enumerated with letter-number coding and written to the file ASV_code_sequence.txt.\nOutput generated: ASV_code_sequence.txt\n10. Create the ASVID_DNAseq.rds file from the ASV_code_sequence.txt.\n         \nRun the Rscript asvid_dna.seq.R.\nThrough this R script, you will create the rds file ASVID_DNAseq.rds that you will use in Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0004].\n11. Transform the tabulate of the ASV_code_sequence.txt to a new line.\nLet's do this through the next bash command:\ncat ASV_code_sequence.txt | tr â\\tâ â\\nâ > ASV_code_sequence.fasta\nThe file ASV_code_sequence.fasta is a multifasta that contains all the ASV sequences (4617 ASV sequences in our studied case).\n12. Linearize the multifasta file.\n         \nawk â{if(NR==1) {print $0} else {if($0 â¼ /^>/) {print â\\nâ$0} else {printf $0}}}â ASV_code_sequence.fasta > ASV_code_sequence_linear.fasta\n13. Split the multifasta file (ASV_code_sequence_linear.fasta)\nRun the bash script split_fasta.sh in the directory that contains the multifasta file ASV_code_sequence_linear.fasta.\nYou can do this through the following:",
    "bash splitfasta.sh ASV_code_sequence_linear.fasta, or directly run the commands in the terminal. As you can see, we use the multifasta file as an argument of the bash script.\nOutput generated: In the same directory, you will generate as many FASTA files as there are ASVs in the original multi-FASTA file.\n14. Move all the independent fasta files to a new directory:\n         \nmkdir fasta_files\nmv *.fasta./fasta_files\nBefore, remember to remove the ASV_code_sequence_linear_fasta in the fasta_files directory.\n15. Run BLASTN for all ASV sequences using the custom database obtained in step 7.\n         \nIn the terminal, run:\t\nbash blastn.sh\nThis is a long process, so we also provide an alternative script to run parallel tasks on an SGE cluster: blastn-sge.sh\nFind the output in the path defined in the previous blastn.sh file.\nCheck that the number of blasted.txt files coincide with the number of input fasta files. In our case the number is 4617. For example, in the folder that contains the output file, run the next command:\nls *blasted.txt | wc -l\n16. Concatenate all blasted.txt files in a single file.\nIn a terminal bash, execute:\ncat *blasted.txt >> blastresults.txt\n17. Select columns 1, 2 and 4 from the BLASTn output:\n         \ncat blastresults.txt | cut -f1,2,4 > blastotab.txt\nColumn 1 states for ASV code, column 2 for the sequence identification number and column 4 is the identical percentage of blastn output towards the query sequence (ASV sequence).\nColumn 2 still appears as 0 in the front of the NCBI taxonomy code, which must be removed. Remove and reorder this to create the final output from BLAST:\ncat blastotab.txt | cut -f1 > column1.txt\ncat blastotab.txt | cut -f2 | sed âs/^0*//â > column2.txt\ncat blastotab.txt | cut -f3 > column3.txt\npaste column1.txt column2.txt column3.txt > blasttotab_def.txt\n18. Perform the taxonomical assignment.\n         \nRun Rscript",
    "blast_assignment.R\nThrough the abovementioned R script, an automatic lineage selection per ASV is proposed based on having a pident value >99.5% and taking into account if the lineage defined for BLAST has the maximal resolution (species level).\nOutput_generated: blastresults_CP.txt\nFrom the initial 4617 ASV sequences, 3811 were blasted to some lineage of the custom BLASTN database. After the blast_assignment.R depuration, 3096 ASV complies with the filters applied to the R script to obtain more confident taxonomical assignments.\n19. Manual check and deduplication.\nThe output of blast for some ASV may not be unique, and more than one species share the same DNA sequence. Deduplication of these cases can only be performed manually because there are many possible situations, and it requires some knowledge on bacterial taxonomy (e.g., Latin knowledge and habitat-specific microbiota lineages).\nFrom the blastresults_CP.txt create a copy where a manual check is done (let's name it, e.g., blastresults_CP_selected.txt). Consider that we do not have to check all the 3096 ASV manually included in blastresults_CP.txt; we only must check the ASV with more than one lineage classification (due to BLAST identical pident value).\nFor ease of use, open blastresults_CP_selected.txt in office software (such as LibreOffice Calc or Microsoft Excel). By using the conditional formatting-based tool on duplicated ASV numbers you can highlight the ASV to accurate the lineage manually (Figure 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-fig-0002]).\nAfter the use of the conditional formatting-based tool, remove the column deep.\nLet us comment on the steps we follow for different scenarios that you can face to reproduce our results:\nCase 1: Two or more lineages with the same pident value and the same level of resolution (example: ASV1008). Then for that case we defined ASV1008 as Granulicatella adiacens/para-adiacens. When we have different species possibilities for the same genus, we separate with â/â and alphabetically arranged.",
    "Case 2: When for the same ASV, we have more than seven different species for the same genus (e.g., ASV1032), we transformed to Genus + sp. For that case, we use Methylobacterium sp.\nCase 3: Sometimes, it is impossible to achieve the species level because we have different genera and species, all of which are human gut inhabitant (e.g., ASV 1287, 1359). Then, we retain the lineage up to the most common taxonomical rank. In these two cases, up to family rank (Enterobacteriaceae). The most complicated cases to establish a lineage in our data were ASV1156 and ASV1168. Nevertheless, considering that DADA2 defined that lineage up to the Lelliotia genus, it helped us define those lineages.\nOnce all the duplicated ASV ID rows are checked, save that document (blastresults_CP_selected.txt).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/ea193d9f-69c9-44e8-8d93-70c6986aa506/cpz1930-fig-0002-m.jpg</p>\nFigure 2\nScreenshot of the blast_results_CP.txt opened in an Excel spreadsheet.",
    "This protocol uses the public 16S database from NCBI to classify the ASV sequences obtained from the DADA2 pipeline. For this purpose, we use the k-mer taxonomical classification algorithm of Kraken2 and Bracken2, creating a custom database.\nNecessary Resources\nHardware\nLinux, MacOS, or Windows (with Subsystem for Linux) operating system, with sufficient available random-access memory (RAM) and disk space (see Strategic Planning)\nSoftware\nThe following software must be installed and available in the PATH environment variable to be executable as a binary system:\nBracken2 (v2.2): (https://github.com/jenniferlu717/Bracken[href=https://github.com/jenniferlu717/Bracken])\nKraken2 (v2.0.8-beta): (https://github.com/DerrickWood/kraken2[href=https://github.com/DerrickWood/kraken2])\nPython (v3.9.12): (https://www.python.org/downloads/[href=https://www.python.org/downloads/])\nR (v4.1.2): (https://cran.r-project.org/bin/windows/base/old/4.1.2/[href=https://cran.r-project.org/bin/windows/base/old/4.1.2/])\nRstudio (v1.4.1106): (https://posit.co/download/rstudio-desktop/[href=https://posit.co/download/rstudio-desktop/])\nWe encourage reading the Kraken2 and Bracken 2 user manuals.\nR packages:\ndplyr (v1.1.1): (https://cran.r-project.org/web/packages/dplyr/index.html[href=https://cran.r-project.org/web/packages/dplyr/index.html])\nreadxl (v1.4.2): (https://cran.r-project.org/web/packages/readxl/index.html[href=https://cran.r-project.org/web/packages/readxl/index.html])\nFiles and sample files\nAvailable on Figshare\n1. Download NCBI RefSeq Targeted Loci project databases.\nFrom a particular directory in your terminal download the last Archaea and Bacteria database available from NCBI RefSeq Targeted Loci Project: https://www.ncbi.nlm.nih.gov/refseq/targetedloci/[href=https://www.ncbi.nlm.nih.gov/refseq/targetedloci/] (Figure 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-fig-0003]):\nwget https://ftp.ncbi.nlm.nih.gov/refseq/TargetedLoci/Archaea/archaea.16SrRNA.fna.gz\nwget https://ftp.ncbi.nlm.nih.gov/refseq/TargetedLoci/Bacteria/bacteria.16SrRNA.fna.gz\nFor traceability in your study, annotate the download date because NCBI updates the database weekly.\nDecompress both downloaded files to obtain fna extension:\ngunzip -d archaea.16SrRNA.fna.gz\ngunzip -d bacteria.16SrRNA.fna.gz\nIn the corresponding folder of Figshare, you can download exactly the two fna files we used.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/2c3b3f92-c862-4e01-8e81-ff6677680848/cpz1930-fig-0003-m.jpg</p>\nFigure 3\nSnapshot of the homepage of the NCBI RefSeq Targeted Loci Project.\n2. Create the custom Kraken2 database.\nIt is highly recommended to read the user manuals of Kraken2 and Bracken2 (https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown[href=https://github.com/DerrickWood/kraken2/blob/master/docs/MANUAL.markdown] for installing Kraken2 and see https://github.com/jenniferlu717/Bracken[href=https://github.com/jenniferlu717/Bracken] for Bracken2).\nWithin the folder you desire to create the Kraken2 custom database, create a subdirectory.\nmkdir customdatabase\n3. Create the Kraken2 custom database only with the two fna files downloaded above from the NCBI RefSeq Targeted Loci.",
    "To do this, in the folder that contains both files and in the computer that has installed Kraken2 and Bracken2, execute the following bash script so that Kraken creates the taxonomy folder inside the customdatabase folder:\nkraken2-build --download-taxonomy --db /PATH/customdatabase\nThis command requires an internet connection to allow kraken2 to download the taxonomy file. Once this first step is finished, execute the next commands to build the custom database with the downloaded fna files:\nfor f in *.fna; do kraken2-build --add-to-library $f --db /PATH/customdatabase; done\nNOTE: Please verify that the directory where you execute the command âfor f in *.fnaâ contains only the two *.fna files indicated on step 1 and no additional files with the .fna extension.\nOnce finished, a new library subdirectory will have been created inside custom database directory.\nTo complete the creation of the Kraken2 custom database, use the following command. You can also execute this command directly in the terminal:kraken2-build --build --threads 28 --db customdatabase\nThree files with extension *.k2d will be created (which are essential to work the Kraken2 algorithm downstream).\n4. Create Bracken2 k-mers for the custom Kraken2 database.\nThen, let's do the bracken build for different k-mer lengths to build a database (we use three k-mer lengths: 150, 200 and 240 bp). We can do it directly in the terminal. The output will be saved in the customdatabase directory.\nStarting for 150 bp kmer:\nbracken-build -d customdatabase -t 28 -k 35 -l 150\nFor 200 bp kmer:\nbracken-build -d customdatabase -t 28 -k 35 -l 200\nFor 240 bp kmer:\nbracken-build -d customdatabase -t 28 -k 35 -l 240\nOnce finished, check that inside the customdatabase directory, you have the next following files: database150mers.kraken, database150mers.kmer_distrib, database200mers.kraken, database200mers.kmer_distrib, database240mers.kraken and, database240mers.kmer_distrib.\n5. Run Kraken2.",
    "To execute Kraken2, use these 4617 FASTA files as input, each corresponding to one of the 4617 ASVs retrieved from the DADA2 pipeline (Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0001]):bash kraken_refseq.sh\n6. Perform Bracken analysis for the 150 bp kmer.\n         \nbash bracken_refseq.sh\nWe want to transform the Kraken + Bracken output format to a more friendly (metaphlan format) output. We use the following bash scripts, two Python scripts downloaded from the Krakentools webpage (https://github.com/jenniferlu717/KrakenTools[href=https://github.com/jenniferlu717/KrakenTools]). Indeed, both Python scripts are available in Figshare.\n7. Transform each Bracken report to the Metaphlan format.\nThe phyton file kreport2mpa.py is used within the following bash script.\nbash krakentools_1_refseq.sh\nMerge all metaphlan format files into one for downstream analysis. The Python script combine_mpa.py is used within the following bash script.\nbash krakentools_2_refseq.sh\nA txt file is generated (combined_allranks_mpa_refseq.txt).\n8. Perform last changes/details on the output generated in order to obtain the R file ASV_lineage_refseq.rds with some convenient reformatting of the lineage established with Kraken 2 and Bracken 2 for each ASV using NCBI RefSeq Targeted Loci as a taxonomical database. To achieve this:\n         \nRun Rscript script_refseqafterdada2.R",
    "From the previous protocols, we obtained the lineage for each ASV on the DADA2 pipeline (Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0001]) from the custom BLASTN database (Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0002]) and 16S NCBI public databases (Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-prot-0003]). This protocol combines the outputs to consider all our lineage information in three ways: analyze them, compare them, and find common patterns. The aim of this protocol is to obtain a more reliable lineage for each ASV after the output comparison of the three methods. Moreover, this protocol presents an extra final step to ensure the update and homogenization of taxonomic lineage ranks according to NCBI taxonomy (ftp.ncbi.nlm.nih.gov/pub/taxonomy/) using myTAI and taxonomizr R packages. In addition, it shows how to construct the phyloseq object, which is the bridge between bioinformatics and statistical analysis in microbiome data.\nNecessary Resources\nHardware\nLinux, MacOS, or Windows (with Subsystem for Linux) operating system, with sufficient available random-access memory (RAM) and disk space (see Strategic Planning)\nSoftware\nThe following software must be installed and available in your PATH environment variable to be executable as a system binary:\nR (v4.1.2): (https://cran.r-project.org/bin/windows/base/old/4.1.2/[href=https://cran.r-project.org/bin/windows/base/old/4.1.2/])\nRstudio (v1.4.1106): (https://posit.co/download/rstudio-desktop/[href=https://posit.co/download/rstudio-desktop/])\nR packages:\ndata.table (v1.14.8): (https://cran.r-project.org/web/packages/data.table/index.html[href=https://cran.r-project.org/web/packages/data.table/index.html])\ndplyr (v1.1.1): (https://cran.r-project.org/web/packages/dplyr/index.html[href=https://cran.r-project.org/web/packages/dplyr/index.html])\nmyTAI (v0.9.3): (https://cran.r-project.org/web/packages/myTAI/index.html[href=https://cran.r-project.org/web/packages/myTAI/index.html])\nopenxlsx (v4.2.5.2): (https://cran.r-project.org/web/packages/openxlsx/index.html[href=https://cran.r-project.org/web/packages/openxlsx/index.html])\nplyr (v1.8.8): (https://cran.r-project.org/web/packages/plyr/index.html[href=https://cran.r-project.org/web/packages/plyr/index.html])\nstringr (v1.5.0): (https://cran.r-project.org/web/packages/stringr/index.html[href=https://cran.r-project.org/web/packages/stringr/index.html])\ntaxize (v0.9.100): (https://cran.r-project.org/web/packages/taxize/index.html[href=https://cran.r-project.org/web/packages/taxize/index.html])\ntidyr (v1.3.0): (https://cran.r-project.org/web/packages/tidyr/index.html[href=https://cran.r-project.org/web/packages/tidyr/index.html])\ntaxonomizr (v0.10.2): (https://cran.r-project.org/web/packages/taxonomizr/index.html[href=https://cran.r-project.org/web/packages/taxonomizr/index.html])\nFiles\nASV_lineage_refseq.rds, blastresults_CP_selected.txt, dada2lineage_ASVDNA.rds and ASVID_DNAseq.rds (all of them available on Figshare)\nSample files\nSee Figshare\n1. Select the lineage among the three methods.\nThe purpose of this step is to automate the lineage selection towards the three methods described previously and update the lineage classification (which differs in some cases for some genera or species if it comes from the SILVA database of NCBI RefSeq Targeted Loci). To do this, the R package myTAI is used.\n         \nRun Rscript script_pre_phyloseq_16S_CP.R",
    "The output file results_16SCP_def.RData file is obtained. This file contains the lineage selected for each of the initial 4617 ASV. In detail, we finally got 4469 ASV with some lineage. This difference is due to not removing human gut, Mitochondria, and Chloroplast lineages.\n2. Perform last lineage check and create the phyloseq object.\nIn this last step, we propose to do some extra lineage checks and use another R package available (apart from myTAI) to update the lineage according to the NCBI Taxonomy database, which is updated periodically. The R package used is taxonomizr. Please note some additional steps are needed to perform after installing it: creating a SQL database.\nTo achieve it, run Rscript taxonomizr.R\nTo finish the protocol, we also consider it opportune to facilitate the R code used to create a phyloseq object before conducting the microbiome statistical analysis through this same R package or others the reader could use.\nThe final phyloseq object that creates our script is species-level defined (through tax_glom phyloseq R package function) and filtered (Navas-Molina etÂ al., 2013[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.930#cpz1930-bib-0012]).\nTo get these analyses run Rscript last_step.R."
  ],
  "subjectAreas": [
    "Bioinformatics"
  ],
  "bigAreas": [
    "Bioinformatics & Computational Biology"
  ]
}