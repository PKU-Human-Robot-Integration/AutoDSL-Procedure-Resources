{
  "id": 3413,
  "origin_website": "Cell",
  "title": "Analysis of population structure and genetic diversity in low-variance Saimaa ringed seals using low-coverage whole-genome sequence data",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nIn this section, we present the preparation of variation data, including mapping short-read sequencing data of 98 individuals against a draft genome assembly, masking the genome, variant calling, imputation of missing data and haplotype phasing, and filtering of the produced variation data. We then demonstrate the analyses of population structure, consisting of PCA and analysis of coancestry, followed by comparison between geographical distance measures and coancestry, and the analysis of genetic diversity with the VCF-formatted variation data. For the sake of convenience, from the PCA onwards, the analysis pipeline will be demonstrated using partial data over the first 20 contigs (250 Mbp in total) of the draft assembly. The test dataset and notebooks for all analysis steps are included in the GitHub project (see software installation[href=https://www.wicell.org#sec1.2] and downloading data[href=https://www.wicell.org#sec1.3] above).\nMapping reads against reference genome\nTiming: 2 days\nFirst, we define paths to the software and prepare the reference genome. Then, we map the read data against the reference genome using software BWA-MEM and carry out realignment around indels using GATK3.\nDefine a working directory and the paths to tools in “bin/”.\n#Working directory\n> cd star_protocols_saimaa\n> work=$(pwd)\n#Directory for software tools\n> apps=$work/bin\n#Software tools\n> bwa=$apps/bwa\n> samtools=$apps/samtools\n> gatk3=$apps/gatk3\n> bcftools=$apps/bcftools\n> vcftools=$apps/vcftools\n> angsd=$apps/angsd\n> pangsd=\"python3 $apps/pcangsd-v.0.99/pcangsd.py\"\n> beagle=\"java -Xmx40G -jar $apps/beagle.08Jun17.d8b.jar\"\n> rmask=$apps/RepeatMasker\n> seqbility=$apps/seqbility-20091110\n> plink=$apps/plink\n> ncpu=4\nCritical: These commands must be executed first as they define variables utilized in other scripts and commands.\nNote: “ncpu” defines the number of computing threads in parallelized tasks and should be adjusted according to one's computing resources.",
    "Note: In this protocol, we assume that the software are placed in the project directory “bin/”. For convenience, we use variables instead of program paths and, if the programs are installed, e.g., at \"/usr/local/bin\" or \"anaconda3/envs/starprot_saimaa/bin\", adjust the paths above accordingly.\nPrepare the reference genome for mapping:\n#Reference\n> refd=$work/data/reference\n> refs=$refd/norppa_12122017\n> gunzip $refs.fa.gz\n> $samtools faidx $refs.fa\n> $samtools dict $refs.fa > $refs.dict\n> $bwa index $refs.fa\nNote: Many analysis software can read files compressed with bgzip and unpacking the reference genome may not be necessary. In our protocol, the GATK3 tools only read unpacked sequence files.\nRun the script shown below for every sample:\n> source scripts/1.mapping-variantcalling/map_fastq.sh SAMPLEID FASTQ1 FASTQ2\nNote: For convenience, all the steps required for mapping the read data, sorting and realigning the reads, and marking the duplicates are collected in one script, found in “scripts/1.mapping-variantcalling/” directory. In the command above, SAMPLEID is the sample name and FASTQ1 and FASTQ2 are the paths to the two fastq files. The script produces the files “SAMPLEID_markdup.bam” and “SAMPLEID-∗.log” (for mapped reads and log output) under processed_data/1.mapping-variantcalling/bams/”.\nNote: The mapping step takes 12–48 h to run per sample, depending on the depth and quality of the data, and requires 15–20 GB of memory. If all samples are run in parallel, running this step takes as long as mapping of the slowest sample.\nscripts/map_fastq.sh:\n#!/bin/bash\nsample=$1\nfastq1=$2\nfastq2=$3\ntemp=$work/processed_data/1.mapping-variantcalling/temp\nbams=$work/processed_data/1.mapping-variantcalling/bams\nmkdir -p $temp\nmkdir -p $bams\nlog=$bams/$sample-`date +%F-%T`.log\nRG=\"@RG∖tID:${sample}∖tSM:${sample}∖tPL:ILLUMINA∖tLB:LIB\"\n( $bwa mem -M -R $RG -t $ncpu $refs.fa $fastq1 $fastq2 ∖\n| $samtools view -h -b -o $temp/${sample}_align.bam - ) 2>> $log\n$samtools fixmate -@ $ncpu -m $temp/${sample}_align.bam $temp/${sample}_fixm.bam &>> $log\n$samtools sort -@ $ncpu $temp/${sample}_fixm.bam -o $temp/${sample}_sort.bam &>> $log\n$samtools index $temp/${sample}_sort.bam &>> $log\n  $gatk3 -T RealignerTargetCreator ∖\n-R $refs.fa -I $temp/${sample}_sort.bam -o $temp/${sample}.intervals ∖",
    "-nt $ncpu &>> $log\n  $gatk3 -T IndelRealigner ∖\n  -R $refs.fa -I $temp/${sample}_sort.bam -o $temp/${sample}_realign.bam ∖\n  -targetIntervals $temp/${sample}.intervals &>> $log\n$samtools markdup -@ $ncpu $temp/${sample}_realign.bam $bams/${sample}_markdup.bam\n$samtools index $bams/${sample}_markdup.bam\nrm $temp/${sample}∗\nVariant calling and imputation\nTiming: 3 days\nThere are many software for calling germline short variants (SNPs and indels), the Genome Analysis Toolkit’s (GATK)6[href=https://www.wicell.org#bib6] HaplotypeCaller+GenotypeGVCFs probably being the most widely used of them. Here, we use ANGSD and PCAngsd which are specifically designed for low-coverage data, but very likely the results would be highly similar with other variant calling approaches.\nCalculate genotype likelihoods with ANGSD.\n> output=$work/processed_data/1.mapping-variantcalling/angsd/saimaa\n> bamsfile=$work/processed_data/1.mapping-variantcalling/angsd/bams.txt\n> ls $bams/∗bam | sort -V > $bamsfile\n#Running ANGSD\n> $angsd -GL 2 -doGlf 2 -doDepth 1 -doCounts 1 -doMajorMinor 1 -doMaf 2 ∖\n  -minMaf 0.05 -SNP_pval 1e-6 -uniqueOnly 1 -minMapQ 30 -minQ 20 -skipTriallelic 1 ∖\n  -nThreads $ncpu -minInd 20 -bam $bamsfile -out $output &> $output.log\nCritical: Notice that the file 'bams.txt' includes paths to all the bam files, and the order of samples in this file will determine the order of samples in the later steps of the data preparation and analysis. ANGSD does not include sample information in the output, and it is crucial to know the sample order and be able to incorporate this information in the data at a later stage of the analysis.\nNote: ANGSD does not perform an on-the-fly realignment of reads similar to that of GATK's HaplotypeCaller but takes the read alignment as given. For that reason, the bam data were realigned using the IndelRealigner tool (see above) from the now-discontinued GATK3 package.",
    "Note: The minimum minor allele frequency, -minMaf, should be adjusted depending on the population/sample you are dealing with. Limit 0.05 may be overly high if one wants to include rarer variants. In this case, however, due to low sequencing coverage and low numbers of singleton variants in our data, not using a MAF limit would produce highly similar results.\nNote: If not parallelized, this step can take up to 70 h to run for the full seal data. It is easy to parallelize the run by dividing the genome into sets of contigs. To do that, write the names of contigs within a set into a file (one per line) and pass that to ANGSD with the option “-rf filename”. In our analyses, the genome was divided into 16 sets of approximately equally large chunks, and then each set required approximately 3–10 GB of memory and took 3–5 h to run. If the process is parallelized, approximately 80 GB of memory and 6 h should be sufficient to run the ANGSD step.\nNote: In case of parallelization, the output files from ANGSD have to be combined before the next steps. Assuming that the output files are named from “file_1” to “file_16”, they can be catenated and then again compressed with the command:\n> ( zcat file_1.beagle.gz && for i in {2..16}; do zgrep -v marker file_$i.beagle.gz; done ) ∖\n| bgzip -c > combined.beagle.gz\nCall genotypes with PCAngsd and convert the output to VCF format.\n> file=$work/processed_data/1.mapping-variantcalling/angsd/saimaa\n> $pcangsd -beagle $file.beagle.gz -e 4 -o $file -post_save\n> awk -f scripts/beagle2vcf.awk $file.post.beagle | bgzip -c > $file.tmp.vcf.gz\n> $bcftools reheader -s $work/data/saimaa_IDs.txt $file.tmp.vcf.gz -o $file.vcf.gz\nCritical: Make sure that the order of sample IDs (here “data/saimaa_IDs.txt”) matches the order of bam files specified in the variant calling stage ('bams.txt', see above).",
    "Critical: The interface and functionality of PCAngsd have evolved and it is crucial to have the right version of the software to obtain the genotype likelihoods in the right format.\nNote: For the full seal data, this step requires 50 GB of memory and takes 4 h to run.\nNote: The PCAngsd step considers the population structure and sets the allele background frequencies accordingly for the genotype likelihood calling. While this sounds reasonable, in some circumstances it may possibly strengthen a signal in the data and create bias.\nImpute the missing genotypes using Beagle, then carry out haplotype phasing.\n> input=$work/processed_data/1.mapping-variantcalling/angsd/saimaa.vcf.gz\n> imputed=$work/processed_data/1.mapping-variantcalling/beagle/saimaa_imputed\n> phased=$work/processed_data/1.mapping-variantcalling/beagle/saimaa_phased\n> $beagle nthreads=10 gl=$input out=$imputed &> $imputed.err\n> $beagle nthreads=10 gt=$imputed.vcf.gz out=$phased &> $phased.err\nCritical: The more recent versions of Beagle do not support imputation in this form and it is critical to use the version specified in the key resources table[href=https://www.wicell.org#key-resources-table].\nNote: Imputation of the full seal data requires 33 GB of memory and takes 12 h to run. The last step, phasing, takes 14 GB of memory and 12 h to run.\nNote: Due to the very low sequencing coverage of some samples, we impute the missing genotypes. When doing that, we group all samples together and the imputation process may thus push some poorly covered samples towards the population average. On the other hand, performing the imputation on subdivided data would strengthen the differences between the subpopulations.\nMasking of the genome and filtering of variation data\nTiming: 2 days",
    "Over two-thirds of a eukaryotic genome can be comprised of repeat elements.30[href=https://www.wicell.org#bib30] Due to false mapping of reads covering repetitive sequences, the repeat elements are a potential source of erroneous signal and mainly a nuisance in population genetic studies. With unfinished reference genomes, another potential source of error is the incorrect resolution of duplicated genome regions and haplotypic copies. With whole-genome sequencing, the scarcity of data is rarely limiting population genetic studies and it is advisable to use too stringent than too lenient criteria for data quality. We took a simple approach and identified repeat elements and unmappable regions, such as segmental duplications, in our draft reference genome and removed variants falling on those from most analyses.\nRun RepeatMasker for the assembly using the dog repeat libraries. RepeatMasker returns a GFF file of repetitive element locations which will be used for filtering variation data.\n> $rmask -xsmall -gff --engine rmblast -dir $refd/RM -pa $ncpu -species \"Canis familiaris\" $refs.fa\n#Making a tab file for repetitive regions\n> rmgff=$refd/RM/norppa_12122017.fa.out.gff\n> awk '!/#/{OFS=\"∖t\";print $1,$4,$5}' $rmgff > $refd/RM/repeats.tab\nNote: RepeatMasker for the genome takes less than 30 h to run, and approximately 400 MB of memory.\nUsing SNPable, split the reference genome into 35 bp fragments and map these back to the reference genome.\n> maskdir=$refd/snpable\n> rm -f $maskdir/∗\n> cd $maskdir\n> $seqbility/splitfa $refs.fa 35 | split -l 20000000\n> for i in $(ls x??); do\n  $bwa aln -R 1000000 -O 3 -E 3 $refs.fa $i > $i.sai\ndone\n> for i in $(ls x??); do\n  $bwa samse $refs.fa $i.sai $i > $i.sam\ndone",
    "Note: For the draft genome used here, the “splitfa” function produces 235 files with names starting with x, referred to in the script below with the command “ls x??”. With this script, the bwa alignment is run for each file, one by one. However, it is more convenient to divide the files into batches and run the alignment for the batches in parallel. Without parallelization, this step takes 50–60 h to run, but with parallelization and ten batches, the run time is only 5–6 h. Each batch requires approximately 3.5 GB of memory, summing up to around 35 GB in total.\nNote: With the parameters used here, the analyses may be overly conservative and exclude genome regions where modern-day 150 bp reads could possibly be uniquely placed. The decision to use the tool was inspired by its inclusion in the MSMC2 best practices instructions29[href=https://www.wicell.org#bib29] and we describe it here in its original form.\nUsing the mapping information, create a masked fasta sequence.\n> cd $maskdir\n> cat x??.sam | $seqbility/gen_raw_mask.pl > rawMask_35.fa\n> $seqbility/gen_mask -l 35 -r 0.5 rawMask_35.fa > mask_35_50.fa\nConvert the masked fasta sequence into bed format using the Python script 'makeMappabilityMask.py' from the MSMC-tools package.\n> python scripts/2.masking-filtering/makeMappabilityMask.py\n> out=$maskdir/norppa_12122017.posmask.bed.gz\n> for i in $(ls $maskdir/posmask); do zcat $maskdir/posmask/$i | bgzip -c >> $out; done\nFilter the variant data (VCF) acquired from the imputation procedure using the GFF file from RepeatMasker (repetitive elements; to be removed) and the BED file from SNPable (positive mask; to be included).\n#Filtering out repeat regions from the imputated and phased VCF:\n> vcf_phased=$work/processed_data/1.mapping-variantcalling/beagle/saimaa_phased.vcf.gz\n> vcf_posmask=$work/processed_data/2.masking-filtering/saimaa_posm.vcf.gz\n> repeats=$refd/RM/repeats.tab\n> mask=$maskdir/norppa_12122017.posmask.bed.gz\n#Filtering the VCF file based on RepeatMasker data and the positive mask produced by SNPable\n> $bcftools view -T ˆ$repeats $vcf_phased | $bcftools view -T $mask -Oz -o $vcf_posmask",
    "Critical: The right use of the caret symbol (ˆ) is crucial: 'bcftools view -T ˆregions_file vcf_file' removes variants within the defined regions, whereas 'bcftools view -T regions_file vcf_file' retains variants within the defined regions.\nPrincipal component analysis\nTiming: 3 h\nThe steps from hereon can be performed on the example data included in the GitHub project without first performing the mapping and variant calling parts.\nPrincipal component analysis (PCA) is one of the most commonly performed exploratory analyses on any large data and, in our case, informative of the coarse structure of the population. We perform the PCA with software smartpca from the Eigensoft package. However, we then continue beyond a typical PCA and illustrate the results in the context of the topography of the lake.\nConvert the variation data into a format suitable for smartpca, prepare a control file and run the PCA.\nUse bcftools and vcftools for creating .map and .ped files based on VCF-formed variation data. Here, integrate Perl code to convert the contig labels into numbers on the fly.\nCreate a parameter file for conversion of the variation data to a format suitable for running smartpca, and run “convertf” to convert the data.\nCreate parameter file and run “smartpca”.\n> convertf=$apps/convertf\n> smartpca=$apps/smartpca\n> data=$work/processed_data/2.masking-filtering/saimaa_20_phased_posmask\n> processed=$work/processed_data/3.smartpca/saimaa_20_phased_posmask\n> results=$work/results/3.smartpca/saimaa_20_phased_posmask\n> $bcftools view $data.vcf.gz | ∖\nperl -pe '($c)=/([0-9]+)F∖|quiver∖|pilon/;++$c;\n$c=∼s/ˆ0+//;s/([0-9]+)F∖|quiver∖|pilon/$c/' | ∖\n$vcftools --gzvcf - --plink --out $processed\n> cat > $work/processed_data/3.smartpca/CONVERT << EOF\ngenotypename: $processed.ped\nsnpname: $processed.map\nindivname: $processed.ped\noutputformat: EIGENSTRAT\ngenotypeoutname: $processed.geno\nsnpoutname: $processed.snp\nindivoutname: $processed.ind\nfamilynames: NO\nEOF\n> $convertf -p $work/processed_data/3.smartpca/CONVERT\n> cat > processed_data/3.smartpca/SMARTPCA << EOF\ngenotypename: $processed.geno\nsnpname: $processed.snp\nindivname: $processed.ind\nevecoutname: $results.evec\nevaloutname: $results.eval\nnumoutevec: 10\nnumthreads: 4\nEOF\n> $smartpca -p $work/processed_data/3.smartpca/SMARTPCA > $results.out",
    "Note: The commands from hereon are defined for the example data consisting of the first 20 contigs. If you want to perform the analyses for the full data, replace \"saimaa_20_phased_posmask\" with \"saimaa_posm\" in all commands.\nVisualize the results by running the code below in an R console. Here, the colors of the dots (individuals) reflect the dots' positions on the axes, and the amount of variation explained by each axis is integrated into the axis labels.\n> library(tidyverse)\n> pca <- read.table(\"results/3.smartpca/saimaa_20_phased_posmask.evec\")\n> eig <- read.table(\"results/3.smartpca/saimaa_20_phased_posmask.eval\")\n> colnames(pca)[2:11] <- paste0(\"PC\",1:10)\n> e1 <- round(100∗eig$V1[1]/sum(eig$V1),2)\n> e2 <- round(100∗eig$V1[2]/sum(eig$V1),2)\n> m3 <- min(pca$PC1)\n> c3 <- max(pca$PC1)-m3\n> m4 <- min(pca$PC2)\n> c4 <- max(pca$PC2)-m4\n> pca$Color <- rgb(\n  red= 1-(pca$PC1-m3)/c3,\n  green=1-(pca$PC2-m4)/c4,\n  blue= (pca$PC2-m4)/c4)\n> colors <- cbind(pca$V1, pca$Color)\n> write.table(colors, \"results/3.smartpca/colors.txt\",\nrow.names=F,quote=T, col.names=F, sep = \"∖t\")\n> pca_20ctgs <- ggplot(pca)+\n  geom_point(aes(PC2,PC1,fill=I(Color)),size=2,shape=21,stroke=.1)+\n  xlab(paste0(\"PC2 (\",e2,\"%)\"))+ylab(paste0(\"PC1 (\",e1,\"%)\"))+\n  scale_x_reverse()+ theme_classic()\n> ggsave(\"results/3.smartpca/pca_20ctgs.png\",pca_20ctgs,width=6)\nWith R, plot the samples on the map of Saimaa using the colors from the PCA.\n> library(rgdal)\n> library(sf)\n> library(tidyverse)\n> coords <- read.csv(\"data/Sample_coordinates_subpop.csv\")\n> pca_cols <- read.table(\"results/3.smartpca/colors.txt\", sep = \"∖t\", header=F)\n#In the case that the samples are not in the same order.\n> k <- match(coords$title,pca_cols$V1)\n> coords <- coords[k,]\n> coordinates(coords) <- c(\"long\", \"lat\")\n> proj4string(coords) <- CRS(\"+proj=utm +zone=35 ellps=WGS84\")\n> coords <- coords %>% st_as_sf(coords = c('long','lat')) %>% st_set_crs(4133)\n> lakes <- read_sf(\"data/saimaa_map/jarvi_laatikko.shp\")\n> saimaa <- lakes[grep('Saimaa',lakes$Nimi),]\n> map <- ggplot()+\n  geom_sf(data=saimaa, fill=\"cadetblue3\",color=gray(0.65),linewidth=0.05)+\n  geom_sf(data=coords,fill=pca_cols$V2, shape=21,size=2.5) +\n  coord_sf(datum = sf::st_crs(4133)) + theme_bw()+xlab(\"\")+ylab(\"\")+\n  theme(\n    panel.grid.minor = element_blank(),\n    panel.grid.major = element_line(size=0.125,color=gray(0.85)),\n    axis.text = element_text(size=6)\n  )\n> ggsave(\"results/3.smartpca/saimaa.png\",map,width=6)\nAnalysis of coancestry\nTiming: 5–40 h",
    "The combination of low-coverage sequencing and highly reduced genetic variation creates challenges for many population genetic analyses, including the quantification of pairwise genetic distances. As an alternative to the widely-used identity-by-state (IBS) metric, we here utilize “coancestry” as defined by the software fineSTRUCTURE. The fineSTRUCTURE analysis is based on chromosome painting that, instead of handling each variant independently, considers the chromosomal context of variants within haplotypes. Below, we perform and compare the analyses of pairwise genetic similarities using coancestry and IBS.\nNote: The time estimate above depends on whether the analysis is carried out using the test dataset of 20 contigs or the full genome\nCalculate coancestry using fineSTRUCTURE.\nConvert the data to suitable format using PLINK and the fineSTRUCTURE tools.\n> VCF=$work/processed_data/2.masking-filtering/saimaa_20_phased_posmask.vcf.gz\n> DIR=$work/processed_data/4.finestructure/for_fs\n> mkdir -p $DIR\n> fs_dir=$apps/fs_4.1.1\n> export PERL5LIB=/path/to/Switch/\n> for ctg in $(head -20 $refs.fa.fai | cut -f1); do\n  num=${ctg:0:6}\n  $plink --vcf $VCF --make-bed --chr $ctg --allow-extra-chr ∖\n    --out $DIR/saimaa_$num --recode ped 12\n  perl $fs_dir/plink2chromopainter.pl -p=$DIR/saimaa_$num.ped ∖\n    -m=$DIR/saimaa_$num.map -o=$DIR/saimaa_$num.phase\n  perl $fs_dir/makeuniformrecfile.pl $DIR/saimaa_$num.phase $DIR/saimaa_$num.rec\n    rm $DIR/saimaa_$num.[blmfp]?? $DIR/saimaa_$num.nosex\ndone\nNote: It is important to use phased variation data (step 6) for fineSTRUCTURE. Our experiences suggest that a population-based approach (Beagle) without a sophisticated reference panel produces a good enough phasing and fineSTRUCTURE is thus applicable to non-model systems.\nNote: In the original study1[href=https://www.wicell.org#bib1], the fineSTRUCTURE analysis was carried out using the unfiltered, phased variation data.",
    "Note: You may have to install module Switch for fineSTRUCTURE. The “perl-switch” package is included in the YAML file; on Ubuntu systems, it can be installed with “sudo apt install libswitch-perl”, and on most systems directly from CPAN with the command “sudo cpan -f Switch”. If Switch has been installed to a non-standard location, the path to Switch.pm must be defined. On Linux, this is done with the following command, using Conda-installed Switch as an example: “export PERL5LIB= /home/user/anaconda3/envs/starprot_saimaa/lib/site_perl/5.26.2/”.\nCreate the fineSTRUCTURE command (command_20ctgs.txt) and start the analysis by executing the command.\n> OUT=$work/processed_data/4.finestructure/\n> cat $work/data/saimaa_IDs.txt | sed 's/ˆ/s/' > $OUT/saimaa.ids\n> cut -c-6 <(head -20 $refs.fa.fai) > $OUT/20ctgs.txt\n> cd $OUT\n#Prepare the command for running fineSTRUCTURE\n> (echo -n \"fs saimaa_20ctgs.cp -idfile saimaa.ids -phasefiles \" && ∖\n  cat 20ctgs.txt | while read a; do echo -n \" for_fs/saimaa_\"$a\".phase\"; done && ∖\n  echo -n \" -recombfiles \" && ∖\n  cat 20ctgs.txt | while read a; do echo -n \" for_fs/saimaa_\"$a\".rec\"; done && ∖\n  echo \" -go\") > command_20ctgs.txt\n#Run the resulting command_20ctgs.txt.\n> source command_20ctgs.txt\n#Copy outputfiles to the \"results\" directory\n> cp saimaa_20ctgs_linked.chunkcounts.out $work/results/4.finestructure/\n> cp saimaa_20ctgs_linked_tree.xml $work/results/4.finestructure/\nNote: fineSTRUCTURE does not accept plain numbers as sample names. Here, we add “s” in the beginning of the sample ID and remove that in the later analyses.\nNote: fineSTRUCTURE will automatically parallelize the analysis by inferring the number of computer CPUs available and setting the number of samples per process.\nNote: With the parallelization applied by the software, this step takes approximately 4 h for 20 contigs (around 250 Mbp in length) on a regular laptop. With the same computational resources available, running the analysis for the entire genome (2.35 Gbp) would take approximately 40 h.\nFor comparison, calculate identity-by-state (IBS) distances using PLINK.\n> out=$work/processed_data/4.finestructure/IBS\n> data=$work/processed_data/2.masking-filtering/saimaa_20_phased_posmask",
    "> mkdir -p $out && cd $out\n> $plink --vcf $data.vcf.gz --allow-extra-chr --distance 'ibs' 'square'\nMake heatmaps of the results from steps 14 and 15 using ggplot2 in R.\nPrepare the dendrogram which defines the order of samples in the heatmap.\n> library(ape)\n> library(XML)\n> library(tidyverse)\n> source(\"bin/fs_4.1.1/FinestructureLibrary.R\")\n> treexml <- xmlTreeParse(\"results/4.finestructure/saimaa_20ctgs_linked_tree.xml\")\n> ttree <- extractTree(treexml)\n> ttree$node.label <- c(1:97)\n#At this point, you may rearrange the matrix by rotating tree clades\n#rotate(ttree, number of parent node)\n> tdend <- myapetodend(ttree,factor=1)\nProcess the fineSTRUCTURE and IBS results.\n# fineSTRUCTURE results:\n> fullorder <- gsub(\"s\",\"\",labels(tdend)) # the order according to the tree\n> data_df <- read.table(\"results/4.finestructure/saimaa_20ctgs_linked.chunkcounts.out\",\n  row.names=1,header=T,skip=1)\n> dataraw <- as.matrix(data_df)\n> colnames(dataraw) <- gsub(\"s\",\"\",colnames(dataraw))\n> rownames(dataraw) <- gsub(\"s\",\"\",rownames(dataraw))\n> datamatrix <- dataraw[fullorder,fullorder] # reorder the data matrix\n> write.table(datamatrix, \"results/4.finestructure/datamatrix.txt\",\n  row.names=T,col.names=T,quote=F)\n> write.table(fullorder, \"results/4.finestructure/fullorder.txt\",\n  row.names=F,col.names=F,quote=F)\n> #range(datamatrix)\n> fsmax <- 395 # cap the heatmap, threshold set on the highest 1.5% of values\n> fsmat <- datamatrix\n> fsmat[fsmat>fsmax] <- fsmax\n> pca_cols <- read.table(\"results/3.smartpca/colors.txt\", sep = \"∖t\", header=F)\n> lcol <- pca_cols$V2[match(fullorder,pca_cols$V1)]\n# IBS results:\n> ibs <- read.table(\"processed_data/4.finestructure/IBS/plink.mibs\", sep = \"∖t\")\n> inds <- read.table(\"data/saimaa_IDs.txt\")$V1\n> ibsmatrix <- as.matrix(ibs)\n> colnames(ibsmatrix) <- inds\n> rownames(ibsmatrix) <- inds\n> ibsmatrix <- ibsmatrix[fullorder,fullorder] # Match order to the finestructure matrix\n> diag(ibsmatrix) <- NA\n> ibsmax <- 0.87 # cap the heatmap, threshold set on the highest 1.5% of values\n> ibsmat <- ibsmatrix\n> ibsmat[ibsmat>ibsmax] <- ibsmax #\nNote: To best show the population structure in the coancestry and IBS heatmaps, the matrices are capped, that is, the very highest values (for the most closely related pairs) are set to a specified maximum value (“fsmax” and “ibsmax” above). The threshold limits here reset the highest 1.5% of the values.",
    "Convert the matrices into a format suitable for ggplot2 and plot them as heatmaps.\nNote: For their greater versatility, we use ggplot2 for the graphics. The notebooks included in the GitHub project also demonstrate plotting with the original tools of the fineSTRUCTURE package.\n> library(tidyverse)\n> library(reshape2)\n> library(cowplot)\n> fs_melt <- melt(fsmat)\n> fs_melt$Var1 <- factor(as.character(fs_melt$Var1),\n  levels = unique(as.character(fs_melt$Var1)))\n> fs_melt$Var2 <- factor(as.character(fs_melt$Var2),\n  levels = unique(as.character(fs_melt$Var2)))\n> ibs_melt <- melt(ibsmat)\n> ibs_melt$Var1 <- factor(as.character(ibs_melt$Var1),\n  levels = unique(as.character(ibs_melt$Var1)))\n> ibs_melt$Var2 <- factor(as.character(ibs_melt$Var2),\n  levels = unique(as.character(ibs_melt$Var2)))\n> fs_col = c(\"papayawhip\",\"navajowhite1\",\"#edc861\",\"darkred\",\"purple4\")\n> fstheme <- theme(axis.text.x = element_text(angle=90),\n      axis.text = element_text(colour = lcol, size = 7),\n      axis.title = element_text(size = 14),\n      legend.position = \"top\",\n      legend.title = element_text(size = 13),\n      legend.text = element_text(size = 14),\n      legend.margin=margin(0,50,0,0),\n      legend.spacing.x = unit(0.5, 'cm'))\n> fs_plot <- ggplot(data = fs_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile()+\n  xlab(\"Seal1\")+ylab(\"Seal2\")+\n  fstheme+\n  guides(fill = guide_colourbar(barwidth = 18, barheight = 0.8))+\n  scale_fill_gradientn(name = \"Coancestry\",\n            breaks = c(0,100,200,300,400,500,600),colours = fs_col)\n> ibs_plot <- ggplot(data = ibs_melt, aes(x=Var1, y=Var2, fill=value)) +\n  geom_tile()+\n  xlab(\"Seal1\")+ylab(\"\")+\n  fstheme+\n  guides(fill = guide_colourbar(barwidth = 18, barheight = 0.8))+\n  scale_fill_gradientn(name = \"IBS\",\n      breaks = c(0.7,0.75,0.8,0.85,0.9,0.95,1),colours = fs_col)\n> fs_grid <- plot_grid(fs_plot, ibs_plot+theme(plot.margin = unit(c(0.4,0.4,0.4,1.2), \"lines\")), ncol = 2, nrow = 1, labels = c(\"A\", \"B\"))\n> ggsave(\"results/4.finestructure/coancestry_grid.png\", fs_grid, height=8, width=16)\nWater distances vs. genetic distances\nTiming: 2 h\nIsolation by distance is a common hypothesis in population genetic analyses. In the case of an aquatic organism living in an extremely labyrinthine lake, the method for defining the physical distances can make a significant difference. We demonstrate this by calculating the Euclidean and water distances between the samples, and then utilize the latter and the coancestry data to test the isolation-by-distance hypothesis and assess the effect of the fragmented lake habitat on the genetic population structure.",
    "Using R, prepare the sample coordinate data (longitude/latitude) and a raster for Lake Saimaa.\n> library(rgdal)\n> library(tidyverse)\n> library(sf)\n> library(fasterize)\n> library(ggthemes)\n> library(gdistance)\n> library(rgeos)\n> library(cowplot)\n> library(magick)\n> library(rlist)\n# Sample coordinate data\n> coords <- read.csv(\"data/Sample_coordinates_subpop.csv\")\n> coordinates(coords) <- c(\"long\", \"lat\")\n> proj4string(coords) <- CRS(\"+proj=longlat +datum=WGS84\")\n> coords <- spTransform(coords, CRS(\"+proj=utm +zone=35 ellps=WGS84\"))\n> pts <- SpatialPoints(coords, CRS(\"+proj=utm +zone=35 ellps=WGS84\"))\n# Lake raster\n> lakes <- read_sf(\"data/saimaa_map/jarvi_laatikko.shp\")\n> saimaa <- lakes[grep('Saimaa',lakes$Nimi),]\n> rsaimaa <- raster(saimaa,ncols=576,nrows=717)\n> rsaimaa <- fasterize(saimaa,rsaimaa)\n> rsaimaa[is.na(rsaimaa)] <- 100000000\n> trCost <- transition(1/rsaimaa, mean, directions=8)\n> trCostC <- geoCorrection(trCost, type=\"c\")\nNote: The raster dimensions depend on the size of the area. The dimensions used (576 × 717) for the map of Lake Saimaa translate to 0.25 km spacing.\nCalculate physical distances between sample pairs with water paths and straight paths.\nCalculate the water distances, storing the distances in a matrix and the actual paths in a list object.\n> dis <- matrix(NA,nrow = 98,ncol = 98)\n> wpths <- list()\n> for(i in 1:97){\n  for(j in (i+1):98){\n    pth <- shortestPath(trCostC, pts[i,], pts[j,], output=\"SpatialLines\")\n    dis[i,j] <- 0\n    if(length(unlist(coordinates(pth@lines[[1]])))>2) {\n    dis[i,j] <- rgeos::gLength(pth)/1000\n    pth@lines[[1]]@ID <- paste0(coords$title[i],\"-\",coords$title[j])\n    wpths <- c(wpths,pth)\n    }\n  }\n}\n> write.table(dis,\"results/5.waterdistance/waterdistance.txt\",\nrow.names=F,col.names=F,quote=F)\n> list.save(wpths, “results/5.waterdistance/wpths.rdata”)\nCalculate the Euclidean distances.\n> coords_df <- as.data.frame(coords)\n> dis <- matrix(NA,nrow = 98,ncol = 98)\n> for(i in 1:97){\n  for(j in (i+1):98){\n    pth=dist(coords_df[c(i,j),3:4], method = \"euclidean\")\n    dis[i,j]=pth/1000\n  }\n}\n> write.table(dis,\"results/5.waterdistance/eucdistance.txt\",\nrow.names=F,col.names=F,quote=F)\nPlot the water distance and Euclidean distance for selected individuals, and compare the geographical and genetic distances to assess the effect of habitat fragmentation.\nGather information into a dataframe.\n> inds <- as.character(read.table(\"data/saimaa_IDs.txt\")$V1)\n> wdist <- read.table(\"results/5.waterdistance/waterdistance.txt\",\nrow.names=inds,col.names=inds,check.names=F)\n> wd_melt <- melt(as.matrix(wdist))\n> wd_melt <- wd_melt[!is.na(wd_melt$value),]\n> colnames(wd_melt) = c(\"seal1\",\"seal2\",\"water_distance\")\n> euc <- read.table(\"results/5.waterdistance/eucdistance.txt\",\nrow.names=inds,col.names=inds,check.names=F)\n> euc_melt <- melt(as.matrix(euc))",
    "> euc_melt <- euc_melt[!is.na(euc_melt$value),]\n> colnames(euc_melt) = c(\"seal1\",\"seal2\",\"euclidean_distance\")\n> distances <- merge(wd_melt,euc_melt)\nPlot the water path and the straight path for four sample pairs (Figure 4[href=https://www.wicell.org#fig4]A).\n#Plotting both distance metrics on the map of Saimaa\n> pairs <- c(\"2474-2563\",\"2382-2474\",\"2549-2550\",\"2549-2573\")\n> wpths <- list.load(“results/5.waterdistance/wpths.rdata”)\n> png(file=\"results/5.waterdistance/distances_6samples.png\",width=1269, height=1500)\n> par(oma=c(0,0,0,0),mar=c(0,0,0,0))\n> plot(r, col = c(\"slategray3\", \"seashell\"), legend = F, axes = F)\n> for(pair in pairs) {\n  pcoord <- coords[coords$title %in% strsplit(pair,\"-\")[[1]],]\n  ind <- which(sapply(wpths, function(x) x@lines[[1]]@ID)==pair)\n  plot(wpths[[ind]], add=TRUE, col=\"slateblue4\", lwd = 5)\n  points(pcoord, pch=20, col=\"red\", cex = 2.5)\n  lines(cbind(pcoord$long,pcoord$lat), lwd = 5, col=\"red\")\n  text(pcoord$long, pcoord$lat+3000, labels = pcoord$title, col = \"black\", cex = 2.5)\n}\n> dev.off()\nPrepare the bar plots demonstrating the difference between the distance measures (Figure 4[href=https://www.wicell.org#fig4]B).\n#Visualize as barplots\n> pairs <- c(\"2474-2563\",\"2474-2382\")\n> pairs <- t(sapply(pairs,function(x) strsplit(x,\"-\")[[1]]))\n> trio1 <- distances[ distances$seal1 %in% pairs[,1] & distances$seal2 %in% pairs[,2] |\ndistances$seal1 %in% pairs[,2] & distances$seal2 %in% pairs[,1], ]\n> trio1$seal1[1] <- 2474\n> trio1$seal2[1] <- 2382\n> trio1 <- melt(trio1,id.vars = c(\"seal1\",\"seal2\"))\n> pairs <- c(\"2549-2550\",\"2549-2573\")\n> pairs <- t(sapply(pairs,function(x) strsplit(x,\"-\")[[1]]))\n> trio2 <- distances[ distances$seal1 %in% pairs[,1] & distances$seal2 %in% pairs[,2] |\ndistances$seal1 %in% pairs[,2] & distances$seal2 %in% pairs[,1], ]\n> trio2 <- melt(trio2,id.vars = c(\"seal1\",\"seal2\"))\n> mytheme <-\n  theme_bw() + theme(\n    panel.background = element_rect(fill = 'seashell'),\n    legend.direction = \"horizontal\",\n    legend.position = \"top\",\n    legend.margin=margin(0,15,0,0),\n    legend.text = element_text(size=10, margin = margin(r = 1.25, l=-4, unit=\"pt\")),\n    legend.key.size = unit(3,\"mm\"),\n    axis.text = element_text(size = 10),\n    axis.title = element_text(size = 10)\n  )\n> p1 <- ggplot(data=trio1, aes(x=as.factor(seal2), y=as.numeric(value), fill=variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  xlab(\"2474\") + ylab(\"Distance\") + ylim(0,120) +\n  scale_fill_manual(\"Distance\", labels= c(\"Euc.\", \"Water\"),\nvalues=c(\"lightcoral\", \"slateblue4\")) + mytheme\n> p2 <- ggplot(data=trio2, aes(x=as.factor(seal2), y=as.numeric(value), fill=variable)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  xlab(\"2549\") + ylab(\"Distance\") + ylim(0,120) +\n  scale_fill_manual(\"Distance\", labels= c(\"Euc.\", \"Water\"),\nvalues=c(\"lightcoral\", \"slateblue4\")) + mytheme",
    "Prepare a scatterplot of water distances and coancestry and test the association between the two within sub-regions, between sub-regions, and in the full data (Figure 4[href=https://www.wicell.org#fig4]C).\n#Prepare water distance and coancestry data\n> coords2 <- read.csv(\"data/Sample_coordinates_subpop.csv\")\n> fs_mat <- as.matrix(read.table(\"results/4.finestructure/datamatrix.txt\",header=T,\nrow.names=1, check.names = F))\n> mean_fs <- (fs_mat+t(fs_mat))/2\n> mean_fs[upper.tri(mean_fs,diag=TRUE)] <- NA\n> fs_melt <- melt(as.matrix(mean_fs))\n> fs_melt <- fs_melt[!is.na(fs_melt$value),]\n> colnames(fs_melt) <- c(\"seal1\",\"seal2\",\"coancestry\")\n> fs_distances <- merge(fs_melt,distances)\n> fs_distances$pop1 <- coords2$pop[match(fs_distances$seal1,coords2$title)]\n> fs_distances$pop2 <- coords2$pop[match(fs_distances$seal2,coords2$title)]\n#Fit regression models for within sub-regions, between sub-regions, and all data\n> within <- fs_distances$pop1 == fs_distances$pop2\n> lim1 <- lm(log10(fs_distances$coancestry[within]) ∼ fs_distances$water_distance[within])\n> lim2 <- lm(log10(fs_distances$coancestry[!within]) ∼ fs_distances$water_distance[!within])\n> lim3 <- lm(log10(fs_distances$coancestry) ∼ fs_distances$water_distance)\n> scatter1 <-\n  ggplot(fs_distances, aes(x=water_distance, y=log10(coancestry), color = (pop1==pop2) ))+\n  geom_point(alpha=0.25, size = 0.8) +\n  scale_color_manual(\"Within subpop\", values = c('palevioletred3','steelblue'))+\n  geom_abline(slope=coef(lim1)[2], intercept = coef(lim1)[1], colour = \"steelblue4\") +\n  geom_abline(slope=coef(lim2)[2], intercept = coef(lim2)[1], colour = \"palevioletred4\") +\n  geom_abline(slope=coef(lim3)[2], intercept = coef(lim3)[1], colour = \"black\") +\n  mytheme\n  > ggsave(\"results/5.waterdistance/correlation_coancestry.png\",scatter1)\nNote: The coancestry matrix is typically unsymmetrical and, as we want one pairwise distance for each sample pair, we take the mean of the two coancestry values. This is done with the command (fs_mat+t(fs_mat))/2. It would be trivial to edit the code to keep the greater of the two values.\nCombine the previously constructed plots into a single figure using the R package cowplot (Figure 4[href=https://www.wicell.org#fig4]).\n> map <- ggplot()+draw_image(\"results/5.waterdistance/distances_6samples.png\",clip=T, x = 0, y = 0) +\n  theme(panel.background = element_rect(fill = 'white'), plot.margin = unit(c(0, 0, 0, 0), \"cm\"))\n> plot_grid(map,\n    plot_grid(\n      p1 + theme(legend.position=\"top\"),\n      p2+ theme(legend.position=\"none\"),\n      scatter1+ theme(legend.position=\"none\"),\n      labels = c(\"B\", \"\", \"C\"),\n      ncol = 1, nrow = 3, rel_heights = c(1,0.9,1)),\n      ncol = 2, nrow=1, rel_widths = c(2.5, 1), labels = c(\"A\", \"\"))\n> ggsave(\"results/5.waterdistance/grid_distance.png\",width=9,height=7)\nAnalysis of genetic diversity\nTiming: 2–8 h",
    "A central part of the original study was to test whether a subdivided population retains more variation than a panmictic one, as originally suggested by Wright.31[href=https://www.wicell.org#bib31],32[href=https://www.wicell.org#bib32] For that, we studied two metrics of genetic diversity, heterozygosity (H) of individuals and nucleotide diversity (π) within subpopulations, across the genome. We found the existing tools for studying genetic diversity rather limited and ill-suited for the windowed analysis required here. More specifically, a comparison of genetic diversity across the genome has to take into account the highly variable composition of the windows and the varying number of informative sites in each window. Because of that, we ended up writing in-house tools, using R, for these analyses. The R scripts take 012-formatted genotype data and the positive mask as the input, and compute H, π and dxy using a user-defined window size.\nNote: The time estimate above depends on whether the analysis is carried out using the test dataset of 20 contigs or the full genome\nConvert the VCF-formatted variation data to the genotype format with numbers of ALT alleles (0, 1 or 2).\n> VCF=$work/processed_data/2.masking-filtering/saimaa_20_phased_posmask.vcf.gz\n> OUT=$work/processed_data/5.diversity/saimaa_20_phased_posmask\n> $vcftools --gzvcf $VCF --012 --out $OUT\nRead the data and prepare the functions for calculating H and π.\n> library(tidyverse)\n> library(data.table)\n> library(IRanges)\n> library(scales)\n> library(cowplot)\n> m <- read_table('data/reference/snpable/norppa_12122017_20ctgs.posmask.bed',\n    col_names=c(\"ctg\",\"start\",\"end\"), show_col_types=F)\n> m$start <- m$start+1\n> m$end <- m$end+1\n> f <- read.table(\"data/reference/norppa_12122017.fa.fai\")\n# read in and transpose the 012-genotypes generated with vcftools\n> d <- t(fread('processed_data/5.diversity/saimaa_20_phased_posmask.012'))\n> d <- d[-1,]\n> colnames(d) <-\n  read.table(\"processed_data/5.diversity/saimaa_20_phased_posmask.012.indv\",header=F)$V1\n> pos <- fread('processed_data/5.diversity/saimaa_20_phased_posmask.012.pos',sep=\"∖t\")\n> colnames(pos) <- c(\"ctg\",\"pos\")\n> d <- cbind.data.frame(pos,d)\n> ns <- read.table(\"data/ns_ids.txt\")$V1  # lists of sample IDs per region\n> cs <- read.table(\"data/cs_ids.txt\")$V1\n> ss <- read.table(\"data/ss_ids.txt\")$V1\n> all <- c(ns,cs,ss)\n> pi_for_set <- function(set,d){\n  n.alleles <- length(set)∗2\n  count.alt <- rowSums(d[,set]);",
    "count.ref <- n.alleles-count.alt\n  count.ref ∗ count.alt ∗ 2 / ( n.alleles ∗ (n.alleles-1) )\n}\n> pi_for_two_sets <- function(set1,set2,d){\n  n.alleles1 <- length(set1)∗2\n  n.alleles2 <- length(set2)∗2\n  count.alt1 <- rowSums(d[,set1]);\n  count.alt2 <- rowSums(d[,set2]);\n  count.ref1 <- n.alleles1-count.alt1\n  count.ref2 <- n.alleles2-count.alt2\n  ( count.ref1 ∗ count.alt2 + count.alt1 ∗ count.ref2 ) / ( n.alleles1 ∗ n.alleles2 )\n}\nCalculate H within 250 kbp windows.\n> ww <- 250000\n> dc <- c()\n> for(ctg in f[,1]){\n  d1 <- d[as.character(d[,1])==ctg,]      # select one contig at time\n  d2 <- cbind(d1,floor(d1$pos/ww)+1)    # add a new column for the window (size 250kbp)\n  colnames(d2)[dim(d2)[2]] <- \"window\"  # label it \"window\"\n    if(dim(d2)[1]>2){\n    d3 <- aggregate(d2[,3:dim(d)[2]],    # count heterozygous sites per sample and per window\n    by=list(d2$window),FUN=function(x){sum(x==1)})\n    colnames(d3)[1] <- \"window\"\n    m1 <- m[m[,1]==ctg,]          # select pos. masked regions for this contig\n    m2 <- IRanges(start=m1$start,end=m1$end)        # convert into IRanges\n    w1 <- seq(1,f$V2[f$V1==ctg],ww)      # make 250kbp windows for this contig\n    w2 <- IRanges(start=w1,width=ww)            # convert into IRanges\n    s <- rep(0,dim(d3)[1])\n    t <- rep(0,dim(d3)[1])\n      for(i in end(w2)/ww){ # iterate over each window (\"i\" being the index)\n      s[i] <- sum(width(restrict(m2,start(w2)[i],end(w2)[i])))    # sum of pos. masked regions\n      t[i] <- i                              # window index\n      }\n    s <- cbind(t,s)\n    colnames(s) <- c(\"window\",\"nsites\")\n    d4 <- merge(d3,s,by.x=1,by.y=1)        # merge the het. counts and pos. mask counts\n    dc <- rbind.data.frame(dc,cbind(ctg,d4))    # merge to the output table, save that in a file\n    }\n  }\n> write.table(dc,paste0(\"results/6.diversity/saimaa_20_H_\",ww,\".tsv\"),row.names=F,quote=F)\nNote: The routines for computing H and π count the sums (of heterozygous sites or mean pairwise differences) for each sample/set within each window. The positive mask is used to count the number of informative sites in each window.\nCalculate π within 250 kbp windows.\n> ww <- 250000\n> dc <- c()\n> ns.c <- match(ns,gsub(\"X\",\"\",colnames(d)))  # columns for each subpoops\n> cs.c <- match(cs,gsub(\"X\",\"\",colnames(d)))\n> ss.c <- match(ss,gsub(\"X\",\"\",colnames(d)))\n> all.c <- match(all,gsub(\"X\",\"\",colnames(d)))\n> for(ctg in f[,1]){",
    "d1 <- d[as.character(d[,1])==ctg,]      # select one contig at time\n  d2 <- cbind(d1,floor(d1$pos/ww)+1)    # add a new column for the window (250 kb)\n  colnames(d2)[dim(d2)[2]] <- \"window\"    # label it \"window\"\n    if(dim(d2)[1]>2){\n    pis <- cbind.data.frame(ns=pi_for_set(ns.c,d2), # compute pi for NS\n    cs=pi_for_set(cs.c,d2),\n    ss=pi_for_set(ss.c,d2),\n    all=pi_for_set(all.c,d2),\n    ns.cs=pi_for_two_sets(ns.c,cs.c,d2), # compute dxy bw. NS and CS\n    ns.ss=pi_for_two_sets(ns.c,ss.c,d2),\n    cs.ss=pi_for_two_sets(cs.c,ss.c,d2)\n    )\n  d3 <- aggregate(pis,by=list(d2$window),FUN=sum) # sum per window\n  colnames(d3)[1] <- \"window\"\n  m1 <- m[m[,1]==ctg,]                # select pos. masked regions for this contig\n  m2 <- IRanges(start=m1$start,end=m1$end)    # convert into IRanges\n  w1 <- seq(1,f$V2[f$V1==ctg],ww)        # make 250kbp windows for this contig\n  w2 <- IRanges(start=w1,width=ww)        # convert into IRanges\n  s <- rep(0,dim(d3)[1])\n  t <- rep(0,dim(d3)[1])\n    for(i in end(w2)/ww){                # iterate over each window (\"i\" = index)\n    s[i] <- sum(width( restrict(m2,start(w2)[i],end(w2)[i]) ))    # sum of pos. masked regions\n    t[i] <- i # window index\n    }\n  s <- cbind(t,s)\n  colnames(s) <- c(\"window\",\"nsites\")\n  d4 <- merge(d3,s,by.x = 1, by.y = 1)          # merge the pi/dxy counts and pos. mask counts\n  dc <- rbind.data.frame(dc,cbind(ctg,d4))        # merge to the output table, save that in a file\n  }\n}\n> write.table(dc,paste0(\"results/6.diversity/saimaa_20_pi_\",ww,\".tsv\"),row.names=F,quote=F)\nVisualize the results using ggplot2.\nPrepare the graph for H.\n# Read the windowed H values and plot their distribution in different individuals\n> het <- read.table(\"results/6.diversity/saimaa_20_H_250000.tsv\",head=T)\n> het[,3:100] <- het[,3:100]/het[,101]\n> het.bins <- apply(het[het$nsites>100000,3:(dim(het)[2]-1)], 2,\n  function(x) hist(x,breaks=c(-0.001,seq(0.0002,0.0052,0.0002),0.027),plot=F)$counts )\n> het.bins <- cbind.data.frame(bins=c(seq(0,0.00519,0.0002),0.025),het.bins)\n> het.bins.m <- gather(het.bins, variable, value, -bins)\n> colnames(het.bins.m) <- c(\"bin\",\"smp\",\"count\")\n> het.bins.m$smp2 <- \"other\"\n> het.bins.m$smp2[het.bins.m$smp==\"X2575\"] <- \"2575\"\n> h_20ctgs <- ggplot()+\n  geom_line(data=subset(het.bins.m,smp2==\"other\"),\n      aes(x=bin,y=count,group=smp,col=smp2),linewidth=0.25)+\n  geom_point(data=subset(het.bins.m,smp2==\"other\"),\n            aes(x=bin,y=count,group=smp,col=smp2),shape=20,size=0.25)+\n  geom_boxplot(data=subset(het.bins.m,smp2!=\"2575\"),\n            aes(x=bin,y=count,group=bin),color=\"darkgray\",outlier.size = 0.25)+\n  geom_line(data=subset(het.bins.m,smp2==\"2575\"),\n            aes(x=bin,y=count,group=smp,col=smp2),linewidth=0.5,linetype=\"solid\")+\n  geom_point(data=subset(het.bins.m,smp2==\"2575\"),\n            aes(x=bin,y=count,group=smp,col=smp2),shape=20,size=4)+\ntheme_classic()+theme(legend.position=\"top\")+\ncoord_cartesian(xlim=c(-0.0001,0.0051))+\nylab(\"#Windows\")+xlab(\"Nucleotide diversity (H)\")+labs(color=\"\")+xlab(\"Nucleotide diversity (H)\")\nNote: The sums of H and π within a window are converted to per-site values by dividing them by the number of informative sites within that window.\nNote: Individual 2575 was of special interest in the original study and is therefore highlighted.",
    "Prepare the graph for π.\n# Read the windowed Pi values and plot their distribution in different subpopulations\n> pi <- read.table(paste0(\"results/6.diversity/saimaa_20_pi_250000.tsv\"),head=T)\n> pi[,3:9] <- pi[,3:9]/pi[,10]\n> pi.bins <- apply(pi[pi$nsites>100000,3:9], 2,\n    function(x) hist(x,breaks=c(-0.001,seq(0.0001,0.0052,0.0001),0.027),plot=F)$counts )\n> pi.bins <- cbind.data.frame(bins=c(seq(0,0.00519,0.0001),0.025),pi.bins)\n> pi.bins.m <- gather(pi.bins, variable, value, -bins)\n> colnames(pi.bins.m) <- c(\"bin\",\"pop\",\"count\")\n> pi.bins.means <- pi.bins.m %>% filter(pop %in% c(\"ns\",\"cs\",\"ss\")) %>% group_by(bin) %>% summarise(mean(count))\n> colnames(pi.bins.means) <- c(\"bin\",\"mean.count\")\n> pi.bins.means$pop <- \"mean\"\n> pi_20ctgs <- ggplot()+\n  geom_line(data=subset(pi.bins.m,pop %in% c(\"ns\",\"cs\",\"ss\")),\n              aes(x=bin,y=count,color=pop),size=0.25,linetype=\"solid\")+\n  geom_point(data=subset(pi.bins.m,pop %in% c(\"ns\",\"cs\",\"ss\")),\n              aes(x=bin,y=count,color=pop),shape=19,size=1)+\n  geom_line(data=pi.bins.means,\n            aes(x=bin,y=mean.count,color=pop),size=1,linetype=\"solid\")+\n  geom_point(data=pi.bins.means,\n          aes(x=bin,y=mean.count,color=pop),shape=19,size=2)+\n  geom_point(data=subset(pi.bins.m,pop==\"all\"),\n          aes(x=bin,y=count,color=pop),shape=19,size=2)+\n  geom_line(data=subset(pi.bins.m,pop==\"all\"),\n        aes(x=bin,y=count,color=pop),size=1)+\n        theme_classic()+\n        theme(legend.position=\"top\", axis.text = element_text(size=13),\n        axis.title = element_text(size=14),\n        legend.text = element_text(size=13))+\n        xlim(-0.0001,0.0051)+\n        ylab(\"#Windows\")+xlab(\"Nucleotide diversity (Pi)\")+\n        labs(color=\"\")+xlab(\"Nucleotide diversity (Pi)\")+\n        scale_color_manual(breaks=c(\"ns\", \"cs\", \"ss\", \"mean\", \"all\"),\n        labels=c(\"NS\", \"CS\", \"SS\", \"Average w. regions\", \"Total Saimaa\"),\n        values=hue_pal()(5))\nNote: In the original study, the amounts of variation within the whole population (π over all samples) and within subpopulations (mean of π over subsets) were of interest and are therefore highlighted.\nPlot the graphs.\n> grid <- plot_grid(\n  h_20ctgs + theme(legend.position=\"top\"),\n  pi_20ctgs + theme(legend.position=\"top\"),\n  labels = c(\"A\", \"B\"),\n  ncol = 1, nrow = 2)\n  > ggsave(\"results/6.diversity/diversity.png\",grid)"
  ],
  "subjectAreas": [
    "Evolutionary Biology",
    "Genomics",
    "Sequence Analysis"
  ],
  "bigAreas": [
    "Molecular Biology & Genetics"
  ]
}