{
  "id": 2970,
  "origin_website": "Cell",
  "title": "Protocol for analysis of liquid chromatography-mass spectrometry metabolomics data using R to understand how metabolites affect disease",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nStep 1: Datasets and codes import\nTiming: 5 min\nImport three datasets which have been prepared in “before you begin”.\nImport “outcome.txt”.\n> outcome.d<-read.table(file=\"outcome.txt\",sep=\"\\t\",quot=\"\",fill=T,head=T,row.names=1)\nImport “metabolites.txt”.\n> meta.d.1<-read.table(file=\"metabolites.txt\",sep=\"\\t\",quot=\"\",fill=T,head=T,row.names=1)\nImport “clinic.txt”.\n> clinic.d.1<-read.table(file=\"clinic.txt\",sep=\"\\t\",quot=\"\",fill=T,head=T,row.names=1)\nReorder three datasets by the same order according to ID.\nLog-transform and reorder metabolomics data.\n> meta.d<-log(meta.d.1[rownames(outcome.d),])\nReorder clinical characteristics data.\n> clinic.d<-clinic.d.1[rownames(outcome.d),]\nImport “functions.R” file.\n> source(\"functions.R\")\nNote: \"functions.R\" contains 9 functions required for data analysis. The 9 functions are: 1) “NT” function for data distribution test; 2) “SC” function for Spearman correlation analysis; 3) “OR”, “split_dataset”, “random_dataset”, and “random_OR” functions for conditional logistic regression analysis and further validation; 4) “CM” function for causal mediation analysis; and 5) “VP” and “VPplot” functions for variance partitioning analysis. The applications of these functions are detailed in the following step-by-step protocols. All the example input data needed to reproduce the codes can be downloaded from https://doi.org/10.5281/zenodo.7601709[href=https://doi.org/10.5281/zenodo.7601709].\nNote: After imported datasets and functions.R file for analysis, the following 5 steps including data distribution test (step 2) and 4 parts of analysis (steps 3, 4, 5 and 6) can be applied separately.\nStep 2: Data distribution test\nTiming: 10 min",
    "It is important to check the distribution of research data before analysis to get an overview and comprehensive understanding of the metabolomic data and clinical parameters. Shapiro-Wilk test is a formal hypothesis test which commonly used for testing normality of data distribution and the statistic of Shapiro-Wilk test (W) is a measure of linearity of the Q–Q plot (Figures 2[href=https://www.wicell.org#fig2]A and 2B). Significant results (p < 0.05) reject the hypothesis that the population has a normal distribution, and indicate a decision that the sample comes from a nonnormal population. However, formal hypothesis tests of assumptions appear to become less useful as sample size increases. In this condition, informal judgments of normality diagnostic graphs including Q-Q plots and histograms are recommended for visualizing and judging the distribution of data (Figures 2[href=https://www.wicell.org#fig2]C and 2D).4[href=https://www.wicell.org#bib4] “NT” function facilitates users for normality test, which gives results of Shapiro-Wilk test, as well as histogram and Q-Q plots automatically for dataset with multiple variables.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2508-Fig2.jpg\nFigure 2. Results of normality test\n(A) Shapiro-Wilk test of metabolomic data (normality.test.csv).\n(B) Shapiro-Wilk test of clinical data (normality.test.csv).\n(C) Histogram and Q-Q plot of metabolomic data (normality.test.pdf).\n(D) Histogram and Q-Q plot of clinical data (normality.test.pdf).\nNormality test by “NT” function.\nNormality test for metabolomic data.\n> NT(data=meta.d)\nNormality test for clinical parameters.\n> NT(data=clinic.d)\nNote: Normality test gives the results of Shapiro-Wilk test, as well as histogram and Q-Q plots. “normality.test.csv” file contains results of Shapiro-Wilk test and “normality.test.pdf” file contains histogram and Q-Q plot of each variable (Figure 2[href=https://www.wicell.org#fig2]).\nNote: For general characteristics of clinical parameters of the study population, differences between cases and controls are commonly evaluated using the Student’s t test for continuous variables with normal distribution, paired Wilcoxon rank-sum test for those with skewed distribution, and the Chi-squared test for categorical variables.",
    "Step 3: Spearman correlation analysis\nTiming: 10 min\nCorrelation coefficients are used to measure the association between two variables. Pearson correlation is a common method used in the context of a linear relationship between two normally distributed continuous variables, while Spearman correlation can be used to measure a monotonic relationship. Considering of 1) some clinical parameters (such as smoke status/diet score) were presented by grade, 2) metabolite profiles were usually non-normal distribution, we use Spearman correlation analysis to assess the associations of individual metabolites with the baseline clinical parameters in our analysis. We provide “SC” function to calculate Spearman correlation coefficient.\nSpearman correlations analysis by “SC” function.\nCalculate Spearman's ρ and p values of Spearman correlation analysis.\n> SCresult<-SC(data1=clinic.d, data2=meta.d)\n> SCmatrix<-SCresult[[1]]\n> pvalues<-SCresult[[2]]\nSave Spearman's ρ and p values to “SCmatrix.csv” (Figure 3[href=https://www.wicell.org#fig3]A) and “SCpmatrix.csv” (Figure 3[href=https://www.wicell.org#fig3]B) file respectively.\n> write.csv (SCmatrix,\"SCmatrix.csv\")\n> write.csv (pvalues,\"SCpmatrix.csv\")\nVisualized by heatmap.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2508-Fig3.jpg\nFigure 3. Results of Spearman correlation analysis\n(A)Results of Spearman's ρ (SCmatrix.csv).\n(B) Results of p values (SCpmatrix.csv).\n(C) Heatmap of Spearman correlation analysis (SC.pdf).\n> pdf(\"SC.pdf\")\n> heatmap.2(SCmatrix,col=bluered(75),key=TRUE, symkey=FALSE, trace=\"none\", cexRow=0.8, margins=c(20,20))\n> dev.off()\nNote: Results of Spearman correlation are visualized by a heatmap named “SC.pdf” (Figure 3[href=https://www.wicell.org#fig3]C). We use hierarchical clustering analysis (with complete linkage) to group metabolites based on absolute Spearman correlations as a measure of similarity.\nResults interpretation\nRelationships with a two-tailed p value < 0.05 is considered statistically significant. Appropriate multiple tests of statistical significance could be considered according to the study design.",
    "Critical: Pearson’s is the most popular correlation coefficient, and works well if a linear relationship is suggested. However, when data are non-normally distributed, a test of the significance of Pearson correlation coefficient may inflate type I error rates and reduce power. Normality checking (see “Data distribution test” part) would be helpful for deciding whether Pearson’s correlation analysis is applicable or not. For skewed data, log transformation might be helpful in some cases, but data transformations must be applied very cautiously. Pearson correlation coefficient can be calculated by changing method parameter “Spearman” in “SC” function into “Pearson”.\nStep 4: Logistic regression analysis\nTiming: 30 min\nRegression analysis is a way to quantify trends in data. Logistic regression analysis uses the logit model to evaluate the association between binary dependent (response) variables and a set of independent (explanatory) variables. However, in the matched data with cases and matched controls, the unconditional logistic expression is biased with overestimated odds ratio (OR). Conditional logistic regression which allows the consideration of matching and stratification, is often used for analysis in the case-control study. The associations between metabolites and risk of disease were estimated using multivariable conditional logistic regression models, with adjustment of age, sex and other clinical parameters. ORs (95%CI) were presented as per SD increment of the metabolites. R package “survival” is required and “OR” function is provided for OR calculating.\nDefine formular for logit model.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2508-Fig4.jpg\nFigure 4. Workflow and results of conditional logistic analysis",
    "Red faces represent cases in the study and black faces are the matched controls. ORs were calculated in full dataset and were validated in both sub datasets and random sampling datasets (OR.csv). Sub datasets were created by splitting the dataset into two independent subsets (dataset1_OR.csv and dataset2_OR.csv). Random sampling datasets were generated by randomly selecting samples of certain size within full datasets for multiple times (randomdataset_OR.csv).\n> form1<-case∼meta+age+as.factor(sex)+CP8+as.factor(CP9)+as.factor(CP10)+CP13+as.factor(CP11)+as.factor(CP12)+CP14+CP1+CP2+CP3+CP6+strata(match)\nCritical: An example formular was given below which adjusted for age, sex and other clinical parameters (CPs). The users should define their own formular according to their variables.\nLogistic regression analysis by “OR” function.\nCalculate ORs.\n> ORresult<-OR(form=form1, meta.d=meta.d, clinic.d=clinic.d, outcome.d=outcome.d)\nSave the results to “OR.csv”file (Figure 4[href=https://www.wicell.org#fig4]).\n> write.csv(ORresult,\"OR.csv\")\nNote: To validate the reliability of the identified metabolites associated with the disease’s onset, internal validation tests and random sampling were usually performed as sensitivity analysis. In our study, internal validation test in two randomly split subsets were performed. Moreover, 200 times of tests were conducted by randomly selecting 80% of all samples as a subset each time. “split_dataset” function is created for splitting dataset to 2 sub datasets in customized proportion. “random_dataset” function is used for generating random sampling dataset and “random_OR function” is used for conditional logistic analysis in random sampling dataset. Figure 4[href=https://www.wicell.org#fig4] shows the workflow of conditional logistic regression and validation.\nValidate ORs in two randomly split subsets.\nGenerate two randomly split subsets as dataset1 and dataset2.\n> split.d<-split_dataset(n.seed=1, proportion.subdataset1 = 0.5)\nNote: We use 50% participants in each subset as example (proportion.subdataset1 = 0.5). Users could define their size of sub datasets by setting the parameter “proportion.subdataset1”.\nCalculate ORs for dataset1 and dataset2.\n> t_ORresult<-OR(form=form1,meta.d=split.d[[1]],clinic.d=split.d[[2]],outcome.d=split.d[[3]])\n> v_ORresult<-OR(form=form1,meta.d=split.d[[4]],clinic.d=split.d[[5]],outcome.d=split.d[[6]])\nExport the results of 2 subsets to “dataset1_OR.csv” and “dataset2_OR.csv” (Figure 4[href=https://www.wicell.org#fig4]).\n> write.csv(t_ORresult,\"dataset1_OR.csv\")\n> write.csv(v_ORresult,\"dataset2_OR.csv\")",
    "Validate ORs in random sampling datasets.\nCalculate and statistic ORs of random sampling datasets.\n> random_ORresult<-random_OR(times.random=200, percentage =0.8)\nNote: We use 200 times of tests for 80% randomly selected samples as example (times.random = 200, percentage = 0.8). Users could define their times of sampling and size of each sampling by setting the parameters “times.random” and “percentage”.\nSave the results to “randomdataset_OR.csv” file.\n> write.csv(random_ORresult,\"randomdataset_OR.csv\")\nNote: In “randomdataset_OR.csv” file, the number of significant ORs (including p < 0.05 and fdr<0.05) was count to assess the repeatability of logistic analysis (Figure 4[href=https://www.wicell.org#fig4]).\nResults interpretation\np values for false discovery rate (FDR) were estimated using the Benjamini-Hochberg method. ORs with FDR < 0.05 are considered to be significant.\nStep 5: Causal mediation analysis\nTiming: 9 h\nAfter previous Spearman correlation analysis (step 3) and logistic analysis (step 4), significant associations on metabolites-disease and metabolites-clinical risk factors have been found. However, it is still unclear that which clinical risk factors could modify the association between level of metabolites and disease risk. Causal mediation analysis can provide causal interpretation for effect of metabolites on disease’s onset.",
    "To calculate the total, direct and indirect effects of exposure on outcome, causal mediation analysis compared 2 regression models in which one model is conditioned on the mediator and the other is not (Figure 5[href=https://www.wicell.org#fig5]A). The following 5 steps are included: 1) examine associations between exposure and mediator (path exposure to mediator, the estimate a in Figure 5[href=https://www.wicell.org#fig5]A); 2) examine associations of mediator as independent variable with outcome (path mediator to outcome, the estimate b in Figure 5[href=https://www.wicell.org#fig5]A); 3) assess simple total effect of exposure on outcome (path exposure to outcome, the estimate c in Figure 5[href=https://www.wicell.org#fig5]A); 4) assess the direct effect of exposure on outcome with mediators controlled (path exposure to outcome not through the mediators, the estimate c’ in Figure 5[href=https://www.wicell.org#fig5]A); and 5) calculate the indirect effect of exposure on outcome by mediators (path exposure to outcome through the mediators, the estimate a×b in Figure 5[href=https://www.wicell.org#fig5]A).5[href=https://www.wicell.org#bib5] It should be noted that total effect = direct effect + indirect effect, in other words, c = c’ + ab.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2508-Fig5.jpg\nFigure 5. Causal mediation analysis\n(A) Mediation model.\n(B) Results of causal mediation analysis (causal mediation.csv).\nR package “mediation” and “CM” function we provided are required for causal mediation analysis.\nDefine formulars for causal mediation analysis.",
    "Note: Three models are required for causal mediation analysis as follows: Y = cX + e1, M = aX + e2, and Y = c’X+ bM + e3, where Y as outcome, X as exposure, M as mediator, c as total effect of metabolites, c’ as direct effect of metabolites, and ab as indirect effect of metabolites mediated by risk factor. Here we give example formulars for performing causal mediation analysis on path of metabolites (exposure)-clinical risk factors (mediator)-disease (outcome). It helps us to get better understanding of potential biological roles of metabolites by assessing their indirect effects on disease operated through clinical risk factors. Users should define their own formulars basing on their hypothesis of exposure, mediator and outcome, respectively.\nDefine formular: M = aX + e2.\n> form.a<-clinic∼meta\nDefine formular: Y = c'X + bM + e3.\n> form.b<-case∼meta+clinic\nDefine formular: Y = cX + e1.\n> form.c<-case∼meta\nPerform causal mediation analysis by “CM” function.\n> CMresult<-CM (form.a, form.b, form.c)\nWrite the results to “causal mediation.csv” file.\n> write.csv (CMresult,\"causal mediation.csv\", row.names=F)\nResults interpretation",
    "The following criteria need to be satisfied for a variable to be considered as a mediator (Figure 5[href=https://www.wicell.org#fig5]B): 1) the exposure variable significantly affects the mediator (beta and p values of estimate a in “causal mediation.csv” file); 2) there is a significant relationship between the mediator and the outcome (beta and p values of estimate b in “causal mediation.csv” file); 3) the exposure variable significantly affects the outcome (beta and p values of estimate c in “causal mediation.csv” file). Direct effects ADE and corresponding p values are also given in “causal mediation.csv” file. ACME represents for indirect effects and Pacme < 0.05 is defined as significant causal mediation relationships. The mediation proportion (prop in “causal mediation.csv” file) is used to assess how much of the effect of metabolites on disease is mediated through clinical measurements. Generally, mediated proportion >10% is recognized as substantial mediation effects.\nStep 6: Variance partitioning analysis\nTiming: 10 min\nVariance partitioning is a statistical method to quantify the contribution of multiple sources of variation and decouple within/between-individual variation. Here we assessed the variance in metabolite levels explained by each clinical measurement at baseline by variance partitioning. R package “variancePartition” and “VP” function are required for variance partitioning calculation.\nDefine formular for variance partitioning analysis.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2508-Fig6.jpg\nFigure 6. Results of variance partitioning analysis\n(A) Bar plot of the variance which explained the most of the total variation of metabolite levels (VPplot.pdf).\n(B) Percentage of variance contribution for metabolites (VP_var.csv).\n(C) Sign of beta in linear model regression of clinical parameters and metabolites (VP_corrtype.csv).\n> form1<-meta∼age+sex+CP8+CP9+CP10+CP13+CP11+CP12+CP14+CP1+CP2+CP3+CP6\nNote: All clinical variables which might related to levels of metabolites were included here. Users should define their own formular according to clinical variables of interest.\nPerform variance partitioning analysis by “VP” function.\nCalculate variance contribution and correlation type.\n> VPresult<-VP(form1)",
    "> vars<-Vpresult[[1]]\n> corr_type<-Vpresult[[2]]\nSave the results of variance contribution and correlation type to “VP_var.csv” (Figure 6[href=https://www.wicell.org#fig6]B) and “VP_corrtype.csv” (Figure 6[href=https://www.wicell.org#fig6]C).\n> write.csv(vars,\"VP_var.csv\")\n> write.csv(corr_type,\"VP_corrtype.csv\")\nFind the biggest variance.\nExtract the variance explained the most of the total variation.\n> name_max=array(,c(nrow(vars),1))\n> var_max=array(,c(nrow(vars),1))\n> corr_type_max=array(,c(nrow(vars),1))\n> for(I in 1:nrow(vars)){\nk<-which.max(vars[i,]) name_max[i,1]=colnames(vars)[k] var_max[i,1]=vars[i,k] corr_type_max[i,1]=corr_type[i,k]\n}\n> rownames(name_max)<-rownames(var_max)<-rownames(corr_type_max)<-rownames(vars)\n> colnames(name_max)=\"name\"\n> colnames(var_max)=\"var\"\n> colnames(corr_type_max)=\"corr_type\"\nVisualize the max variance contribution for each metabolite as bar plot (Figure 6[href=https://www.wicell.org#fig6]A).\n> pdf(\"VPplot.pdf\")\n> VPplot(var=var_max,corr_type=corr_type_max)\n> dev.off()\nResults interpretation\nVariance explained over 2% of total variation are recognized as prominent. If a metabolite can be rarely explained by traditional clinical measures, these metabolites might be meaningful as new prediction factors since it is hardly replaced by traditional clinical measures."
  ],
  "subjectAreas": [
    "Health Sciences",
    "Metabolism",
    "Bioinformatics"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Bioinformatics & Computational Biology"
  ]
}