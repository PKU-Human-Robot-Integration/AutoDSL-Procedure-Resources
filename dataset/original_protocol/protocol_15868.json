{
  "id": 19694,
  "origin_website": "Wiley",
  "title": "Testing Graphical Causal Models Using the R Package “dagitty”",
  "procedures": [
    "Testing a DAG in dagitty requires two inputs: (1) the hypothesized, graphical DAG model to test, and (2) the corresponding data. This protocol describes how to obtain the former. Although it is possible to enter a DAG in the R package directly, the easiest way to construct a DAG is to use the web interface at dagitty.net—where the model can be exported for easy loading into R.\nNecessary Resources\nHardware\nAny computer that can run R and a web browser\nSoftware\nWeb browser: any major standards-compliant browser\nOperating system: any operating system that can run R and a web browser\nR: version 3.6 or higher\nR package ‘dagitty’ version 0.3.0 or higher\nFiles\nNone\n1. Go to dagitty.net. You will see a caption Launch, where you can click on Launch dagitty online in your browser. This will open up the GUI with an example DAG already drawn (Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0001]). The GUI has a left, upper, and right menu, as well as the canvas where the DAG is drawn—see Figure 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0001]. Note that the upper menu has a “How to” that can be used in addition to this tutorial.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/42d099d3-1dbb-45f0-81cd-c778ecc43eea/cpz145-fig-0001-m.jpg</p>\nFigure 1\nThe dagitty web GUI. Use this interface to build a DAG and export it for further processing in the R package.\n2. In the upper menu, click Model > New model. This will clear the canvas to draw a new DAG from scratch. Instead of drawing a DAG from scratch, users can also load and continue with a previous DAG by copying and pasting code in the Model code tab in the right menu. See step 8 for details.\n3. Click anywhere on the empty canvas to add a new variable to the DAG. Give it a name and click “OK.” Repeat to include all variables of the DAG.",
    "IMPORTANT NOTE: Although it is possible to give a variable a name with spaces in it, we recommend that users avoid this if they wish to test their DAG using the R package. For example, rather than naming a variable My variable, use ‘camel case’ and type MyVariable. Furthermore, the names in the DAG model should correspond to the column names in the dataset used. While it is possible to rename variables in either the DAG model code or the imported data later on, users may save time by entering the correct variable names at this point.\n4. Optional: Rearrange variables on the canvas if necessary. To select a variable, click it. You can delete or rename it in the Variable panel on the upper left of the page. To move it, just drag it to its new place.\n5. Add edges. For example, to add a directed edge from a variable A to a variable B, first click once on A and then once on B. Users can remove an existing edge by clicking once more first on A, then on B. To make a bidirected edge, first add the edge A→B as described, and then add the reverse edge B→A by clicking B, then A. Note that a bidirected edge A↔B in a DAG is shorthand for a structure A←U→B, where A and B are correlated because they have some (unobserved) common cause U.\n6. Optional: Clean up the DAG layout. For DAGs with many variables and edges, it can be difficult to find a clear and “readable” layout that properly shows the model. In the upper menu, try clicking Layout > Generate layout automatically to auto-generate a layout. Users can try this step multiple times until they find a satisfactory option.",
    "7. Optional: Save an image of your DAG. Click the Model tab in the upper menu, and select your export option of choice. Users can save their DAG in PDF, PNG, JPEG, or SVG format. Alternatively, LateX users can export LateX code for drawing the DAG using the tikz package (Tantau, 2007[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0012]).\n8. Optional: Save the DAG for later use. Copy-paste the text in the Model code tab in the right menu into any text editor, and save it for later use. This code can be used for importing the DAG into R or in the dagitty web interface, see step 2.\n9. Open R and load the dagitty package:\n         \nlibrary( dagitty )\nFirst-time users first need to install the dagitty package—please refer to Support Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-prot-0002].\n10. Import your DAG into R and save it into a variable myDAG using the following command:\n         \nmyDAG <- dagitty( \"[code here]\" )\nIn the place of [code here], copy-paste the code from the Model code tab in the right menu of the dagitty web GUI. The entire code becomes something like this:\n         \nmyDAG <- dagitty( 'dag {\nbb=\"0,0,1,1\"\nAnotherVariable [pos=\"0.571,0.261\"]\nMyVariable [pos=\"0.250,0.265\"]\nMyVariable -> AnotherVariable\n}' )\nDo not forget the quotation marks around the code as copy-pasted from the web GUI. Your DAG is now ready for use in the R package; see the other basic protocols in this article.\n11. Optional: Plot the DAG to verify that it has been successfully imported:\n         \nplot( myDAG )",
    "For users unfamiliar with R, this protocol describes how to get started with R and install the dagitty and lavaan packages.\nNecessary Resources\nHardware\nAny computer that can run R and a web browser\nSoftware\nWeb browser: any major standards-compliant browser\nOperating system: any operating system that can run R and a web browser\nR: version 3.6 or higher\nR package ‘dagitty’ version 0.3.0 or higher\nFiles\nNone\n1. Download and install R on your computer. Visit https://cloud.r-project.org/[href=https://cloud.r-project.org/] and follow the instructions.\n2. Download and install RStudio from https://rstudio.com/products/rstudio/download/[href=https://rstudio.com/products/rstudio/download/]. Open RStudio; you will see a GUI as in Figure 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0002].\nThe RStudio window has several areas, including the console and plotting area (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0002]). When the bottom line in the console reads >, this means R is ready for users to type their commands.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/ac18d4fe-2668-4a09-8e19-50fad6d635ac/cpz145-fig-0002-m.jpg</p>\nFigure 2\nPlotting a DAG using dagitty. In RStudio, commands are supplied to the console (bottom left), and plots appear in the bottom right window.\n3. Install the dagitty package. In the console (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0002]), type:\n         \ninstall.packages( \"dagitty\" )\nand hit Enter. This will install the dagitty package in R.\nNote that a working internet connection is required to download and install R packages. The install.packages() command may ask users to choose a “CRAN mirror” from where to download the package. If this occurs, type 1 and hit enter. This will install the package from the cloud, which is always a safe option. You will see text appearing in the console while the package is installing—this is normal. Wait until you see > again, which means the installation is finished.\n4. Load the dagitty package by typing into the console (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0002]):\n         \nlibrary( dagitty )",
    "dagitty should now be ready for use. By default, R does not load all functions it comes with because this would be inefficient. Users therefore have to load a package first if they wish to use it.\n5. If all went well, you should now be able to plot a first DAG. Try:\n         \nmyDAG <- dagitty( \"dag {A -> B -> C A -> C}\" )\nplot( myDAG )\nA DAG with three variables (A, B, and C) should now appear in the plot area (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0002]). Beware of typos when entering commands in R. Like any programming language, R is not very smart and does not understand you when you type something slightly different from what you intended. For example, if you have called your dag myDAG, R will not understand you if you try to plot(MyDAG): it is case sensitive.\n6. You can open a help page by typing ? immediately followed by the function name. For example, try ?dagitty to find out what the dagitty() function does.\n7. Install the lavaan package. Similar to step 3, in the console (Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0002]), type:\n         \ninstall.packages( \"lavaan\" )\nand hit Enter. The installation can be verified by loading the package using:\n         \nlibrary( lavaan )\nAfter successful installation, this command should not give any error messages.",
    "This protocol describes how to test the implied conditional independencies of a DAG against a dataset with only categorical variables. We use the DAG shown in Figure 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0003] along with simulated data to show an example of testing. We also provide a supplementary tutorial in Supporting Information, which contains a specific, worked-out example to illustrate how statistical testing and domain knowledge can be combined to test a DAG.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/380e86b4-1bdb-436a-8ce7-9dba9955d0af/cpz145-fig-0003-m.jpg</p>\nFigure 3\nTesting a DAG against categorical data. An example DAG with categorical variables is plotted.\nNecessary Resources\nHardware\nAny computer that can run R and a web browser\nSoftware\nWeb browser: any major standards-compliant browser\nOperating system: any operating system that can run R and a web browser\nR: version 3.6 or higher\nR package ‘dagitty’ version 0.3.0 or higher\nFiles\nbrca.txt (download at: https://github.com/ankurankan/2020-dagitty-manual/[href=https://github.com/ankurankan/2020-dagitty-manual/])\n1. Open R and load the dagitty package using:\n         \nlibrary( dagitty )\n2. Import the DAG to be tested into R as described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-prot-0001] (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0003]).\n         \nmyDAG <- dagitty( 'dag {\nAge -> Irradiation\nAge -> Menopause\nIrradiation -> Recurrence\nMenopause -> Recurrence\n}' )\nWe recommend plotting the DAG at this point, to check that it has been correctly imported:\n         \nplot( myDAG )\n3. Import the data. In this case, since our data is a text file, we can use the read.table function:\n         \ndata <- read.table( \"brca.txt\", header=TRUE )",
    "The exact commands required to import data in R depend on how the data are stored. Since dagitty is an R package, we assume here that readers are familiar with the methods for importing data into R. If not, readers can refer to ?read.table to see the options for reading in data—or to the free book Grolemund & Wickham (2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0004]), which is an excellent resource for learning how to handle data in R. We also provide a simulated example dataset as a text file with a format that can be imported using the line of code mentioned above, so readers unfamiliar with R can use this example to get their data in the correct format before loading into R.\n4. Test if implied independencies are contradicted by the data (Fig. 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0004]). To do this, we will first generate a list of the independencies that are implied by the DAG.\n         \nimpliedConditionalIndependencies( myDAG )\nThis will show that there are two conditional independencies that are implied: “Age ⊥ Recurrence | Irradiation, Menopause” and “Irradiation ⊥ Menopause | Age.” To test these, we use the command ciTest as follows:\n         \nciTest( \"Age\", \"Recurrence\", c( \"Irradiation\", \"Menopause\" ),\ndata, type= \"cis.chisq\" )\nciTest( \"Irradiation\", \"Menopause\", \"Age\", data, type=\"cis.chisq\" )\nSince the variables are categorical, a chi-square test is used to test for correlations between them. If one of the independencies is contradicted, this is reflected by a low p-value and high RMSEA and χ2. See Guidelines for Understanding Results for more information.\nThere is an alternative method available that automatically lists relevant independencies implied by the input DAG, tests them, and returns all results in a single table (shown in Fig. 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0004]):\n         \nlocalTests( myDAG, data, type=\"cis.chisq\" )",
    "In this case, the results do not support the first independence, “Age ⊥ Recurrence | Irradiation, Menopause.” This could mean, for example, that a direct effect exists from Age to Recurrence that is not mediated by either Irradiation or Menopause. It could also mean that one or both of the variables “Irradiation” or “Menopause” have not been measured sufficiently precisely to capture the corresponding indirect effects in full, which can create the illusion of a residual direct effect. See Guidelines for Understanding Results for more information.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/5e0f58b3-7fdd-4a9d-93e1-4c8daa611a90/cpz145-fig-0004-m.jpg</p>\nFigure 4\nTesting the provided DAG against a dataset. A fully described tutorial with this DAG and this dataset is provided in the Supporting Information.\n5. Optional: Correct the DAG if necessary. If the model is falsified by significant associations between variables that should be independent of each other, users may opt to adapt their DAG model by adding an extra arrow, such that the problematic independency is no longer implied by the model. For an example, see the tutorial in Supporting Information. However, changing models is a process that should be handled with care and always supported by domain knowledge. Significant p-values are not necessarily proof that the model is incorrect; they can also indicate problems with the data (pre)processing or the dataset itself. See Guidelines for Understanding Results for more information.",
    "In the case of continuous datasets, dagitty tests for conditional independence using linear regression. Given a conditional independence condition X⊥Y|Z, first the residuals for the regressions X∼Z and Y∼Z are computed for each data point. Then, a statistical test determines whether a significant correlation exists between the residuals. If the independence condition holds, the residuals from these regressions should be statistically independent, and therefore uncorrelated.\nSince this test relies on linear regression, it makes the following assumptions about the dataset: (1) linearity (relationships between variables are linear); (2) multivariate normality (data are distributed as a multivariate Gaussian distribution); and (3) homescedasticity (residuals of all regressions have zero mean and constant variance). If these assumptions do not hold, users may consider using transformations of the input data or use non-linear instead of linear regression (see Support Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-prot-0005]).\nThis protocol gives instructions on loading the dataset, defining the model structure, and testing the conditional independencies implied by a DAG with linearly related continuous variables. We use a flow cytometry dataset from a cellular signaling study (Sachs, Perez, Pe'er, Lauffenburger, & Nolan, 2005[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0011]) as an example. While the goal of this study was to use a structure learning algorithm to automatically learn the causal influences in cellular signaling networks, the authors also presented a “consensus network” representing the current biological knowledge about the signaling pathways. This protocol will show how this consensus network can be tested for its consistency with the data gathered by the authors.\nNote that the data used in this protocol are a logicle-transformed (Parks, Roederer, & Moore, 2006[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0007]) version of the original dataset, since data originating from flow cytometers are not normally distributed. For more details on this transformation, please refer to Support Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-prot-0005].\nNecessary Resources\nHardware\nAny computer that can run R and a web browser",
    "Software\nWeb browser: any major standards-compliant browser\nOperating system: any operating system that can run R and a web browser\nR: version 3.6 or higher\nR package ‘dagitty’ version 0.3.0 or higher\nFiles\nprotein-signal.csv (download dataset at: https://github.com/ankurankan/2020-dagitty-manual/[href=https://github.com/ankurankan/2020-dagitty-manual/])\n1. Open R and load the dagitty package.\n         \nlibrary( dagitty )\n2. Import the dataset into R.\n         \ndata <- read.csv( file=\"protein_signal.csv\", header=TRUE )\nSince the example data file is in csv format, we are using the read.csv function. The file argument specifies the name of the data file; header = TRUE specifies that the first row of the file contains the column names. If you are working with a non-csv format file, please refer to the R documentation for instructions to load it into R.\n3. Import the DAG to be tested into R as described in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-prot-0001]\nmodel <- dagitty( 'dag {\nbb=\"-0.5,-0.5,0.5,0.5\"\nP38 [pos=\"-0.155,-0.141\"]\nPIP2 [pos=\"-0.337,0.063\"]\nPIP3 [pos=\"-0.278,-0.068\"]\nPKA [pos=\"-0.127,-0.200\"]\nPKC [pos=\"-0.111,-0.287\"]\nErk [pos=\"-0.061,-0.001\"]\nAkt [pos=\"-0.115,0.052\"]\nJnk [pos=\"-0.208,-0.149\"]\nPlcg [pos=\"-0.337,-0.177\"]\nMek [pos=\"-0.063,-0.096\"]\nRaf [pos=\"-0.066,-0.204\"]\nPIP2 -> PKC [pos=\"-0.485,-0.379\"]\nPIP3 -> PIP2\nPIP3 -> Akt\nPIP3 -> Plcg\nPKA -> P38\nPKA -> Erk\nPKA -> Akt\nPKA -> Jnk\nPKA -> Mek\nPKA -> Raf\nPKC -> P38 [pos=\"-0.166,-0.227\"]\nPKC -> Jnk [pos=\"-0.188,-0.258\"]\nPKC -> Mek [pos=\"-0.021,-0.245\"]\nPKC -> Raf\nPlcg -> PIP2\nPlcg -> PKC [pos=\"-0.248,-0.271\"]\nMek -> Erk\nRaf -> Mek\n}' )\nThe imported model can be plotted in R using plot(model) to verify the structure. The plot for the model defined above is shown in Figure 5[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0005].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/deb72fee-58a3-4720-af65-cbfc54b98dc2/cpz145-fig-0005-m.jpg</p>\nFigure 5\nThe structure of the model.\n4. After defining the model and importing the data, use the localTests function to test whether all the implied conditional independencies of the model hold in the data.\n         \nres <- localTests( x=model, data=data, type=\"cis\" )\nprint( res )",
    "The output of localTests is shown in Figure 6[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0006]. The localTests function requires the model and data to be specified. Additionally, users can specify the test type. In the case of continuous data, to use the linear-regression based test, the type argument should be specified as cis.\nThe localTests function outputs the Pearson correlation coefficient, p-value, and confidence interval of the correlation coefficient for each of the conditional independencies implied by the model structure. The Pearson's correlation coefficient varies between −1 and 1, with 0 implying no correlation and −1 or 1 implying a perfect linear correlation. The p-value for the test indicates the probability of observing the given dataset under the hypothesis that the independence condition is true. Hence, a correlation coefficient of around 0 with a high p-value (>0.05) is typically observed if the conditional independence holds in the data. By contrast, a high value of the correlation coefficient with a low p-value suggests that the conditional independence does not hold in the dataset. The 2.5% and 97.5% columns indicate the 95% confidence interval for the correlation coefficient; the more narrow the confidence interval and the further it is away from zero, the stronger the evidence that conditional independence does not hold.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/49529474-6d01-4ca6-a550-69c9a88db8f7/cpz145-fig-0006-m.jpg</p>\nFigure 6\nExample code showing some of the output of localTests. Note that the real output table is much longer and we are showing only a few lines of it here for simplicity.\n5. Optional: Test only some of the implied conditional independencies.\n         \nres <- localTests( x=model, data=data,\nmax.conditioning.variables=2 )\nprint( res )",
    "In the case that we do not wish to test all the implied conditional independencies of the model, the max.conditioning.variables argument of localTests allows users to specify a subset of independencies to test. In the example above, only independence conditions with less than 3 conditional variables are tested.\n6. Optional: Plot test results using the plotLocalTestResults method to visually inspect the correlation coefficient and its confidence interval. This can be useful if there are many test results to consider. Here, we will generate a plot that will focus on the 20 most problematic test results (i.e., the ones for which the effect sizes are the furthest way from 0).\nTo do this, we first sort the test results by their effect size:\n         \nres <- res[order( abs( res$estimate ) ),]\nNext, we plot the 20 results with the largest effect size:\n         \nplotLocalTestResults( tail( res, 20) )\nThe resulting plot shows the point estimate of the correlation coefficient along with its confidence interval for each conditional independence tested (Fig. 7[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0007]). From the result, it is very clear that the independence between the Akt and Erk protein, which is implied by the consensus network, is very strongly contradicted by the data with effect sizes of more than 0.9. Although there are other violations of independence implications, those are far more modest in comparison. For more details on interpreting these results, refer to Guidelines for Understanding Results.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/eb7e5c19-f8ab-418b-afb1-c15f271c439d/cpz145-fig-0007-m.jpg</p>\nFigure 7\nThe plot generated by the plotLocalTestResults.",
    "In the previous protocol for testing the model against continuous data, we used a linear regression-based test that is only valid when the relationship between the variables is approximately linear. In this protocol, we describe methods for testing conditional independencies against continuous data when the relationship between variables is not linear. There are, broadly, two approaches to such situations: (1) transform variables to a different scale, on which relationships become linear; (2) employ non-linear regression techniques. Since the choice of data transformation is highly dependent on the distribution of the dataset, we suggest that readers refer to the relevant literature for commonly used transformations for a specific type of dataset. Here, we show examples of using: (1) the log transformation, a very common general-purpose transformation applied to data that is approximately log-normally distributed; and (2) the logicle transformation (Parks et al., 2006[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0007]), a biexponential transformation that is frequently applied to flow cytometry data of the type used in Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-prot-0003]. For the non-linear regression, we describe the method using LOESS (Locally Estimated Scatterplot Smoothing; Cleveland, 1979[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0001]).\nNecessary Resources\nThis protocol has the same prerequisites as Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-prot-0004] and assumes that users have already performed steps 1-3 of that protocol to load their data and model into R. In addition, step 2 of this protocol requires the R package ‘premessa’ and the Bioconductor package ‘flowCore’. Please see https://github.com/ankurankan/2020-dagitty-manual/[href=https://github.com/ankurankan/2020-dagitty-manual/] for installation instructions.",
    "1. Optional: Log-transform one or more of the variables such that non-linear variable relations become approximately linear after transformation. Variables can be transformed if they have log-normal distributions (such that the transformed variable is normally distributed and fulfills the normality condition of linear regression). The quantile-quantile plot (QQ plot) is a useful tool to assess normality of a variable and its log-transformed version. Normally distributed data roughly follow a straight line on a QQ plot. When the data instead have a log-normal distribution, the QQ plot looks more similar to a parabola. When log transformation succeeds in making the distribution of a variable approximately normal, the QQ plot of the transformed variable should be approximately linear (Fig. 8[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0008]).\nAs an example, we show the effect of log-transformation on a simulated log-normally distributed variable.\n         \nX <- rlnorm( 500 )\npar( font.main=1, cex.main=1, bty='l',\nmfrow=c( 1, 2 ), pch=19, mar=c( 4, 4, 2, 1 ) )\nqqnorm( X, main=\"Before log transformation\", cex=.5 )\nqqnorm( log( X ), main=\"After log transformation\", cex=.5 )\nThe code snippet above first generates 500 random samples from a log-normal distribution with mean 0 and standard deviation 1 on the log scale. The par function sets the parameters for our plot, and then we plot the qqplots for the variable “X” and its log-transformed version.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6f746b51-ea20-4488-8e5d-0123488b31be/cpz145-fig-0008-m.jpg</p>\nFigure 8\nUsing a QQ-plot to assess normality of a variable.",
    "2. Optional: The logicle transformation (Parks et al., 2006[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0007]) is commonly applied to flow cytometry datasets. Data from flow cytometry are often roughly log-normally distributed, making log-transformation a possible alternative. However, the signal from flow cytometers actually follows a more normal distribution for small values and only becomes log-normal for large enough values. Furthermore, spillover compensation can make some of the values negative, such that log transformation can no longer be applied. Logicle transformation solves these problems by doing asymptotic linear transformation near 0 values and asymptotic log transformation at higher (positive or negative) values. The transformation is actually parametric, but its two parameters are typically estimated from the data at hand.\nIn the example below, we use the implementation of logicle transform from the Bioconductor R package flowCore (Ellis et al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0003]).\n         \nlibrary( readxl )\nlibrary( flowCore )\nlibrary( premessa )\ndataset <- as.matrix( read_excel( \"cd3cd28.xls\" ) )\nd_flow <- as_flowFrame( dataset )\nlgcl <- estimateLogicle( d_flow,\nchannels=colnames( d_flow))\nd_trans <- exprs( transform( d_flow, lgcl ) )\npar( font.main=1, cex.main=1, bty=\"l\",\nmfrow=c( 1, 2 ), pch=19, mar=c( 4, 4, 2, 1 ) )\nqqnorm( dataset[, 4], main=\"Before logicle transformation\", cex=.5 )\nqqnorm( d_trans[, 4], main=\"After logicle transformation\", cex=.5 )\nIn the code snippet above, we start with the loaded dataset d_flow, and then first convert it to a flowFrame object using the as_flowFrame function from the package premessa. We then estimate the parameters for the logicle transformation using the estimateLogicle function. The estimateLogicle function accepts the channels argument to specify the variables to transform. The transform function then applies the transformation with the estimated parameters on the data. In Figure 9[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0009], we show the effect of the transformation on one of the variables of the flow cytometry dataset from Sachs et al. (2005[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0011]).\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4599bda6-2d74-492d-94d6-a894af1e7f63/cpz145-fig-0009-m.jpg</p>\nFigure 9",
    "Using a QQ-plot to assess normality of a variable.\n3. Optional: Use LOESS. In some cases, the non-linear relationship is more complex and a data transformation does not suffice to make it linear. In these cases, users can compute regression residuals with LOESS rather than linear regression. LOESS fits simple models to localized subsets of data to build a (non-linear) fitting function that describes the deterministic part of variation in the data.\nTo use LOESS for testing, the type parameter needs to be specified as cis.loess:\n         \nlocalTests( myDAG, data, type=\"cis.loess\",\nR=100, max.conditioning.variables=4 )\nIn this case, the confidence interval of the estimate is computed using bootstrapping, which is controlled by an additional parameter R giving the number of bootstrap replicates. Additional parameters for LOESS can also be specified in localTests by passing them to the loess.pars argument. A restriction of using this method is that the implementation works only for conditional independencies with less than 5 conditional variables. Hence, the implied conditional independencies will need to be filtered using the parameter max.conditioning.variables before testing them.",
    "The previous protocols described DAG testing in cases where the dataset contained either only categorical or only continuous variables. In this protocol, we show how to test models against datasets containing both categorical and continuous variables. This method involves computing the so-called polychoric correlation (Pearson, 1900[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0010]) matrix of the dataset, which is used to test whether the implied conditional independencies of the model hold in the data. When we compute the polychoric correlation, we assume that each of the categorical variables in the dataset has been derived by “binning” some underlying “latent” variable, which we assume is continuous and normally distributed. For example, assume we have in our dataset a categorical variable education level with the categories Non-HS-Grad, HS-grad, College-Associate, Academic-Degree. We can then assume that each person's education level has an underlying continuous distribution, where people with similar education levels have the same graduation or degree. This underlying education level is then a “latent” continuous variable (that is, we do not observe it directly), which we can use for our polychoric correlation.\nThis approach works for so-called ordinal categorical variables, where the categories have a natural ordering (e.g., from low to high). Yet the dataset may also contain variables that are non-ordinal (e.g., “color,” where there is no natural ordering of “red,” “blue,” and “yellow”). Polychoric correlation works with continuous and ordinal variables, and can also be used with binary variables because binary variables can be given an arbitrary ordering. The approach cannot directly incorporate categorical variables with more than two levels. If there are few such variables, users could decide to forego testing conditional independencies that involve them, or consider merging some of the categories to allow them being tested to some extent.",
    "To illustrate the polychoric correlation technique, we chose a dataset that is easy to interpret for readers from different backgrounds rather than a specific bioinformatics dataset as in the previous protocol. This choice will allow us to focus on the actual technique rather than the specific nature and meaning of the data. For this purpose, we use the “adult income” dataset (Kohavi, 1996[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0006]) available at the UCI machine learning repository (Dua & Graff, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-bib-0002]). We will show the required preprocessing steps for both ordinal and non-ordinal variables, computing the polychoric correlation matrix, and then testing it with our model structure. For simplicity, we have pre-processed the dataset by merging and cleaning some variables, and provide the cleaned version as a supplementary csv file, adult.csv.\nNecessary Resources\nHardware\nAny computer that can run R and a web browser\nSoftware\nWeb browser: any major standards-compliant browser\nOperating system: any operating system that can run R and a web browser\nR: version 3.6 or higher\nR package ‘dagitty’ version 0.3.0 or higher\nR package ‘lavaan’ version 0.6 or higher\nFiles\nadult.csv (download dataset at: https://github.com/ankurankan/2020-dagitty-manual/[href=https://github.com/ankurankan/2020-dagitty-manual/])\n1. Open R and load the dataset.\n         \ndata <- read.csv( \"adult.csv\" )\n2. Define the ordinal categorical variables. The categories of Age, Education, HoursPerWeek, and Income in our dataset have a natural ordering, hence they are defined as ordinal variables.\n         \ndata$Age <- ordered( data$Age,\nlevels=c( \"<20\", \"20-34\", \"35-49\", \"50-65\", \">65\" ) )\ndata$Education <- ordered(data$Education,\nlevels=c( \"Non-HS-Grad\", \"HS-grad\",\n\"College-Associate\", \"Academic-Degree\" ) )\ndata$HoursPerWeek <- ordered( data$HoursPerWeek,\nlevels=c( \"<20\", \"20-39\", \"40\", \">40\" ) )\ndata$Income <- ordered(data$Income,\nlevels=c( \"<=50K\", \">50K\" ) )",
    "3. Define binary variables. The rest of the variables in the dataset are binary and we will convert them to integers. This will assign an arbitrary order to each variable: the lexicographically smaller value will be assigned a 1, and the larger value a 2. For example, the variable “Immigrant” has the values “no” and “yes,” so “no” will be translated to 1 and “yes” to 2.\n         \ndata$Race <- as.integer( data$Race )\ndata$Sex <- as.integer( data$Sex )\ndata$Immigrant <- as.integer( data$Immigrant )\n4. Optional: Deal with non-ordinal categorical variables with more than 2 categories. In the last step, we defined the non-ordinal variables with two categories as binary but this does not work for variables with more than 2 categories, and they need to be dummy encoded. In our dataset, we have the variable Marital Status with the categories: Is-Married, Never-Married, Was-Married. We decide to merge the two categories Never-Married and Was-Married into a single category and rename the resulting two categories, which can be accomplished as follows:\n         \nlevels(data$MaritalStatus) <- list(\nMarried=\"Is-Married\",\nNotMarried=c( \"Was-Married\", \"Never-married\" ) )\nThen we convert this variable to a binary number like previously:\n         \ndata$MaritalStatus <- as.integer( data$MaritalStatus )\nHere, 1 will now mean “Married” and 2 will mean “Not Married.”\n5. Compute the correlation matrix. We can use the lavCor function implemented in the lavaan package to compute the polychoric correlation matrix from the dataset as:\n         \nlibrary( lavaan )\ncorr <- lavCor( data )\nIn the current version of lavaan (0.6), the function lavCor gives a warning message: “estimation of the baseline model failed.” This warning can be safely ignored.",
    "6. Create the model structure. We can use the dagitty web interface to create the model structure and import it to R as shown in Basic Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-prot-0001]. Below, we directly give an abbreviated DAG code for reference.\n         \nlibrary( dagitty )\nmodel <- dagitty( 'dag {\nbb=\"-4.6,-3.8,3.7,3.7\"\nAge [pos=\"-2,-2.6\"]\nEducation [pos=\"1.4,0.5\"]\nHoursPerWeek [pos=\"-0.6,-0.1\"]\nImmigrant [pos=\"1.1,-2.6\"]\nIncome [pos=\"-1.6,2.7\"]\nMaritalStatus [pos=\"-3.7,-2.6\"]\nRace [pos=\"3.0,-2.5\"]\nSex [pos=\"-0.5,-2.6\"]\nAge -> { Education HoursPerWeek Income MaritalStatus }\nEducation -> Income\nHoursPerWeek -> Income\nImmigrant -> { Education HoursPerWeek }\nImmigrant <-> Race\nMaritalStatus -> Income\nRace -> Education\nSex -> { Education HoursPerWeek }\n}' )\nThe imported model can be plotted in R using plot( model ) to verify the structure. The plot for the model defined above is shown in Figure 10[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0010].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/4968d048-d800-4a10-85a5-20877d5fae8c/cpz145-fig-0010-m.jpg</p>\nFigure 10\nThe structure of the model.\n7. Test the implied conditional independencies of the model structure against the correlation matrix.\n         \nlocalTests( model, sample.cov=corr, sample.nobs=nrow( data ) )\nThe output of localTests is shown in Figure 11[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0011]. When testing using the sample covariance matrix, localTests returns an estimate, the p-value of the test, and the confidence interval for the estimate. An estimate of around 0 with a p-value higher than 0.05 would mean that the data do not provide evidence against the implied conditional independence being tested. Note that there is a strong negative relationship between being female and being married, indicating a major bias in the collection of this dataset.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/b2506747-7b7a-4d48-b5c0-8282fec4c906/cpz145-fig-0011-m.jpg</p>\nFigure 11\nExample output of localTests. Here, we only show the tests with at most 1 conditioning variable (see step 8).\n8. Optional: Test only some of the implied conditional independencies.\n         \nlocalTests( x=model, sample.cov=corr,\nsample.nobs=nrow( data ),\nmax.conditioning.variables=2 )",
    "In the case that we only want to test some of the implied conditional independencies of the model, the tests argument of localTests allows users to specify a subset of the independencies to test. In the example above, only independence conditions with less than 3 conditional variables are tested.\n9. Optional: Plot the test results.\n         \nplotLocalTestResults( localTests( model,\nsample.cov=corr, sample.nobs=nrow( data ) ) )\nSimilar to the previous protocols, all the test results can be plotted to inspect the results visually. An example output is shown in Figure 12[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.45#cpz145-fig-0012].\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/257e0e2c-cbb3-4145-b699-40318b44bc14/cpz145-fig-0012-m.jpg</p>\nFigure 12\nThe plot generated by the plotLocalTestResults."
  ],
  "subjectAreas": [
    "Bioinformatics"
  ],
  "bigAreas": [
    "Bioinformatics & Computational Biology"
  ]
}