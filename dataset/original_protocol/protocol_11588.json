{
  "id": 12810,
  "origin_website": "Jove",
  "title": "Protocol for Data Collection and Analysis Applied to Automated Facial Expression Analysis Technology and Temporal Analysis for Sensory Evaluation",
  "procedures": [
    "Ethics Statement: This study was pre-approved by Virginia Tech Institutional Review Board (IRB) (IRB 14-229) prior to starting the project.\nCaution: Human subject research requires informed consent prior to participation. In addition to IRB approval, consent for use of still or video images is also required prior to releasing any images for print, video, or graphic imaging. Additionally, food allergens are disclosed prior to testing. Participants are asked prior to panel start if they have any intolerance, allergies or other concerns.\nNote: Exclusion Criteria: Automated facial expression analysis is sensitive to thick framed glasses, heavily bearded faces and skin tone. Participants who have these criteria are incompatible with software analysis due to an increased risk of failed videos. This is attributed to the software's inability to find the face.\n1. Sample Preparation and Participant Recruitment\nPrepare beverage or soft food samples.\n\t\nPrepare intensified dairy solutions using 2% milk and suggested flavors from Costello and Clark (2009)22 as well as other flavors. Prepare the following solutions: (1) unflavored milk (2% reduced fat milk); (2) unflavored water (drinking water); (3) vanilla extract flavor in milk (0.02 g/ml) (imitation clear vanilla flavor); and (4) salty flavor in milk (0.004 g/ml iodized salt).\n\t\tNote: These solutions are used for demonstration purposes only.\nPour half ounce aliquots (~15 g) of each solution into 2 oz. transparent plastic sample cups and cap with color coded lids.\n\t\tNote: It is recommended to use transparent cups; however, it is up to the researcher's discretion.\nRecruit participants from the campus or the local community to participate in the study.\n\tNote: Participant sample size needed for a study is up to the discretion of the researcher. We recommend a range of 10 to 50 participants.\nObtain human subject consent prior to participation in the study.",
    "2. Preparation of Panel Room for Video Capture\nNote: This protocol is for data capture in a sensory evaluation laboratory. This protocol is to make AFEA data capture useful for a sensory booth setting.\nUse individual booths with a touchscreen monitor in front of them (face level) to keep their focus forward and to prevent looking down.\nUse adjustable height chairs with back support.\n\tNote: These are essential for allowing participants to be vertically adjusted and placed in a suitable range for video capture. Use stationary chairs (no rolling feature) with adjustable back height support so the participant's movements are reduced.\nSet overhead lighting at \"100% daylight\" for optimal facial emotional video capture (Illuminant 6504K; R=206; G=242; B=255).\n\tNote: To avoid intense shadowing, diffuse frontal lighting is ideal while the light intensity or color is not as relevant20. Ultimately, it is up to the discretion of the researcher, individual protocol/methodology, and environment to control lighting for capture.\nAffix an adjustable camera above the touchscreen monitor for recording.\n\t\nUse a camera with a resolution of at least 640 x 480 pixels (or higher)20. Discuss the required camera capabilities with the software provider before purchase and installation20. Note: The aspect ratio is not important20.\nSet camera capture speed to 30 frames per second (or other standard speed) for consistency.\nConnect and ensure media recording software is set up to the camera to record and save participant videos.\n3. Participant Adjustment and Verbal Directions\nHave only one participant at a time evaluate the samples in the sensory booth.\n\tNote: Testing more than one participant at the same time may interfere with the testing environment and disrupt the concentration of the participant or create bias.\nUpon arrival, give participants verbal instructions about the process and standard operating procedures.",
    "Have the participants sit straight up and against the back of the chair.\nAdjust chair height, position of the chair (distance from the camera), and camera angle so that the participant's face is captured in the center of the video recording, with no shadows on chin or around eyes.\n\t\tNote: In the sensory booth, the participant's head is roughly 20 - 24 inches away from the camera and the monitor with the face centered in the camera video feed.\nInstruct participants to remain seated as positioned and focused facing towards the monitor display. Additionally, instruct participants to refrain from any sudden movements post-sample consumption during the 30 sec evaluation period per sample.\nInstruct the participant to consume the entire beverage or liquefied food sample and swallow.\nInstruct the participant to quickly move the sample cup below the chin and down to the table immediately after the sample is in the mouth. This is to eliminate facial occlusion. Remind them to keep looking toward the monitor.\n\t\tNote: The sample carrier to deliver the sample is up to the discretion of the researcher. A straw or cup may be used. Regardless, initial facial occlusion is unavoidable because the face will be occluded or distorted due to consumption.\nInstruct the participant to follow the instructions as they appear on the touchscreen monitor. Note: Instructions are automatically sequenced as programmed into the automated sensory software.\n4. Individual Participant Process for Video Capture\nConfirm video camera is optimally capturing participant’s face while the participant is seated comfortably in the booth (before sample presentation) by viewing the computer monitor on which the video capture is displayed. Begin recording by clicking the record button on the computer monitor.\nInstruct participants to sip water to cleanse their palate.",
    "Provide treatments one at a time, starting with a baseline or control treatment (unflavored water). Identify each sample by a unique colored index card placed on top of each sample relating to the sample color code for sample treatment identification within the video.\n\tNote: Programmed guidance on the touchscreen monitor instructs participants. The instructions direct the participant through a series of standardized steps for each treatment sample.\nVia the touchscreen monitor, direct the participant to:\n\t\nHold up the associated color index card pre-consumption for sample identification in the video.\n\t\tNote: The color card is a way researchers can identify treatments in the video and mark the appropriate time frame (time zero) for sample evaluation.\nAfter holding the card briefly, place the card back on the tray.\nFully consume the sample and wait approximately 30 seconds, enforced through the programmed guidance on the monitor, while facing towards the camera.\n\t\tNote: The 30 sec controlled sampling period encompasses a time span adequate for the entire sampling evaluation period (i.e., showing the index card, opening a sample (removing the lid), consumption, and emotional capture).\nEnter their hedonic acceptability score on the touchscreen monitor (1=dislike extremely, 2=dislike very much, 3=dislike moderately, 4=dislike slightly, 5=neither like nor dislike, 6=like slightly, 7=like moderately, 8=like very much, 9=like extremely).\nRinse mouth with drinking water before the next sample process.\n5. Evaluating Automated Facial Expression Analysis Options\nNote: Many facial expression analysis software programs exist. Software commands and functions may vary. It is important to follow the manufacturer's user guidelines and reference manual20.\nSave recordings in a media format and transfer to the automated facial expression analysis software.\nAnalyze participant videos using automated facial analysis software.\n\t\nDouble click on the software icon on the computer desktop.\nOnce the program is open, click \"File\", select \"New…\", and select \"Project…\"",
    "In the pop up window, name the project and save the project.\nAdd participants to the project by clicking the \"Add participants\" icon (Person with a (+) sign). More participants can be added by repeating this step.\nAdd participant's video to the respective participant for analysis.\n\t\t\nOn the left side of the screen click the icon of the film reel with a plus (+) sign to add a video to analyze.\nClick the \"magnifying glass\" under the participant of interest to browse the video to add.\nAnalyze videos frame-by-frame under continuous calibration analysis settings in the software.\n\t\nClick the pencil icon to adjust settings at the bottom of the window, under the \"settings\" tab for each participant video.\n\t\t\nSet \"Face Model\" to General. Set \"Smoothen classifications\" to Yes. Set \"Sample Rate\" to Every frame.\nSet \"Image rotation\" to No. Set \"Continuous calibration\" to Yes. Set \"Selected calibration\" to None.\nSave project settings.\nPress the batch analysis icon (the same red and black target-like symbol) to analyze the project videos.\nSave the results once analysis is completed.\n\t\tNote: Other video settings exist in the software if researcher preference warrants another analysis method.\nConsider videos failures if serious facial occlusions or the inability to map the face persists during the specified post-consumption window (Figure 1). Additionally, if the model fails data will say \"FIT_FAILED\" or \"FIND_FAILED\" in the exported output files (Figure 2). This represents lost data since the software cannot classify or analyze the participant's emotions.\n\t\tNote: AFEA translates facial muscle motion to neutral, happy, disgusted, sad, angry, surprised and scared on a scale from 0 (not expressed) to 1 (fully expressed) for each emotion.\nExport the AFEA data output as log files (.txt) for further analysis.\n\t\nOnce analyses are complete, export the whole project.\n\t\t\nClick \"File\", \"Export\", \"Export Project Results\".",
    "When a window opens, choose the location of where the exports should be saved and save the log files (.txt) to a folder.\nConvert each participant log life to a data spreadsheet (.csv or .xlsx) to extract relevant data.\n\t\t\t\nOpen data spreadsheet software and select the \"Data\" tab.\nOn the \"Data\" tab, in the \"Get External Data\" group, click \"From Text\".\nIn the \"Address bar\", locate, double-click the participant text file to import, and follow the on screen wizard instructions.\nContinue the export process for all relevant participant files.\n6. Timestamp Participant Videos for Data Analysis\nUsing the AFEA software, manually review each participant’s video and identify post-consumption time zero for each sample. Record the timestamp in a data spreadsheet. Post-consumption is defined when the sample cup is below the participant’s chin and no longer occludes the face.\n\tNote: The placement of the timestamp is critical for evaluation. The point where the cup no longer occludes the face is the optimal recommendation and timestamps need to be consistent for all participants.\nSave the timestamp data spreadsheet (.csv) as a reference for extracting relevant data from videos.\n\tNote: Participant videos may also be coded internally in the software as \"Event Marking\".\n7. Time Series Emotional Analysis\nNote: Consider the \"baseline\" to be the control (i.e., unflavored water in this example). The researcher has the ability to create a different \"baseline treatment stimulus\" or a \"baseline time without stimulus\" for paired comparison dependent on the interests of the investigation. The method proposed accounts for a \"default\" state by using a paired statistical test. In other words, the procedure uses statistical blocking (i.e., a paired test) to adjust for the default appearance of each participant and therefore reduces the variability across participants.\nExtract relevant data from the exported files (.csv or .xlsx).",
    "Identify a time frame relevant to the study evaluation (seconds).\nManually extract respective data (time frame) from the exported participant files consulting the participant timestamp (time zero).\nCompile each participant's treatment data (participant number, treatment, original video time, and emotion response) per emotion (happy, neutral, sad, angry, surprised, scared, and disgusted) for the select time frame (seconds) in a new data spreadsheet for future analysis (Figure 3).\nContinue this process for all participants.\nIdentify the corresponding time zero from the timestamp file for each participant-treatment pair and adjust video time to a true time \"0\" for direct comparison (Figure 4, Figure 5).\n\tNote: Participant data is collected in a continuous video therefore each treatment \"time zero\" is different (i.e., unflavored water video time zero is 02:13.5 and unflavored milk video time zero is 03:15.4) in Figure 4. Due to the different treatment \"time zeroes\", the video times need to be readjusted and realigned to start at \"0:00.0\" or other standard start time in order for direct time comparison of treatment emotional response data.\nFor each participant, emotion, and adjusted time point, extract the paired treatment (e.g., unflavored milk) and control treatment (e.g., unflavored water) quantitative emotional score. In other words, align a participant's treatment and control time series of responses for each emotion (Figure 5).\nCompile all participant's information (participant, adjusted time, and paired treatment (e.g., unflavored water and unflavored milk) at each time point (Figure 6).\n\tNote: The steps below demonstrate the steps for a paired Wilcox test by hand. Most data analysis software programs will do this automatically. It is recommended to discuss the statistical analysis process with a statistician.",
    "Once the samples are reset and aligned with new adjusted video times, directly compare between the emotional results of a respective sample and the control (unflavored water) using sequential paired nonparametric Wilcoxon tests across the participants (Figure 7).\n\tNote: The new time alignment of the samples will allow for direct comparison within the 5 seconds post-consumption time frame. If a paired observation is not present in a treatment, drop the participant from that time point comparison.\n\t\nCalculate the difference between the control and the respective sample for each paired comparison using data spreadsheet management software.\n\t\tNote: The comparison will be dependent on the frame rate selected for emotional analysis in the software. The protocol demonstrates 30 individual comparisons per second for 5 seconds (selected time frame).\n\t\tNote: Use Figure 7 as a reference for columns and steps.\n\t\t\nSubtract the value of milk (e.g., unflavored milk) from the value of the control (e.g., unflavored water) to determine the difference. In the data spreadsheet management software in a new column titled \"Treatment Difference\", enter \"=(C2)-(D2)\", where \"C2\" is the control emotional values and \"D2\" is the selected treatment emotional values. Continue this process for all time points.\nCalculate the absolute value of the treatment difference. In the data spreadsheet management software in a new column, enter \"=ABS(E2)\", where \"E2\" is the Treatment Difference. Continue this process for all time points.\nDetermine the rank order of the treatment difference. In the data spreadsheet management software in a new column, enter \"=RANK(G2, $G$2:$G$25, 1)\" where \"G2\" is the Absolute Difference and \"1\" is \"ascending\". Continue this process for all time points.\nDetermine the signed rank of the rank order on the spreadsheet. Change the sign to negative if the treatment difference was negative (Column I).",
    "Calculate the positive sum (=SUMIF(I2:I25, \">0\", I2:I25) and negative sum =SUMIF(I2:I25,\"<0\",I2:I25) of the rank values.\nDetermine the test statistic. The test statistic is the absolute value lower sum.\nConsult statistical tables for Wilcoxon Signed Ranked Test Statistic using the number of observations included at the specific time and a selected alpha value to determine the critical value.\nIf the test statistic is less than the critical value reject the null hypothesis. If it is greater, accept the null hypothesis.\nGraph the results on the associated treatment graph (i.e., unflavored milk compared to unflavored water) for the times when the null hypothesis is rejected. Use the sign of the difference to determine which treatment has the greater emotion (Figure 8).\n\t\nIn the data spreadsheet management software, create a graph using the values of presence or absence of significance.\n\t\t\nClick \"Insert\" tab.\nSelect \"Line\"\nRight click on the graph box.\nClick \"select data\" and follow the screen prompts to select and graph relevant data (Figure 8).\n\t\t\tNote: The graphs will portray emotional results where the sample or control is higher and significant. Graph dependent, the emotion is higher at that specific time allowing the ability to discern how participant's emotions evolve over the 5 second time period between two samples.\n\t\t\tNote: Statistical support with a statistician is highly recommended to extract relevant data. Development of statistical coding is required to analyze emotional results.\nSubscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Behavior"
  ],
  "bigAreas": [
    "Ecology & Environmental Biology"
  ]
}