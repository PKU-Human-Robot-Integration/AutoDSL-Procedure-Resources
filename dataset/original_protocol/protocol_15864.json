{
  "id": 19690,
  "origin_website": "Wiley",
  "title": "Learned Embeddings from Deep Learning to Visualize and Predict Protein Sets",
  "procedures": [
    "This protocol serves as non-technical overview of what is available out-of-the-box through the bio_embeddings pipeline. The premise is simple: you will use software to plot protein sequences and color them by a property. For this purpose, we prepared three files for download: one containing about 100 protein sequences in FASTA format, a CSV file containing DisProt (Hatos et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0017]) classifications for these sequences (whether their 3D structure presents mostly disorder or little disorder), and a configuration file that specifies parameters for the computation. Apart from downloading these files and the steps necessary to install the bio_embeddings software, executing the computation is a single step. The following basic protocols present greater detail about the technical aspects surrounding inputs, outputs and parameters of the pipeline.\nThe output obtained by us when executing this protocol is available for comparison at http://data.bioembeddings.com/disprot/disprot_sampled[href=http://data.bioembeddings.com/disprot/disprot_sampled]; the plot file resulting from executing the steps is available at http://data.bioembeddings.com/disprot/disprot_sampled/plotly_visualization/plot_file.html[href=http://data.bioembeddings.com/disprot/disprot_sampled/plotly_visualization/plot_file.html].\nNOTE: This visualization is produced for a small sample of DisProt sequences; as such it is by no means representative of the power of the embeddings in distinguishing DisProt classes.\nMaterials\nHardware\nA modern computer (newer than 2012), with about 8 GB of available RAM, 2 GB of available disk space, and an Internet connection.\nSoftware\nWindows users may need to install Windows Subsystem for Linux (https://docs.microsoft.com/en-us/windows/wsl[href=https://docs.microsoft.com/en-us/windows/wsl]). All users should have Python 3.7 or 3.8 installed (https://www.python.org/downloads[href=https://www.python.org/downloads]).\nData\nYou will need a FASTA sequence for some proteins in DisProt, which can be downloaded from http://data.bioembeddings.com/disprot/sequences.fasta[href=http://data.bioembeddings.com/disprot/sequences.fasta]; you will need annotations of disorder content, which can be downloaded from http://data.bioembeddings.com/disprot/disprot_annotations.csv[href=http://data.bioembeddings.com/disprot/disprot_annotations.csv]; finally, you need a configuration file for the bio_embeddings pipeline, which can be downloaded from http://data.bioembeddings.com/disprot/config.yml[href=http://data.bioembeddings.com/disprot/config.yml].\n1. Ensure that all software and hardware requirements are met (see Materials, above).\nInstall Python 3.7 or 3.8 on your system using https://www.python.org/downloads[href=https://www.python.org/downloads].",
    "If you already have a Python installation with a different version (e.g., 2.7) that you must keep, consider installing Python 3.8 through Anaconda (“Anaconda Software Distribution,” 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0004]): https://docs.anaconda.com/anaconda/install[href=https://docs.anaconda.com/anaconda/install].\n2. Download required files.\nThrough your browser, navigate to http://data.bioembeddings.com/disprot[href=http://data.bioembeddings.com/disprot] and download the files: sequences.fasta, config.yml, and disprot_annotations.csv.\nNote that you might need to right click and select “Save Link As” to download the files.\nIf you prefer to use the terminal, run the following three commands:\n         \ntable:\n﻿0\nwget http://data.bioembeddings.com/disprot/sequences.fasta wget http://data.bioembeddings.com/disprot/config.yml wget http://data.bioembeddings.com/disprot/disprot_annotations.csv\n3. Create a project directory and move files into it.\nCreate a new directory called disprot on your computer and move the files downloaded in step 2 into this directory.\nWe suggest creating the directory in an easy-to-find location, for example the Downloads folder.\n4. Open a new terminal window.\nTo open a terminal on MaxOS or Linux, search for the application “Terminal” and open it. On Windows, after having installed the Windows Subsystem for Linux (https://docs.microsoft.com/en-us/windows/wsl[href=https://docs.microsoft.com/en-us/windows/wsl]), search for and open the application called “bash” through the start menu.\n5. Install bio_embeddings.\nTo install the pipeline and all of its dependencies, open a terminal window and type in the command:\n         \ntable:\n﻿0\n\"pip install ---user \"\"bio-embeddings[all]\"\"\"\nThis command may take up to 10 min to execute, depending on the speed of the connection. If you experience warnings regarding incompatible packages (e.g., “bio-embeddings requires Y>X, but you have Y Z which is incompatible”), please try using a new conda environment (see Troubleshooting).\n6. Navigate to the project directory from the terminal window.\nIf you called your project directory disprot inside the Downloads folder, the command to navigate to the directory through the MacOS and Linux Terminal is:\n         \ntable:\n﻿0\ncd ∼/Downloads/disprot\n7. Run the bio_embeddings pipeline.",
    "To start running the bio_embeddings pipeline, type the following in your terminal window:\n         \ntable:\n﻿0\nbio_embeddings config.yml\nThen, press Enter.\nThis will start a job using parameters defined in the text configuration file (config.yml; detail about the parameters in the next protocols). Opening the file with a text editor will display the following content:\n         \ntable:\n﻿0\nglobal: sequences_file: sequences.fasta prefix: disprot_sampled protbert_embeddings: type: embed protocol: prottrans_bert_bfd reduce: true discard_per_amino_acid_embeddings: true umap_projections: type: project protocol: umap depends_on: protbert_embeddings n_components: 2 plotly_visualization: type: visualize protocol: plotly annotation_file: disprot_annotations.csv display_unknown: false depends_on: umap_projections\nThere are four major text blocks, each defining a job stage. The parameters in the first block (starting with general) define where protein sequences live and where to store results. The second block (protbert_embeddings) defines parameters to generate computational representations using a language model (more in the following). The third (uma_projections) contains options to transform the representations, while the forth (plotly_visualizations) defines options to plot the proteins.\nYou should see output that resembles:\n         \ntable:\n﻿0\n\"2020-11-09 20:37:13,753 INFO Created the prefix directory disprot_sampled 2020-11-09 20:37:13,756 INFO Created the file disprot_sampled/input_parameters_file.yml 2020-11-09 20:37:13,970 INFO Created the file disprot_sampled/sequences_file.fasta 2020-11-09 20:37:14,118 INFO Created the file disprot_sampled/mapping_file.csv …\"\nPlease note that sometimes warnings may appear as dependencies used by the bio_embeddings pipeline get updated and introduce slight changes in how bio_embeddings is expected to interface with them. Warnings are usually harmless and get addressed by the bio_embeddings team within a few weeks. The command will take up to 15 min to execute and will download a 1.5-GB file in your home directory.\n8. Open the plot file.",
    "After the execution of the bio_embedding pipeline has finished, your system should automatically have opened up a browser window displaying a 2D graph of the proteins contained in the FASTA file colored by their disorder content according to DisProt (Hatos et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0017]; Fig. 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-fig-0001]). If not, you can navigate to the disprot directory, which will contain a new directory (disprot_sampled), with yet another directory (plotly_visualization), which contains the plot file as plot_file.html . You can open this file in any modern browser.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/2370ce44-79ad-4f93-837d-1a3eac7150be/cpz1113-fig-0001-m.jpg</p>\nFigure 1\n2D visualization of protein sequences with disorder annotation. The points are projections of embeddings of a subset of protein sequences contained in DisProt (Hatos et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0017]). Proteins annotated with high disorder content (red) tend to cluster to the bottom-right, while proteins annotated with little disorder content (blue) tend to cluster to the top-left. The figure is available interactively at http://data.bioembeddings.com/figures/figure_1.html[href=http://data.bioembeddings.com/figures/figure_1.html].",
    "Through this protocol, you may generate machine-readable representations (embeddings) from a set of protein sequences using the “embed” stage of the bio_embeddings pipeline. The sequence file utilized for the example was written by the prediction program DeepLoc (Almagro Armenteros, Sønderby, Sønderby, Nielsen, & Winther, 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0002]), but you can also provide your own FASTA file. Embeddings constitute an abstract encoding of the information contained in protein sequences, and are the building block of the pipeline and its analytical tools. In this protocol, we use BERT (Devlin, Chang, Lee, & Toutanova, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0013]) trained on BFD (Steinegger & Söding, 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0056]; Steinegger et al., 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0054]) to extract embeddings from protein sequences. This model is part of the ProtTrans protein LMs (Elnaggar et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0014]), referred to as ProtBERT in text or prottrans_bert_bfd in the following code. You can find out how to choose a protein LM based on your requirements on our website (http://bioembeddings.com[href=http://bioembeddings.com]). The salient output of the embed stage are the embedding files. These come in two flavors: per-residue (embeddings_file.h5) and per-protein (reduced_embeddings.h5). While the per-residue embeddings are taken directly out of the LMs, per-protein embeddings are generated post-processing the information extracted by the LM through global average pooling (Shen et al., 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0053]) on all combined per-residue embeddings of a sequence. Per-residue embeddings are useful to analyze properties of residues in a protein (e.g., which residues bind ligands), while per-protein representations capture annotations describing entire proteins (e.g., native localization).\nMaterials\nHardware\nComputer (newer than 2012), >8 GB of available RAM, ∼2 GB of available disk space\nOptional: Graphical Processing Unit (GPU) with >4 GB of vRAM and supporting CUDA® 11.0\nThis will speed up the embedding process manyfold\nInternet connection\nSoftware (MacOS and Linux)\nPython 3.7 or 3.8 (https://www.python.org/downloads[href=https://www.python.org/downloads])\nWindows users: Windows Subsystem for Linux (https://docs.microsoft.com/en-us/windows/wsl[href=https://docs.microsoft.com/en-us/windows/wsl])",
    "Optional: CUDA® (https://developer.nvidia.com/cuda-downloads[href=https://developer.nvidia.com/cuda-downloads]; at time of writing: version 11.1)\nData\nDeepLoc (Almagro Armenteros et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0002]): http://data.bioembeddings.com/deeploc/deeploc_data.fasta[href=http://data.bioembeddings.com/deeploc/deeploc_data.fasta]\nDeepLoc (reduced sample) FASTA-formatted sequences: http://data.bioembeddings.com/deeploc/sampled_deeploc_data.fasta[href=http://data.bioembeddings.com/deeploc/sampled_deeploc_data.fasta]\nNOTE: as input, we begin with two files containing protein sequences in a simplified FASTA format (first line begins with “>” followed by protein name, all subsequent lines contain the sequence in single-letter amino acid code).\n1. Install bio_embeddings from pip.\nTo install the pipeline and all of its dependencies, open a terminal window and type in the command:\n         \ntable:\n﻿0\n\"pip install --user \"\"bio-embeddings[all]\"\"\"\n2. Create a project directory.\nWe suggest you create a new project directory on your disk. You can generate it through the terminal:\n         \ntable:\n﻿0\nmkdir deeploc\nThen, open the directory through the terminal:\n         \ntable:\n﻿0\ncd deeploc\n3. Download the DeepLoc FASTA file inside the project directory.\nFrom the terminal (within the project directory):\n         \ntable:\n﻿0\nwget http://data.bioembeddings.com/deeploc/deeploc_data.fasta\nAlternatively, download the file using your browser, and move it to the project directory.\nCAUTION: If you are using a system not equipped with a GPU, we suggest picking a smaller FASTA set for the next steps. This will facilitate executing subsequent steps. A smaller FASTA file is available at: http://data.bioembeddings.com/deeploc/sampled_deeploc_data.fasta[href=http://data.bioembeddings.com/deeploc/sampled_deeploc_data.fasta]. If you pick this file, make sure to note the name change for the following steps.\n4. Create a configuration file.\nA configuration file defines what the pipeline should do (files and parameters it should use and stages it should run). Many examples of configuration files are provided at http://examples.bioembeddings.com[href=http://examples.bioembeddings.com], including the one you will create here (called deeploc). To create the configuration file from the terminal:\n         \ntable:\n﻿0\nnano config.yml\nThen, type in the following and save the file (to save: press Ctrl+x, then “y”, then the Return key):\n         \ntable:\n﻿0",
    "global: sequences_file: deeploc_data.fasta prefix: deeploc_embeddings simple_remapping: True  prottrans_bert_embeddings: type: embed protocol: prottrans_bert_bfd reduce: True\nThe global section defines a global parameter; mandatory are the input sequence file (called deeploc_data.fasta in the config) and the prefix where outputs will be stored (in this case, a new directory deeploc_embeddings, which will be created inside the deeploc project directory).\nThe sections following global define stages of the pipeline and can have arbitrary names. In this case, you have one stage called prottrans_bert_embeddings, which will execute an “embed” stage (type: embed), using the BERT language model trained on BFD (Elnaggar et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0014]) (protocol: prottrans_bert_bfd). The “embed” stage produces per-residue embeddings by default. To get per-protein embeddings you must specify the reduce parameter (reduce: True).\n5. Run the bio_embeddings pipeline.\nAll that is left to do is to supply the configuration file to bio_embeddings and let the pipeline execute the job. To do so, type on the terminal:\n         \ntable:\n﻿0\nbio_embeddings config.yml\nYou should see output that resembles:\n         \ntable:\n﻿0\n\"2020-11-09 20:37:13,753 INFO Created the prefix directory deeploc_embeddings 2020-11-09 20:37:13,756 INFO Created the file deeploc_embeddings/input_parameters_file.yml 2020-11-09 20:37:13,970 INFO Created the file deeploc_embeddings/sequences_file.fasta 2020-11-09 20:37:14,118 INFO Created the file deeploc_embeddings/mapping_file.csv …\"\n6. Locate the embedding files.\nAfter the job has finished, you should have a new directory called deeploc_embeddings (prefix) in your deeploc project directory. This directory will contain several files, and another directory, prottrans_bert_embeddings (config.yml after section global), with the outputs of the “embed” stage. The most salient files are embeddings_file.h5 and reduced_embeddings_file.h5 (only produced if “reduce: True”) inside the prottrans_bert_embeddings directory. These files are what you will use for your analyses and to train prediction tools (following protocols).",
    "The previous protocol generated embeddings from protein sequences in your dataset (here DeepLoc dataset). In Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003] you use functions from the bio_embeddings package to visualize “protein spaces” spanned by the embeddings extracted. These visualizations reveal whether or not the LM chosen for the “embed” stage (Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0002]) can roughly separate your data based on a desired property/phenotype. The property/phenotype in our example is subcellular location in 10 states. Alternate Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0006] uses the same data and similar steps to visualize protein solubility. While visualizations are useful, the discriminative power of embeddings can be boosted many times by training machine learning models on the embeddings to predict the desired property (Basic Protocol 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0004]).\nBetween embedding generation and protein space visualization, another step has to be inserted. In the pipeline, we refer to this step as a “project” stage. Its purpose is to reduce the dimensionality of the embeddings (e.g., 1024 for ProtBERT) such that it can be visualized in either 2D or 3D. Here, we project embeddings onto 2D; Alternate Protocol 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0005] uses the same data and slight variations in parameters to 3D plots instead.\nThe final notebook constructed here is available at http://notebooks.bioembeddings.com[href=http://notebooks.bioembeddings.com] as deeploc_visualizations.ipynb to be downloaded and executed locally, or executed directly online. The file also includes steps presented in Alternate Protocols 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0005] and 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0006].\nThe Support Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0007] 1 explains how to integrate the final visualization options in a configuration file as instruction for the pipeline to manage the entire process—from sequences to visualizations. This is useful to enable colleagues to reproduce all your results from a few files.\nMaterials\nSoftware\nJupyter Notebook (Kluyver et al., 2016[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0024])\nNotebooks can be run locally, provided that the necessary dependencies are installed (python 3.7 and the Jupyter suite). Installation steps are described here: https://jupyter.org/install[href=https://jupyter.org/install].",
    "Notebooks can be run on Google Colaboratory (Bisong, 2019[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0008]), without having to install software locally, given an internet connection and a Google account.\nData\nDeepLoc embeddings input files, which you either calculated through Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0002] or you can be download from http://data.bioembeddings.com/deeploc/reduced_embeddings_file.h5[href=http://data.bioembeddings.com/deeploc/reduced_embeddings_file.h5]\nAnnotations of properties/phenotypes of the proteins; for DeepLoc, subcellular location annotations can be downloaded from http://data.bioembeddings.com/deeploc/annotations.csv[href=http://data.bioembeddings.com/deeploc/annotations.csv]\n1. Create new Jupyter Notebook on Google Colaboratory (a) or locally (b).\n         \nWe suggest running the following through Google Colaboratory. To open a new Google Colaboratory, navigate to: https://colab.research.google.com/#create=true[href=https://colab.research.google.com/#create=true].\nIf you prefer to execute the steps on your local computer, through the terminal, navigate to the deeploc folder created previously, or to a new folder. Then, start a Jupyter notebook through the terminal:\ntable:\n﻿0\njupyter-notebook\nThis should open a browser window. From the top-right drop-down menu called “new”, select “Python 3”.\n2. Install bio_embeddings\n         \nOn Google Colaboratory paste in the following code in the first code block:\n                  \ntable:\n﻿0\n\"!pip3 install -U pip !pip3 install -U \"\"bio-embeddings[all]\"\"\"\nThen, press the play button on the left of the code cell. Given some version differences in Google Colaboratory, warnings may arise. These, however, can be ignored.\nIf you already executed Protocol 1, you are set. Otherwise, open a new terminal window and type:\n                  \ntable:\n﻿0\n\"pip install --user \"\"bio-embeddings[all]\"\"\"\n3. Download files.\n         \nOn Google Colaboratory, create a new code block (by pressing the “+ code” button). Then, paste in the following code:\n                  \ntable:\n﻿0\n!wget http://data.bioembeddings.com/deeploc/reduced_embeddings_file.h5 !wget http://data.bioembeddings.com/deeploc/annotations.csv\nOn your local computer, simply download the files listed in the Materials list for this protocol and move them into the folder in which the notebook was started (see step 1).\n4. Import dependencies.",
    "From here on, the execution steps are identical on Google Colaboratory and your local Jupyter notebook. You will now import the functions that allow you to open embedding files, reduce the dimensionality, and visualize scatter plots. To do so, in a new code block, type and execute the following:\n         \ntable:\n﻿0\n\"import h5py import numpy as np from pandas import read_csv, DataFrame from bio_embeddings.utilities import QueryEmbeddingsFile from bio_embeddings.project import umap_reduce from bio_embeddings.visualize import render_scatter_plotly\"\n5. Read annotations file.\nAssume that the original FASTA file, for which you generated embeddings, was the following:\n         \ntable:\n﻿0\n>Q9H400-2 SEQVENCE >P12962 SEQVVNCE >P12686 MNQVENCE\nYou can define a set of annotations for the sequences in this set as a CSV file, containing minimally two columns called “identifier” and “label” such as:\n         \ntable:\n﻿0\n\"identifier,label Q9H400-2,Cell membrane P12962,Cytoplasm P12686,Mitochondrion\"\nThe identifiers have to match to the identifiers in the FASTA header of the protein sequences for which embeddings have been computed. They can, however, only contain a subset of identifiers with respect to the embeddings.\nYou can now load the annotations.csv file which we have created based on the DeepLoc data. These annotations contain experimentally validated subcellular location in 10 classes. To load them into the notebook, execute the following in a new code block:\n         \ntable:\n﻿0\nannotations = read_csv(`annotations.csv')\n6. Read the embeddings file.\nIn a new code block, type and execute the following:\n         \ntable:\n﻿0\n\"identifiers = annotations.identifier.values embeddings = list()  with h5py.File(`reduced_embeddings_file.h5', `r') as embeddings_file: embedding_querier = QueryEmbeddingsFile(embeddings_file)  for identifier in identifiers: embeddings.append(embedding_querier.query_original_id(identifier))\"",
    "This will store the embeddings in the “embeddings” list in the same order as the identifiers. To access the embeddings, you can use a helper class called “QueryEmbeddingsFile”. This class allows you to retrieve embeddings either using the identifier extracted from the FASTA header (as done here, via the query_original_id function), or by using the pipeline's internal identifier for protein sequences. You can find more information about these functions at https://docs.bioembeddings.com[href=https://docs.bioembeddings.com].\n7. Project embeddings to 2D using UMAP (McInnes, Healy, & Melville, 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0030]).\nIn a new code block, type and execute the following:\n         \ntable:\n﻿0\n\"options = { `min_dist': .1, `spread': 8, `n_neighbors': 160, `metric': `euclidean', `n_components': 2, `random_state': 10 } projected_embeddings = umap_reduce(embeddings, **options)\"\nThis code block will take some minutes to execute (4 min on Google Colaboratory), as projecting the embeddings is a compute-intensive operation. Projecting embeddings onto fewer dimensions is necessary because data in dimensions d>3 is very tricky to plot (and even d = 3, i.e., 3D plots of scientific data, are often difficult to grasp quickly). RAW embeddings have much higher dimensions, e.g., d = 1024 dimensions for ProtBERT (Elnaggar et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0014]). In “options”, you can define UMAP parameters. These parameters can be tuned to generate different visualizations, e.g., you could change the “metric” to “manhattan”. To graphically see the effect of changing options, you may execute the steps from here onward again. The “projected_embeddings” variable contains a Numpy (Harris et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0016]) matrix of size N×2, where N is the number of proteins for which there are embeddings in the embedding file, while 2 is dictated by the “n_components” in “options” (number of output dimensions of the projection).\n8. Merge projected embeddings and annotations.\nIn a new code block, type and execute the following:\n         \ntable:\n﻿0",
    "\"projected_embeddings_dataframe = DataFrame( projected_embeddings, columns=[\"\"component_0\"\", \"\"component_1\"\"], index=identifiers ) merged_annotations_and_projected_embeddings = annotations.join( projected_embeddings_dataframe, on=\"\"identifier\"\", how=\"\"left\"\" )\"\nHere, you create a DataFrame (similar to a table) from the projected embeddings. Rows are indexed by the “identifiers”, while the two columns contain the two components of te projected embeddings. In other words: you are constructing a table of coordinates for your protein sequences. Lastly, you merge these coordinates with the annotations. You can inspect the first five rows of the dataframe by typing the following into a new code block and executing it:\n         \ntable:\n﻿0\nmerged_annotations_and_projected_embeddings[:5]\nThis should resemble the content reported in Table 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-tbl-0001].\nTable 1.\n                Example of Merged Annotations and Projected Embeddings\ntable:\n﻿Identifier,Label,Component_0,Component_1\nQ9H400,Cell.membrane,2.474637,–8.919042\nQ5I0E9,Cell.membrane,32.507015,10.355012\nP63033,Cell.membrane,18.500378,–0.299981\nQ9NR71,Cell.membrane,2.420154,18.161064\nQ86XT9,Cell.membrane,–4.937888,–1.767011\n9. Plot the protein space spanned by the projected embeddings\nIn a new code block, type and execute the following:\n         \ntable:\n﻿0\nfigure = render_scatter_plotly(merged_annotations_and_projected_embeddings) figure.show()\nThis will display an interactive plot (of which a static screenshot is provided in Fig. 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-fig-0002]). Interactive plots make it possible to disentangle complex annotations/datasets, e.g., by toggling the display of some annotations (click on the legend). Even more useful: zoom in and out plots, especially when visualizing 3D plots.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/88a35acc-a5d5-4847-bbaa-1a68471c0501/cpz1113-fig-0002-m.jpg</p>\nFigure 2\n2D protein space drawn by projected DeepLoc embeddings. Points are projections of embeddings of protein sequences in the DeepLoc set (Almagro Armenteros et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0002]). Coloring is provided according to their subcellular location. Of note: “Extracellular” proteins seem to be particularly keen on forming a cluster, while proteins in other localizations barely separate into groups inside a bigger cluster. The figure is available interactively at: http://data.bioembeddings.com/figures/figure_2.html[href=http://data.bioembeddings.com/figures/figure_2.html].",
    "Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0002] generated embeddings for proteins in DeepLoc (Almagro Armenteros et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0002]). Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003] visualized the projected embeddings in a 2D plot and annotated the proteins in this 2D plot by colors signifying subcellular location. In the following steps, you will use the embeddings generated through the pipeline and the location annotations from DeepLoc to machine-learn the prediction of location from protein sequence embeddings. Once trained, you can apply this prediction method to annotate/predict location for any protein sequence. The simplest recipe to build a generic machine learning model is as follows:\n         \n1.Divide data into train and test sets (these should be sequence-non-redundant with respect to each other, i.e., no protein sequence in one should be more sequence-similar than some threshold to any protein in the other; what this threshold is depends on your task)\n2.Split a subset from the train set to construct a validation set (non-redundant to split-off)\n3.Evaluate some machine learning hyper-parameters using the validation set (e.g., which type of machine learning model—such as ANN, CNN, or SVM, what particular choice of parameters—such as number of hidden units/layers for ANN/CNN). Construct a leaderboard (i.e., a table keeping track of the relative performance of all the models/hyper-parameters).\n4.Select the best model from the leaderboard, and evaluate on the test set (by NO MEANS apply all models to the test set and pick the best; instead, it is essential to choose the best using the validation set and to stick to that choice to avoid over-fitting).\n5.Report performance for a diversity of relevant evaluation metrics for the final model using the test set (include estimates for standard errors)",
    "The following steps explore this recipe using sci-kit learn (Pedregosa et al., 2011[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0037]). You will produce a classifier which roughly separates the ten location classes from DeepLoc (Almagro Armenteros et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0002]). The objective of this protocol is not to produce the best prediction method for subcellular location classification, which would require more parameter testing and tuning! Instead, the objective is to showcase the ease of going from data to prediction method when using embeddings. The final notebook constructed here is available at http://notebooks.bioembeddings.com[href=http://notebooks.bioembeddings.com] as downloadable file called deeploc_machine_learning.ipynb.\nMaterials\nSee Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003]\n1. Complete steps 1-5 of Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003].\n2. Import additional dependencies.\nVia a new code block, you will import a set of dependencies from the popular machine learning library scikit-learn (Pedregosa et al., 2011[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0037]) in order to train and evaluate the machine learning model:\n         \ntable:\n﻿0\nfrom sklearn.neural_network import MLPClassifier from sklearn.model_selection import GridSearchCV from sklearn.metrics import accuracy_score\n3. Split annotations into train and test sets\nThe first task for any supervised machine learning is the split of the data into training and testing sets. The testing set (also referred to as “hold out set”) is used exclusively to evaluate the performance of the final machine learning model. The training set serves the optimization of the model and hyper-parameters.",
    "In computational biology/bioinformatics, informed decisions on how to split data are pivotal, for example, by ascertaining that no protein in the training set has more than 20% pairwise sequence identity (PIDE) to any protein in the test set (Reeb, Goldberg, Ofran, & Rost, 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0045]). While packages such as scikit-learn (Pedregosa et al., 2011[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0037]) include functions to easily split data into train and test sets, they completely fail to account for domain knowledge such as the concept of homology or evolutionary connections relevant to reduce redundancy between bio-sequences. Therefore, users of such packages have to address these issues manually when starting a new project, or they will join the many who produce overconfident methods.\nDeepLoc annotations come with a column “set” which is either “train” or “test”. The split into these two categories has been made such that any pair of sequences in train and test share at most 30% PIDE. To split the data, execute the following block of code:\n         \ntable:\n﻿0\n\"train_set = annotations[annotations.set == \"\"train\"\"] test_set = annotations[annotations.set == \"\"test\"\"]\"\n4. Load embeddings into train and test sets.\nOnce you have split the annotations into train and test sets, you need to create input and output for the machine learning model. The input will be the sequence embeddings (in the following, “training_embeddings”), while the output will be the subcellular location associated to those proteins (in the following, “training_labels”). In a new code block, type the following:\n         \ntable:\n﻿0\n\"training_embeddings = list() training_identifiers = train_set.identifier.values training_labels = train_set.label.values  testing_embeddings = list() testing_identifiers = test_set.identifier.values testing_labels = test_set.label.values  with h5py.File(`reduced_embeddings_file.h5', `r') as embeddings_file:  embedding_querier = QueryEmbeddingsFile(embeddings_file)  for identifier in training_identifiers:  training_embeddings.append(embedding_querier.query_original_id(identifier))  for identifier in testing_identifiers:  testing_embeddings.append(embedding_querier.query_original_id(identifier))\"\n5. Define basic machine learning architecture and parameters to optimize\nIn a new code block, type and execute the following:\n         \ntable:\n﻿0",
    "\"multilayerperceptron = MLPClassifier( solver=`lbfgs', random_state=10, max_iter=1000 ) parameters = {  `hidden_layer_sizes': [(30,), (20,15)] }\"\nThis will create a basic neural network architecture (“multilayerperceptron”) and a set of parameters that you want to test during parameter optimization. The basic architecture uses the \"Limited-memory Broyden–Fletcher–Goldfarb–Shanno Algorithm\" solver (Saputro & Widyaningsih, 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0052]) and a maximum of 1000 training iterations (max_iter). Using the “lbfgs” solver, maximum training iterations correspond to how many embeddings the algorithm will maximally see before training is stopped. Training may automatically be stopped before the maximum number of iterations if the model converges (in other words: if its validation error stays within a certain threshold). In the DeepLoc set, there are more than ten thousand samples, so max_iter could be set to a higher value, but for the purpose of this protocol, to have reasonable execution time, we propose limiting the number of iterations to 1000.\nThe parameter that you will optimize is the number of hidden layers and the amount of neurons in each layer. In one case, you will try a network with one hidden layer containing 30 neurons, while in the second case you will test a network with two hidden layers with 20 and 15 neurons, respectively.\n6. Train classifiers and pick the best performing model.",
    "Usually, this step is performed in various sub-steps, for example: first you define the number of training splits (e.g., Nsplit=3), which would give you data for training (optimization of free parameters) and for cross-training/validation (optimization of hyper-parameters and model choice). Then, you train Nsplit-1 (i.e., 2 for Nsplit=3) network variants describing each split, evaluate on the respective validation data, and finally select the network performing best (on the cross-training/validation split). Luckily, all of these steps can be summarized into three lines of code using sci-kit learn. For this example, we have ignored homology/redundancy when splitting the data set for brevity, but in real-life applications, accounting for homology/redundancy when splitting is essential to obtain valid models!\nIn a new block of code, write and execute the following:\n         \ntable:\n﻿0\n\"classifiers = GridSearchCV( multilayerperceptron, parameters, cv=3, scoring=\"\"accuracy\"\" ) classifiers.fit(training_embeddings, training_labels) classifier = classifiers.best_estimator_\"\nNote this code takes about 15 min to execute on Google Colab. No output is produced during this time. Visual clues from the notebook assist you in understanding when the computation is over. Another important note on scope: while you will obtain a classifier that is roughly able to classify sequences in ten subcellular location compartments, your method will not beat the state-of-the-art for this problem due to extensive development in the field! The goal of this protocol is to give you the tools to build a classifier, as well as to require little time to execute. If you want to obtain the best classifier, you will need to test and tune more parameters, and especially consider more training iterations (as defined by max_iter in the previous step).\n7. Predict subcellular location for test set and calculate performance,",
    "Lastly, to evaluate the performance of you final model, you predict the location for all proteins in the test set and calculate accuracy as follows:\n         \ntable:\n﻿0\n\"predicted_testing_labels = classifier.predict(testing_embeddings) accuracy = accuracy_score( testing_labels, predicted_testing_labels )  print(f\"\"Our model has an accuracy of {accuracy:.2}\"\")\"\nThe reported accuracy should be 0.72.\n8. Optional: Embed a novel sequence and predict its subcellular location.\nIn this optional step, you generate the sequence embedding for an arbitrary sequence and use the classifier developed in the previous steps to predict its subcellular location. To do so, type and execute the following:\n         \ntable:\n﻿0\n\"from bio_embeddings.embed import ProtTransBertBFDEmbedder  embedder = ProtTransBertBFDEmbedder()  sequence = \"\"DDCGKLFSGCDTNADCCEGYVCRLWCKLDW\"\" per_residue_embedding = embedder.embed(sequence) per_protein_embedding = embedder.reduce_per_protein(per_residue_embedding) sequence_subcellular_prediction = classifier.predict([per_protein_embedding])[0]  print(\"\"The arbitrary sequence is predicted to be located in: \"\" f\"\"{sequence_subcellular_prediction}\"\")\"\nAbove, you import the “ProtTransBertBFDEmbedder” and initialize it. You then define an amino acid sequence using the standard IUPAC alphabet. The sequence is then embedded per-residue (per_residue_embedding), and the per-residue embedding is transformed to a per-protein embedding via a helper function (per_protein_embedding). Finally, the per-protein embedding is used to predict subcellular location through the classifier you developed, and the prediction (Extracellular) is printed to screen.\nYou may see a warning about “padding” appear in the output; you can ignore this as it will not affect execution.\nFor scikit-learn the function “predict” expects a list of protein embeddings. This (usually helpful) feature implies that additional steps are required to predict for a single sequence, namely that first you have to put the embedding into a list. You can then grab the prediction of the first (and only) item in the list, which will be the prediction of the arbitrary sequence.",
    "The following steps introduce minimal code changes with respect to the steps and code outlined in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003] to visualize in 3D instead of 2D. We assume that the code from Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003] has been written in a Jupyter/Colab Notebook and highlight code changes in orange. Visit the docs at https://docs.bioembeddings.com[href=https://docs.bioembeddings.com] to find out more about the functions of the bio_embeddings package.\nThe code from Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003] is available at http://notebooks.bioembeddings.com[href=http://notebooks.bioembeddings.com] as downloadable file called deeploc_visualizations.ipynb. It includes the steps presented here in an alternate form.\nMaterials\nSee Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003]\n1. Project embeddings onto 3D instead of onto 2D.\nThe first change to the previous steps requires only augmenting the number of components UMAP will project embeddings to.\nTake the code block written in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003], step 7, and locate and change the line:\n         \ntable:\n﻿0\n`n_components': 2\nto:\n         \ntable:\n﻿0\n`n_components': 3\nThen, re-run the code cell.\n2. Import 3D scatter plot renderer instead of 2D.\nChange the import of the visualization function from Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003], step 4, from:\n         \ntable:\n﻿0\nfrom bio_embeddings.visualize import render_scatter_plotly\nto:\n         \ntable:\n﻿0\nfrom bio_embeddings.visualize import render_3D_scatter_plotly\nand execute the code block.\n3. Add a third component to the projected embeddings DataFrame.\nChange the number of components in the projected DataFrame defined in B.asic Protocol 3, step 8 from:\n         \ntable:\n﻿0\n\"columns=[\"\"component_0\"\", \"\"component_1\"\"],\"\nto:\n         \ntable:\n﻿0\n\"columns=[\"\"component_0\"\", \"\"component_1\"\", \"\"component_2\"\"],\"\nand execute the code block.\n4. Swap the plotting function with the 3D variant:\nLastly, swap out the plotting function name in the code block created in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003], step 9, from:\n         \ntable:\n﻿0\nfigure = render_scatter_plotly(  merged_annotations_and_projected_embeddings )\nto:\n         \ntable:\n﻿0\nfigure = render_3D_scatter_plotly(  merged_annotations_and_projected_embeddings )\nand execute the code block.\nAt this point, a 3D interactive plot (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-fig-0003]) will be displayed on your notebook.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/ed1357ae-d700-4c37-a7ce-6ef9f3953eea/cpz1113-fig-0003-m.jpg</p>",
    "Figure 3\n3D protein space drawn by projected DeepLoc embeddings. Points are projections of embeddings of protein sequences in the DeepLoc set (Almagro Armenteros et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0002]). Coloring is provided according to their subcellular localizations. The 3D figure is best explored interactively: http://data.bioembeddings.com/figures/figure_3.html[href=http://data.bioembeddings.com/figures/figure_3.html].",
    "The following steps introduce minimal code changes with respect to the steps and code outlined in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003] in order to visualize the classification into membrane/soluble proteins as annotated in DeepLoc (Almagro Armenteros et al., 2017[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-bib-0002]) instead of location. We assume that the code from Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003] has been written up and highlights code changes in orange.\nThe code from Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003] is available at http://notebooks.bioembeddings.com[href=http://notebooks.bioembeddings.com] as downloadable file called deeploc_visualizations.ipynb. It includes the steps presented here in an alternate form.\nMaterials\nSoftware and Hardware\nSee Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003]\nData\nDeepLoc solubility annotations: http://data.bioembeddings.com/deeploc/solubility_annotations.csv[href=http://data.bioembeddings.com/deeploc/solubility_annotations.csv]\n1. Download additional file solubility_annotations.csv.\n         \nOn Google Colaboratory create a new code block (by pressing the “+ code” button). Then, paste in the following code:\n                  \ntable:\n﻿0\n!wget http://data.bioembeddings.com/deeploc/solubility_annotations.csv\nOn your local computer, simply download the file listed in the Materials list for this protocol and move into the folder in which the notebook was started (see Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003], step 1).\n2. Change the annotations file.\nIn the code block created in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003], step 5, change the input file from:\n         \ntable:\n﻿0\nannotations = read_csv(`annotations.csv')\nto:\n         \ntable:\n﻿0\nannotations = read_csv(`solubility_annotations.csv')\n3. Re-run the subsequent code blocks.\nRe-run every code block following the code block just changed. This will display a graph, this time colored according to protein solubility, i.e., whether a protein is annotated as membrane-bound, soluble or lacks an annotation).",
    "Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003] presents an explorative approach towards producing protein-space visualizations. In this Support Protocol[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0007], you will use the parameters chosen in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003] to define a pipeline configuration file. These files allow reproducible workflows. You will do so by extending the bio_embeddings configuration presented in Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0002], step 4, to also generate protein space visualizations. Noteworthy differences with previous files will be highlighted in orange.\nMaterials\nSoftware and Hardware\nSee Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0002]\nData\nDeepLoc FASTA file: http://data.bioembeddings.com/deeploc/deeploc_data.fasta[href=http://data.bioembeddings.com/deeploc/deeploc_data.fasta]\nDeepLoc subcellular location annotations: http://data.bioembeddings.com/deeploc/annotations.csv[href=http://data.bioembeddings.com/deeploc/annotations.csv]\n1. Execute steps 1 through 3 of Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0002].\n2. Download the annotations file into the project directory.\nFrom the terminal (within the project folder):\n         \ntable:\n﻿0\nwget http://data.bioembeddings.com/deeploc/annotations.csv\nAlternatively, download the file using your browser (link in the Materials of this protocol), and move it to the project directory.\n3. Define a configuration file to embed, project and visualize protein sequences.\nSimilarly to Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0002], step 4, we define a text file (config.yml) that contains the following text:\n         \ntable:\n﻿0\nglobal: sequences_file: deeploc_data.fasta prefix: deeploc_embeddings simple_remapping: True  prottrans_bert_embeddings: type: embed protocol: prottrans_bert_bfd reduce: True discard_per_amino_acid_embeddings: True  umap_projections: type: project protocol: umap depends_on: prottrans_bert_embeddings min_dist: 0.1 spread: 8 n_neighbors: 160 metric: euclidean n_components: 2 random_state: 10  plotly_visualization: type: visualize protocol: plotly depends_on: umap_projections annotation_file: annotations.csv display_unknown: False\nThe first part of this config (“global” and “prottrans_bert_embeddings”) are almost identical to the config presented in Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0002]. The addition of the “discard_per_amino_acid_embeddings” parameter tells the pipeline that we are only interested in the per-protein embeddings (reduced_embeddings_file.h5), and that the per-residue embeddings (embedding_file.h5) should not be stored on disk. This will save significant storage space.",
    "A stage (umap_projections) of type “project” that uses the protocol umap was added. The “depends_on” directive tells the pipeline that the embeddings generated by “prottrans_bert_embeddings” should be used for the project stage. We add the same UMAP parameters as in Basic Protocol 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0003], step 7. This stage will output a DataFrame of the projected embeddings (projected_embeddings.csv).\nFinally, we use this data for a “visualize” type stage (by depending on the umap_projections). We annotate the visualization using the annotation file called “annotations.csv”. Sequences without annotations (but that might be present in the input FASTA file) will not be plotted (“display_unknown: False”). The “plotly_visualization” stage will produce a file containing the 2D interactive figure (figure.html).\n4. Run the bio_embeddings pipeline.\nWhat remains is to supply the configuration file to bio_embeddings and let the pipeline execute the job. For that type into the terminal:\n         \ntable:\n﻿0\nbio_embeddings -o config.yml\nThe “-o” option instructs the pipeline to overwrite a previous pipeline run at the same prefix, which might have remained in the current project directory (deeploc) from the previously executed Basic Protocol 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-prot-0002].\n5. Locate the interactive figure file.\nAfter the job has finished, you should see a “deeploc_embeddings” directory in your project directory. This directory will contain three subdirectories called: prottrans_bert_embeddings, umap_projections, and plotly_visualization. Each directory contains the output of the corresponding stage. The newly created interactive figure will be stored in the “plotly_visualization” directory as “figure.html”. You can use a browser, such as Safari, to open this figure. It should resemble Figure 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.113#cpz1113-fig-0002]."
  ],
  "subjectAreas": [
    "Bioinformatics"
  ],
  "bigAreas": [
    "Bioinformatics & Computational Biology"
  ]
}