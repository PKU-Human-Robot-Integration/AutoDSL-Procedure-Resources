{
  "id": 10389,
  "origin_website": "Jove",
  "title": "Databases to Efficiently Manage Medium Sized, Low Velocity, Multidimensional Data in Tissue Engineering",
  "procedures": [
    "NOTE: See Table of Materials for the software versions used in this protocol.\n1. Evaluate if the data would benefit from a database organization scheme\nDownload the example codes and databases (see Supplemental Coding Files, which are summarized in Table 1).\nUse Figure 1 to evaluate if the data set of interest is \"multi-dimensional\".\n\tNOTE: Figure 1 is a graphical representation of a multi-dimensional database provided for the example data set.\nIf the data can be visualized in a \"multi-dimensional\" form like the example and if the ability to relate a specific experimental outcome to any of the dimensions (i.e., conditions) would allow for greater scientific insight into the available data, proceed to construct a relational database.\n2. Organize the database structure\nNOTE: Relational databases store information in the form of tables. Tables are organized in schema of rows and columns, similar to spreadsheets, and can be used to link identifying information within the database.\nOrganize the data files, so they have well thought out unique names. Good practice with file naming conventions and folder-subfolder structures, when done well, allow for broad database scalability without compromising the readability of accessing files manually. Add date files in a consistent format, such as \"20XX-YY-ZZ\", and name subfolders according to metadata is one such example.\nAs the data-base structure is designed, draw relationships between the fields in different tables. Thus, multi-dimensionality is handled by relating different fields (i.e., columns in the tables) in individual tables to each other.\nCreate readme documentation that describes the database and relationships that were created in step 2.2. Once an entry between different tables is linked, all associated information is related to that entry and can be used to call complex queries to filter down to the desired information.",
    "NOTE: Readme documents are a common solution for providing supplemental information and database structural information about a project without adding non-uniform data to the structure.\nFollowing steps 2.1−2.3, make the end result similar to this example where the differing characteristics of individuals (Figure 2A) are related to associated experimental data of those individuals (Figure 2B). The same was done through relating columns of pattern types (Figure 2C) and data types (Figure 2D) to matching entries in the main data values table to explain various shorthand notations (Figure 2B).\nDetermine all the essential and merely helpful data points that need to be recorded for long range data collection.\n\tNOTE: A key advantage of using databases over spreadsheet programs, as mentioned earlier, is scalability: additional data points can be trivially added at any point and calculations, such as averages, are instantly updated to reflect newly added data points.\n\t\nIdentify the necessary information for creating distinct data points prior to beginning. Leave raw data untouched, instead of modifying or saving over it, so that reanalysis is possible and accessible.\n\t\tNOTE: For the given example (Figure 2), the \"Designator\" corresponding to an individual, \"Pattern type\", \"Coverslip #\", and \"Variable type\" were all vital fields for distinctness of the associated value.\nIf desired, add other helpful, non-vital information such as the \"Total # of Coverslips\" to indicate the number of repetitions conducted and help determine if data points are missing in this example.\n3. Set up and organize the pipeline\nIdentify all the various experiments and data analysis methods that might lead to data collection along with the normal data storage practices for each data type. Work with open source version control software such as GitHub to ensure necessary consistency and version control while minimizing user burden.",
    "If possible, create procedure for consistent naming and storing of data to allow for an automated pipeline.\n\tNOTE: In the example, outputs were all consistently named, thus creating a data-pipeline that looked for specific attributes was straightforward once the files were selected. If consistent naming is not possible, the tables in the database will need to be populated manually, which is not recommended.\nUse any convenient programming language to generate new data entries for the database.\n\t\nCreate small \"helper\" tables (files #8−#10 in Table 1) in separate files that can guide automated selection of data. These files serve as a template of possibilities for the pipeline to operate under and are easy to edit.\nTo generate new data entries for the data-pipeline (Figure 3D), program the code (LocationPointer.m, file #1 in Table 1) to use the helper tables as inputs to be selected by the user (files #8−#10 in Table 1).\nFrom here, assemble a new spreadsheet of file locations by combining the new entries with the previous entries (Figure 3E). Create a code to automate this step as shown in LocationPointerCompile.m (file #2 in Table 1).\nAfterwards, check this merged spreadsheet for duplicates, which should be automatically removed. Create a code to automate this step as shown in LocationPointer_Remove_Duplicates.m (file #3 in Table 1).\nAdditionally, check the spreadsheet for errors, and notify the user of their reason and location (Figure 3F). Create a code to automate this step as shown in BadPointerCheck.m (file #4 in Table 1). Alternatively, write a code that will check the compiled database and identify duplicates in one step as shown in LocationPointer_Check.m (file #5 in Table 1).\nCreate a code to let the user manually remove bad points without losing the integrity of the database as shown in Manual_Pointer_Removal.m (file #6 in Table 1).",
    "Then use the file locations to generate a data value spreadsheet (Figure 3G, file #12 in Table 1) as well as to create a most updated list of entries that can be accessed to identify file locations or merged with future entries (Figure 3H). Create a code to automate this step as shown in Database_Generate.m (file #7 in Table 1).\nDouble check that the pipeline adds to the experimental rigor by checking for inclusion of rigorous naming conventions, automated file assembly codes, and automated error checks as previously described.\n4. Create the database and queries\nNOTE: If tables store information in databases, then queries are requests to the database for information given specific criteria. There are two methods to create the database: starting from a blank document or starting from the existing files. Figure 4 shows a sample query using SQL syntax that is designed to run using the database relationships shown in Figure 2.\nMethod 1: Starting from scratch in creating the database and queries\n\t\nCreate a blank database document.\nLoad the helper tables (files #8−#10 in Table 1) by selecting External Data | Text File Import | Choose File (files #8−#10) | Delimited | First Row Contains Headers, Comma | leave default | Choose My Own Primary Key (Designator for Cell Lines File #8, Variable Name for Data Types File #9, Pat Name for Pattern Type File #10) | leave default | Finish.\nLoad the Data value table (file #12 in Table 1) by selecting External Data | Text File Import | Choose File (file #12) | Delimited | First Row Contains Headers, Comma | leave default | Let Access Add primary key | Import to Table: DataValues | Finish.",
    "Create the relationships by selecting Database Tools | Relationships | Drag all Tables to the board | Edit Relationships | Create New | Match the DataValue fields with Helper Tables Designators | Joint Type 3.\nSelect Create | Query Design.\nSelect or drag all relevant tables into the top window. In this example ‘Cell Lines', ‘Data Values', ‘Data Types', and ‘Pattern Type'. The relationships should automatically set up based on the previous Relationship design.\nFill out the query columns for desired results, for example:\n\t\t\nClick on Show | Totals.\nFill out the first column (Table: DataValues, Field: DataVar, Total: GroupBy, Criteria: \"Act_OOP\"), the second column (Table: DataValues, Field: PatVar, Total: GroupBy, Criteria: \"Lines\"), and the third column (Table: Cell_Lines, Field: Designator, Total: GroupBy, Sort: Ascending).\nFill out the fourth column (Table: DataValues, Field: Parameter, Total: Ave), the fifth column (Table: DataValues, Field: Parameter, Total: StDev), and the sixth column (Table: DataValues, Field: Parameter, Total: Count).\nRun the query.\nAlternatively, use the provided example database as a basis for examples. Open the database file Database_Queries.accdb (file #13 in Table 1) that was downloaded earlier. Use it as a template by replacing existing tables with the data of interest.\n5. Move the output tables to a statistical software for significance analysis\nFor this sample experimental data, use the one-way analysis of variance (ANOVA) using Tukey's test for mean comparisons between various conditions.\n\tNOTE: Values of p < 0.05 were considered statistically significant.\nSubscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Bioengineering"
  ],
  "bigAreas": [
    "Bioengineering & Technology"
  ]
}