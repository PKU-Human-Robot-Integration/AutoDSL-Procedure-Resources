{
  "id": 19671,
  "origin_website": "Wiley",
  "title": "Accurate and Effective Detection of Recurrent Copy Number Variants in Large SNP Genotype Datasets",
  "procedures": [
    "PennCNV (Wang et al., 2007[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0018]) has been the de facto standard for CNV calling since its initial release 15 years ago. While it is fairly easy to use, implementing a pipeline to process tens or hundreds of thousands of samples can be challenging. Moreover, there are many checks the users need to perform in order to ensure all commands run as intended, as well as some files that are not straightforward to generate or obtain. The pipeline we refined takes care of most of these issues. This protocol details how to run the pipeline and obtain the raw CNV calls and sample QC files to use for further processing.\nRequired Files\nTo run the protocol three main files plus the intensity files are required, and they must be in the correct format. We provide GC content file (requirement 4); however, the user needs to generate the samples list (requirement 3) and the SNPs position file (requirement 2).",
    "1.Intensity files. These files store the raw information regarding each Illumina SNP probe. Each file should contain at least the following three columns: “Name,” “Log R Ratio,” “B Allele Freq.” Column names must be exact; other columns can be present as long as these are the first three. These columns contain, namely: the name of the SNP markers, the value of Log2 R ratio (a relative measure of the light intensity), and B Allele Frequency (a measure of the allelic composition). Intensity files are also called “Final Reports” and may contain a multi-line header. Moreover, it is possible that multiple samples are stored in the same file (e.g., an entire genotyping batch) and that the files are compressed. In such cases, PennCNV will not work. All these problems can be very different from case to case, but they can be solved using standard GNU utilities tools (included in every major linux distribution) such as sed or the program awk. To solve the second problem, the PennCNV script split_illumina_report.pl can also be used. Note that in this case the user may need to regenerate a correct samples_list.txt file. Note that it is important to know in which build of the human genome the intensity data is (e.g., “hg19”), and that all samples in the same project must be on the same version.",
    "2.snppos.txt. This is a tab-separated text file that contains at least the following three columns: “Name,” “Chr,” and “Position,” which are the name and genomic location of each SNP marker. The first column must use the same names as in the intensity files. In projects where each intensity file contains this information, any sample can be used to generate the SNP position file. Otherwise, there should be a file called “PFB” (Population Frequency of the B allele) that contains all the columns required by the snppos.txt format.\n3.samples_list.txt. This must be a two-column tab-separated text file with a header. The two columns must be “sample_ID” and “file_path.” “sample_ID” needs to be the identifier for the specific sample, and “file_path” needs to be the complete path (thus starting from root, “/”; therefore avoid using links and the home directory, “∼/”, in the path) to the intensity file for that sample. Again, this protocol assumes there is one intensity file per sample. If this is not the case, see step 4. In the case where samples were genotyped in waves or batches, these can be used; otherwise, batches of approximately 2000 samples will be created. In the first instance, an additional column called “batch” must be present, and the content must be integers indicating the specific batch.\n4.GC content file. This file is used by the PennCNV script cal_gc_snp.pl. This file is not straightforward to generate, but we provide the hg38, hg19 and hg18 version as part of the IBPcnv repository. The hg38 version is available from the PennCNV repository: https://github.com/WGLab/PennCNV/blob/master/gc_file/[href=https://github.com/WGLab/PennCNV/blob/master/gc_file/]. It is also included in the IBPcnv repository for user convenience. Finally, the original hg18 version can also be obtained directly from UCSC Genome Browser at http://hgdownload.cse.ucsc.edu/goldenPath/hg18/database/gc5Base.txt.gz.\nProtocol steps",
    "1. Setup. We provide all required software and scripts in a docker/singularity container and a GitHub repository. Excluding the raw data, all files and subdirectories need to be in the same directory ($workingdir hereafter). Support Protocol 2 details the installation process, as well as some suggestions on how to set up directories and the environment for the analysis.\n2. Initial checks and required files generation:\n         \nThis will do the following. First, it will run a series of tests to check that all intensity files are present and in the correct format. Then, it will check if samples were already divided into batches and that this is in the correct format. If not, it will create the batches and separate the samples into groups of approximately 2000 each; the actual number may vary in order to not have the last batch significantly smaller than the rest.\nTo complete this step, move to the main working directory (cd $workingdir) and run:\nsingularity exec ibpcnv.simg Rscript \\\nIBPcnv/penncnv_pipeline/01_preprocess.R \\\n$workingdir 1 2000 $tabix_folder\nwhere $tabix_folder is the complete path to the directory where the tabix-indexed files will be created in step 5. The approximate batch size can be changed via the third parameter; however it is not recommended to use a small batch size, as the population B allele frequency (PFB) files will be created per batch.\n         \nc.If the script fails checking the batches but no errors are found in the intensity files, the second parameter can be set to 0 to skip the initial checks that constitute the slower part of the testing.\nd.If all checks are successfully completed, the script will write the per-batch sample list files needed by PennCNV in $workingdir/listfile/. The script will also print the number of batches that will be used.\n3. Select the SNP markers:",
    "This step will do the following. First it will extract the marker names and positions from an intensity file. It will then download the HRC SNP list, selecting only SNPs that are strictly biallelic, known (with a name in dbSNP142, no “.”) and with a minor allele frequency of at least $minMAF (MAF values for each SNP are obtained from the HRC); we suggest 0.001 (0.1%). Then, it will merge the two tables, remove markers with duplicated “SNP_ID,” remove markers that map to the same position, and finally it will create the snppos.txt file needed by PennCNV.\nTo complete this step, move to the main working directory and run:\nsingularity exec ibpcnv.simg Rscript \\\n  IBPcnv/penncnv_pipeline/02_select_SNPs.R \\\n  $workingdir $minMAF $hgversion TRUE\nThe removal of duplicated markers can be avoided by setting the last parameter to FALSE. At the moment of this writing, the HRC SNP list is available only in hg19 coordinates; thus, the last argument can take only “hg19” as value. This may change in the near future. Any other list of SNPs that follows the same format will work. See the Sanger Institute documentation for details: http://ngs.sanger.ac.uk/README and ftp://ngs.sanger.ac.uk/production/hrc/HRC.r1/README. When using a different SNP list, it is suggested to first use the default one in order to be able to easily check the format.\n4. PennCNV calling pipeline:\nFirst, run the calling pipeline in parallel on each wave. To complete this step, move to the main working directory and run the command:\nbash IBPcnv/penncnv_pipeline/03_penncnv_pipeline.sh \\\n  $workingdir $ibpcnvdir $n_batches hg19\nwhere $n_batches is the number of batches from step 2.\n         \nb.PennCNV calling parameters (minimum number of SNPs and minimum length of each call) can be changed in the script $ibpcnvdir/penncnv_pipeline/03_2_cnv_calling.sh, before launching the previous step. By default, these are set to 5 and 1000 bp respectively.",
    "c.Check that all batches are completed successfully. Catching any PennCNV error is quite straightforward—the following command can be used\ncd $workingdir && grep 'ERROR' pennlogs/*\nHowever, SLURM and PBS problems can be more complex to find. To check that all samples have been processed, run:\n         \nif [$(wc -l samples_list.txt | cut -d ' ' -f1) == \\\n  $(wc -l results/autosome.qc | cut -d ' ' -f1)]; then\n  echo \"All good\"; fi\nIf this check fails, the following command can be used to list the samples that have not been processed by PennCNV:\n         \njoin -v2 -1 1 -2 2 <(LANG=C tail -n+2 1 results/autosome.qc | sort -k) \\\n  <(LANG=C tail -n+2 samples_list.txt | sort -k 2)\nOne common problem is for a full batch or a batch chunk to fail, usually due to the job scheduler. If that is the case, the full batch can be relaunched running (for PBS systems, use qsub instead of sbatch):\n         \nsbatch IBPcnv/penncnv_pipeline/03_1_per_wave.sh $workingdir \\\n$ibpcnvdir $n_wave\nwhere $n_batch is the number of the failed batch. We suggest redoing the whole batch also in the case of a partial failure.\n         \nd.After the calling pipeline is completed, the batches can be combined into two single files (CNV calls and QC) running (for PBS systems, use qsub instead of sbatch)\nsbatch IBPcnv/penncnv_pipeline/04_combine_results.sh \\\n  $workingdir $ibpcnvdir $maxgap\ne.This will also perform a “soft stitching” step, meaning that for each sample, close calls with the same copy number will be stitched together. This is controlled via the $maxgap parameter; we recommend a value of 0.2 and not higher than 0.4, as higher values could alter the raw call-set. A stronger stitching will be performed in Basic Protocol 2 when selecting putative CNVs in a specific locus.",
    "5. Tabix indexing the intensity files. In Basic Protocol 2, we take advantage of the speed of tabix-indexed files as well as of the GC waviness-adjusted LRR values.\n         \nThe GC model file for the specific array in use should have already been generated. If the user is interested only in the second half of the protocol, the following command can be used:\nbash IBPcnv/misc/create_gcmodel.sh $workingdir $ibpcnvdir\nb.To perform the indexing run:\nbash IBPcnv/penncnv_pipeline/05_launch_tabix.sh $workingdir $ibpcnvdir\nThis will also compute the GC waviness-adjusted LRR values for each marker. This can be used in the visual inspection and is also discussed in the Commentary. This step concludes Basic Protocol 1.",
    "CNV calling using SNP-array data is an intrinsically imprecise process, and strongly depends on the quality of the initial SNP-array raw data. In particular, it is prone to pick up noise as signal, as well as to “over-segment,” that is, incorrectly splitting a (often large) CNV call into several smaller ones. This ultimately may lead to unreliable results. Moreover, precise CNV boundaries (at the level of 1-2 SNP probes) are very difficult to obtain. Different research groups have developed different strategies to overcome these problems. To counter over-segmentation, it is common practice to “stitch” close, consecutive calls with the same copy number (CN). To reduce noise, some kind of filtering is almost always included. At the CNV call level, the filtering can act on the call length, the number of SNP probes, or, more rarely, on the confidence score. Some filtering is usually also performed at sample level; this may include removing samples with too high an LRR standard deviation (LRRSD), extreme BAF drift, or too many calls. A few studies, such as this protocol, also perform an initial filtering on the SNPs in order to reduce the general noise of the raw data. Notably, these approaches tend to lead to false positive calls being a larger issue than false negative ones. Changing the type and strength of the CNV call level filtering enables researchers to balance between false negatives and the number of calls to validate (i.e., false positives to manually screen). In order to reduce the number of false positives, two main strategies have typically been used. The first is to visually validate all putative CNV calls.",
    "This approach is time consuming and prone to human error; however, when done correctly, it is the overall best approach, and it is commonly used when calling CNVs in a limited set of specific genomic loci, such as recurrent CNV loci (Calle Sánchez et al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0002]; Stefansson et al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0015]). This approach should always be preferred when the CNVs of interest are rare, and thus a few false positives or negatives could significantly affect estimated prevalence and downstream analysis. The second approach is to use multiple algorithms to perform the CNV calling and intersect the resulting callsets (often using a form of reciprocal overlap in terms of base pairs or probes) and filter the results. An example is using more than one program [such as PennCNV (Wang et al., 2007[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0018]), QuantiSNP (Colella et al., 2007[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0003]), and others) and considering “validated” only calls where two or more programs made the call. In this case the reasoning is that even if some false positives are introduced, with a large sample size, these errors will be equally distributed among, e.g., cases and controls. Such studies usually also perform some kind of grouping before doing any analysis (e.g., deletions affecting at least one gene) and rarely consider individual CNVs on their own.",
    "In this protocol, we follow the first approach (i.e., always relying on visual inspection as the final validation step), and we integrate it with newly developed filters that do not depend on the PennCNV output but directly on the raw data. By doing so we are able to reduce the number of putative carriers to inspect, while also minimizing the number of false negatives as much as possible. This approach is particularly suited for large cohorts where the number of false positives can be high. In smaller datasets, it may not be equally useful to perform the filtering step, and the user can simply use our package to standardize the files and then use the graphical interface to validate all putative carriers. It is important to notice that, as stated in the main introduction, this protocol has been designed with a strong focus on recurrent CNV, meaning CNV with relatively fixed and precise start and end positions. As an example, a researcher interested in all recurrent CNVs in the larger 15q region should specify each specific locus individually in the loci.txt file (see required files), while a researcher interested in all CNVs overlapping the NRXN1 gene should use a very low minoverlap value (see step 3c) and avoid the advanced filtering (see step 4d).\nFigure 2[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0002] shows a schematic representation of the Basic Protocol 2.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/58daf4ff-8d25-4e37-9cd0-14daad6092f5/cpz1621-fig-0002-m.jpg</p>\nFigure 2\nMore detailed schematics of Basic Protocol 2. Colors and icons are kept consistent with Figure 1[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0001], indicating starting data files, QCtreeCNV, and DeepEYE respectively.\nRequired Files",
    "The only additional file needed to run the second part of the protocol is a list of the loci of interest. The file loci.txt must be a four-column tab-separated text file with a header. The columns must be called “locus,” “chr,” “start,” and “end.” The chromosome must be in integer format. This format is used in all the results and intermediate R objects, as well as the tabix-indexed intensity files. It simply consists of integers from 1 to 22 that are used for autosomes, plus 23 for X, 24 for Y, 25 for XY, and 26 for MT.\nProtocol steps\n1. Setup. All software and scripts required are provided in a docker/singularity container and a GitHub repository. Support Protocol 2 details the installation process. It is assumed the user has successfully completed Basic Protocol 1. Throughout this protocol, all files are loaded in R as data.table objects. Please note that they behave slightly differently than data.frame in certain situations. If the user prefers using data.frame to explore the results, after the protocol completion the main objects can be converted by running:\n         \nobjectA_df <- as.data.frame(objectA_dt)\nThis will ensure full consistency when using rbase commands and the tidyverse framework.\n2. QCtreeCNV filtering pipeline. This step is meant to be run interactively in an R session. The user can use the provided commands in an R script; however, we suggest exploring at least a couple of loci interactively before setting the final parameters.\n3. Preparation. All code lines are shown in code block 1.\n         \nLoad data into R. To launch an interactive R session using the provided singularity image, run:\nsingularity exec ibpcnv.simg R",
    "At this point, the four main files can be loaded into R. Assuming the user followed the suggested naming in Basic Protocol 1, this can be done by running lines 1 to 5.\n         \nb.Check formats and select the calls in the loci of interest. These steps can be performed with a single function, qctree_pre(). It takes the four main objects from the previous step and the parameters for stitching close calls. Line 7 shows the code for default values. If there is any problem with the inputs, the function will fail with an error message explaining the specific problem. By default, the function will also take care of multiple calls in a locus from a single locus, keeping only the largest call, regardless of the copy number. The user can avoid this by setting rm_dup = F. Note however that the downstream steps do not support multiple calls per sample in a single locus and will throw an error if any is found.\nc.The stitching function takes three main parameters, minimum number of SNPs for calls to be considered, maximum gap between two consecutive calls with same CN in order to stitch them together, and minimum overlap between a call and a locus for the call to be selected as a putative CNV. Default values are respectively 20, 0.5 and 0.2. These values can be changed with the following parameters: minsnp, maxgap, minoverlap, e.g., pre <- qctree_pre(loci, cnvs, qc, samples, minsnp = 15, maxgap = 0.4, minoverlap = 0.5).",
    "d.Compute CNV Regions. We provide two different functions to compute CNVRs (CNV Regions), cnvr_fast() and cnvrs_create(). Additionally, the user is free to use a different method as long as the results are in the correct format. CNVRs are used here to separate groups of largely overlapping calls within a certain locus, in particular groups with different lengths. CNVRs and their computation are further discussed in the Commentary and in the package manuals and vignette. The suggested function can be run as shown in line 9:\n1> library(data.table); setDTthreads(2); library(QCtreeCNV)\n2> cnvs <- fread(\"results/autosome.cnv\")\n3> qc <- fread(\"results/autosome.qc\")\n4> loci <- fread(\"loci.txt\")\n5> samples <- fread(\"samples_list.txt\")\n6>\n7> put_cnvs <- qctree_pre(loci, cnvs, qc, samples)\n8>\n9> cnvrs <- cnvr_fast(put_cnvs)\nCode block 1.\n4. qctree() filtering:\n         \nThe filtering function is structured as a decision-making tree consisting of five main steps, and each step has multiple parameters the user can change. More technical details on each step and how the main parameters are connected with the outputs of Support Protocol 1 (quality control) are further discussed in the Commentary. To run the function with default values type:\ncnvs_out <- qctree(cnvrs[[1]], cnvrs[[2]], loci)\nIf no filtering is deemed necessary (for example when the number of putative calls is small), the user can proceed directly to step 4d.\n         \nb.Step 1 of the filtering tree is removing QC outliers sample-wise, and this is the most accessible step to customize. By default, samples will be removed if they have LRRSD > 0.35, BAF drift > 0.01, or GCWF outside of the window –0.02 to 0.02. These values can be changed with the following parameters: maxLRRSD, maxBAFdrift, maxGCWF, minGCWF.",
    "c.The resulting table will contain the column “excl” with the value 0, meaning the line is a good putative CNV call, or 1, meaning the line is a bad putative CNV and can be skipped in the visual inspection step. This table can be exported in the correct format for the visual inspection interface with:\nexport_cnvs(cnvs_out[excl == 0,], \"putative_cnvs.txt\")\nThis will write the file putative_cnvs.txt in the $workingdir containing only the good putative CNV calls in the format expected by DeepEYE. To export all calls, type:\n         \nexport_cnvs(cnvs_out, \"putative_cnvs.txt\")\nd.If no advanced filtering is needed, the user can simply apply the standard filters (LRRSD, BAFdrift, GCWF) when exporting the table. Using the data.table syntax, run:\nexport_cnvs(put_cnvs[LRRSD <= 0.35 & BAFdrift <= 0.01 &\n    between(GCWF, -0.02, 0.02, incbounds=T),],\n  \"putative_cnvs.txt\")\n5. Visual inspection.\n         \nVisual inspection of the putative CNVs is necessary to validate the true CNV carriers with precision. The in-house program DeepEYE (included in the provided container) provides the user with a graphical interface to assign a label (true, false, unknown) to each CNV candidate. The results are stored in an extra column in the putative CNVs file.\nDeepEYE requires three main inputs—the samples and loci lists, plus the putative CNVs table. It can be run from the singularity image, assuming all files have the standard names described in the protocol:\nsingularity exec ibpcnv.simg python3 /opt/eyeCNV/visualizer.py \\\n$workingdir putative_cnvs.txt loci.txt samples_list.txt GC_YES\nThis will open a graphical window as shown in Figure 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0003].\n         \nInitially, the window will display only a series of buttons and boxes (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0003] left side). In order to start the actual visual inspection (Fig. 3[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0003] right side), the user needs to follow these steps:",
    "Name the project (box a); the name will dictate the name of the output file, i.e., visual_output_projectname.txt. The project name should be meaningful, e.g., when user “abc” is evaluating deletions in the TAR locus, a good project name could be TAR_del_abc.\nSelect the loci to inspect. Left clicking on button b will open a secondary window (i) where it is possible to select the loci to inspect in the current run.\nSelect the condition. Left clicking on button c will open a menu with the following options: true, false, unknown, all. In a new project “all” should be selected; while re-evaluating a previous project the user can select only a portion of the calls (e.g., unknown).\nSelect the type. Left clicking on button d will open a menu with the following options: duplications, deletions, any. This lets the user select a specific type of CNV to inspect.\nOnce this is set, the user can click button e, “start.”\nc.If at least one CNV call was selected, the right panel will appear. The panel consists of two plots (h); the top is for LRR and the bottom is for BAF. Each dot represents a marker. The red dots are within the locus of interest, the blue dots are outside.\nd.For each plot, the user can evaluate if a CNV is present within the red region and record the decision using the dedicated buttons (f):\nTrue: there is a CNV in the red region (with the correct CN, as shown in g).\nFalse: there is not a CNV in the red region (or the CN is not correct).\nUnknown: it is not possible to tell whether a CNV is present or not, likely due to excessive noise in the region.\nError: useful to mark samples with problematic intensity data.",
    "e.The progress bar and text in g mark the session's progress. Once all selected calls have been inspected, the result file will be written in $workingdir. The last column, “visual_output,” contains the record of the visual inspection as integers: 1 (true), 2 (false), 3 (unknown), –7 (error). This file can be used again as a putative CNV file, for example to re-evaluate unknown calls only.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/01639876-46b1-4c58-a096-e1bdca074b79/cpz1621-fig-0003-m.jpg</p>\nFigure 3\nDeepEYE graphical interface (no CNV is present in the region, simulated data). Refer to step 5b of Basic Protocol 2 for the usage instruction.\n6. Visual evaluation concludes Basic Protocol 2. The file visual_output_projectname.txt will contain the results. The visual output codes (step 5e) can be used to filter the relevant CNVs. See also the final section Understanding Results.",
    "This protocol, similar to any other CNV calling pipeline, implements a certain set of filters. We propose valid default values based on the scientific literature and our research experience (Calle Sánchez et al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0002]; Stefansson et al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0015]). However, since this protocol is mostly aimed at CNV calling in recurrent loci where the actual CNVs are quite rare, we need to be extremely careful that our processing does not introduce any false negatives that may severely bias our estimates (false positives are controlled via visual inspection). In large cohorts, this ultimately means balancing the strength of the filters and the amount of manual validation required. Finally, some filters are applied sample-wise on values that directly reflect the noise in the data, for the specific sample (LRRSD and BAF drift in particular). We found that it is often quite possible to use relaxed filters, thus eliminating fewer samples. However, in doing so, one must be able to assess that the ability to detect CNVs is not significantly different in “noisier” samples, otherwise biases may be introduced in the results. Here, we show how to produce a series of plots that help identify such potential issues in the results of the protocol, and briefly discuss the interpretation of each one. Specific problems and solutions are then highlighted in the troubleshooting section.\nProtocol steps\n1. Setup. This support protocol is meant to help evaluate the performance of the CNV calling protocol. First, start an R session. From $workingdir:\n         \nsingularity exec ibpcnv.simg R\nLoad the visual inspection results, e.g., assuming the visual inspection results were saved as visual_output_ALL.txt in $workingdir:\n         \nlibrary(data.table); vi_res <- fread(\"visual_output_ALL.txt\")",
    "2. Create the plots. The function qc_plots_cnvs() will create three plots for the main filtering arguments and more supporting ones. Here we will discuss briefly only the main one, for the complete discussion refer to the Commentary. As an example, to create the QC plots for the CNVs in all loci, simply run:\n         \nlibrary(QCtreeCNV); qc_plots_cnvs(vi_res, \"all_loci\")\nThis will create the folder all_loci in the working directory and save all QC plots in it. Note that the plots show the results regarding only the CNVs marked as true (visual_ouput == 1).\n3. Interpret the results. Figure 4[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0004] shows a good and a bad example of the two plot types. Also seeUnderstanding Results for more discussion.\n         \nPlot 1 (example in Figure 4A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0004]) shows the CNV prevalence in different LRRSD chunks (low, medium, high), separated for deletions and duplications. It illustrates the ability to detect true CNVs in different groups of samples from the noise perspective (high LRRSD can be considered the main indication of a noisy sample). Ideally, the prevalence should not strongly differ across the three groups, especially the one with higher LRRSD compared to the rest. A significant change means that the ability to detect CNVs is significantly affected in noisy samples, usually becoming lower. This means the LRRSD filter threshold may need to be increased. On the other hand, if a strong LRRSD filter was used and plot 1 shows very high consistency, the user may want to explore lowering it to possibly exclude fewer samples from the analysis.",
    "Plots 2 and 3 (example in Figure 4B[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0004]) show the distribution of the number of SNPs and overlap proportion with the locus per call for true CNVs, numsnp, and overlap, respectively. Both these measures are used as filters when selecting putative CNV calls. Ideally, the distribution should not seem to be “cut” at the threshold values for numsnp and overlap. If that is the case, it might indicate that some potential true CNVs are being filtered out, and it may be worth trying to relax the filters. This is especially important for very rare CNV loci, where a small increase in carriers can have an impact on power. Note that plot 2 may be more meaningful when used on individual loci or a group of loci with very similar marker coverage, since it is the absolute number of markers of each call. If different thresholds were used for different loci, they must be treated separately to obtain meaningful results.\nPlots 4 and 5 can be interpreted in the same way as plot 1 (Fig. 4A[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-fig-0004]) but regarding two other noise measures, BAF drift and GCWF, respectively. They can be considered secondary since these dimensions are less prone to affect the CNV detection accuracy.\n<p>imgsrc:https://currentprotocols.onlinelibrary.wiley.com/cms/asset/6824be90-ef9f-4ad4-8c2f-dd259b0ff62c/cpz1621-fig-0004-m.jpg</p>\nFigure 4\nA good and a problematic example of QC plots 1, 4, and 5 (panel A), and 2 and 3 (panel B). In both plots, the deletion in light red represent the ideal situation, and the duplication in light blue represent the problematic situation. (A) the group of samples with high LRRSD have fewer true duplications than the other two groups, and that the confidence interval is relatively small. (B) The threshold value we selected for numsnp appears to be cutting the left tail of the distribution for the true duplications.",
    "4. Deal with possible detection bias. If the dataset includes sample groups selected differently (e.g., cases and controls), it may be a good idea to analyze them separately. The CNV prevalence is often expected to differ in different groups (e.g., some recurrent CNVs are more frequent in neuropsychiatric case groups than in population controls); thus, combining them may lead to confusion in the interpretation of the QC plots, especially if LRRSD distribution also differs across those groups. Possible problems in these plots are described in the last two points of the troubleshooting section. Assuming the sample_IDs for one group are in vector groupA QC analysis can be run separately with:\n         \nqc_plots_cnvs(vi_res[sample_ID %in% groupA,], \"groupA\")\n5. Explore an individual locus or groups of loci. The process described in previous steps 2 and 3 can be applied to groups of loci as well as to individual loci. This is useful especially when there seems to be some problems, but only in a fraction of the loci or in a particular one. For example, to look only at the results for loci “A” and “B” run:\n         \nqc_plots_cnvs(vi_res[locus %in% c(\"A\", \"B\"),], \"loci_A_B\")\nAs a rule, it can be helpful to divide the loci of interest into two or three groups based on length (e.g., small/large) and inspect the QC plots for the groups in addition to the ones for all loci. Groups can also be based on prevalence (very rare/not very rare) or other measures. The general idea is that, while looking at all loci at the same time can give an overall impression of the results quality, it can also mask problems linked to one or very few loci. Smaller groups can help identify those, if any, and the QC plot for individual loci can pinpoint the actual problem.",
    "We provide a docker image containing all software required to run the protocol on Docker Hub at https://hub.docker.com/r/sinomem/docker_cnv_protocol[href=https://hub.docker.com/r/sinomem/docker_cnv_protocol]. This container has the following software installed: htslib (Bonfield et al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0001]; Danecek et al., 2021[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0005]) (1.14), PennCNV (Wang et al., 2007[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0018]) (1.0.5), R (R Core Team, 2018[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0011]) (4.1.2), and DeepEYE2, as well as several R packages, including data.table (Dowle et al., 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0006]), fpc (Hennig, 2020[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0008]), and VariantAnnotation (Obenchain et al., 2014[href=https://currentprotocols.onlinelibrary.wiley.com/doi/10.1002/cpz1.621#cpz1621-bib-0010]).\nRegarding the in-house software used in this protocol, the QCtreeCNV R package is available on GitHub at https://github.com/SinomeM/QCtreeCNV[href=https://github.com/SinomeM/QCtreeCNV]. Instructions to install the package are given in the README. DeepEYE2 is available at https://github.com/XabierCS/eyeCNV[href=https://github.com/XabierCS/eyeCNV]. All other scripts are collected in a GitHub repository at https://github.com/SinomeM/IBPcnv[href=https://github.com/SinomeM/IBPcnv].\nThe statistical programming language R is available at https://www.r-project.org/[href=https://www.r-project.org/]. Tabix is part of the HTSlib suite, together with SAMtools and BCFtools. It can be obtained at https://www.htslib.org/download/[href=https://www.htslib.org/download/]. PennCNV is the de facto standard in CNV calling from array data, in particular Illumina. It is available at http://penncnv.openbioinformatics.org/en/latest/user-guide/download/. In the following section, we detail how to install the docker/singularity image and use it to run the protocol.\nProtocol steps\n1. Setup. Throughout the protocol, $workingdir is used to indicate the main project folder. This folder will contain all the input and output files. For simplicity, the user can define an environmental variable:\n         \nexport workingdir=/path/to/workingdir",
    "2. Install singularity. The software should be already installed on most modern HPCs. If not, users should ask the system administrator to install it for them. To install it on a Linux workstation, one should follow the official instructions available at https://sylabs.io/guides/3.0/user-guide/installation.html[href=https://sylabs.io/guides/3.0/user-guide/installation.html]. A precompiled RPM package is available at https://dl.fedoraproject.org/pub/epel/8/Everything/x86_64/Packages/s/[href=https://dl.fedoraproject.org/pub/epel/8/Everything/x86_64/Packages/s/] and the program alien can be used to convert it to DEB (https://github.com/apptainer/singularity/issues/5390[href=https://github.com/apptainer/singularity/issues/5390]). Finally, in systems where the use of conda environments is encouraged or enforced, it should be possible to use the conda package at https://anaconda.org/conda-forge/singularity[href=https://anaconda.org/conda-forge/singularity].\n3. Download the provided container image. We provide the container on DockerHub, and it can be pulled directly by singularity. To do so, first move in the desired folder (cd $workingdir) and then type:\n         \nsingularity pull ibpcnv.simg docker://sinomem/docker_cnv_protocol:latest\nNote that the protocol expects the image to have the suggested name (ibpcnv.simg) and be stored (or linked) in the main working directory ($workingdir). If singularity fails with an error regarding the /tmp folder, it may help to set the environmental variables SINGULARITY_TMPDIR and SINGULARITY_CACHEDIR to some non-protected location (such as ∼/tmp or $workingdir/tmp).\n4. Download the IBPcnv repository. We provide two versions, one that uses SLURM (srun/sbatch) and one that uses PBS (qsub). They can be obtained running:\n         \nwget https://github.com/SinomeM/IBPcnv/archive/refs/heads/master.zip&&\\\n   unzip master.zip && mv IBPcnv-masterIBPcnv && rm master.zip\n# or\nwget https://github.com/SinomeM/IBPcnv/archive/refs/heads/pbs.zip&&\\\nunzip pbs.zip && mv IBPcnv-pbs IBPcnv && rm pbs.zip\nfor the SLURM and PBS versions respectively. For convenience, we can define the environmental variable $ibpcnvdir:\nexport ibpcnvdir=${workingdir}/IBPcnv\n5. Add the SLURM/PBS account if needed. The four scripts that use the job scheduler (03, 03.1, 03.2, and 04 in $ibpcnvdir/penncnv_pipeline/) are designed to be easily edited if the system requires the use of a specific account name. Throughout the text we provide both versions of the commands when run interactively.",
    "6. Run the protocol. All scripts assume that the singularity image is used. The pipeline in Basic Protocol 1 is designed to take advantage of the SLURM or PBS job scheduler, depending on which branch of the IBPcnv repository was chosen.\n7. Docker versus singularity. To download the container using docker we run:\n         \ndocker pull sinomem/docker_cnv_protocol:latest\nThen, to print the tabix help page using singularity image or docker we type respectively:\n         \nsingularity exec /path/to/ibpcnv.simg tabix --help\n# or\ndocker run sinomem/docker_cnv_protocol:latest tabix --help\nOne of the main differences is that, conveniently, singularity automatically mounts several file storage locations to the container while Docker does not. Moreover, in order to use docker a user needs to be added to the docker group and this process may require sudo permissions. Refer to the docker documentation for further details, https://docs.docker.com/[href=https://docs.docker.com/]."
  ],
  "subjectAreas": [
    "Bioinformatics"
  ],
  "bigAreas": [
    "Bioinformatics & Computational Biology"
  ]
}