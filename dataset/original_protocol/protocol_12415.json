{
  "id": 13821,
  "origin_website": "Jove",
  "title": "Topographical Estimation of Visual Population Receptive Fields by fMRI",
  "procedures": [
    "1. Data Acquisition\nPrepare a stimulus protocol that is effective in eliciting a reliable retinotopic visual response as previously described in Dumoulin and Wandell1 and Lee et al.2. However, other well established paradigms are also applicable depending on the specific experimental question to be addressed.\nPresent bar stimuli drifting across the screen sequentially along 8 directions of space, in steps of 45 degrees. Ensure that the motion is in synchrony with scanner frame acquisition (TR ~2 sec) so that the bar moves a step once an fMRI frame starts and stays at the new location until the frame ends.\nTo measure a correct baseline signal, add epochs without bar stimulation1.\n\t\nDefine a field of view (10 to 15° radius) in visual angle over which the stimulus is presented. Present moving or flickering checkerboard patterns (checker size = 0.94 x 0.94 deg2, pattern update rate = 250 msec/pattern) within the bar to elicit strong visual responses.\nInput the following specific parameters: 8 evenly spaced directions of motion, bar width equal to 1.875 deg, and bars move by half the bar width per frame (2 sec). Additional details can be found in Lee et al.2.\nGenerate a spot (~0.25°) in the screen center on which the subject’s eyes fixate during the experiment. Change color of the spot randomly in time.  \nScan the brain of a subject in an MRI scanner using a typical echo-planar-imaging (EPI) scan that has 192 frames duration (24 frames in each direction of motion). Repeat the scans 4-8 times to increase signal-to-noise ratio.",
    "Set parameters for the EPI sequence as follows: TR = 2 sec, TE = 40 msec, matrix size = 64 x 64, 28 slices, voxel size = 3 x 3 x 3 mm3, flip angle = 90°, Alternatively, apply sequences with a finer resolution (e.g., 2 x 2 x 2 mm3) or a short TR (e.g., 1-1.5 sec) covering only the visual cortex2.\nTrack eye movements with an eyetracker system during functional scans to ensure fixation is maintained to within 1-1.5° of the fixation point.\n\tNOTE: Here, a head-coordinate based eyetracker in a goggle system is used, but other suitable eyetracker systems can be used instead.\nInstruct the subjects to fixate the spot on the screen center generated in step 1.3.2. To ensure the subjects are fixating, instruct them to report the color changes of the fixation spot.\nObtain anatomical scans, at 1 x 1 x 1 mm3 resolution (e.g., T1-MPRAGE; TR = 1,900 msec, TE = 2.26 msec, TI = 900 msec, flip angle = 9°, 176 partitions).\n\tNOTE: These anatomical scans will be used for segmentation as well as for aligning the functional images to the anatomy both within and across scans. For better alignment between functional (EPI) images and the anatomy, obtain also an inplane anatomy scan, with resolution identical to the EPI, using T1-weighted fast spoiled gradient echo (SPGR) sequence1.\n2. Data Pre-processing\nNOTE: Prior to estimating pRF properties, several typical fMRI data pre-processing steps are needed, such as head motion correction and alignment of functional volumes to the anatomical scan. In this article, all pre-processing, estimation, analysis and presentation of results obtained are performed using the open source MATLAB-based software toolbox VISTA LAB available on the VISTA software site. http://white.stanford.edu/newlm/index.php/Main_Page.",
    "Load the anatomical scan into MATLAB and prepare a volume anatomy using a function called createVolAnat.\nSegment Gray matter, White matter, and CSF using the function “ItkGray”.\nPrepare functional data by converting DICOM (i.e., raw MRI file format for Siemens) files into NIFTI (i.e., standard functional MRI file format) files, and load data into VISTA using a function called mrInit.\nCorrect head-motion and align functional images to the anatomy loaded in step 2.1 using rxAlign based on an affine matrix transformation.\nAverage functional motion-corrected scans for improving signal-to-noise ratio by clicking mrVISTA Analysis TimeSeries Average tSeries. Exclude from averaging scans during which eye movements deviates from fixation more than 1-1.5°. If signals from different runs have different dc-drifts, average functional scans after removing the dc-drifts.\nCalculate the mapping coordinates between functional scans and Gray matter and identify corresponding Gray-matter voxels in the functional scans by selecting the following menus: mrVISTA Window Open Gray 3-View Window. Assign BOLD signals in the Gray matter voxels by interpolation, choosing one of the options available in mrVISTA.\n3. Estimation of pRF Topography and Parametric Modeling\nDownload the code files through the following link: https://sites.google.com/site/leesangkyun/prf/codes.zip, extract the compressed file and place them in a preferred location of the local computer. Add the path of the folder in MATLAB.\nSet the stimulus parameters used in the experiment by selecting the following menus: mrVISTA Analysis Retinotopic Model Set Parameters. Specify the following parameters such as stimulus images, the stimulus size, the canonical hemodynamic function, the frame rate of the fMRI scanner.\nPrior to the pRF estimation, prepare the initial parameter sets (Figure 1B).",
    "Set the cross-validation sets in “tprf_set_params.m” from the code files. Divide timeseries into at least two subsets (one set for testing and the remaining sets for training) that are long enough for the bar to sweep the entire stimulus space. Alternatively, without averaging scans in step 2.4, validate scans by leaving out one scan for testing and using the remaining scans for training.\nSet a coarse parameter set (λ in Figure 1; λ = [10-2 10-1 1 101 102]) in “tprf_set_params.m”. Then, set a fine scale range ([0.1 0.3 0.5 0.7 0.9 1 3 5 7 9]) in “tprf_set_params.m”.\n\t\tNOTE: The program uses the coarse set to select the λ resulting in the highest explained variance. Then, the program searches the space around the selected λ using the fine scale range, further refining the selection of λ that yields the highest explained variance.\nSet a threshold (0.2) of the explained variance for visually responsive voxels in “tprf_set_params.m”.\n\t\tNOTE: This threshold is used as the reference for selection of visually responsive voxels. Alternatively, make an ROI for a non-visually responsive region (e.g., by drawing a sphere with a radius of 1 cm in a non-visually responsive brain area), where the threshold can be automatically calculated.\nSet a set of thresholds ([0.3, 0.5, 0.7]) for defining the pRF center region in the normalized topography in “tprf_set_params.m” (i.e., [0 to 1] or [-1 to 1] with epochs without bar stimulation in step 1.3.1).\n\t\tNOTE: From the set of thresholds the program provided selects the “best” threshold, i.e. the threshold that defines a pRF central region for which the pRF center model explains the greatest signal variance. Alternatively, choose a different set of threshold values depending on the characteristics of the topography.",
    "Execute “tprf_runpRFest.m” calculate the pRF topography (Figure 1) and fit a 2D anisotropic Gaussian. After specifying all parameters described in this protocol, and running the code, obtain the final estimation results.\nimgsrc://cloudfront.jove.com/files/ftp_upload/51811/51811fig1highres.jpg\nFigure 1: PRF estimation process. (A) Schematic illustration of the process followed for pRF topography estimation. h(t): hemodynamic response function, A(t): stimulus, m: pRF, Reg: L2-norm regularization. (B) Specific steps for pRF topography estimation and pRF center modeling. The set of parameters required for the estimation is listed in each step. A one-dimensional section of topography and its model are illustrated. Under “Model Fitting”, black and red curves represent the topography and its pRF center model with a center threshold of 0.5, respectively. The blue dashed line indicates a threshold for the pRF central region.Subscription Required. Please recommend JoVE to your librarian."
  ],
  "subjectAreas": [
    "Behavior"
  ],
  "bigAreas": [
    "Ecology & Environmental Biology"
  ]
}