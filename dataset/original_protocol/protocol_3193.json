{
  "id": 3382,
  "origin_website": "Cell",
  "title": "Protocol for predicting peptides with anticancer and antimicrobial properties by a tri-fusion neural network",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nTriNet is a peptide identification method that can be leveraged by using a well-trained model. In addition, to enable researchers and developers to effectively apply this framework by training it on their own peptide data, we provide a comprehensive step-by-step guide that details the process of training TriNet on the input peptide information with fixed format files.\nData preparation\nTiming: <5 min\nThis step illustrates the progress of data preparation and processing to satisfy the formatting requirements of TriNet.\nNote: TriNet captures relevant information from peptide sequences, including serial fingerprints, sequence evolution, and physicochemical properties. These features are then fed into a well-trained deep learning model for classification purposes. TriNet automatically calculates the serial fingerprints and physicochemical properties directly from the primary peptide sequences in the form of a FASTA file. However, for the sequence evolution information, the standard mode of TriNet needs you to provide the position-specific scoring matrix (PSSM) feature profile generated via a sequence similarity alignment by using the BLAST (Basic Local Alignment Search Tool). For convenience, we also provide a fast mode of TriNet without calculating the PSSM matrix, which may slightly reduce the prediction accuracy of TriNet. The process of calculating the PSSM files by using BLAST is also detailed as follows.\nProvide a FASTA file.\nNote: The FASTA format is a widely used text format in bioinformatics for storing nucleic acid or peptide sequences.\nTo utilize TriNet, submit a FASTA format file containing the peptide sequence(s) to be predicted.\nNote: If you are providing training data instead of predictive data, you will need to include both the peptide sequences and a corresponding label for each peptide.\nAppend a label (1 for positive and 0 for negative) at the end of the corresponding peptide representation separated by a ‘|’.",
    "Note: For example, the first peptide is represented by “>ACP_1” for predictive purposes, while it should be “ACP_1|1” for training purposes (Figure 2[href=https://www.wicell.org#fig2]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2964-Fig2.jpg\nFigure 2. Display of FASTA files and downloading interfaces of BLAST and uniref. 90 database\n(A) A FASTA format file.\n(B) The downloading interface of BLAST.\n(C) The download interface of the uniref. 90 database.\nGenerate PSSM profiles.\nNote: To obtain PSSM profiles as the inputs of the standard mode of TriNet, you should download the BLAST program and make sequence similarity alignments as follows.\nNote: The installers and source code are available from https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/[href=https://ftp.ncbi.nlm.nih.gov/blast/executables/blast+/LATEST/] (Figure 2[href=https://www.wicell.org#fig2]B). Download the corresponding installation packages that are compatible with your operating system. Here is an example of how to install the software on Windows and Linux systems.\nDownload and install the BLAST tool on a Windows system.\nDownload the installation file ncbi-blast-2.13.0+-win64.exe.\nInstall BLAST to the specified location by double-clicking on the ncbi-blast-2.13.0+-win64.exe file.\nDownload and install the BLAST tool on a Linux system.\nDownload the installation package ncbi-blast-2.13.0+-x64-linux.tar.gz.\nExtract the contents of the package.\n>tar -zxvf ncbi-blast-2.13.0+-x64-linux.tar.gz\nAdd the absolute PATH of the BLAST executable directory (bin) to the environment variable $PATH to enable direct calls by program name.\n>echo “export PATH={path to Blast}/bin:\\$PATH” >> ∼/.bashrc\nDownload the uniref. 90 protein FSATA file and unzip it.\nNote: The uniref. 90 FSATA file can be downloaded from https://ftp.ebi.ac.uk/pub/databases/uniprot/uniref/uniref90/[href=https://ftp.ebi.ac.uk/pub/databases/uniprot/uniref/uniref90/] (Figure 2[href=https://www.wicell.org#fig2]C).",
    "Note: The UniRef18[href=https://www.wicell.org#bib18] (UniProt Reference Clusters) provides clustered sets of sequences from the UniProt Knowledgebase (UniProtKB), an internationally recognized protein sequence resource to obtain complete coverage of sequence space at several resolutions while hiding redundant sequences. UniRef100 combines identical sequences and sub-fragments with 11 or more residues into a single entry. UniRef90 is built by clustering UniRef100 sequences such that each cluster is composed of sequences that have at least 90% sequence identity to, and 80% overlap with, the longest sequence in the cluster (the seed sequence).\n>gunzip uniref90.fasta.gz # To obtain the uniref90.fasta file.\nBuild the uniref. 90 database using the makeblastdb command in the BLAST program.\n>makeblastdb -in {path to uniref90.fasta file}/uniref90.fasta -dbtype prot -out {path to the database to be stored}/uniref90 -parse_seqids\nStart alignment. Run the following command to align the query peptide sequence(s) among the database you have built to obtain the PSSM profile (Figure 3[href=https://www.wicell.org#fig3]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2964-Fig3.jpg\nFigure 3. Illustration of a PSSM profile\n>psiblast -db {path to the uniref90 database} -evalue 0.001 -num_iterations 3 -num_threads 1 -query {path of the query FASTA file} -out_ascii_pssm {path of the output PSSM profile}\nSet the parameters of blast as follows.\n-db: The path of the database you built, type = str.\n-evalue: The evalue represents the number of subject sequences that are expected to be retrieved from the database with a bit score equal to or greater than the one calculated from the alignment of the query and subject sequences, based solely on chance rather than homology, type = float.\n-num_iterations: The number of iterations (integer types usually set to 3), type = int.\n-num_threads: The number of threads you want to use (limited by CPU performance), type = int.\n-query: The path of the sequence(s) you want to align (must be in FASTA format), type = str.",
    "-out_ascii_pssm: The path of the output profile as a PSSM matrix of your query sequence(s), type = str.\nNote: The file names of PSSM profiles should be numbers and be related to the orders of the peptide sequences. For example, if there are N peptide sequences in the FASTA file, the corresponding N PSSM files should be named 1.pssm, 2.pssm, ..., N.pssm.\nDisplay the PSSM matrix.\nNote: PSSM is used to describe the conservation degree of amino acid residues in peptide sequences (Figure 3[href=https://www.wicell.org#fig3]).\nCollect and screen peptide sequences.\nNote: In AMP/ACP collection, there is a rigorous set of criteria that encompasses the assurance of reliable sources, high-grade purity, antimicrobial activity, compliance with relevant safety standards, and commendable stability.19[href=https://www.wicell.org#bib19] Here, we provide an example for collecting and screening peptides. Additionally, we provide several public datasets and other sources for collecting potential AMPs to be predicted.\nNote: For training, validation, and testing datasets, it is imperative to ascertain the reliability of the data sources. In this study, we describe the process of generating the AMPlify3[href=https://www.wicell.org#bib3] dataset to explicate the criteria that should be followed when constructing datasets.\nSelect positive samples.\nCombine publicly accessible AMP datasets, namely, the Antimicrobial Peptide Database (APD3, accessible at http://aps.unmc.edu/AP[href=http://aps.unmc.edu/AP]) and the Database of Anuran Defense Peptides (DADP, accessible at http://aps.unmc.edu/AP[href=http://aps.unmc.edu/AP]).\nRemove redundant sequences to yield a non-redundant positive dataset of 4173 AMP sequences.\nNote: Each sequence within this positive dataset is characterized by a length not exceeding 200 amino acid residues (see Table 1[href=https://www.wicell.org#tbl1] for examples of selected AMPs).\ntable:files/protocols_protocol_2964_1.csv\nSelect negative samples.\nCollect sequences with no annotations related to antimicrobial activities and with a length not exceeding 200 amino acid residues from the UniProtKB/Swiss-Prot repository.",
    "Remove duplicates and sequences comprising residues beyond the scope of the 20 canonical amino acids, generating a set Sn of candidate negative samples.\nRemove the peptide sequences in Sn that were annotated as potential AMPs in the UniProtKB/Swiss-Prot database or included in the 4173 positive samples.\nRandomly select 4173 negative samples from the remaining candidate negatives to match the number and length distribution of positive samples (see Table 1[href=https://www.wicell.org#tbl1] for examples of selected non-AMPs).\nNote: DRAMP19[href=https://www.wicell.org#bib19] (Data Repository of Antimicrobial Peptides) stands as a publicly accessible, curated repository of resources pertaining to antimicrobial peptides. It provides many candidate peptides whose antimicrobial activities have not been assayed yet. These peptides can be subjected to predictive analysis using TriNet to assess their antimicrobial potentials, followed by experimental validations. You can also refer to the following four publicly available AMP databases: ADAM,17[href=https://www.wicell.org#bib17] APD,20[href=https://www.wicell.org#bib20] CAMP21[href=https://www.wicell.org#bib21] and LAMP.22[href=https://www.wicell.org#bib22] In addition to the potential AMPs in public databases, you can collect candidate AMPs from other sources. For example, in a recent study,12[href=https://www.wicell.org#bib12] all the microbial genomes of the human gut microbiome were assembled, and then many potential AMPs were extracted via predicted ORFs on the assembled genomes.\nRunning TriNet with its default trained model\nTiming: <5 min\nThis step is to execute the prediction module, which requires you to submit a FASTA file containing one or more peptide sequences. In addition, you may opt to submit corresponding PSSM profiles, which provide evolutionary information for the input peptide sequence(s) and can potentially improve the prediction accuracy. After running, TriNet returns to you an output file in the format of CSV (detailed in the section of “expected outcomes[href=https://www.wicell.org#expected-outcomes]”).\nNote: The amount of time consumed is related to the numbers and lengths of the submitted peptide sequences.",
    "Access the TriNet directory from the PowerShell Prompt Console (as shown in Figure 1[href=https://www.wicell.org#fig1]B).\nNote: For example, if the TriNet.py file is located in the TriNet-master folder, which is in the download folder, you should type the following command into the console prompt.\n>cd {downloads_path}/TriNet-master\nLaunch the TriNet tool.\nRun TriNet by the following command.\n>TriNet.py [-h] [--PSSM_file PSSM_FILE] [--sequence_file SEQUENCE_FILE] [--output OUTPUT] [--operation_mode OPERATION_MODE]\nSet the required parameters as follows.\n--sequence_file, -s SEQUENCE_FILE: The path of a FASTA format peptide sequence file, type = str.\n--operation_mode, -mode OPERATION_MODE: Four options are offered for the ‘-mode’ item, including ‘sc’, ‘sm’, ‘fc’ and ‘fm’, where ‘s’ represents the standard mode (needing you to provide PSSM profiles), ‘f’ represents the fast mode (only needing a FASTA file), ‘c’ means the prediction of anticancer peptides, and ‘m’ means the prediction of antimicrobial peptides, type = str. e.g., ‘sc’ represents the standard prediction of anticancer peptides.\nSet the optional parameters as follows.\n-h, --help: Printing the help message.\n--output, -o OUTPUT: The path of the prediction result, default with the path of {current path}/output, type = str.\n--PSSM_file, -p PSSM_FILE: The path of PSSM feature profiles, which is a required parameter if you choose the standard mode for prediction, type = str.\nNote: The following command represents the prediction of anticancer peptides by the standard mode, using the ACP_example.fasta file and the PSSM profiles under the ./pssm_acp_example path, and returns the output file acpout.csv.\n>python TriNet.py -mode sc -s ./ACP_example.fasta -p ./pssm_acp_example/ -o ./acpout.csv\nUtilizing online web service with its default trained model\nTiming: <5 min",
    "In addition to the approach for utilizing TriNet mentioned above, we also provide an online web service that enables the prediction of both anticancer and antimicrobial peptides. This alternative approach provides a convenient and accessible means of accessing the functionality of TriNet.\nNavigate to the following web address http://liulab.top/TriNet/server[href=http://liulab.top/TriNet/server] to access the online service page (see Figure 4[href=https://www.wicell.org#fig4]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2964-Fig4.jpg\nFigure 4. The online web server of TriNet\n(A) The web server homepage (http://liulab.top/TriNet/server[href=http://liulab.top/TriNet/server]) offers a “standard mode” using PSSM features and a “fast mode” without using PSSM features for both ACP and AMP predictions. For the fast mode, you only need to upload a FASTA file as the input.\n(B) For the standard mode, you need to upload a FASTA file and corresponding PSSM files as the inputs.\nReview the notes before using the TriNet online services.\nNote: These notes contain essential information that you should be aware of before proceeding with the service.\nNote: The use of the fast mode of TriNet for predicting anticancer or antimicrobial peptides requires only a FASTA file containing the peptide sequences to be predicted.\nUpload a FASTA file and click the “Submit” button to initiate the fast mode analysis process (see Figure 5[href=https://www.wicell.org#fig5]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2964-Fig5.jpg\nFigure 5. Usage of the online services by the ACP_example.fasta file\n(A) Prediction of the anticancer peptides by using the fast mode.\n(B) Prediction of the anticancer peptides by using the standard mode.\nNote: If you opt for the standard mode of TriNet to predict anticancer or antimicrobial peptides, additional PSSM profiles are needed.\nUpload all the required files and click the “Submit” button to initiate the standard mode analysis process (see Figure 5[href=https://www.wicell.org#fig5]B).",
    "Note: The file names of the PSSM profiles should be numbers and be related to the orders of the peptide sequences. For example, if there are N peptide sequences in the FASTA file, the corresponding N PSSM files should be named 1.pssm, 2.pssm, ..., N.pssm.\nNote: Upon submission of the task, the program will execute and generate a result file, which can be downloaded (see Figure 6[href=https://www.wicell.org#fig6]). Additionally, the interface provides an option for you to view the results online (see Figure 6[href=https://www.wicell.org#fig6], detailed in the “expected outcomes[href=https://www.wicell.org#expected-outcomes]” section).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/2964-Fig6.jpg\nFigure 6. Illustration of the prediction output interface\nRetraining TriNet by using a new training dataset\nTiming: <20 min\nIn this step, we give the detailed steps of retraining TriNet by using a new dataset.",
    "Note: The TriNet project provides the ability to adjust hyperparameters when retraining the model by using your own data as follows. First, download the training code from https://github.com/hjy23/TriNet-Reproducing[href=https://github.com/hjy23/TriNet-Reproducing]. Second, activate the TriNet virtual environment and ensure that all necessary dependencies are available. Third, the desired training parameters are adjusted as necessary, and the training process is performed. Here, we use the ACPmain dataset, which is already available in the TriNet-Reproducing file, as an example to demonstrate the training process. If the you intend to train TriNet on your own datasets, you must simply replace the ACPmain dataset with your new datasets. In addition, if you are not interested in evaluating the accuracy of the trained model on an independent test dataset, then there is no need to set a test dataset. However, it is recommended to set aside a portion of the dataset as a test set and evaluate the performance of the trained model on it. For the training set, we split it into a training set and a validation set in a fixed 4:1 ratio. TriNet employs our developed Training and Validation Interaction (TVI) (i.e., iteratively interacting samples between the training and validation sets) method to construct more appropriate training and validation sets for model training.\nPerform model training as follows.\nDownload the train code from https://github.com/hjy23/TriNet-Reproducing[href=https://github.com/hjy23/TriNet-Reproducing] and unzip it.\nNavigate to the root directory of the TriNet-Reproducing project.\n>cd TriNet-Reproducing\nActivate the TriNet virtual environment.\n>conda activate TriNet\nRun the training script train.py and pass in all necessary parameters to retrain TriNet.\n>python train.py -type ACP -use_PSSM True -train_fasta ./data/ACPmain.txt -train_pssm ./data/pssm_acpmain/ -use_test True -test_fasta ./data/ACPmaintest.txt -test_pssm ./data/pssm_acpmaintest/ -use_TVI True",
    "Note: When running the Python script via the command line, you have the option to pass in parameters and modify them as desired. However, if no parameters are passed, the script will automatically use default settings. This flexibility allows for customized execution of the script based on specific requirements or preferences.\nSet the necessary parameters as follows.\n--type: The type of training model is antimicrobial peptide (AMP) or anticancer peptide (ACP), type = str, ‘ACP’ or ‘AMP’.\n--use_PSSM: Whether to use the PSSM feature, type = bool, ‘Ture’ or ‘False’.\n--use_TVI: Whether to use the TVI approach to automatically select the more appropriate training and validation sets, type = bool, ‘Ture’ or ‘False’.\n--use_test: Whether to use an independent test set to evaluate the trained model, type = bool, ‘Ture’ or ‘False’.\n--train_fasta: The path of the training FASTA file.\n--train_pssm: The path of the train PSSM files while the parameter --use_PSSM is set to be True.\n--test_fasta: The path of the test FASTA file while the parameter --use_test is set to be True.\n--test_pssm: The path of the test PSSM files while the parameters --use_PSSM and --use_test are both set to be True.\nSet the optional parameters as follows.\n--dense_unit_dcgr: The dimension of features learned from the DCGR feature matrix extracted as serial fingerprint information in the first stage, type = int.\n--dense_unit_pssm: The dimension of features learned from the PSSM feature extracted as evolutionary information in the second stage, type = int.\n--dense_unit_all: The dimension of the concatenation features containing the fingerprint, evolutionary and physicochemical property information, type = int.\n--dff: The dimension of hidden units in the middle linear layer of a pointwise feed-forward network in the encoder of the third stage, type = int.",
    "--d1: The probability of randomly dropping input units during each update in the training process of the first stage, type = float.\n--d2: The probability of randomly dropping input units during each update in the training process after the concatenation of the three stages, type = float.\n--lr: The initial learning rate used to control the step size of parameter updates, type = float.\n--batch_size: The number of samples used in each iteration, type = int.\n--decay_rate: The decline percentage of the learning rate in each step of the dynamic learning rate adjustment, type = float.\n--decay_step: The number of steps of learning rate decay in each step of the dynamic learning rate adjustment, type = int.\n--epoch: The number of epochs used for training, type = int.\nNote: An example of the training command by setting full parameters.\n>python train.py -type ACP -use_PSSM True -use_TVI True -use_test True -train_fasta ./data/ACPmain.txt -train_pssm ./data/pssm_acpmain/ -test_fasta ./data/ACPmaintest.txt -test_pssm ./data/pssm_acpmaintest/ -dense_unit_dcgr 350 -dense_unit_pssm 120 -dense_unit_all 200 –dff 6 -d1 0.5 -d2 0.5 -lr 0.0004 -decay_rate 0.95 -decay_step 500 -batch_size 64 – epoch 50\nNote: In deep neural networks, it is imperative to recognize the importance of hyperparameters because they highly influence the performance of a model.\nTune hyperparameters.\nTune learning rate.",
    "Note: The learning rate is a hyperparameter that governs the amplitude of weight adjustments during each iteration of the model. For smaller-scale datasets, as well as for relatively simple models, employing a smaller learning rate can contribute to a steadier convergence. In contrast, for larger datasets, a larger learning rate has the potential to expedite the training phase while exploiting the statistical properties of the data. It is recommended to initialize the learning rate within the range of 0.1 to 0.0001, followed by fine-tuning through various learning rate schedulers, while diligently monitoring the loss and accuracy metrics.\nTune batch size and epoch.\nNote: The batch size is important in defining the quantity of samples processed in each training iteration, while the epoch determines the total number of traversals across the entire dataset. Employing larger batch sizes can improve the training efficiency, especially for larger datasets. However, this also amplifies the susceptibility to overfitting. Conversely, smaller batch sizes usually foster superior model generalization ability.\nTune dropout rate.\nNote: Dropout constitutes a prominent regularization strategy aimed at reducing overfitting.23[href=https://www.wicell.org#bib23] The use of smaller datasets, datasets with a higher degree of sequence similarity, datasets comprising shorter peptide sequences, and more complex models inherently increases the susceptibility of the model to overfitting. In such situations, increasing the dropout rate is usually effective in improving the generalization ability of the model. While increasing the dropout rate is effective in reducing overfitting, an excessive dropout rate can hinder the ability of the model to discern complex patterns. It is recommended to initiate the dropout rate at approximately 0.5, followed by adjustments based on the performance of the model with the validation set.\nTune hidden size.",
    "Note: Hidden size is also a crucial aspect that demands fine-tuning based on the dataset characteristics. For larger datasets or those containing many longer sequences, a higher hidden size can improve the complexity and representative capacity of the model. Conversely, a smaller hidden size is suitable for relatively modest datasets to reduce overfitting and promote more robust generalization ability by diminishing the complexity of the model.\nTune feature embedding dimension.\nNote: The three hyperparameters, dense_unit_dcgr, dense_unit_pssm, and dense_unit_all, should be also determined by considering the characteristics of a dataset. For datasets that are either smaller in scale, have lower sequence similarity, or contain shorter sequences, it is advisable to select lower dimensional embeddings. Conversely, for larger datasets, adding the dimensions of feature embeddings is an effective approach to obtain an enriched contextual understanding of the data.\nNote: TriNet employs binary cross-entropy as the loss function and the Adam optimizer for model optimization. Moreover, the learning rate is adeptly managed by a learning rate scheduler, which fine-tunes the learning rate in real time to ensure the optimal adaptation to the training data. During training, the model may encounter the problems of overfitting and underfitting.\nSolve the possible overfitting and underfitting issues.\nApply data augmentation.\nNote: Increasing the amount of data and adding more diverse types of data can reduce the risk of overfitting and improve the generalization ability of the model.\nApply regularization techniques.\nNote: Employing regularization techniques can penalize model parameters if they are too large, thereby preventing overfitting. The batch normalization technique is used in TriNet to avoid overfitting.\nAdd dropout layers.\nNote: Adding dropout layers in the network can prevent the model from becoming overly reliant on any specific neurons and hence avoid overfitting. The dropout technique is also employed to avoid overfitting in TriNet.\nApply early stopping.",
    "Note: Monitoring the validation loss during training and stopping once the validation loss ceases to decrease (indicating potential overfitting) can prevent overfitting. The early stopping technique is also applied to avoid overfitting in TriNet.\nAdjust model complexity.\nNote: Opting for a simpler model if overfitting occur, or conversely, a more complex model if underfitting is observed can address these problems. You can adjust the model complexity by tuning the hidden sizes and the embedding sizes.\nIncrease training epochs.\nNote: Adding training iterations is effective in capturing the underlying patterns of the data and hence avoiding underfitting.\nNote: The advantages and disadvantages of model retraining with a new dataset. The advantages include the potentials to enhance its predictive capabilities by leveraging new data to recalibrate the parameters of the model. Furthermore, it may overcome the problem of model overfitting and improve its ability to generalize to unseen data. However, it is imperative to note the disadvantages associated with the retraining process. One such disadvantage is the additional expenditure of time and computational resources in conducting the retraining. Additionally, the collections and relabeling of new datasets increase the development cycle and incurs elevated costs. Another potential disadvantage is the risk of performance attrition. If retraining is not effectively implemented with appropriate data management and training strategies, it can cause performance degradation or induce new errors into the model. Based on these considerations, it is crucial to weigh the trade-offs of time, resources, and performance enhancements when determining the retraining of a model.\nUtilize the retrained TriNet model.",
    "Note: After retraining, the output is archived in the “checkpoints” folder. To utilize the model for the prediction of ACPs or AMPs, you should execute the “test.py” script and pass in all required parameters. The parameters “-mode”, “-s”, “-p”, and “-o” are the same as those described in the “running TriNet with its default trained model[href=https://www.wicell.org#sec2.2]” section, while the “-mp” parameter is explained as follows.\n--model_path, -mp: The path of the model to be loaded, type = str.\nNote: The command below demonstrates the utilization of a retrained model for the prediction of anticancer peptides. In this instance, the ACP_example.fasta file and the PSSM profiles located within the ./pssm_acp_example directory serve as the inputs through the standard mode. The output is stored in the form of a file named acpout.csv.\n>python test.py -mode sc -s ./ACP_example.fasta -p ./pssm_acp_example/ -o ./acpout.csv -mp ./checkpoints/2023-06-28-19_26_42"
  ],
  "subjectAreas": [
    "Sequence Analysis",
    "Bioinformatics",
    "Computer Sciences",
    "Cancer",
    "Microbiology"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}