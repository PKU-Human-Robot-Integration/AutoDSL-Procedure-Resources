{
  "id": 2439,
  "origin_website": "Cell",
  "title": "Protocol for quantitative ethology on natural social interactions in Drosophila",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nWe first describe how to acquire videos with MIAS. We then demonstrate the analysis workflow utilizing SoAL with a series of graphical user interfaces and scripts (Figure 4[href=https://www.wicell.org#fig4], right part).\nExperiment\nTiming: 2–4 days for daily experiments. 18–24 h for starvation. 0.5–1.5 h for fly loading and adaptation. 1 h for video acquisition\nCarry out the experiments in the incubator at 22°C and 50% humidity, within 0–4 h after the beginning of the day cycle.\nNote: We have provided a 10 min example video acquired during the experiment. The example video is included in our tutorial data (https://doi.org/10.6084/m9.figshare.19711738[href=https://doi.org/10.6084/m9.figshare.19711738], containing an example video along with all the analysis results after “Pose estimation”). If you want to skip the experiment and practice the pose estimation (refer to step-by-step method details[href=https://www.wicell.org#step-by-step-method-details] “Pose estimation”), download this tutorial data and perform the pose estimation (refer to step-by-step method details[href=https://www.wicell.org#step-by-step-method-details] step 7).\nNote: We combined single-housed males with group-housed females to enhance courtship, as single-housing promotes male courtship while group-housing suppresses female receptivity.\nFemale starvation.\nNote: Females need to be starved 18–24 h before the beginning of the experiment.\nShift the group-housed flies (refer to before you begin[href=https://www.wicell.org#before-you-begin] step 9) into an empty vial with a sheet of wet paper.\nPut the empty vial back into the rearing incubator.\nFly loading.\nSwitch the incubator on and set the temperature to 22°C and the humidity to 50%.\nAssemble the chambers.\nRemove the chamber walls and top plates in Figure 2[href=https://www.wicell.org#fig2].\nHeat the fly food (refer to before you begin[href=https://www.wicell.org#before-you-begin] step 3) by microwave until melted.\nPour the food into the holes at the center of each chamber on the bottom plate (Figure 1[href=https://www.wicell.org#fig1]B) with a pipette.",
    "Stack up the chamber walls and top plates with both the top and left edges aligned (Figures 1[href=https://www.wicell.org#fig1]C and 2[href=https://www.wicell.org#fig2]).\nInsert the four separators into the slits by rows.\nIntroduce males into the parts with food (Figure 1[href=https://www.wicell.org#fig1]B, the hole below the separation line) guided by a soft tube. Cover the holes on the top plate with the cover plates immediately after the fly is guided into the chambers.\nNote: To make an introducing tube, cut the two ends of a 2 mL pipette, and make sure that one end fits the microcentrifuge tube and the other end fits the introducing hole. Quickly open the lid of the microcentrifuge tube when the fly stays away from the lid, while aligning the end of the tube with the microcentrifuge tube. After the fly moves into the soft tube, pinch the big end to prevent escaping. Place the small end above the introducing hole and wait until the fly moves into the chamber.\nNote: Anesthetization affects the neural activity of the flies that may not recover immediately after waking up. We therefore recommend not to anesthetize the males to maintain the best condition of the animals at the cost of experiment difficulty.\nAnesthetize the group-housed flies by ice.\nPick the females with tweezers and gently moved them into the parts without food (Figure 1[href=https://www.wicell.org#fig1]B, the introducing hole above the separation line), thus preventing them from eating before the experiment.\nCover the holes on the top plate with the cover plates.\nAdaptation.\nLeave the flies in the chamber for 20–30 min for recovery and adaptation.\nClosing chamber.\nJust before the video acquisition, remove the separators and slide the top plate up to cover the introducing holes (Figure 1[href=https://www.wicell.org#fig1]D).\nRemove the cover plates to clear the view.",
    "Note: This operation should finish within 2 min to reduce the interaction time before video acquisition.\nCritical: When sliding the top plate, take care not to crush the flies with the introducing hole. When there are flies in the introducing hole, wait a short time until all the flies leave the holes.\nVideo acquisition.\nRun the video acquisition software (e.g., MIAS_FLIR.exe for FLIR cameras). This program shows a black console (Figure 3[href=https://www.wicell.org#fig3]) and two preview windows.\nInspect the preview window to ensure the field of view is fine.\nPress “c” and “Enter” in the console to start the recording.\nPress “q” and “Enter” in the console to stop the recording and quit MIAS.\nNote: In our example, the video is recorded with a resolution of 1280 by 1024 pixels and a fixed acquisition rate of 66.6 frames/s.\nNote: The videos acquired are stored in the folders named “video” under MIAS directory, with the folder structure as follows:\n- MIAS\n    config.json\n    MIAS_All.exe\n    MIAS_FLIR.exe\n    [Other files]\n    -video\n        -20200703_141812_1\n            20200703_141812_1.avi\n            20200703_141812_1.log\n        -20200703_141812_2\n            20200703_141812_2.avi\n            20200703_141812_2.log\nFor each camera, MIAS generates two files (“xxx.avi” is the recorded video. “xxx.log” stores the timestamps of each frame) in the same folder. The name (e.g., “20200703_141812_2”) is composed of date, time, and the camera name, separated by “_”.\nChamber cleaning.\nAfter each experiment, anesthetize the flies by ice or CO2 and discard them. Rinse the surface of the supporters and all the parts of the chambers with deionized water.\nPose estimation\nTiming: hours to days\nTiming: For keypoint detection (step 13), about 2 h for the example video on the Basic Hardware Configuration\nThe following steps use the tutorial data to demonstrate the analysis pipeline. The analysis result includes the time series of 5 keypoint positions.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1923-Fig6.jpg\nFigure 6. The top-down approach for keypoints detection",
    "(A and B) Segment a local region around each fly, and centralize by transforming the segmented image to ensure that the fly body appears vertically at the center of a 64 by 48 pixels image (B).\n(C) Detect the 5 keypoints of a fly.\n(D) Transform the keypoints back to the original coordinate.\nNote: To perform pose estimation, we used a top-down approach, applying ROI extraction, body detection, and keypoints prediction on the video frames in parallel (Figure 6[href=https://www.wicell.org#fig6]).\nDownload tutorial data.\nDownload and unzip the tutorial data (https://doi.org/10.6084/m9.figshare.19711729[href=https://doi.org/10.6084/m9.figshare.19711729]).\nNote: The folder “video” under the SoAL code directory corresponds to the “video” folder under the MIAS directory (refer to step-by-step method details[href=https://www.wicell.org#step-by-step-method-details] step 5).\nNote: As a reference, another package of tutorial data (https://doi.org/10.6084/m9.figshare.19711738)[href=https://doi.org/10.6084/m9.figshare.19711738)] already contain configuration files and analysis results. The files will be replaced, after performing the following steps.\nConfiguration (for steps 8–12 see demonstration in Methods video S1[href=https://www.wicell.org#mmc1]).\nExecute the following command to call the configuration UI.\n>python SoAL_PreProcess.py ∖\nvideo/20200703_141812_2/20200703_141812_2.avi\nNote: The command line parameter “video/20200703_141812_2/20200703_141812_2.avi” is the path of the video file to be configured (the symbol “∖” is a line continuation, do not input this symbol and the following line break). This configuration shows 3 dialog boxes in series (described in the following steps 9–11) to set the parameters for keypoint detection and behavior annotation.\nSetting the scale.\nThe first dialog shows a random sampled frame from the video for setting the scale, defined as the number of pixels occupying a 1-mm length.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1923-Fig7.jpg\nFigure 7. UI for setting the scale factor",
    "(A) This UI shows the image of a frame with an auto-detected chamber diameter (blue line). Optionally, modify the textboxes below the image to change the experiment date (“ExpDate”), female eclosion date (“FemaleDoB”), and temperature (“Temperature”). The textbox “Diameter (mm)” is for specifying the real length of the chamber diameter. “Scale (px/mm)” is updated automatically when the diameter and the real length are both specified. Click button “Random” to change a frame to show.\n(B and C) If the detected diameter is incorrect, label two points on the image by clicking to indicate a diameter. First, click the start point (B, red point). Second, click the end point (C).\nNote: The scale factor is calculated by automatic detection of the chamber diameter (Figure 7[href=https://www.wicell.org#fig7]A blue line). Alternatively, the chamber diameter can also be specified manually (Figures 7[href=https://www.wicell.org#fig7]B and 7C).\nInput the real diameter (in mm) of the chamber in the textbox “Diameter (mm)”. The textbox “Scale (px/mm)” then shows the calculated scale factor.\nClick “Confirm” to proceed to the next step.\nROI extraction.\nNow another dialog shows an image superimposed with 16 blue circles indicating the auto-detected chamber regions (Figure 8[href=https://www.wicell.org#fig8]A).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1923-Fig8.jpg\nFigure 8. UI for ROI extraction\n(A) This UI shows the image of a frame with auto-detected ROIs (blue circles).\n(B) Deselect ROIs as needed by clicking in the circle (red circle, the third one of the fourth row, which includes a dead fly). Click again to re-include the circle as a ROI.",
    "(C) Input fly information for each ROI in the textbox “FlyInfo”. Specify all ROIs according to the order of top to bottom and left to right (skipping the excluded ROIs with red circles). The text for one ROI is composed of a group name and an age number, separate by “_”. The adjacent ROIs with the same group and age can be merged and represented by adding “∗N” (N is the number of the same ROIs). Different ROIs are separated by “,”. For example, the text “Ctrl_9∗7,Exp_8,Exp_9∗7” (textbox “FlyInfo”) means the first 7 males are from “Ctrl” group and are 9 d old, the eighth male is from “Exp” group and are 8 d old, and the last 7 males are from “Exp” group and are 9 d old. The red one (the third one of the fourth row) is excluded.\nOptional: If the detection is incorrect, change the parameters for circle detection (the textbox “CircleParam”) or use the manual mode (see problems 10[href=https://www.wicell.org#sec7.19] and 14[href=https://www.wicell.org#sec7.27] for the details).\nSome of the regions should not be analyzed due to the unsuccessful preparation. Click in any circle to exclude the region from being ROI (the circle will change to red, Figure 8[href=https://www.wicell.org#fig8]B).\nClick “Confirm” to proceed to the next step.\nBackground subtraction.\nWait several seconds of computing until the appearance of the next dialog box.\nNote: The program computes the background image by averaging 100 frames that are uniformly sampled in time throughout the video.\nFly segmentation.\nNote: The program segments the flies in the images by detecting connected regions in binarized images.\nThe dialog shows the binarized ROI regions (Figure 9[href=https://www.wicell.org#fig9], 15 ROIs are shown because one ROI is excluded).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1923-Fig9.jpg\nFigure 9. UI for fly segmentation",
    "(A–C) The UI shows the images of 15 extracted ROIs. The histogram under the images is the gray scale histogram of current image. Slide the bottom bar to change the binarization threshold (the value is shown on the right, “150”). Switch the radio buttons at the left bottom to change the viewing mode: original (A), background-subtracted (B), and binary (C). The default viewing mode is binary (C).\nSlide the bar at the bottom to change the binarization threshold.\nCritical: A rule of thumb for choosing a good threshold is to make sure that the segmented fly body as previewed in Figure 9[href=https://www.wicell.org#fig9]C is complete and does not include the wings.\nOptional: For different videos with the same illumination condition, the same threshold can be used (the default value of the threshold can be changed in the source file “SoAL_Constants.py” named “DEFAULT_GRAY_THRESHOLD”).\nOptional: Change the preview mode by clicking the buttons at the bottom (Figure 9[href=https://www.wicell.org#fig9]).\nClick “Confirm” to finish the configuration.\nNote: The configuration results are stored in two files, a text file “xxx_config.json” and a background image “xxx.bmp” (“xxx” is the video name), both in the same directory as the video file.\nRunning keypoint detection (Methods video S2[href=https://www.wicell.org#mmc2]).\nChoosing the model. Modify the configuration item “MODEL_FILE” (under the item “TEST”) in “hrnet/fly_w32.yaml” to specify the model to use for keypoint detection (same as the step “before you begin[href=https://www.wicell.org#before-you-begin], step 12b”). By default, the provided model “hrnet_w32_SDPD-15k.pth” is used.\nExecute the script to automatically analyze all the videos in the directory.\n>python SoAL_KptDetect.py all\nThe program will print the processing progress. Wait until the message “[Main]: all finished” is printed. Close the console window to quit.",
    "Note: The directory is specified by the item “VIDEO_TODO_DIR” in “SoAL_Constants.py”, and the default path is “video/”. The videos are automatically processed in parallel based on the hardware configuration.\nNote: To determine whether a video has been successfully processed, inspect the text file “.state” in each video folder to see if “finish” is shown.\nNote: Processing speed is about 150 centralized images per second on the Basic Hardware Configuration, and 400 centralized images per second on the Advanced Hardware Configuration with at least two videos processing simultaneously.\nOptional: Before running keypoint detection, edit the following items in the file “SoAL_Constants.py” to modify some parameters.\nMODEL_CONFIG = \"hrnet/fly_w32.yaml\"\nVIDEO_TODO_DIR = \"video/\"\nMAX_TASK = 2\nBATCH_SIZE = 60\nMODEL_SHAPE = 64, 48\nFLY_NUM = 2\n“MODEL_CONFIG” provides the path of the model configuration file. “VIDEO_TODO_DIR” is the parent folder of the video to be analyzed. “MAX_TASK” is the number of videos processed in parallel (limited by the clock speed of the CPU and the RAM), which is 2 with the Basic Hardware Configuration. “BATCH_SIZE” is the number of centralized images processed in a batch on the GPU (limited by the VRAM capacity of the GPU), which is 60 with the Basic Hardware Configuration and 200 with the Advanced Hardware Configuration. “MODEL_SHAPE” defines the size (width and height, in pixels) of the centralized image as input to the HRNet model. “FLY_NUM” defines the number of flies in one ROI, this number is 2 in cases of male-female courtship.\nInspecting keypoints (Methods video S3[href=https://www.wicell.org#mmc3]).\nNote: Here is an example of part of the folder structure after the keypoint detection:\n- video\n    - 20200703_141812_2\n        20200703_141812_2.avi\n        20200703_141812_2.bmp\n        20200703_141812_2_config.json\n          - 0\n            20200703_141812_2_0_kpt.csv\n            20200703_141812_2_0_config.json\n          - 1\n            20200703_141812_2_1_kpt.csv\n            20200703_141812_2_1_config.json\n          - 2\n            20200703_141812_2_2_kpt.csv\n            20200703_141812_2_2_config.json",
    "The digital folders include the keypoint information files “xxx_kpt.csv” of each ROI (the last number in the name indicates the ROI number). The file “xxx_config.json” contains the configuration information of this ROI.\nInspect the keypoint information “xxx_kpt.csv” by executing the following script (Figure 10[href=https://www.wicell.org#fig10]).\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1923-Fig10.jpg\nFigure 10. UI for Inspecting detected keypoints\nThis UI shows 4 diagrams of a frame: the image of the ROI, the keypoints and heading directions (triangles, two flies are color-coded in blue and red), and two centralized flies with the keypoints. Move the slide bar to change the showing frame. Press “Left arrow” or “Right arrow” to navigate to the next and previous frame respectively. Directly input the frame number and press “Enter” to navigate to the requested frame.\n>python tools/SoAL_ViewKptUI.py ∖\nvideo/20200703_141812_2/0/20200703_141812_2_0_kpt.csv\nNote: The command line parameter “video/20200703_141812_2/0/20200703_141812_2_0_kpt.csv” is the keypoint information file to inspect. This UI shows the image of the specific ROI of the current frame, the 5 keypoints of each fly, and two centralized flies superimposed with the keypoints and the skeletons.\nNavigate to desired frames by the bottom slide bar.\nNote: The “SoAL_ViewKptUI” command can be executed alongside “SoAL_KptDetect” simultaneously. This is useful for inspecting the partial results quickly to find the problems. If the results are not satisfactory, you can stop the keypoint detection and make adjustments before running the “SoAL_KptDetect.py” command again.",
    "Note: The table file “xxx_kpt.csv” contains the following columns. “frame” is the frame number. The same frame number will appear twice for the two flies. “reg_n” is the number of regions detected. This number is generally 2 for social behavior, but it becomes 1 when the two flies overlap. “area” is the area of one region. “pos:x” and “pos:y” is the position of the region center in the coordinate of the ROI (origin point on the upper left corner). “ori” is the orientation angle. “e_maj” and “e_min” is the major and minor axis of the fitted ellipse. “point:xs” and “point:ys” are the horizontal and vertical positions of the 5 keypoints.\nBehavior annotation\nTiming: Several minutes depending on the specific case\nThe following step assigns the correct identities to the detected keypoints and annotates behaviors of each fly. The results include the motion parameters and the behavior bouts.\nRun identity assignment and behavior annotation (Methods video S4[href=https://www.wicell.org#mmc4]).\nRun the following script to assign identities and annotate behaviors of one ROI:\n>python SoAL_ID_Anno.py ∖\nvideo/20200703_141812_2/0/20200703_141812_2_0_kpt.csv\nNote: The command line parameter “video/20200703_141812_2/0/20200703_141812_2_0_kpt.csv” is the keypoint information file to analyze. To analyze an entire folder, pass the video folder as the parameter.\n>python SoAL_ID_Anno.py video/20200703_141812_2\nNote: The annotation process loads the keypoint information “xxx_kpt.csv”. The motion parameters for each frame are then calculated. Next, identities are assigned according to the wing extension angle. Finally, we detect and annotate specific behavior bouts, such as copulation, wing extension, crabwalking, and circling.\nOptional: To enable anti-distortion (refer to before you begin[href=https://www.wicell.org#before-you-begin] step 6), modify the value of “NEED_UNDISTORT” in the file “SoAL_ID_Anno.py”. In this way, executing “SoAL_ID_Anno.py” command also loads the camera calibration information file.\n    Your browser does not support HTML5 video.",
    "Methods video S5. Preparing the dataset for training network model, related to “before you begin 11\"\n    Your browser does not support HTML5 video.\n  \nMethods video S6. Training and testing the network model, related to “before you begin 12\""
  ],
  "subjectAreas": [
    "Bioinformatics",
    "Neuroscience",
    "Model Organisms",
    "Behavior"
  ],
  "bigAreas": [
    "Biomedical & Clinical Research",
    "Molecular Biology & Genetics",
    "Ecology & Environmental Biology",
    "Bioinformatics & Computational Biology"
  ]
}