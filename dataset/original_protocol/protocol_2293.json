{
  "id": 2413,
  "origin_website": "Cell",
  "title": "End-to-end pipeline for differential analysis of pausing in ribosome profiling data",
  "procedures": [
    "Step-by-step method details\nStep-by-step method details\nProcessing raw ribosome profiling data and RNA-sequence data\nTiming: 5 h\nThe analyses detailed in this protocol are meant to be used to make comparisons between two datasets. These two datasets will be referred to as “condition1” and “condition2” in the protocol’s example code. This section is an adaptation of a protocol by Ingolia et al. (2012)[href=https://www.wicell.org#bib10] and includes details on the pre-processing of the ribosome profiling data, the generation of genome indices, and the alignment to a reference genome. Troubleshooting 2[href=https://www.wicell.org#sec6.3]. The ribosome profiling data are aligned to both the genome and the transcriptome to facilitate downstream processing steps.\nNote: Ribosome profiling data will often be referenced by the acronym “RPF” while mRNA sequencing data will be referenced by the shortened “RNA”.\nNote: The example code found below assumes the user is using the example references and data within the “Datasets/reference_files” and “Datasets/testing_fastq_files” folders on our OSF repository[href=https://osf.io/5qcwk/]. This code should be run using the Datasets folder as a working directory.\nCritical: The testing fastq files found on our OSF page have already had their adapter sequences trimmed. Therefore, step 1 of this protocol must be skipped when working with these testing files. Failure to skip this step will empty the data in the fastq files and they will need to be re-downloaded from OSF.\nTrim adapter sequences off of ribosome footprinting reads using fastx_clipper. This command will need to be run for each ribosome profiling sample you are analyzing: {Bash}\n> fastx_clipper -Q33 -a TGGAATTCTCGGGTGCCAAGG -l 25 -c -n -i <untrimmed_RPF.fastq> -o condition1_RPF_1.trimmed.fastq\n-Q: Minimum Phred quality score to keep, set to 33 in the example.",
    "-a: The adapter sequence used during the ribosome profiling experiment to be clipped. Set as “TGGAATTCTCGGGTGCCAAGG” in example. This input must be changed to the adapter sequence used during the ribosome profiling experiment.\n-l: Minimum read length to keep. Set as 25 in example.\n-c: Discard non-clipped sequences.\n-n: Keep nucleotides with unknown sequences.\n-i: Name of input file to be clipped. Set as “<untrimmed_RPF.fastq>” in example.\n-o: Preferred name of output file. Set as “condition1_RPF_1.trimmed.fastq” in example.\nGenerate a bowtie2 index for the non-coding RNA within a new directory: {Bash}\n> mkdir ncrna_indices\n> cp reference_files/assembly.ncrna.fa ncrna_indices\n> cd ncrna_indices\n> bowtie2-build assembly.ncrna.fa ncrna_index\n> cd ..\n-f: Specify that the command input files will be fasta/fastq files.\n<example.ncrna.fa>: A positional argument specifying a fasta file or list of fasta files containing the references sequences to be aligned to. Set as “example.ncrna.fa” in example.\n<ncrna_index>: A positional argument specifying a prefix that will be appended to the start of all output files. Set as “ncrna_index” in example.\nRemove reads which mapped to ncRNA from the fastq files using bowtie2 and then save the resulting files in a new directory. This step greatly increases the speed of the final alignment step by removing any ribosome profiling reads that align to non-coding RNA. Again, this command will need to be run for each ribosome profiling sample you are analyzing: {Bash}\n> mkdir norrna_fastq_files\n> bowtie2 -L 23 --un=norrna_fastq_files/condition1_RPF_1.norrna.fastq -x ncrna_indices/ncrna_index testing_fastq_files/condition1_RPF_1.trimmed.fastq > outputtemp.sam\n-L: Set seed substring length. Set to “23” in example.\n--un: The path which unpaired reads will be written to. Set to “condition1_RPF.norrna.fastq” in example. For the purpose of this protocol, this is the main output we are looking for in this step.",
    "-x: Path to the reference genome index generated in step 2. Set to “ncrna_indices/ncrna_index” in example.\n<condition1_RPF.trimmed.fastq > Positional argument which specifies the file to be aligned to the ncRNA. Set to “testing_fastq_files/condition1_RPF_1.trimmed.fastq” in example. This will be the same file which was trimmed in step 1.\n<outputtemp.sam>: The name of the output file. Set to “outputtemp.sam” in example. For the purpose of this protocol, this output is not important and is written to a temporary output file.\nGenerate a STAR genome index within a new directory. Troubleshooting 3[href=https://www.wicell.org#sec6.5]. {Bash}\n> mkdir star_indices\n> STAR --runThreadN 8 --runMode genomeGenerate --genomeDir star_indices --genomeFastaFiles reference_files/assembly.fa --sjdbGTFfile reference_files/annotation.gtf --genomeSAindexNbases 13\n--runThreadN: The number of threads to be used to generate the indices. Set to 8 in example. Should be set at or below the number of available cores on the system.\n--runMode: The type of process STAR is running. Set to “genomeGenerate” in example.\n--genomeDir: The path to a directory where the genome indices will be stored. Set as “star_indices” in example.\n--genomeFastaFiles: The path to a fasta file containing the complete genome assembly for the organism of study. Set as “assembly.fa” in example.\n--sjdbGTFfile: The path to a gtf file containing a genome annotation file for the organism of study. Set to “annotation.gtf” in example.\n--genomeSAindexNbases: The length of the SA pre-indexing string in base pairs. Should be chosen depending on genome size using the equation min(14, log2(length of genome)/2 -1). Set as “13” in example.\nAlign the ribosome profiling data to the reference genome. This command will need to be repeated for each sample you are analyzing:",
    "Note: Each time this step is run it will output two files. One that is aligned to the genome and has the format “condition_RPF_#_Aligned.sortedByCoord.out.bam” and one that is aligned to the transcriptome and has the format “condition_RPF_#_Aligned.toTranscriptome.out.bam.”\n{Bash}\n> mkdir star_alignments\n> STAR --genomeDir star_indices --runThreadN 8 --readFilesIn norrna_fastq_files/condition1_RPF_1.norrna.fastq --outFileNamePrefix star_alignments/condition1_RPF_1_ --outSAMtype BAM SortedByCoordinate --quantMode TranscriptomeSAM\n--genomeDir: A path to a directory where STAR genome indices are stored. Set to “star_indices” in example.\n-- runThreadN: The number of threads to be used for the gene mapping. Set to 8 in example. Should be set at or below the number of available cores on the system.\n--readFilesIn: The path and name of the file containing the sequences to be mapped to the reference genome. Set to “condition1_RPF.norrna.fastq” in example.\n--outFileNamePrefix: A prefix that will be appended to the start of all output file names. Set to “condition1_RPF_” in example.\n--outSAMtype: The type of SAM/BAM file which will be outputted. Set to “BAM SortedByCoordinate” in example.\n--quantMode: The types of different quantifications requested. In the example it is set to “TranscriptomeSAM” so that SAM/BAM alignments to the transcriptome are outputted into separate files.\nAlign the mRNA sequencing data to the reference genome. This command will need to be repeated for each sample you are analyzing:\n{Bash}\n> STAR --genomeDir star_indices --runThreadN 8 --readFilesIn testing_fastq_files/condition1_RNA_1.fastq --outFileNamePrefix star_alignments/condition1_RNA_1_ --outSAMtype BAM SortedByCoordinate\nSee previous step for list of arguments.\nSeparate the genome alignment and transcriptome alignment files into two different folders:\n{Bash}\n> cp star_alignments/∗Aligned.sortedByCoord.out.bam testing_genome_alignments\n> cp star_alignments/∗Aligned.toTranscriptome.out.bam testing_transcriptome_alignments\nCreate BAM index files for all of the genome alignment files generated in the last 2 steps:\n{Bash}\n> cd testing_genome_alignments\n> samtools index condition1_RPF_1_Aligned.sortedByCoord.out.bam\n> samtools index condition1_RNA_1_Aligned.sortedByCoord.out.bam\n> cd ..",
    "Pause point: Their will be many different intermediate outputs from these first few steps. The only files that are needed for the rest of the protocol are the BAM files outputted by steps 5 and 6 and the associated BAM index files generated in step 8. For each ribosome profiling sample there will be two BAM files, one for the genome alignment and one for the transcriptome alignment. The RNA samples will have only one BAM file each which will be for the genome alignment.\nDetecting changes in translation efficiency\nTiming: 1 h\nTranslation efficiency is defined as the ratio of normalized ribosome footprint reads to RNA-seq reads for individual transcripts. We use featureCounts to count the number of ribosome profiling reads and mRNA sequencing reads that map to CDS regions and exons respectively for both of our datasets. Then, a short Python script is used to convert the output of featureCounts into suitable inputs for RiboDiff. Finally, RiboDiff is used to quantify the changes in translation efficiency for each gene and determine the significance of the change.\nCritical: In order to run RiboDiff it is necessary for there to be at least 2 replicates.\nNote: This section can be tested using the example GTF file found in the “Datasets/reference_files” folder and the example Bam files found in the “Datasets/testing_genome_alignments” folder provided on our OSF repository[href=https://osf.io/5qcwk/]. The following commands should be run using the Datasets folder as a working directory.\nUse featureCounts to count the individual number of reads that align to specific genomic features. We are interested in reads that align to exons for RNA-seq experiments and reads that align to CDS (protein coding) regions for ribosome profiling experiments: {Bash}\n> mkdir TE_results\n> featureCounts -T 8 -t CDS -a reference_files/annotation.gtf -o TE_results/counts_condition1_RPF_1.txt testing_genome_alignments/condition1_RPF_1_Aligned.sortedByCoord.out.bam",
    "-T: The number of threads to be used when counting features. Set to 8 in example. Should be set at or below the number of available cores on the system.\n-t: The specific feature type to be used for read counting. Set to “CDS” in example.\n-a: The path and name of a GTF genome annotation file for the organism of choice. Set to “reference_files/annotation.gtf” in example.\n-o: The preferred name of the output file. Set to “counts_condition1_RPF.txt” in example.\n<testing_genome_alignments /condition1_RPF_1_Aligned.sortedByCoord.out.bam > Positional argument for the alignment files to be inputted into featureCounts. The reads in the file must be aligned using the same genome annotation file as the one specified in “-a”.\nCritical: When running this step on the mRNA sequencing data, “-t CDS” should be changed to “-t exon”. This step must be run for all the genome alignment files from both datasets before proceeding to step 10.\nUse the process_counts.py file located in the “Python_scripts” folder on our OSF project page[href=https://osf.io/5qcwk/] to automatically take all of the count files generated by featureCounts and arrange them into the correct format for RiboDiff. This Python script should be run within the directory containing the output from featureCounts. {Bash}\n> cp <path/to/process_counts.py> TE_results\n> cd TE_results\n> python3 process_counts.py --path-to-counts . --treated-prefix condition1 --control-prefix condition2\n--path-to-counts: The path to a folder which contains the count arrays outputted by feature counts.\n--treated-prefix: A prefix that is contained within the name of the count files for the condition1 data.\n--control-prefix: A prefix that is contained within the name of the count files for the condition2 data.\nCurrently RiboDiff has not been updated to work with Python3+. As such, we need to activate the py2 environment created in installing RiboDiff section in order to run the program:\n> conda activate py2",
    "Run the RiboDiff program using python 2: {Bash}\n> python2 <path/to/RiboDiff/scripts/TE.py> -e experimental_design.csv -c raw_read_count.txt -o ribo_output.txt -p 1\n-e: A text file describing the format of the experiment. This should be set to the output of the process_counts.py script. Set to “experimental_design.csv” in example.\n-c: A text file containing the count data. This should be set to the output of the process_counts.py script. Set to “raw_read_count.txt” in example.\n-o: Preferred name for the tab delimited text file which will be outputted by RiboDiff. Set to “ribo_output.txt” in example.\n-p: Make plots to show the data and results (Figure 2[href=https://www.wicell.org#fig2]).\nDeactivate the py2 environment and change working directory back to Datasets:\n{Bash}\n> conda deactivate\n> cd ..\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1899-Fig2.jpg\nFigure 2. RiboDiff outputs\n(A) Scatterplot comparing the log transformed translation efficiency ratios vs the mean read counts from the ribosome profiling data. Genes found to have significant changes in translation efficiency are colored yellow while non-significant genes are colored in gray. Genes exhibiting high K-S statistics are named and circled in red.\n(B) Histogram showing the distribution of log transformed translation efficiency ratios for all genes. The distribution for genes found to have significantly increased or decreased translation efficiencies are shown in red and blue respectively.\n(C) Comparison of dispersion measurements from sequencing data. Scatterplot showing similar amounts of dispersion in RNA-seq and ribosome profiling data, indicating that the use of a single dispersion measurement is sufficient.\nDetermining P-site offsets\nTiming: 1 h",
    "Ribosome protected fragments are often a variety of different lengths, so we do not know the exact location of the ribosome along the transcript from the ribosome profiling data. We will map each footprint to a precise location along a transcript using P-site offsets. The P-site offset is the distance between the P-site of the ribosome and the extremities (the 5ʹ or 3ʹ end) of the read. By estimating the most likely P-site offset for each read length we can accurately determine the position of the ribosome along the transcript. For this analysis, P-site offsets were determined using the riboWaltz R package Lauria et al. (2018)[href=https://www.wicell.org#bib14].\nNote: This section can be run using the GTF file found in the “Datasets/reference_files” folder and the Bam files found in the “Datasets/testing _transcriptome_alignments” folder on our OSF repository[href=https://osf.io/5qcwk/]. An example R script containing the code for this section can be found in the “R_scripts” folder on OSF and our GitHub repository[href=https://github.com/greenblattlab/STAR_protocol]. This example R script should be run while using the “R_scripts” folder as a working directory.\nLoad the riboWaltz package into your R session.\n> library(ribowaltz)\nLoad the GTF annotation file used during genome alignment into R using riboWaltz’s create_annotation function. Troubleshooting 4[href=https://www.wicell.org#sec6.7].\n{R}\n> df = create_annotation(gtfpath = \"<path/to/annotation.gtf>\")\nLoad the bam files from the transcriptome alignment into R using the bamtolist function. Note that this function operates by automatically loading up all of the bam files within a folder specified by the user:\n{R}\n> bam_list = bamtolist(\"<path/to/transcriptome_alignments_folder>\", annotation = df)",
    "Critical: This step can be RAM intensive and may cause R to crash if insufficient RAM is available. If R consistently crashes at this step, check to ensure that the machine being used meets the 16 GB RAM requirement stated in the Materials and equipment section and close additional processes running on the machine.\nCalculate the P-site offsets for all of the bam files by running the psite() function:\n> offsets = psite(data = bam_list, start = FALSE)\nNote: In this example the “start” argument is set to “FALSE”. Setting this argument to false directs riboWaltz to use the second to last codon on a transcript as a reference codon instead of the first codon. This was set to false because the datasets originally used to test this protocol did not exhibit initiation peaks, so it was unsuitable to use the first codon as a reference for p-site offsetting. If this protocol is used on data collected from cell cultures which demonstrate an initiation peak, it may be beneficial to alter this argument to “TRUE”.\nExtract the important information from the output of psite by creating a new dataframe using R’s built-in subset function. The values needed from the output are length, which should be saved in a column called length, and corrected_offset_from_3, which should be saved in a column called p_offset. Save the newly created dataframe as a tab delimited text file using write.table:\n{R}\n> samples = unique(offsets$sample)\n> for (i in 1:length(samples)) {\n  > sam_offs = subset(offsets, sample == samples[i],\n  select = c(\"length\", \"corrected_offset_from_3\"))\n  > colnames(sam_offs) = c(\"length\", \"p_offset\")\n  > write.table(sam_offs, paste(\"<path/to/save/p_site_offsets/>\",\n    samples[i],\"_p-site-offsets\", sep=\"\"),sep = \"\\t\",\n    row.names = FALSE)\n}\nCreating count arrays\nTiming: 1 h",
    "Before the analysis of translation limitation can begin the data from the ribosome profiling experiments must be organized into count arrays. Count arrays are basically lists that record the number of reads which map to each base pair or codon position along a transcript. The count arrays will be created inside of a Jupyter notebook which is running inside of the Plastid Conda environment set up in the Plastid and Python environment preparations section. Using Plastid to create the count arrays will allow for important adjustments to be made to the data such as applying the p-site offsets made in the determining p-site offsets[href=https://www.wicell.org#sec3.3] section and sub-setting the data to only look at the coding regions of the transcripts. The count arrays will be saved as simple csv tables which can be easily incorporated into further analyses in later sections.\nNote: This section can be run using the GTF file found in the “Datasets/reference_files” folder, the text files found in the “Datasets/testing_Psite_offsets” folder, and the Bam files found in the “Datasets/testing_genome_alignments” folder on our OSF repository[href=https://osf.io/5qcwk/]. An example notebook containing the code for this section can be found in the “Notebooks” folder on OSF and our GitHub repository[href=https://github.com/greenblattlab/STAR_protocol]. This example notebook should be run within the “Notebooks” directory.\nCritical: This section requires that the utilities.py file located on our OSF project page[href=https://osf.io/5qcwk/] is within the module search path for your python session. This can be done either by saving the utilities.py file within the same directory as your Jupyter notebooks or by appending the path to the folder containing the utilities.py file to your python session path using the following code:\n> import sys\n> sys.path.append(\"<path/to/Python_scripts>\")\nLoad in the python libraries and functions necessary for this pipeline. This includes several functions from plastid and the contents of our utilities.py file:",
    "{Python3}\n> from plastid import BAMGenomeArray, GTF2_TranscriptAssembler, Transcript\n> import numpy as np\n> import pandas as pd\n> from plastid.plotting.plots import ∗\n> import utilities as utils\n> import matplotlib.pyplot as plt\n> from matplotlib.pyplot import figure\n> %matplotlib inline\nLoad in the table of P-site offsets created in the determining p-site offsets[href=https://www.wicell.org#sec3.3] section using the Pandas function read_csv:\n{Python3}\n> # Load in the P-site offsets for condition 1\n> p_offsets_cond1 = pd.read_csv(\"<path/to/condition1_RPF_1_Aligned.toTranscriptome.out_p-site-offsets>\", sep=\"\\t\")\n> # Load in the P-site offsets for condition 2\n> p_offsets_cond2 = pd.read_csv(\"path/to/condition2_RPF_1_Aligned.toTranscriptome.out_p-site-offsets\", sep=\"\\t\")\nLoad in a GTF genome annotation file into python using Plastid’s GTF2_TranscriptAssembler function. This function will load in the transcripts as an iterator of Plastid’s transcript type objects which we will then convert to a list using Python’s list function:\n{Python3}\n> # Load in the transcript information\n> transcripts = list(GTF2_TranscriptAssembler(open(\"<path/to/annotation.gtf>\"), return_type=Transcript))\nLoad in the Bam file containing the Ribosome Profiling data as a Bam Genome Array using Plastid’s BamGenomeArray() function and map the reads to their corresponding P-sites via the VariableThreePrimeMapFactory custom function in utilities.py and Plastid’s set_mapping function:\n{Python3}\n> # Load in the alignments from both the condition 1 and condition 2 datasets\n> alignments_cond1 = BAMGenomeArray(\"path/to/condition1_RPF_1_Aligned.sortedByCoord.out.bam\")\n> alignments_cond2 = BAMGenomeArray(\"path/to/condition2_RPF_1_Aligned.sortedByCoord.out.bam\")\n> # Set the P-site offset mappings for both datasets\n> alignments_cond1.set_mapping(utils.VariableThreePrimeMapFactory(p_offsets = p_offsets_cond1))\n> alignments_cond2.set_mapping(utils.VariableThreePrimeMapFactory(p_offsets = p_offsets_cond2))\nFor each transcript object in our list use Plastid’s get_counts function to create a numpy array that contains the number of counts at each position in the transcript:\n{Python3}\n> # Initialize two lists to contain the count arrays\n> count_arrays_cond1 = []\n> count_arrays_cond2 = []\n> # Iterate through each transcript in the GTF file and extract the count arrays for that transcript from the alignments.\n> for transcript in transcripts:\n  > count_arrays_cond1.append(\n    transcript.get_counts(alignments_cond1))",
    "> count_arrays_cond2.append(\n    transcript.get_counts(alignments_cond2))\nOnce the count arrays have been created the information on CDS regions contained in the transcript type objects can be used to alter the count arrays to only cover the CDS regions:\n{Python3}\n> # Initialize two lists which will contain the start and end positions of the CDS region of each transcript.\n> cds_starts = []\n> cds_ends = []\n> # Iterate through each transcript and add the cds information to the lists\n> for transcript in transcripts:\n  > cds_starts.append(transcript.cds_start)\n  > cds_ends.append(transcript.cds_end)\n> # subset each count array to only look at the cds region.\n> for i in range(len(count_arrays_cond1)):\n  > count_arrays_cond1[i] =\n    list(count_arrays_cond1[i][cds_starts[i]:cds_ends[i]])\n  > count_arrays_cond2[i] =\n    list(count_arrays_cond2[i][cds_starts[i]:cds_ends[i]])\nUse the add_gene_ids function from utilities.py to append the transcript ID and gene ID of each transcript to the start of the count array:\n{Python3}\n> utils.add_gene_ids(transcripts, count_arrays_cond1)\n> utils.add_gene_ids(transcripts, count_arrays_cond2)\nFilter out any count arrays that are of insufficient length or have insufficient read density. In this example, count arrays which were under 200 base pairs in length or which had a read density cut-off below 0.15 reads per base pair were filtered out:\n{Python3}\n> # Initialize two lists to hold the filtered arrays\n> filtered_array_cond1 = []\n> filtered_array_cond2 = []\n> # Iterate through each of the count arrays and save any count arrays that pass our filtering parameters\n> for array_1, array_2 in zip(count_arrays_cond1,\n  count_arrays_cond2):\n    > if len(array_1) > 200 and\n      sum(array_1 [2:])/len(array_1 [2:]) > 0.15 and\n      sum(array_2 [2:])/len(array_2 [2:]) > 0.15:\n        > filtered_array_cond1.append(array_1)\n        > filtered_array_cond2.append(array_2)",
    "Note: The 0.15 reads per base pair cutoff was determined empirically in Flanagan et al. (2022)[href=https://www.wicell.org#bib5] using simulations of ribosome profiles with different read densities. Depending on the amount of noise present within the data it may be appropriate to use a larger or smaller cutoff. If K-S statistic values are inconsistent across replicates, a more stringent cutoff should be applied.\nSave the count arrays to be used in future notebooks. Use the custom save_count_positions function from utilities.py so that the count arrays are saved with a header that describes each column which it is easier to read. Troubleshooting 5[href=https://www.wicell.org#sec6.9].\n{Python3}\n> utils.save_count_positions(filtered_array_cond1,\n  \"<path/to/save/condition1_1_counts.csv>\")\n> utils.save_count_positions(filtered_array_cond2,\n  \"<path/to/save/condition2_1_counts.csv>\")\nDifferential ribosome distribution analysis\nTiming: 30 min\nThis section will cover how to determine if the difference between our two datasets induces rate limiting pauses during translation elongation. First, the count arrays from the last section will be loaded into a Jupyter notebook and filtered so that only transcripts with sufficient length and read coverage are analyzed. Then, LOESS smoothing will be performed on the filtered count arrays and the cumulative distributions of the smoothed count arrays will be calculated. These cumulative distributions will be used to calculate the K-S statistic for each gene. All of the genes will then be sorted into 3 separate bins based on whether they have low, medium, or high K-S statistics. The fold enrichment of genes within each bin is then calculated. Finally, Fisher’s exact test will be used to determine if the observed enrichment of the target genes in various K-S bins is statistically significant.",
    "Note: This section can be run using the csv files from the “Datasets/testing_count_arrays” folder and the csv file from the “Datasets/condition_targets” folder on our OSF repository[href=https://osf.io/5qcwk/]. An example notebook containing the code for this section can be found in the “Notebooks” folder on OSF and our GitHub repository[href=https://github.com/greenblattlab/STAR_protocol]. This example notebook should be run within the “Notebooks” directory.\nCritical: This section requires that the utilities.py file located on our OSF project page[href=https://osf.io/5qcwk/] is within the module search path for your python session. This can be done either by saving the utilities.py file within the same directory as your Jupyter notebooks or by appending the path to the folder containing the utilities.py file to your python session path using the following code:\n> import sys\n> sys.path.append(“<path/to/utilities_folder>”)\nLoad in all of the necessary Python packages:\n{Python3}\n> import numpy as np\n> import pandas as pd\n> import utilities as utils\n> import scipy.stats as stats\n> from statsmodels.stats.multitest import multipletests\n> from decimal import Decimal\n> import matplotlib.pyplot as plt\n> from matplotlib.pyplot import figure\n> %matplotlib inline\n> from multiprocess import Pool\nLoad in the count arrays for both datasets:\n{Python3}\n> # Load in the count arrays from the condition 1 dataset\n> condition1, names_1 = utils.load_count_positions(\"<path/to/condition1_1_counts.csv>\")\n> # Load in the count arrays from the condition 1 dataset\n> condition2, names_2 = utils.load_count_positions(\"<path/to/condition2_1_counts.csv>\")\nSmooth out the count arrays using LOESS smoothing and calculate the cumulative read distributions. The get_smoothed_array function from the diff_utils.py file is set up to perform Loess smoothing with a window size equal to 5% of the transcript length and calculates the cumulative read distribution. This step can take some time, so it is recommended to use multiprocess’ Pool() function to complete this using multiple cores:\n{Python3}",
    "> # Define the number of processers to use.\n> max_pool = 8\n> # Iterate through each dataset and calculate the smoothed density array\n> with Pool(max_pool) as p:\n  > pool_1, pool_2 = list(\n      p.imap(utils.get_smoothed_array,\n          condition1)\n    ), list(\n      p.imap(utils.get_smoothed_array,\n          condition2)\n    )\nFor each gene in both datasets, calculate the K-S-statistic as the maximum distance between their smoothed cumulative distributions and then save this K-S-statistic as a list:\n{Python3}\n> # initialize 2 lists to hold the length and K-S statistic for each gene\n> ks_list = []\n> len_list = []\n> # iterate through each transcript in both datasets and calculate the length and K-S Statistic\n> for tr_1, tr_2, index in zip(condition1, condition2,\n  list(range(len(condition1)))):\n    > smoothed_array_1, cumul_1 = pool_1[index]\n    > smoothed_array_2, cumul_2 = pool_2[index]\n    > ks = max(abs(cumul_1 - cumul_2))\n    > ks_list.append(ks)\n    > len_list.append(len(tr_1))\nCombine the lists for the gene IDs, gene lengths, and K-S statistics into a pandas dataframe using the Pandas’ DataFrame function and rename the columns of the dataframe to something more suitable:\n{Python3}\n> # Create a pandas dataframe of K-S statistics\n> ks_table = pd.DataFrame(list(zip(names_1, ks_list,\nlen_list)))\n> # Rename the dataframe columns\n> ks_table.columns = [\"gene_ID\", \"ks_stat\", \"gene_length\"]\nLoad up a table containing a list of names for the genes affected by the differing conditions between our 2 datasets. The affected genes will be referred to as target genes or targets. A list of gene targets for our testing dataset can be found in the “Datasets/condition_targets” folder on our OSF project.\n{Python3}\n> target_names = pd.read_csv(\"<path/to/target_table.csv>\", names = [\"gene_ID\"])",
    "Merge the table of K-S statistics and the table of target gene names into a new table using Pandas’ merge function. This new table will have an indicator column that shows if one of the target genes matched to one of the genes in the table of K-S statistics:\n{Python3}\n> temp_df = pd.merge(ks_table, target_names, how = \"left\", on\n= \"gene_ID\", indicator = True)\nClean up the new table by removing any duplicates that may have been caused by multiple transcripts having the same gene name:\n{Python3}\n> temp_df.drop_duplicates(subset =\"gene_ID\",keep = \"first\", inplace = True)\nUse the indicator column created in step 34 to create 2 subsets of our table of K-S statistics; one that only includes genes which matched with our target genes and one that only includes genes which did not match with our target genes:\n{Python3}\n> # Filter the temporary dataframe to only include targets\n> targets = temp_df[temp_df._merge == \"both\"]\n> # Filter the temporary dataframe to only include non-targets\n> non_targets = temp_df[temp_df._merge == \"left_only\"]\nUse matplotlib’s scatter and violinplot functions to create plots which visualize the K-S statistic for all of the target and non-target genes (Figure 3[href=https://www.wicell.org#fig3]A):\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1899-Fig3.jpg\nFigure 3. Differential analysis of ribosome profiling data for data treated to induce elongation limitation",
    "Treatments included the depletion of selenocysteine carrying tRNA which arrested the elongation of selenoproteins (Fradejas-Villar et al., 2017[href=https://www.wicell.org#bib6]), and the introduction of Torin 1 which inhibited the initiation of translation for transcripts that contain 5′TOP regions (Philippe et al., 2020[href=https://www.wicell.org#bib18]). Dot plots with overlayed violin plots show the distribution of K-S statistics for genes which are targets and non-targets for the treatment. Bar plots show the enrichment of treatment targets in low, medium, and high K-S fractions. P-values were calculated using Fisher’ exact test and adjusted using the Benjamini Hochberg method. Enrichment in the high K-S fraction and changes in K-S statistic distributions are only present for selenoproteins and non-selenoproteins (A), and not for TOP mRNAs and non-TOP mRNAs (B). This suggests that only limiting elongation rates impacts our K-S statistic metric.\n{Python3}\n> # Initialize axes objects\n> fig = figure(figsize = (5.5,4.5))\n> ax = fig.add_axes([0,0,.9,.9])\n> # Add violin plots\n> violin_parts =\nax.violinplot([non_targets.ks_stat,targets.ks_stat],\nshowmeans = True)\n# Alter violin plot colours\n> for pc in violin_parts[\"bodies\"]:\n  > pc.set_facecolor(\"grey\")\n  > pc.set_edgecolor(\"grey\")\n> for partname in (\"cbars\",\"cmins\",\"cmaxes\",\"cmeans\"):\n  > vp = violin_parts[partname]\n  > vp.set_edgecolor(\"grey\")\n> # Add dotplots\n> x = np.random.normal(1, 0.04, size=len(non_targets.ks_stat))\n> ax.scatter(x, non_targets.ks_stat, s = 12, color =\n\"darkorange\", alpha=0.8)\n> x = np.random.normal(2, 0.04, size=len(targets.ks_stat))\n> ax.scatter(x, targets.ks_stat, s = 12, color =\n\"darkturquoise\", alpha=0.8)\n> # Determine x and y axis limits\n> ax.set_xlim(0.5,2.5)\n> ax.set_ylim(0,0.7)\n> # Label the axes.\n> positions = (1,2)\n> labels = (\"Non-targets\", \"Targets\")\n> ax.set_xticks(positions, labels, fontsize = 13)\n> plt.ylabel(\"K-S Statistic\", fontsize = 13)\n> # Create grid lines\n> axes = plt.gca()\n> axes.yaxis.grid(linestyle = \"--\")",
    "Divide the data into low, medium, and high K-S fractions and determine the fold enrichment of the target genes in each fraction using the determine_enrichment function from utilities.py. The K-S fractions in this example are set as genes with a K-S statistic less than 0.15, genes with a K-S statistic between 0.15 and 0.3, and genes with a K-S statistic above 0.3:\n{Python3}\n> # Define the cut-off for the high K-S fraction and the number of fractions to create.\n> upper_ks = 0.3\n> N_cats = 2\n> enrich, sections = utils.determine_enrichment(targets, non_targets, upper_ks, N_cats)\nNote: The 0.3 cut-off for the high K-S fraction was empirically determined based on numerical simulations of initiation limited and elongation limited translation (Flanagan et al., 2022[href=https://www.wicell.org#bib5]). This value can be altered to increase or decrease the sensitivity of the method.\nPerform Fisher’s exact test to determine if the enrichment of targets in any of the K-S fractions is significant. The Fisher_exact_p_values function from utilities.py can be used to automatically calculate these P-values for each of the K-S fractions. This function automatically performs three tests, so the outputted P-values should be adjusted using the Benjamini Hochberg method. This can be done using statsmodels’ multipletests function:\n{Python3}\n> # Perform Fisher’s exact for all three fractions simultaneously.\n> p_values = utils.Fisher_exact_p_values(targets, non_targets, sections)\n> # Adjust the P-values using the Benjamini Hochberg method\n> adj_p_values = multipletests(p_values, method = \"fdr_bh\")[1]\nUse matplotlib’s pyplot.bar function to create a series of barplots that show the fold enrichment of genes that are targets in each fraction (Figure 3[href=https://www.wicell.org#fig3]A):\n{Python3}\n> # Specify the figure size.\n> figure(figsize = (6.5,5.5))\n> # Create a barplot showing enrichment in each fraction.\n> bps = plt.bar([1,2,3],enrich, width = 0.5,\ntick_label = [\"Low K-S fraction\",\"Medium K-S fraction\",",
    "\"High K-S fraction\"], color = [\"g\", \"b\", \"m\"], edgecolor =\n\"black\")\n> # Adjust fontsize and labels.\n> plt.xticks(fontsize = 13)\n> plt.ylabel(\"Fold Enrichment\", fontsize = 13)\n> # Increase plot margins\n> plt.margins(0.1,0.1)\n> # Create grid lines\n> axes = plt.gca()\n> axes.yaxis.grid(linestyle = \"--\")\n> # Write the adjusted p-values on top of each bar.\n> for b, p in zip(bps, adj_p_values):\n  > height = b.get_height()\n  > plt.annotate(\"p = \" + \"{}\".format(\"%.2E\" % Decimal(p)),\n    xy=(b.get_x() + b.get_width() / 2, height),\n    xytext=(0, 3), textcoords=\"offset points\",\n    ha=\"center\", va=\"bottom\")\nOptional: After completing the analysis, the following optional steps can be used to create graphs showing the count arrays and smoothed cumulative read distributions for individual genes of interest in order to observe the changes in read distribution. To recreate the plots seen in Figures 4[href=https://www.wicell.org#fig4] or 5[href=https://www.wicell.org#fig5] using the testing data, “gene_of_interest” should be replaced with “Selenop” or “Aox3” respectively.\nChoose a gene of interest and then find its count array for both the condition 1 and condition 2 datasets:\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1899-Fig4.jpg\nFigure 4. Determination of the K-S statistic for elongation limited selenoprotein Selenop\n(A and B) Barplots depicting the normalized read density distribution for a gene from the condition 1 dataset (A) and the condition 2 dataset (B) (see also Figure 3A[href=https://www.wicell.org#fig3]).\n(C and D) Lineplots comparing the smoothed, normalized read densities (C) and the cumulative read densities (D) between condition 1 and 2. The K-S statistic is calculated as the maximum distance between the two cumulative distributions.\nimgsrc:https://prod-shared-star-protocols.s3.amazonaws.com/protocols/1899-Fig5.jpg\nFigure 5. Determination of the K-S statistic for initiation limited non-selenoprotein Aox3 (see also Figure 3[href=https://www.wicell.org#fig3]A)\n(A and B) Barplots depicting the normalized read density distribution for a gene from the condition 1 dataset (A) and the condition 2 dataset (B).",
    "(C and D) Lineplots comparing the smoothed, normalized read densities (C) and the cumulative read densities (D) between condition 1 and 2. The K-S statistic is calculated as the maximum distance between the two cumulative distributions.\n{Python3}\n> # Select gene of interest\n> goi = \"gene_of_interest\"\n> # Initialize lists to hold the count arrays.\n> goi_array_cond1 = []\n> goi_array_cond2 = []\n> # Iterate through the condition 1 and condition 2 datasets and extract count arrays from the gene of interest.\n> for tr_1, tr_2, name in zip(condition1, condition2,\nnames_1):\n  > if name == goi:\n    > goi_array_cond1 = tr_1\n    > goi_array_cond2 = tr_2\n  > if len(goi_array_cond1) == 0:\n    > raise ValueError(\"Gene name not found\")\nCreate smoothed count arrays for the gene of interest:\n{Python3}\n> # Smoothed array from condition 1 dataset\n> smoothed_array_1, cumul_1 =\nutils.get_smoothed_array(goi_array_cond1 + 0.00000000001)\n> # Smoothed array from condition 2 dataset\n> smoothed_array_2, cumul_2 =\nutils.get_smoothed_array(goi_array_cond2 + 0.00000000001)\nUse Matplotlib’s bar function to create bar plots that show the raw count arrays for the gene of interest:\n{Python3}\n> # Find the maximum read density between both arrays so it can be used to define the y-axis range.\n> maxi = max([max(goi_array_cond1/sum(goi_array_cond1)),\nmax(goi_array_cond2/sum(goi_array_cond2))])\n> # Create the bar plot for the condition 1 count array.\n> plt.bar(list(range(len(goi_array_cond1))),\ngoi_array_cond1/sum(goi_array_cond1), width = 4)\n> # Define the y-axis range\n> plt.ylim([0,maxi∗1.1])\n> # Add the axis labels and title\n> plt.ylabel(\"Read Density (normalized)\", fontsize = 11)\n> plt.xlabel(\"Transcript Position\", fontsize = 11)\n> plt.title(\"Condition 1\" + goi, fontsize = 13)\n> # Create grid lines\n> axes = plt.gca()\n> axes.yaxis.grid(linestyle = \"--\")\n> # Display plot\n> plt.show()\n> # Create the bar plot for the condition 1 count array.\n> plt.bar(list(range(len(goi_array_cond1))),\ngoi_array_cond2/sum(goi_array_cond2), color = \"darkorange\",\nwidth = 4)",
    "> # Define the y-axis range\n> plt.ylim([0,maxi∗1.1])\n> # Add the axis labels and title\n> plt.ylabel(\"Read Density (normalized)\", fontsize = 11)\n> plt.xlabel(\"Transcript Position\", fontsize = 11)\n> plt.title(\"Condition2 \" + goi, fontsize = 13)\n> # Create grid lines\n> axes = plt.gca()\n> axes.yaxis.grid(linestyle = \"--\")\nUse Matplotlib’s basic plot function to create line graphs that show the smoothed count arrays and the cumulative smoothed count arrays for the gene of interest:\n{Python3}\n> # Plot the smoothed count arrays from both datasets\n> plt.plot(smoothed_array_1, label = \"condition1\")\n> plt.plot(smoothed_array_2, label = \"condition2\", color =\n\"darkorange\")\n> # Add the axis labels, title, and legend\n> plt.ylabel(\"Read Density\", fontsize = 11)\n> plt.xlabel(\"Transcript Position\", fontsize = 11)\n> plt.title(\"Smoothed and Normalized Count Arrays\", fontsize =\n13)\n> plt.legend()\n> # Create grid lines\n> axes = plt.gca()\n> axes.yaxis.grid(linestyle = \"--\")\n> # Display plot\n> plt.show()\n> # Calculate the K-S statistic from the cumulative distributions.\n> ks = max(abs(cumul_1 - cumul_2))\n> # Plot the smoothed count arrays from both datasets\n> plt.plot(cumul_1, label = \"condition1\")\n> plt.plot(cumul_2, label = \"condition2\")\n> # Write the K-S statistic on the plot\n> plt.text(len(cumul_2)∗0.66, 0.2, \"KS stat = \" +\nstr(round(ks,3)), fontsize = 11)\n> # Add the axis labels, title, and legend\n> plt.ylabel(\"Cumulative Read Density\", fontsize = 11)\n> plt.xlabel(\"Transcript Position\", fontsize = 11)\n> plt.title(\"Cumulative Distributions\", fontsize = 13)\n> plt.legend()\n> # Create gridlines\n> axes = plt.gca()\n> axes.yaxis.grid(linestyle = \"--\")"
  ],
  "subjectAreas": [
    "Genomics",
    "Sequence Analysis",
    "Bioinformatics"
  ],
  "bigAreas": [
    "Molecular Biology & Genetics",
    "Bioinformatics & Computational Biology"
  ]
}